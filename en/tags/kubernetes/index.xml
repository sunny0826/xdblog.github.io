<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on GuoXD Blog</title>
    <link>https://guoxudong.io/en/tags/kubernetes/</link>
    <description>Recent content in kubernetes on GuoXD Blog</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&lt;a rel=&#39;license&#39; href=&#39;http://creativecommons.org/licenses/by-nc/4.0/&#39; target=&#39;_blank&#39;&gt;知识共享署名-非商业性使用 4.0 国际许可协议&lt;/a&gt;</copyright>
    <lastBuildDate>Wed, 04 Dec 2019 17:09:51 +0800</lastBuildDate>
    
	    <atom:link href="https://guoxudong.io/en/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>小工具介绍：KubeWatch</title>
      <link>https://guoxudong.io/en/post/kubewatch/</link>
      <pubDate>Wed, 04 Dec 2019 17:09:51 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kubewatch/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;这次要介绍一个 Kubernetes 资源观测工具，实时监控 Kubernetes 集群中各种资源的新建、更新和删除，并实时通知到各种协作软件/聊天软件，目前支持的通知渠道有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;slack&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hipchat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mattermost&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;flock&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;webhook&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我这边开发了钉钉的通知渠道，但是在上游 &lt;a href=&#34;https://github.com/bitnami-labs/kubewatch/issues/198&#34; target=&#34;_blank&#34;&gt;ISSUE#198&lt;/a&gt; 中提出的贡献请求并没有得到回应，所以这边只能 fork 了代码，然后自己进行了开发，以支持钉钉通知。&lt;/p&gt;

&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;

&lt;p&gt;这里推荐使用 helm 进行安装，快速部署&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm install kubewatch stable/kubewatch \
--set rbac.create=true \
--set slack.channel=&#39;#YOUR_CHANNEL&#39; \
--set slack.token=&#39;xoxb-YOUR_TOKEN&#39; \
--set resourcesToWatch.pod=true \
--set resourcesToWatch.daemonset=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想使用钉钉通知，则可以在 &lt;a href=&#34;https://github.com/sunny0826/kubewatch-chat&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; 上拉取我的代码，代码中包含 helm chart 包，可直接进行安装&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/sunny0826/kubewatch-chat.git
cd kubewatch-chat
helm install kubewatch kubewatch \
--set dingtalk.sign=&amp;quot;XXX&amp;quot; \
--set dingtalk.token=&amp;quot;XXXX-XXXX-XXXX&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;钉钉配置&#34;&gt;钉钉配置&lt;/h2&gt;

&lt;p&gt;在钉钉中创建 &lt;code&gt;智能群助手&lt;/code&gt; ，之后&lt;/p&gt;

&lt;h3 id=&#34;获取-token&#34;&gt;获取 token&lt;/h3&gt;

&lt;p&gt;复制的 webhook 中 &lt;code&gt;https://oapi.dingtalk.com/robot/send?access_token={YOUR_TOKEN}&lt;/code&gt;, &lt;code&gt;{YOUR_TOKEN}&lt;/code&gt; 就是要填入的 token。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva4.sinaimg.cn/large/ad5fbf65gy1g9ku2hvs16j20ep05smxk.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;安全设置&#34;&gt;安全设置&lt;/h2&gt;

&lt;p&gt;钉钉智能群助手在更新后新增了安全设置，提供三种验证方式 &lt;code&gt;自定义关键词&lt;/code&gt; &lt;code&gt;加签&lt;/code&gt; &lt;code&gt;IP地址（段）&lt;/code&gt;，这里推荐使用 &lt;code&gt;IP地址（段）的方式&lt;/code&gt;，直接将 Kubernetes 集群的出口 IP 填入设置即可。同时也提供了 &lt;code&gt;加签&lt;/code&gt; 的方式，拷贝秘钥，将其填入 &lt;code&gt;dingtalk.sign&lt;/code&gt; 中。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/ad5fbf65gy1g9ku6qjwy2j20fo077glw.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;项目配置&#34;&gt;项目配置&lt;/h2&gt;

&lt;p&gt;编辑 &lt;code&gt;kubewatch/value.yaml&lt;/code&gt; ，修改配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;## Global Docker image parameters
## Please, note that this will override the image parameters, including dependencies, configured to use the global value
## Current available global Docker image parameters: imageRegistry and imagePullSecrets
##
# global:
#   imageRegistry: myRegistryName
#   imagePullSecrets:
#     - myRegistryKeySecretName

slack:
  enabled: false
  channel: &amp;quot;&amp;quot;
  token: &amp;quot;xoxb&amp;quot;

hipchat:
  enabled: false
  # room: &amp;quot;&amp;quot;
  # token: &amp;quot;&amp;quot;
  # url: &amp;quot;&amp;quot;
mattermost:
  enabled: false
  # channel: &amp;quot;&amp;quot;
  # url: &amp;quot;&amp;quot;
  # username: &amp;quot;&amp;quot;
flock:
  enabled: false
  # url: &amp;quot;&amp;quot;
webhook:
  enabled: false
  # url: &amp;quot;&amp;quot;
dingtalk:
  enabled: true
  token: &amp;quot;&amp;quot;
  sign: &amp;quot;&amp;quot;

# namespace to watch, leave it empty for watching all.
namespaceToWatch: &amp;quot;&amp;quot;

# Resources to watch
resourcesToWatch:
  deployment: true
  replicationcontroller: false
  replicaset: false
  daemonset: false
  services: false
  pod: true
  job: false
  persistentvolume: false

image:
  registry: docker.io
#  repository: bitnami/kubewatch
  repository: guoxudongdocker/kubewatch-chart
#  tag: 0.0.4-debian-9-r405
  tag: latest
  pullPolicy: Always
  ## Optionally specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ##
  # pullSecrets:
  #   - myRegistryKeySecretName

## String to partially override kubewatch.fullname template (will maintain the release name)
##
# nameOverride:

## String to fully override kubewatch.fullname template
##
# fullnameOverride:

rbac:
  # If true, create &amp;amp; use RBAC resources
  #
  create: true

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

resources: {}
  # limits:
  #   cpu: 100m
  #   memory: 300Mi
  # requests:
  #   cpu: 100m
  #   memory: 300Mi

# Affinity for pod assignment
# Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
# affinity: {}

# Tolerations for pod assignment
# Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: []

# Node labels for pod assignment
# Ref: https://kubernetes.io/docs/user-guide/node-selection/
nodeSelector: {}

podAnnotations: {}
podLabels: {}
replicaCount: 1

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 &lt;code&gt;value.yaml&lt;/code&gt; 安装&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/sunny0826/kubewatch-chat.git
cd kubewatch-chat
helm install my-release -f kubewatch/values.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;slack-配置&#34;&gt;Slack 配置&lt;/h2&gt;

&lt;p&gt;Slack 为 kubewatch 默认的通知软件，这里就不简介 Slack 的安装和注册，直接从创建 APP 开始&lt;/p&gt;

&lt;h3 id=&#34;创建一个-app&#34;&gt;创建一个 APP&lt;/h3&gt;

&lt;p&gt;进去创建 &lt;a href=&#34;https://api.slack.com/apps&#34; target=&#34;_blank&#34;&gt;APP 页面&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/ad5fbf65gy1g9kum3x5npj21h40p6tdx.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;选择 &lt;code&gt;App Name&lt;/code&gt; 和 &lt;code&gt;Development Slack Workspace&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/ad5fbf65gy1g9kupp0av1j210c0uejvj.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;添加-bot-用户&#34;&gt;添加 Bot 用户&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax3.sinaimg.cn/large/ad5fbf65gy1g9kuszmgggj21n4156gu2.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;添加-app-到-workspace&#34;&gt;添加 App 到 Workspace&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://tva3.sinaimg.cn/large/ad5fbf65gy1g9kuyzwzetj21qu0wmq9n.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;获取-bot-token&#34;&gt;获取 Bot-token&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax3.sinaimg.cn/large/ad5fbf65gy1g9kv06dva8j21s60uajxf.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;通知效果&#34;&gt;通知效果&lt;/h2&gt;

&lt;p&gt;在 Slack 中，&lt;code&gt;创建&lt;/code&gt; &lt;code&gt;更新&lt;/code&gt; &lt;code&gt;删除&lt;/code&gt; 分别以绿、黄和红色代表&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax1.sinaimg.cn/large/ad5fbf65gy1g9kv23nvmoj213c0mewj4.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在钉钉中，我进行了汉化&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax3.sinaimg.cn/large/ad5fbf65gy1g9kv5fppglj20dd08zdgs.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax4.sinaimg.cn/large/ad5fbf65gy1g9kv5uuxn4j20ea08fgmk.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;对于 kubewatch 我们这里主要用作监控各种 CronJob 的定时触发状态，已经 ConfigMap 和 Secrets 的状态变化，同时也观察 HPA 触发的弹性伸缩的状态，可以实时观测到业务高峰的到来，是一个不错的小工具。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Velero 进行集群备份与迁移</title>
      <link>https://guoxudong.io/en/post/aliyun-velero/</link>
      <pubDate>Wed, 13 Nov 2019 09:13:22 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/aliyun-velero/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;在近日的一个风和日丽的下午，正在快乐的写 bug 时，突然间钉钉就被 call 爆了，原来是 k8s 测试集群的一个 namespace 突然不见了。这个 namespace 里面有 60 多个服务，瞬间全部没有了……虽然得益于我们的 CI/CD 系统，这些服务很快都重新部署并正常运行了，但是如果在生产环境，那后果就是不可想象的了。在排查这个问题发生的原因的同时，集群资源的灾备和恢复功能就提上日程了，这时 Velero 就出现了。&lt;/p&gt;

&lt;h2 id=&#34;velero&#34;&gt;Velero&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/vmware-tanzu/velero&#34; target=&#34;_blank&#34;&gt;Velero&lt;/a&gt; 是 VMWare 开源的 k8s 集群备份、迁移工具。可以帮助我们完成 k8s 的例行备份工作，以便在出现上面问题的时候可以快速进行恢复。同时也提供了集群迁移功能，可以将 k8s 资源迁移到其他 k8s 集群的功能。Velero 将集群资源保存在对象存储中，默认情况下可以使用 &lt;a href=&#34;https://velero.io/docs/v1.1.0/aws-config&#34; target=&#34;_blank&#34;&gt;AWS&lt;/a&gt;、&lt;a href=&#34;https://velero.io/docs/v1.1.0/azure-config&#34; target=&#34;_blank&#34;&gt;Azure&lt;/a&gt;、&lt;a href=&#34;https://velero.io/docs/v1.1.0/gcp-config&#34; target=&#34;_blank&#34;&gt;GCP&lt;/a&gt; 的对象存储，同时也给出了插件功能用来拓展其他平台的存储，这里我们用到的就是阿里云的对象存储 OSS，阿里云也提供了 Velero 的插件，用于将备份存储到 OSS 中。下面我就介绍一下如何在阿里云容器服务 ACK 使用 Velero 完成备份和迁移。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Velero 地址：&lt;a href=&#34;https://github.com/vmware-tanzu/velero&#34; target=&#34;_blank&#34;&gt;https://github.com/vmware-tanzu/velero&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ACK 插件地址：&lt;a href=&#34;https://github.com/AliyunContainerService/velero-plugin&#34; target=&#34;_blank&#34;&gt;https://github.com/AliyunContainerService/velero-plugin&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;下载-velero-客户端&#34;&gt;下载 Velero 客户端&lt;/h3&gt;

&lt;p&gt;Velero 由客户端和服务端组成，服务器部署在目标 k8s 集群上，而客户端则是运行在本地的命令行工具。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;前往 &lt;a href=&#34;https://github.com/vmware-tanzu/velero/releases&#34; target=&#34;_blank&#34;&gt;Velero 的 Release 页面&lt;/a&gt; 下载客户端，直接在 GitHub 上下载即可&lt;/li&gt;
&lt;li&gt;解压 release 包&lt;/li&gt;
&lt;li&gt;将 release 包中的二进制文件 &lt;code&gt;velero&lt;/code&gt; 移动到 &lt;code&gt;$PATH&lt;/code&gt; 中的某个目录下&lt;/li&gt;
&lt;li&gt;执行 &lt;code&gt;velero -h&lt;/code&gt; 测试&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;创建-oss-bucket&#34;&gt;创建 OSS bucket&lt;/h3&gt;

&lt;p&gt;创建一个 OSS bucket 用于存储备份文件，这里也可以用已有的 bucket，之后会在 bucket 中创建 &lt;code&gt;backups&lt;/code&gt;、&lt;code&gt;metadata&lt;/code&gt;、&lt;code&gt;restores&lt;/code&gt;三个目录，这里建议在已有的 bucket 中创建一个子目录用于存储备份文件。&lt;/p&gt;

&lt;p&gt;创建 OSS 的时候一定要选对区域，要和 ACK 集群在同一个区域，存储类型和读写权限选择&lt;strong&gt;标准存储&lt;/strong&gt;和&lt;strong&gt;私有&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva3.sinaimg.cn/wap720/ad5fbf65gy1g8w7t8c4xbj21021d8thq.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;创建阿里云-ram-用户&#34;&gt;创建阿里云 RAM 用户&lt;/h3&gt;

&lt;p&gt;这里需要创建一个阿里云 RAM 的用户，用于操作 OSS 以及 ACK 资源。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;新建权限策略&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax4.sinaimg.cn/large/ad5fbf65gy1g8w80cjiv2j21uo18cag8.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;策略内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;Version&amp;quot;: &amp;quot;1&amp;quot;,
    &amp;quot;Statement&amp;quot;: [
        {
            &amp;quot;Action&amp;quot;: [
                &amp;quot;ecs:DescribeSnapshots&amp;quot;,
                &amp;quot;ecs:CreateSnapshot&amp;quot;,
                &amp;quot;ecs:DeleteSnapshot&amp;quot;,
                &amp;quot;ecs:DescribeDisks&amp;quot;,
                &amp;quot;ecs:CreateDisk&amp;quot;,
                &amp;quot;ecs:Addtags&amp;quot;,
                &amp;quot;oss:PutObject&amp;quot;,
                &amp;quot;oss:GetObject&amp;quot;,
                &amp;quot;oss:DeleteObject&amp;quot;,
                &amp;quot;oss:GetBucket&amp;quot;,
                &amp;quot;oss:ListObjects&amp;quot;
            ],
            &amp;quot;Resource&amp;quot;: [
                &amp;quot;*&amp;quot;
            ],
            &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;新建用户&lt;/p&gt;

&lt;p&gt;在新建用户的时候要选择 &lt;code&gt;编程访问&lt;/code&gt;，来获取 &lt;code&gt;AccessKeyID&lt;/code&gt; 和 &lt;code&gt;AccessKeySecret&lt;/code&gt;，这里请创建一个新用于用于备份，不要使用老用户的 AK 和 AS。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax2.sinaimg.cn/large/ad5fbf65gy1g8w8h4ek4uj21h40ue785.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;部署服务端&#34;&gt;部署服务端&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;拉取 &lt;a href=&#34;https://github.com/AliyunContainerService/velero-plugin&#34; target=&#34;_blank&#34;&gt;Velero 插件&lt;/a&gt; 到本地&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/AliyunContainerService/velero-plugin
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置修改&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;修改 &lt;code&gt;install/credentials-velero&lt;/code&gt; 文件，将新建用户中获得的 &lt;code&gt;AccessKeyID&lt;/code&gt; 和 &lt;code&gt;AccessKeySecret&lt;/code&gt; 填入，这里的 OSS EndPoint 为之前 OSS 的访问域名（&lt;strong&gt;注：这里需要选择外网访问的 EndPoint。&lt;/strong&gt;）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax2.sinaimg.cn/large/ad5fbf65gy1g8w8xd1sgzj21c20cm75z.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ALIBABA_CLOUD_ACCESS_KEY_ID=&amp;lt;ALIBABA_CLOUD_ACCESS_KEY_ID&amp;gt;
ALIBABA_CLOUD_ACCESS_KEY_SECRET=&amp;lt;ALIBABA_CLOUD_ACCESS_KEY_SECRET&amp;gt;
ALIBABA_CLOUD_OSS_ENDPOINT=&amp;lt;ALIBABA_CLOUD_OSS_ENDPOINT&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改 &lt;code&gt;install/01-velero.yaml&lt;/code&gt;，将 OSS 配置填入：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;---
apiVersion: velero.io/v1
kind: BackupStorageLocation
metadata:
labels:
    component: velero
name: default
namespace: velero
spec:
config: {}
objectStorage:
    bucket: &amp;lt;ALIBABA_CLOUD_OSS_BUCKET&amp;gt;  # OSS bucket 名称
    prefix: &amp;lt;OSS_PREFIX&amp;gt;    # bucket 子目录
provider: alibabacloud
---
apiVersion: velero.io/v1
kind: VolumeSnapshotLocation
metadata:
labels:
    component: velero
name: default
namespace: velero
spec:
config:
    region: &amp;lt;REGION&amp;gt;    # 地域，如果是华东2（上海），则为 cn-shanghai
provider: alibabacloud
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;k8s 部署 Velero 服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 新建 namespace
kubectl create namespace velero
# 部署 credentials-velero 的 secret
kubectl create secret generic cloud-credentials --namespace velero --from-file cloud=install/credentials-velero
# 部署 CRD
kubectl apply -f install/00-crds.yaml
# 部署 Velero
kubectl apply -f install/01-velero.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;测试 Velero 状态&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ velero version
Client:
    Version: v1.1.0
    Git commit: a357f21aec6b39a8244dd23e469cc4519f1fe608
Server:
    Version: v1.1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到 Velero 的客户端和服务端已经部署成功。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;服务端清理&lt;/p&gt;

&lt;p&gt;在完成测试或者需要重新安装时，执行如下命令进行清理：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl delete namespace/velero clusterrolebinding/velero
kubectl delete crds -l component=velero
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;备份测试&#34;&gt;备份测试&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;velero-plugin&lt;/code&gt; 项目中已经给出 &lt;code&gt;example&lt;/code&gt; 用于测试备份。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;部署测试服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -f examples/base.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对 &lt;code&gt;nginx-example&lt;/code&gt; 所在的 namespace 进行备份&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero backup create nginx-backup --include-namespaces nginx-example --wait
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;模拟 namespace 被误删&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl delete namespaces nginx-example
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用 Velero 进行恢复&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero restore create --from-backup nginx-backup --wait
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;集群迁移&#34;&gt;集群迁移&lt;/h3&gt;

&lt;p&gt;迁移方法同备份，在备份后切换集群，在新集群恢复备份即可。&lt;/p&gt;

&lt;h3 id=&#34;高级用法&#34;&gt;高级用法&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定时备份&lt;/p&gt;

&lt;p&gt;对集群资源进行定时备份，则可在发生意外的情况下，进行恢复（默认情况下，备份保留 30 天）。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 每日1点进行备份
velero create schedule &amp;lt;SCHEDULE NAME&amp;gt; --schedule=&amp;quot;0 1 * * *&amp;quot;
# 每日1点进行备份，备份保留48小时
velero create schedule &amp;lt;SCHEDULE NAME&amp;gt; --schedule=&amp;quot;0 1 * * *&amp;quot; --ttl 48h
# 每6小时进行一次备份
velero create schedule &amp;lt;SCHEDULE NAME&amp;gt; --schedule=&amp;quot;@every 6h&amp;quot;
# 每日对 web namespace 进行一次备份
velero create schedule &amp;lt;SCHEDULE NAME&amp;gt; --schedule=&amp;quot;@every 24h&amp;quot; --include-namespaces web
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;定时备份的名称为：&lt;code&gt;&amp;lt;SCHEDULE NAME&amp;gt;-&amp;lt;TIMESTAMP&amp;gt;&lt;/code&gt;，恢复命令为：&lt;code&gt;velero restore create --from-backup &amp;lt;SCHEDULE NAME&amp;gt;-&amp;lt;TIMESTAMP&amp;gt;&lt;/code&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;备份删除&lt;/p&gt;

&lt;p&gt;直接执行命令进行删除&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero delete backups &amp;lt;BACKUP_NAME&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;备份资源查看&lt;/p&gt;

&lt;p&gt;备份查看&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero backup get
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看定时备份&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero schedule get
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看可恢复备份&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero restore get
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;备份排除项目&lt;/p&gt;

&lt;p&gt;可为资源添加指定标签，添加标签的资源在备份的时候被排除。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 添加标签
kubectl label -n &amp;lt;ITEM_NAMESPACE&amp;gt; &amp;lt;RESOURCE&amp;gt;/&amp;lt;NAME&amp;gt; velero.io/exclude-from-backup=true
# 为 default namespace 添加标签
kubectl label -n default namespace/default velero.io/exclude-from-backup=true
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;问题汇总&#34;&gt;问题汇总&lt;/h3&gt;

&lt;h4 id=&#34;时区问题&#34;&gt;时区问题&lt;/h4&gt;

&lt;p&gt;进行定时备份时，发现备份使用的事 UTC 时间，并不是本地时间，经过排查后发现是 &lt;code&gt;velero&lt;/code&gt; 镜像的时区问题，在调整后就会正常定时备份了，这里我重新调整了时区，直接调整镜像就好，修改 &lt;code&gt;install/01-velero.yaml&lt;/code&gt; 文件，将镜像替换为 &lt;code&gt;registry-vpc.cn-shanghai.aliyuncs.com/keking/velero:latest&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: velero
  namespace: velero
spec:
  replicas: 1
  selector:
    matchLabels:
      deploy: velero
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: &amp;quot;8085&amp;quot;
        prometheus.io/scrape: &amp;quot;true&amp;quot;
      labels:
        component: velero
        deploy: velero
    spec:
      serviceAccountName: velero
      containers:
      - name: velero
        # sync from gcr.io/heptio-images/velero:latest
        image: registry-vpc.cn-shanghai.aliyuncs.com/keking/velero:latest   # 修复时区后的镜像
        imagePullPolicy: IfNotPresent
        command:
          - /velero
        args:
          - server
          - --default-volume-snapshot-locations=alibabacloud:default
        env:
          - name: VELERO_SCRATCH_DIR
            value: /scratch
          - name: ALIBABA_CLOUD_CREDENTIALS_FILE
            value: /credentials/cloud
        volumeMounts:
          - mountPath: /plugins
            name: plugins
          - mountPath: /scratch
            name: scratch
          - mountPath: /credentials
            name: cloud-credentials
      initContainers:
      - image: registry.cn-hangzhou.aliyuncs.com/acs/velero-plugin-alibabacloud:v1.2
        imagePullPolicy: IfNotPresent
        name: velero-plugin-alibabacloud
        volumeMounts:
        - mountPath: /target
          name: plugins
      volumes:
        - emptyDir: {}
          name: plugins
        - emptyDir: {}
          name: scratch
        - name: cloud-credentials
          secret:
            secretName: cloud-credentials

&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;版本问题&#34;&gt;版本问题&lt;/h4&gt;

&lt;p&gt;截止发稿时，Velero 已经发布了 v1.2.0 版本，目前 ACK 的 Velero 的插件还未升级。&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;近日正好有 k8s 集群服务迁移服务的需求，使用 Velero 完成了服务的迁移，同时也每日进行集群资源备份，其能力可以满足容器服务的灾备和迁移场景，实测可用，现已运行在所有的 k8s 集群。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Grafana 展示阿里云监控指标</title>
      <link>https://guoxudong.io/en/post/aliyun-cms-grafana/</link>
      <pubDate>Thu, 07 Nov 2019 11:08:36 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/aliyun-cms-grafana/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;对于阿里云用户来说，阿里云监控是一个很不错的产品，首先它在配额内使用是免费的！免费的！免费的！重要的事情说三遍。他的功能类似于 zabbix，但是比 zabbix 提供了更多的监控项，基本上在云上使用的资源都可以通过云监控来实时监控。而它提供的开箱即用方式，天然集成云资源，并提供多种告警方式，免去了监控与告警系统搭建与维护的繁琐，并且减少了资源的消耗，比购买 ECS 自己搭建 zabbix 要少消耗很多资源。同时阿里云监控和阿里云其他服务一样，也提供了比较完整的 OpenApi 以及各种语言的 sdk，可以基于阿里云的 OpenApi 将其与自己的系统集成。我们之前也是这么做的，但是随着监控项的增加，以及经常需要在办公场地监控投屏的专项监控页，光凭我们的运维开发工程师使用 vue 写速度明显跟不上，而且页面的美观程度也差很多。&lt;/p&gt;

&lt;h3 id=&#34;手写前端-vs-grafana&#34;&gt;手写前端 VS Grafana&lt;/h3&gt;

&lt;p&gt;手写前端虽然可定制化程度更高，但是需要消耗大量精力进行调试，对于运维人员，哪怕是运维开发也是吃不消的（前端小哥哥和小姐姐是不会来帮你的，下图就是我去年拿 vue 写的伪 Grafana 展示页面，花费了大约一周时间在调整这些前端元素）。
&lt;img src=&#34;https://tva4.sinaimg.cn/large/ad5fbf65gy1g8pfrw1licj22ye1gg4qp.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Grafana 则标准化程度很高，展示也更加符合大众审美，某些定制化需求可以通过自定义 DataSource 或者 AJAX 插件的 iframe 模式完成。开发后端 DataSource 肯定就没有前端调整 css 那么痛苦和耗时了，整体配置开发一个这样的页面可能只消耗一人天就能完成。而在新产品上线时，构建一个专项监控展示页面速度就更快了，几分钟内就能完成。
&lt;img src=&#34;https://tva4.sinaimg.cn/large/ad5fbf65gy1g8pfvp0keej22yc1g2khm.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;关于阿里云监控&#34;&gt;关于阿里云监控&lt;/h2&gt;

&lt;p&gt;云监控（CloudMonitor）是一项针对阿里云资源和互联网应用进行监控的服务。&lt;/p&gt;

&lt;p&gt;云监控为云上用户提供开箱即用的企业级开放型一站式监控解决方案。涵盖 IT 设施基础监控，外网网络质量拨测监控，基于事件、自定义指标、日志的业务监控。为您全方位提供更高效、更全面、更省钱的监控服务。通过提供跨产品、跨地域的应用分组管理模型和报警模板，帮助您快速构建支持几十种云产品、管理数万实例的高效监控报警管理体系。通过提供 Dashboard，帮助您快速构建自定义业务监控大盘。使用云监控，不但可以帮助您提升您的系统服务可用时长，还可以降低企业 IT 运维监控成本。&lt;/p&gt;

&lt;p&gt;云监控服务可用于收集获取阿里云资源的监控指标或用户自定义的监控指标，探测服务可用性，以及针对指标设置警报。使您全面了解阿里云上的资源使用情况、业务的运行状况和健康度，并及时收到异常报警做出反应，保证应用程序顺畅运行。&lt;/p&gt;

&lt;h2 id=&#34;关于-grafana&#34;&gt;关于 Grafana&lt;/h2&gt;

&lt;p&gt;Grafana 是一个跨平台的开源的度量分析和可视化工具，可以通过将采集的数据查询然后可视化的展示，并及时通知。由于云监控的 Grafana 还没有支持告警，所以我们这里只用了 Grafana 的可视化功能，而告警本身就是云监控自带的，所以也不需要依赖 Grafana 来实现。而我们的 Prometheus 也使用了 Grafana 进行数据可视化，所以有现成的 Grafana-Server 使用。&lt;/p&gt;

&lt;h2 id=&#34;阿里云监控对接-grafana&#34;&gt;阿里云监控对接 Grafana&lt;/h2&gt;

&lt;p&gt;首先 Grafana 服务的部署方式这里就不做介绍了，请使用较新版本的 Grafana，最好是 5.5.0+。后文中也有我开源的基于阿里云云监控的 Grafana 的 helm chart，可以使用 helm 安装，并会直接导入云监控的指标，这个会在后文中介绍。&lt;/p&gt;

&lt;h3 id=&#34;安装阿里云监控插件&#34;&gt;安装阿里云监控插件&lt;/h3&gt;

&lt;p&gt;进入插件目录进行安装&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /var/lib/grafana/plugins/
git clone https://github.com/aliyun/aliyun-cms-grafana.git 
service grafana-server restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是使用 docker 或者部署在 k8s 集群，这里也可以使用环境变量在 Grafana 部署的时候进行安装&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;...
spec:
  containers:
  - env:
    - name: GF_INSTALL_PLUGINS  # 多个插件请使用,隔开
      value: grafana-simple-json-datasource,https://github.com/aliyun/aliyun-cms-grafana/archive/master.zip;aliyun-cms-grafana
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;您也可以下载 aliyun-cms-grafana.zip 插件解压后，上传服务器的 Grafana 的 plugins 目录下，重启 grafana-server 即可。&lt;/p&gt;

&lt;h3 id=&#34;配置云监控-datasource&#34;&gt;配置云监控 DataSource&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Grafana 启动后，进入 &lt;code&gt;Configuration&lt;/code&gt; 页面，选择 &lt;code&gt;DataSource&lt;/code&gt; Tab 页，单击右上方的&lt;code&gt;Add data source&lt;/code&gt;，添加数据源。&lt;/li&gt;
&lt;li&gt;选中&lt;code&gt;CMS Grafana Service&lt;/code&gt;，单击&lt;code&gt;select&lt;/code&gt;。
&lt;img src=&#34;https://tvax2.sinaimg.cn/large/ad5fbf65gy1g8ph0ukr0pj21nm0jk76m.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;li&gt;填写配置项，URL 根据云监控所在地域填写，并且填写阿里云账号的 accessKeyId 和 accessSecret，完成后单击&lt;code&gt;Save&amp;amp;Test&lt;/code&gt;。
&lt;img src=&#34;https://tvax3.sinaimg.cn/large/ad5fbf65gy1g8ph4bg2bij218m194n9f.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;创建-dashboard&#34;&gt;创建 Dashboard&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;单击 &lt;code&gt;Create&lt;/code&gt; -&amp;gt; &lt;code&gt;Dashboard&lt;/code&gt; -&amp;gt; &lt;code&gt;Add Query&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;配置图标，数据源选择之前添加的 &lt;code&gt;CMS Grafana Service&lt;/code&gt;，然后文档中的配置项填入指标即可（这里要注意的是，云监控 API 给返回的只有实例 ID，并没有自定义的实例名称，这里需要手动将其填入 &lt;code&gt;Y - column describe&lt;/code&gt; 中；而且只支持输入单个 Dimension，若输入多个，默认选第一个，由于这些问题才有了后续我开发的 &lt;code&gt;cms-grafana-builder&lt;/code&gt; 的动机）。
&lt;img src=&#34;https://tva4.sinaimg.cn/large/ad5fbf65gy1g8phck0irbj22ye13in79.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;li&gt;配置参考 &lt;a href=&#34;https://help.aliyun.com/document_detail/28619.html&#34; target=&#34;_blank&#34;&gt;云产品监控项&lt;/a&gt;，
&lt;img src=&#34;https://tva2.sinaimg.cn/large/ad5fbf65gy1g8phg832uvj21a40vo793.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;使用-helm-chart-的方式部署-grafana&#34;&gt;使用 helm chart 的方式部署 Grafana&lt;/h2&gt;

&lt;p&gt;项目地址：&lt;a href=&#34;https://github.com/sunny0826/cms-grafana-builder&#34; target=&#34;_blank&#34;&gt;https://github.com/sunny0826/cms-grafana-builder&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;cms-grafana-builder&#34;&gt;cms-grafana-builder&lt;/h3&gt;

&lt;p&gt;由于上文中的问题，我们需要手动选择每个实例 ID 到 Dimension 中，并且还要讲该实例的名称键入 &lt;code&gt;Y - column describe&lt;/code&gt; 中，十分的繁琐，根本不可能大批量的输入。&lt;/p&gt;

&lt;p&gt;这就是我开发这个 Grafana 指标参数生成器的原因，起初只是一个 python 脚本，用来将我们要监控的指标组装成一个 Grafana 可以使用 json 文件，之后结合 Grafana 的容器化部署方法，将其做成了一个 helm chart。可以在启动的时候自动将需要的参数生成，并且每日会对所有指标进行更新，这样就不用每次新购或者释放掉资源后还需要再跑一遍脚本。&lt;/p&gt;

&lt;h3 id=&#34;部署&#34;&gt;部署&lt;/h3&gt;

&lt;p&gt;只需要将项目拉取下来运行 &lt;code&gt;helm install&lt;/code&gt; 命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm install my-release kk-grafana-cms \
--namespace {your_namespace} \
--set access_key_id={your_access_key_id} \
--set access_secret={your_access_secret} \
--set region_id={your_aliyun_region_id} \
--set password={admin_password}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多详情见 &lt;a href=&#34;https://github.com/sunny0826/cms-grafana-builder&#34; target=&#34;_blank&#34;&gt;github README&lt;/a&gt;，欢迎提 issue 交流。&lt;/p&gt;

&lt;h3 id=&#34;指标选择&#34;&gt;指标选择&lt;/h3&gt;

&lt;p&gt;在部署成功后，可修改 ConfigMap：&lt;code&gt;grafana-cms-metric&lt;/code&gt;，然后修改对应的监控指标项。&lt;/p&gt;

&lt;h3 id=&#34;效果&#34;&gt;效果&lt;/h3&gt;

&lt;p&gt;ECS:
&lt;img src=&#34;https://tvax1.sinaimg.cn/large/ad5fbf65gy1g8pi9toh3dj21gv0pldyf.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;RDS:
&lt;img src=&#34;https://tva2.sinaimg.cn/large/ad5fbf65gy1g8pi9o91ejj21h80q316p.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;EIP:
&lt;img src=&#34;https://tva4.sinaimg.cn/large/ad5fbf65gy1g8pi9i9if3j21h70q3aif.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Redis:
&lt;img src=&#34;https://tvax1.sinaimg.cn/large/ad5fbf65gy1g8pi8ss733j21h30pz7b6.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;为了满足公司需求，后续还开发 DataSource 定制部分，用于公司监控大屏的展示，这部分是另一个项目，不在这个项目里，就不细说了，之后有机会总结后再进行分享。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>解决 Nginx-Ingress 重定向失败问题</title>
      <link>https://guoxudong.io/en/post/nginx-ingress-error/</link>
      <pubDate>Fri, 16 Aug 2019 11:15:37 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/nginx-ingress-error/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;最近对公司 Kubernetes 集群的 &lt;code&gt;nginx-ingress-controller&lt;/code&gt; 进行了升级，但是升级后却出现了大问题，之前所有采用 &lt;code&gt;nginx.ingress.kubernetes.io/rewrite-target: /&lt;/code&gt; 注释进行重定向的 Ingress 路由全部失效了，但是那些直接解析了域名，没有进行重定向的却没有发生这个问题。&lt;/p&gt;

&lt;h2 id=&#34;问题分析&#34;&gt;问题分析&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;首先检查对应服务健康状态，发现所有出问题的服务的状态均正常，同时受影响的之后 http 调用，而 RPC 调用却不受影响，这时问题就定位到了 ingress。&lt;/li&gt;
&lt;li&gt;然后检查 nginx-ingress-controller ，发现 nginx-ingress-controller 的状态也是正常的，路由也是正常的。&lt;/li&gt;
&lt;li&gt;最后发现受影响的只有添加了重定向策略的 ingress 。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;问题解决&#34;&gt;问题解决&lt;/h2&gt;

&lt;p&gt;问题已经定位，接下来就是着手解决问题，这时候值得注意的就是之前进行了什么变更：升级了 nginx-ingress-controller 版本！看来问题就出现在新版本上，那么就打开官方文档：&lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/examples/rewrite/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.github.io/ingress-nginx/examples/rewrite/&lt;/a&gt; 看一下吧。&lt;/p&gt;

&lt;h3 id=&#34;attention&#34;&gt;Attention&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Starting in Version 0.22.0, ingress definitions using the annotation &lt;code&gt;nginx.ingress.kubernetes.io/rewrite-target&lt;/code&gt; are not backwards compatible with previous versions. In Version 0.22.0 and beyond, any substrings within the request URI that need to be passed to the rewritten path must explicitly be defined in a &lt;a href=&#34;https://www.regular-expressions.info/refcapture.html&#34; target=&#34;_blank&#34;&gt;capture group&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;文档上给出了非常明显的警告⚠️：从 V0.22.0 版本开始将不再兼容之前的入口定义，再查看一下我的 nginx-ingress-controller 版本，果然问题出现来这里。&lt;/p&gt;

&lt;h3 id=&#34;note&#34;&gt;Note&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.regular-expressions.info/refcapture.html&#34; target=&#34;_blank&#34;&gt;Captured groups&lt;/a&gt; are saved in numbered placeholders, chronologically, in the form &lt;code&gt;$1&lt;/code&gt;, &lt;code&gt;$2&lt;/code&gt; &amp;hellip; &lt;code&gt;$n&lt;/code&gt;. These placeholders can be used as parameters in the &lt;code&gt;rewrite-target&lt;/code&gt; annotation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;示例&#34;&gt;示例&lt;/h3&gt;

&lt;p&gt;到这里问题已经解决了，在更新了 ingress 的配置之后，之前所有无法重定向的服务现在都已经可以正常访问了。修改见如下示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ echo &#39;
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  name: rewrite
  namespace: default
spec:
  rules:
  - host: rewrite.bar.com
    http:
      paths:
      - backend:
          serviceName: http-svc
          servicePort: 80
        path: /something(/|$)(.*)
&#39; | kubectl create -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;解决这个问题的实际时间虽然不长，但是着实让人出了一身冷汗，同时也给了我警示：变更有风险，升级需谨慎。在升级之前需要先浏览新版本的升级信息，同时需要制定完善的回滚策略，确保万无一失。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>如何构建混合Kubernetes平台</title>
      <link>https://guoxudong.io/en/post/how-we-built-our-hybrid-kubernetes-platfor/</link>
      <pubDate>Tue, 06 Aug 2019 14:01:30 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/how-we-built-our-hybrid-kubernetes-platfor/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;随着3年前重构 &lt;a href=&#34;https://www.dailymotion.com/&#34; target=&#34;_blank&#34;&gt;Dailymotion&lt;/a&gt; 核心API的决定，我们希望提供一种更有效的方式来托管应用程序，&lt;a href=&#34;https://medium.com/dailymotion/deploying-apps-on-multiple-kubernetes-clusters-with-helm-19ee2b06179e&#34; target=&#34;_blank&#34;&gt;促进我们的开发和生产工作流程&lt;/a&gt;。 最终决定使用容器编排平台来实现这一目标，那么自然就选择了 Kubernetes。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;那么为什么要建立自己的Kubernetes平台？&lt;/p&gt;

&lt;h2 id=&#34;借由-google-cloud-快速推动的-api-投入生产&#34;&gt;借由 Google Cloud 快速推动的 API 投入生产&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;2016年夏&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;三年前，在 &lt;a href=&#34;https://www.vivendi.com/&#34; target=&#34;_blank&#34;&gt;Vivendi&lt;/a&gt; 收购 Dailymotion 之后，所有开发团队都专注于一个目标：提供全新的 Dailymotion 产品。&lt;/p&gt;

&lt;p&gt;根据对容器、编排解决方案和以前的经验的分析，使我们确信 Kubernetes 是正确的选择。许多开发人员已经掌握了这一概念并知道如何使用 Kubernetes ，这对我们的基础设施转型来说是一个巨大的优势。在基础架构方面，我们需要一个强大而灵活的平台来托管这些新型的云原生应用程序。而公有云为我们提供了极大的便利，于是我们决定在 Google Kubernetes Engine 上部署我们的应用程序，即使之后我们也会在自己的数据中心中进行混合部署。&lt;/p&gt;

&lt;h3 id=&#34;为何选择-gke&#34;&gt;为何选择 GKE ？&lt;/h3&gt;

&lt;p&gt;我们做出这个选择主要是出于技术原因，但也因为我们需要快速提供基础设施来满足 Dailymotion 的业务需求。并且对托管的应用程序（如地理分布，可伸缩性和弹性）有一些要求。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws2.sinaimg.cn/large/ad5fbf65gy1g5py1vm2k2j20hd0bbjtq.jpg&#34; alt=&#34;&#34; /&gt;
&lt;center&gt;Dailymotion 的 GKE 集群&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Dailymotion 作为一个全球性的视频平台，需要通过减少延迟来改善用户体验。之前我们仅在巴黎提供 &lt;a href=&#34;https://developer.dailymotion.com/&#34; target=&#34;_blank&#34;&gt;API&lt;/a&gt; ，但这样并非最佳，我们希望能够在欧洲、亚洲以及美国托管我们的应用程序。&lt;/p&gt;

&lt;p&gt;这种延迟限制意味着我们在平台的网络设计方面面临着巨大的挑战。大多数云供应商要求我们在每个地区创建一个网络，并将所有这些网络通过 VPN 与托管服务互连，但 Google Cloud 允许我们在所有 Google 地区创建一个完全路由的单一网络，该网络在运营方面提供了便利并提高了效率。&lt;/p&gt;

&lt;p&gt;此外，Google Cloud 的网络和负载均衡服务非常棒。它可以将我们的用户路由到最近的集群，并且在发生故障的情况下，流量会自动路由到另一个区域而无需任何人为干预。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g5pytelbwnj20jg0avq4x.jpg&#34; alt=&#34;&#34; /&gt;
&lt;center&gt;Google 负载均衡监控&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;我们的平台同样需要使用 GPU，而 Google Cloud 允许我们以非常有效的方式直接在我们的 Kubernetes 集群中使用它们。&lt;/p&gt;

&lt;p&gt;所有这一切使我们在启动后6个月开始接入 Google Cloud 基础架构上的生产流量。&lt;/p&gt;

&lt;p&gt;但是，尽管具有整体优势，但使用共有云服务还是要花费不少成本。这就是为什么我们要评估采取的每项托管服务，以便将来将其内部化。事实上，我们在2016年底开始构建我们的本地集群，并启动了我们的混合策略。&lt;/p&gt;

&lt;h2 id=&#34;在-dailymotion-的内部构建容器编排平台&#34;&gt;在 Dailymotion 的内部构建容器编排平台&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;2016年秋&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;看到整个技术栈已经准备好在生产环境中应用，但&lt;a href=&#34;https://tartiflette.io/&#34; target=&#34;_blank&#34;&gt;API仍在开发中&lt;/a&gt;，这使得我们有时间专注搭建我们的本地集群。&lt;/p&gt;

&lt;p&gt;Dailymotion 多年来在全球拥有自己的内容分发网络，每月有超过30亿的视频播放量。显然，我们希望利用现有的优势并在我们现有的数据中心部署自己的 Kubernetes 集群。&lt;/p&gt;

&lt;p&gt;我的目前拥有6个数据中心的2500多台服务器。所有这些都使用 Saltstack 进行配置，我们开始准备所有需要的公式来创建主节点、工作节点以及 Etcd 集群。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g5pzm4m985j20jg06tgm7.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;网络部分&#34;&gt;网络部分&lt;/h3&gt;

&lt;p&gt;我们的网络是一个完全路由的网络。每个服务器使用Exabgp通过网络广播自己的IP。我们比较了几个网络插件， &lt;a href=&#34;https://www.projectcalico.org/&#34; target=&#34;_blank&#34;&gt;Calico&lt;/a&gt; 使用的是三层网络，因此这是唯一满足我们需求的网络插件。&lt;/p&gt;

&lt;p&gt;由于我们想要重用基础架构中的所有现有工具，首先要解决的问题是插入一个自制网络工具（我们所有服务器都使用它），通过我们的 Kubernetes 节点通过网络广播 IP 范围。我们让 Calico 为 pod 分配 IP，但不使用它与我们的网络设备进行BGP会话。路由实际上是由Exabgp处理的，它宣布了Calico使用的子网。这使我们可以从内部网络访问任何pod，尤其是来自我们的负载均衡器。&lt;/p&gt;

&lt;h3 id=&#34;我们如何管理入口流量&#34;&gt;我们如何管理入口流量&lt;/h3&gt;

&lt;p&gt;为了将传入的请求路由到正确的服务，我们希望使用 Ingress Controllers 与 Kubernetes 的入口资源集成。&lt;/p&gt;

&lt;p&gt;3年前，nginx-ingress-controller 是最成熟的控制器 ，并且 Nginx 已经使用多年，并以其稳定性和性能而闻名。&lt;/p&gt;

&lt;p&gt;在我们的设计中，我们决定在专用的 10Gbps 刀片服务器上托管我们的控制器。每个控制器都插入其所属集群的 kube-apiserver 端点。在这些服务器上，我们还使用Exabgp来广播公共或私有IP。我们的网络拓扑允许我们使用来自这些控制器的BGP将所有流量直接路由到我们的pod，而无需使用NodePort服务类型。这样可以避免节点之间的水平流量，从而提高效率。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws3.sinaimg.cn/large/ad5fbf65gy1g5q05ex27bj20in0fbt9q.jpg&#34; alt=&#34;&#34; /&gt;
&lt;center&gt;从 Internet 到 pods 的流量&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;现在我们已经看到了我们如何构建混合平台，我们可以深入了解流量迁移本身。&lt;/p&gt;

&lt;h2 id=&#34;将流量从-google-cloud-迁移到-dailymotions-基础架构&#34;&gt;将流量从 Google Cloud 迁移到 Dailymotions 基础架构&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;2018年秋&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;经过近2年的构建、测试和微调，我们发现自己拥有完整的 Kubernetes 技术栈，可以接收部分流量。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g5q0b3o8laj20jg06sq36.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;目前，我们的路由策略非常简单，但足以解决我们的问题。除了我们的公共IP（Google Cloud和Dailymotion）之外，我们还使用AWS Route 53 来定义策略并将终端用户流量引入我们选择的集群。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws2.sinaimg.cn/large/ad5fbf65gy1g5q0ds3spjj20jg07a0tk.jpg&#34; alt=&#34;&#34; /&gt;
&lt;center&gt;使用Route 53的路由策略示例&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;在 Google Cloud 上很简单，因为我们为所有群集使用唯一的IP，并且用户被路由到他最近的 GKE 群集。对于我们来说，我们不使用相同的技术，因此我们每个群集都有不同的IP。&lt;/p&gt;

&lt;p&gt;在此次迁移过程中，我们将目标国家逐步纳入我们的集群并分析其收益。&lt;/p&gt;

&lt;p&gt;由于我们的GKE集群配置了自动调节自定义指标，因此它们会根据传入流量进行扩展/缩小。&lt;/p&gt;

&lt;p&gt;在正常模式下，区域的所有流量都路由到我们的内部部署集群，而GKE集群则使用Route 53提供的运行状况检查作为故障转移。&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;我们接下来的步骤是完全自动化我们的路由策略，以实现自动混合策略，不断增强我们的用户体验。在效益方面，我们大大降低了云的成本，甚至改善了API响应时间。我们相信我们的云平台足以在需要时处理更多流量。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Helm 在多集群部署应用</title>
      <link>https://guoxudong.io/en/post/deploying-apps-on-multiple-kubernetes-clusters-with-hel/</link>
      <pubDate>Sun, 14 Jul 2019 14:16:56 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/deploying-apps-on-multiple-kubernetes-clusters-with-hel/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.dailymotion.com/&#34; target=&#34;_blank&#34;&gt;Dailymotion&lt;/a&gt; 在生产环境使用 Kubernetes 已经3年了，但是也面临着多集群部署应用的挑战，这也是在过去的几年中我一直努力优化工具和改进工作流的原因。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;本文将重点介绍我们如何在全球多个 Kubernetes 集群上部署我们的应用程序。&lt;/p&gt;

&lt;p&gt;为了将应用一次部署到多个 Kubernetes 集群，我们使用了 &lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt;，并将所有 chart 存储在一个 git 仓库中。我们使用 &lt;strong&gt;umbrella&lt;/strong&gt; 来部署由多个服务组成的完整应用程序，这基本上是一个声明依赖关系的 chart ，其允许我们在单个命令行中引导我们的 API 及其服务。&lt;/p&gt;

&lt;p&gt;此外，我们在使用 Helm 之前会运行一个 python 脚本，用来进行检查，构建 chart ，添加 secrets 并部署我们的应用程序。所有这些任务都是使用 docker 镜像在 CI 平台上完成的。&lt;/p&gt;

&lt;p&gt;下面就进行详细介绍&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意！：&lt;/strong&gt; 当你阅读这篇博文的时候，Helm 3 的第一个 &lt;a href=&#34;https://github.com/helm/helm/releases/tag/v3.0.0-alpha.1&#34; target=&#34;_blank&#34;&gt;release&lt;/a&gt; 已经发布。这个版本带来了一系列增强功能，肯定会解决我们过去遇到的一些问题。&lt;/p&gt;

&lt;p&gt;image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;h2 id=&#34;charts-开发流程&#34;&gt;Charts 开发流程&lt;/h2&gt;

&lt;p&gt;在开发应用程序时，我们使用&lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Branching-Branching-Workflows&#34; target=&#34;_blank&#34;&gt;分支工作流&lt;/a&gt;，开发 chart 时也使用相同流程。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先，&lt;strong&gt;dev&lt;/strong&gt; 分支用于构建要在开发集群上进行测试的 chart 。&lt;/li&gt;
&lt;li&gt;然后，当发起 PR 请求到 &lt;strong&gt;master&lt;/strong&gt; 分支时，将发布到演示环境中进行验证。&lt;/li&gt;
&lt;li&gt;最终，我们将 PR 请求提交的修改合并到 &lt;strong&gt;prod&lt;/strong&gt; 分支，将这个修改应用于生产环境。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们使用 &lt;a href=&#34;https://chartmuseum.com/&#34; target=&#34;_blank&#34;&gt;Chartmuseum&lt;/a&gt; 作为私有仓库来存储 chart ，每个环境都有一个 。这样我们就可以在&lt;strong&gt;环境之间实现明确的隔离&lt;/strong&gt;，并且确保该 chart 在生产环境中使用之前已经过测试。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g50h10d4xbj20ys0ee75e.jpg&#34; alt=&#34;&#34; /&gt;
&lt;center&gt;每个环境的 Chart 仓库&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;值得注意的是，当开发人员 push 代码到他们的 dev 分支时，他们的 chart 版本也会自动 push 到 dev 环境的 Chartmuseum 。因此，所有开发人员都使用相同的 dev 存储库，他们必须小心的指定自己的 chart 版本，以避免使用其他人的对 chart 的更改。&lt;/p&gt;

&lt;p&gt;此外，我们的 python 脚本通过使用 &lt;a href=&#34;https://kubeval.instrumenta.dev/&#34; target=&#34;_blank&#34;&gt;Kubeval&lt;/a&gt; 在它们推送到 Chartmusem 之前验证 Kubernetes 对象与 Kubernetes OpenAPI 规范。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;chart 开发工作流&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://ws3.sinaimg.cn/large/ad5fbf65gy1g50hg9gmh2j20gr047t8o.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;根据 &lt;a href=&#34;https://gazr.io/&#34; target=&#34;_blank&#34;&gt;gazr.io&lt;/a&gt; 规范设置我们的 pipeline 任务（lint，unit-test）。&lt;/li&gt;
&lt;li&gt;push docker 镜像，该镜像包含部署应用程序的 Python 工具。&lt;/li&gt;
&lt;li&gt;根据分支名称设置相应环境。&lt;/li&gt;
&lt;li&gt;使用 Kubeval 检查 Kubernetes yamls 。&lt;/li&gt;
&lt;li&gt;自动增加 chart 版本及其父项（取决于更改的 chart ）。&lt;/li&gt;
&lt;li&gt;将 chart push 到与其环境对应的 Chartmuseum 。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;管理集群差异&#34;&gt;管理集群差异&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Cluster federation&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们使用 &lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/federation/&#34; target=&#34;_blank&#34;&gt;Kubernetes cluster federation&lt;/a&gt;，它允许我们从单个 API 端声明 Kubernetes 对象。但是我们遇到的问题是，无法在 federation 端中创建某些 Kubernetes 对象，因此很难维护 federation 对象和其他的群集对象。&lt;/p&gt;

&lt;p&gt;为了解决这个问题，我们决定独立管理我们的集群，反而使这个过程变得更加容易（我们使用的是 federation v1，v2 可能有所改善）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;平台地理分布&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;目前，我们的平台分布在6个地区，3个在自己的数据中心，3个在公有云。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g50klup6yaj212w0ftq4h.jpg&#34; alt=&#34;&#34; /&gt;
&lt;center&gt;分布式部署&lt;/center&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Helm global values&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;4个全局的 Helm value 定义集群间的差异。这些是我们所有 chart 的最小默认值。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;    global:
        cloud: True
        env: staging
        region: us-central1
        clusterName: staging-us-central1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;center&gt;global values&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;这些信息有助于我们为应用程序定义上下文，它们可用于监控，跟踪，记录，进行外部调用，扩展等许多内容&amp;hellip;&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;cloud&lt;/strong&gt;：我们有一个混合 Kubernetes 集群。例如，我们的 API 部署在 GCP 和我们自己的数据中心。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;env&lt;/strong&gt;：对于非生产环境，某些值可能会发生变化。本质上是资源定义和自动扩展配置。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;region&lt;/strong&gt;：此信息用于标识群集的位置，并可用于定义外部服务的最近端点。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;clusterName&lt;/strong&gt;：如果我们想要为每个群集定义一个值。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面是一个具体的示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;{{/* Returns Horizontal Pod Autoscaler replicas for GraphQL*/}}
{{- define &amp;quot;graphql.hpaReplicas&amp;quot; -}}
{{- if eq .Values.global.env &amp;quot;prod&amp;quot; }}
{{- if eq .Values.global.region &amp;quot;europe-west1&amp;quot; }}
minReplicas: 40
{{- else }}
minReplicas: 150
{{- end }}
maxReplicas: 1400
{{- else }}
minReplicas: 4
maxReplicas: 20
{{- end }}
{{- end -}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;center&gt;helm template example&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;请注意，此逻辑在帮助模板中定义，以保持 Kubernetes YAML 的可读性。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;声明应用&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们的部署工具基于几个 YAML 文件，下面是我们声明服务及其每个集群的扩展拓扑（副本数量）的示例。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;releases:
  - foo.world
 
foo.world:                # Release name
  services:               # List of dailymotion&#39;s apps/projects
    foobar:
      chart_name: foo-foobar
      repo: git@github.com:dailymotion/foobar
      contexts:
        prod-europe-west1:
          deployments:
            - name: foo-bar-baz
              replicas: 18
            - name: another-deployment
              replicas: 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;center&gt;service definition&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;这是部署工作流的所有步骤，最后一步将在多个生产集群上同时部署应用程序。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g50ldllp33j20mw0bxglz.jpg&#34; alt=&#34;&#34; /&gt;
&lt;center&gt;Jenkins deployment steps&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;secrets-怎么办&#34;&gt;Secrets 怎么办&lt;/h2&gt;

&lt;p&gt;在安全领域，我们专注于跟踪可能在不同位置传播的所有的 Secrets ，并将其存储在巴黎的 &lt;a href=&#34;https://www.vaultproject.io/&#34; target=&#34;_blank&#34;&gt;Vault&lt;/a&gt; 。&lt;/p&gt;

&lt;p&gt;我们的部署工具负责从 Vault 检索加密的值，并在部署时将其注入 Helm 。&lt;/p&gt;

&lt;p&gt;为此，我们定义了存储在 Vault 中的 Secrets 与我们的应用程序所需的 Secrets 之间的映射，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;secrets: 
     - secret_id: &amp;quot;stack1-app1-password&amp;quot;
       contexts:
         - name: &amp;quot;default&amp;quot;
           vaultPath: &amp;quot;/kv/dev/stack1/app1/test&amp;quot;
           vaultKey: &amp;quot;password&amp;quot;
         - name: &amp;quot;cluster1&amp;quot;
           vaultPath: &amp;quot;/kv/dev/stack1/app1/test&amp;quot;
           vaultKey: &amp;quot;password&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;定义将 Secrets 写入 Vault 时要遵循的通用规则。&lt;/li&gt;
&lt;li&gt;如果 Secrets 有特定的上下文/集群，则必须添加特定条目。&lt;/li&gt;
&lt;li&gt;否则，将使用默认值。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对于此列表中的每个项目，将在 Kubernetes Secrets 中插入一个 key/value 。这样我们 chart 中的 Secrets 模板非常简单。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
data:
{{- range $key,$value := .Values.secrets }}
{{ $key }}: {{ $value | b64enc | quote }}
{{ end }}
kind: Secret
metadata:
name: &amp;quot;{{ .Chart.Name }}&amp;quot;
labels:
chartVersion: &amp;quot;{{ .Chart.Version }}&amp;quot;
tillerVersion: &amp;quot;{{ .Capabilities.TillerVersion.SemVer }}&amp;quot;
type: Opaque
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;警告与限制&#34;&gt;警告与限制&lt;/h2&gt;

&lt;p&gt;image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false-1&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;h3 id=&#34;操作多个存储库&#34;&gt;操作多个存储库&lt;/h3&gt;

&lt;p&gt;目前，chart 和应用程序开发是分离的。这意味着开发人员必须处理两个 git 存储库，一个用于应用程序，另一个用于定义如何在 Kubernetes 上部署。而2个 git 存储库意味着两个工作流程，这对于新手来说可能相当复杂。&lt;/p&gt;

&lt;h3 id=&#34;管理-umbrella-charts-可能很棘手&#34;&gt;管理 umbrella charts 可能很棘手&lt;/h3&gt;

&lt;p&gt;如前所述，umbrella charts 非常适合定义依赖关系并快速部署多个应用程序。同时我们使用 &lt;code&gt;--reuse-values&lt;/code&gt; 选项，以避免每次部署作为 umbrella charts 一部分的应用程序时都要传递所有值。&lt;/p&gt;

&lt;p&gt;在我们的 CD 工作流中，只有2个值会定期更改：副本数量和镜像标签（版本）。对于其他更稳定的值，需要手动更新，而且这些值并不是很容易弄清楚。此外，我们曾遇到过部署 umbrella charts 的一个错误导致严重的中断。&lt;/p&gt;

&lt;h3 id=&#34;更新多个配置文件&#34;&gt;更新多个配置文件&lt;/h3&gt;

&lt;p&gt;添加新应用程序时，开发人员必须更改多个文件：应用程序声明， Secrets 列表，如果应用程序是 umbrella charts 的一部分，则将其添加到依赖。&lt;/p&gt;

&lt;h3 id=&#34;在-vault-上-jenkins-权限过大&#34;&gt;在 Vault 上， Jenkins 权限过大&lt;/h3&gt;

&lt;p&gt;目前，我们有一个 &lt;a href=&#34;https://www.vaultproject.io/docs/auth/approle.html&#34; target=&#34;_blank&#34;&gt;AppRole&lt;/a&gt; 可以读取 Vault 的所有 Secrets 。&lt;/p&gt;

&lt;h3 id=&#34;回滚过程不是自动化的&#34;&gt;回滚过程不是自动化的&lt;/h3&gt;

&lt;p&gt;回滚需要在多个集群上运行该命令，这可能容易出错。我们制作本操作手册是因为我们要确保应用正确的版本 ID 。&lt;/p&gt;

&lt;h2 id=&#34;gitops-实践&#34;&gt;GitOps 实践&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;目标&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们的想法是将 chart 放回到它部署的应用程序的存储库下。工作流程与应用同时开发，例如，无论何时在主服务器上合并分支，都会自动触发部署。这种方法与当前工作流程的主要区别在于，所有内容都将通过 git 进行管理（应用程序本身以及我们在 Kubernetes 中部署它的方式）。&lt;/p&gt;

&lt;p&gt;这样做优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;从开发人员的角度来看，更容易理解。学习如何在本地 chart 中应用更改将更容易。&lt;/li&gt;
&lt;li&gt;将服务 deployment 定义在与此服务的代码相同的位置。&lt;/li&gt;
&lt;li&gt;移除 umbrella charts 管理。服务将拥有自己的 Helm 版本。这使得应用程序生命周期管理（回滚，升级）形成闭环，不会影响其他服务。&lt;/li&gt;
&lt;li&gt;git 功能对 chart 管理的好处：回滚，审计日志&amp;hellip;&amp;hellip;如果要还原 chart 更改，可以使用 git 进行更改。同时部署将自动触发。&lt;/li&gt;
&lt;li&gt;我们考虑使用 Skaffold 等工具改进开发工作流程，这些工具允许开发人员在类似于生产的环境中测试他们的更改。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;2步迁移&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们的开发人员已使用上述工作流程2年，因此我们需要尽可能顺利地进行迁移。这就是为什么我们决定在达到目标之前添加一个中间步骤。&lt;/p&gt;

&lt;p&gt;第一步很简单：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;我们将保留一个类似的结构来配置我们的应用程序部署，但是在名为 “DailymotionRelease” 的单个对象中&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: &amp;quot;v1&amp;quot;
kind: &amp;quot;DailymotionRelease&amp;quot;
metadata:
name: &amp;quot;app1.ns1&amp;quot;
environment: &amp;quot;dev&amp;quot;
branch: &amp;quot;mybranch&amp;quot;
spec:
slack_channel: &amp;quot;#admin&amp;quot;
chart_name: &amp;quot;app1&amp;quot;
scaling:
- context: &amp;quot;dev-us-central1-0&amp;quot;
  replicas:
    - name: &amp;quot;hermes&amp;quot;
      count: 2
- context: &amp;quot;dev-europe-west1-0&amp;quot;
  replicas:
    - name: &amp;quot;app1-deploy&amp;quot;
      count: 2
secrets:
- secret_id: &amp;quot;app1&amp;quot;
  contexts:
    - name: &amp;quot;default&amp;quot;
      vaultPath: &amp;quot;/kv/dev/ns1/app1/test&amp;quot;
      vaultKey: &amp;quot;password&amp;quot;
    - name: &amp;quot;dev-europe-west1-0&amp;quot;
      vaultPath: &amp;quot;/kv/dev/ns1/app1/test&amp;quot;
      vaultKey: &amp;quot;password&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;每个应用程序一个版本（不再使用 umbrella charts ）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;将 chart 加入应用程序 git 存储库中&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们已经开始向所有开发人员科普这个词，并且迁移过程已经开始。第一步仍然使用 CI 平台进行控制。我将在短期内撰写另一篇博文，介绍第二步：我们如何通过 &lt;a href=&#34;https://github.com/weaveworks/flux&#34; target=&#34;_blank&#34;&gt;Flux&lt;/a&gt; 实现向 GitOps 工作流程的迁移。将描述我们的设置和面临的挑战（多个存储库，Secrets 等）。 敬请期待！&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>阿里云 ACK 挂载 NAS 数据卷</title>
      <link>https://guoxudong.io/en/post/nas-k8s/</link>
      <pubDate>Mon, 08 Jul 2019 15:09:56 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/nas-k8s/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;今天接到一个将 NAS 数据卷挂载到 Kubernetes 集群的需求，需要将一个 NAS 数据卷挂载到集群中。这一很简单的操作由于好久没有操作了，去翻看了一下官方文档，发现官方文档还在停留在去年7月份&amp;hellip;为了防止之后还有相似情况的发生，这里将所有操作做一个简单记录。&lt;/p&gt;

&lt;h2 id=&#34;购买存储包-创建文件系统&#34;&gt;购买存储包（创建文件系统）&lt;/h2&gt;

&lt;p&gt;在挂载 NAS 之前，首先要先购买 NAS 文件存储，这里推荐购买存储包，100G 的 SSD 急速型一年只需1400多，而容量型只要279，对于我这种只有少量 NAS 存储需求的人来说是是靠谱的，因为我只需要5G的左右的存储空间，SSD 急速型 NAS 一年只要18块，完美。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g4sglwrx0gj22wa09gae4.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;选择想要创建 NAS 所在 VPC 和 区域&lt;/p&gt;

&lt;h2 id=&#34;添加挂载点&#34;&gt;添加挂载点&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;点击添加挂载点
&lt;img src=&#34;https://wx3.sinaimg.cn/large/ad5fbf65gy1g4sgp0dos2j22ky0iowkr.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;选择 VPC 网络、交换机和权限组
&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g4sgpwqrgoj20xu0vowib.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;linux-挂载-nas-数据卷&#34;&gt;Linux 挂载 NAS 数据卷&lt;/h2&gt;

&lt;p&gt;在挂载点创建成功后，就可以将 NAS 数据卷挂载到 Linux 系统，这里以 CentOS 为例：&lt;/p&gt;

&lt;h3 id=&#34;安装-nfs-客户端&#34;&gt;安装 NFS 客户端&lt;/h3&gt;

&lt;p&gt;如果 Linux 系统要挂载 NAS ，首先需要安装 NFS 客户端&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo yum install nfs-utils
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;挂载-nfs-文件系统&#34;&gt;挂载 NFS 文件系统&lt;/h3&gt;

&lt;p&gt;这里阿里云早就进行了优化，点击创建的文件系统，页面上就可以 copy 挂载命令。页面提供了挂载地址的 copy 和挂载命令的 copy 功能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g4sh2i33wnj22w40yyn55.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;挂载命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo mount -t nfs -o vers=4,minorversion=0,noresvport xxxxx.cn-shanghai.nas.aliyuncs.com:/ /mnt
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;查看挂载结果&#34;&gt;查看挂载结果&lt;/h3&gt;

&lt;p&gt;直接在挂载数据卷所在服务上执行命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;df -h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就可以看到结果：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g4sh6xwyt8j20lj0850tq.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-集群挂载-nas-数据卷&#34;&gt;Kubernetes 集群挂载 NAS 数据卷&lt;/h2&gt;

&lt;p&gt;K8S 的持久数据卷挂载大同小异，流程都是：&lt;strong&gt;创建PV&lt;/strong&gt; -&amp;gt; &lt;strong&gt;创建PVC&lt;/strong&gt; -&amp;gt; &lt;strong&gt;使用PVC&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;下面就简单介绍在阿里云上的操作：&lt;/p&gt;

&lt;h3 id=&#34;创建存储卷-pv&#34;&gt;创建存储卷（PV）&lt;/h3&gt;

&lt;p&gt;首先要创建存储卷，选择 &lt;strong&gt;容器服务&lt;/strong&gt; -&amp;gt; &lt;strong&gt;存储卷&lt;/strong&gt; -&amp;gt; &lt;strong&gt;创建&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这里要注意的是：&lt;strong&gt;挂载点域名使用上面面的挂载地址&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g4shuiiyyqj20hc0hp0tz.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;创建存储声明-pvc&#34;&gt;创建存储声明（PVC）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;选择 NAS&lt;/strong&gt; -&amp;gt; &lt;strong&gt;已有存储卷&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;选择刚才创建的存储卷&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws4.sinaimg.cn/large/ad5fbf65gy1g4shv5vs1kj20hx0bvt9g.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;使用pvc&#34;&gt;使用PVC&lt;/h3&gt;

&lt;p&gt;使用的方法这里就不做详细介绍了，相关文章也比较多，这里就只记录 Deployment 中使用的 yaml 片段：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;...
volumeMounts:
- mountPath: /data      # 挂载路径
    name: volume-nas-test
...
volumes:
- name: volume-nas-test
persistentVolumeClaim:
    claimName: nas-test     # PVC 名称
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;这里只是做一个简单的记录，仅适用于阿里云 ACK 容器服务，同时也是 ACK 的一个简单应用。由于不经常对数据卷进行操作，这里做简单的记录，防止以后使用还要再看一遍文档。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>【转】Kubernetes 儿童插图指南</title>
      <link>https://guoxudong.io/en/post/the-childrens-illustrated-guide-to-kubernetes/</link>
      <pubDate>Fri, 05 Jul 2019 09:50:58 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/the-childrens-illustrated-guide-to-kubernetes/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;转自掘金社区，原文地址：&lt;a href=&#34;https://juejin.im/post/5d1b2a656fb9a07edc0b7058&#34; target=&#34;_blank&#34;&gt;https://juejin.im/post/5d1b2a656fb9a07edc0b7058&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;kubernetes-儿童插图指南&#34;&gt;Kubernetes 儿童插图指南&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://ws4.sinaimg.cn/large/ad5fbf65gy1g4nwsdbr8wj20qo0hs0w2.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://wx3.sinaimg.cn/large/ad5fbf65gy1g4nwt5keovj210u0shmyc.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g4nwteb85hj20qo0hsk5v.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;献给所有试图向孩子们解释软件工程的家长。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g4nwu2t40sj20qo0hswqv.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;很久很久以前，有一个叫 Phippy 的应用程序。她是一个简单的应用程序，由 PHP 编写且只有一个页面。她住在一个需要和其他可怕的应用程序分享环境的主机中，她不认识这些应用程序并且不愿意和他们来往。她希望她能拥有一个属于自己的环境：只有她自己和她可以称之为家的 Web 服务器。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx1.sinaimg.cn/large/ad5fbf65gy1g4nwutz6f2j20qo0hsq90.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;每个应用程序都有个运行所依赖的环境。对于 PHP 应用程序来说，这个环境可能包括 Web 服务器，一个可读文件系统和 PHP 引擎本身。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g4nwvduq0mj20qo0hsdu8.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;有一天，一只善良的鲸鱼出现了。他建议小 Phippy 住在容器里，这样可能会更快乐。所以应用程序 Phippy 迁移到了容器中。这个容器很棒，但是……它有点像一个漂浮在大海中央的豪华起居室。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g4nwwr0uk3j20qo0hs0zi.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;容器提供了一个独立的环境，应用程序可以在这个环境中运行。但是这些孤立的容器常常需要被管理并与外面的世界连接。对于孤立的容器而言，共享文件系统、网络通信、调度、负载均衡和分发都是要面对的挑战。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g4nwx3kj32j20qo0hswr7.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;鲸鱼耸了耸肩。“对不起，孩子。”他说着，消失在海面下。就在 Phippy 甚至开始绝望时，一位驾驶着巨轮的船长出现在海平线上。这艘船由几十个绑在一起的木筏组成，但从外面来看，它就像一艘巨轮。
“你好呀，这位 PHP 应用程序朋友。我是 Kube 船长。”睿智的老船长说。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx3.sinaimg.cn/large/ad5fbf65gy1g4nwxm9w44j20qo0hsn3b.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;“Kubernetes” 在希腊语中是船长的意思。我们可以从这个单词中得到 Cybernetic 和 Gubernatorial 这两个词组。Kubernetes 项目专注于构建一个健壮的平台，用于在生产环境中运行数千个容器。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g4nwxzqi7vj20qo0hsgxn.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;“我是 Phippy。”小应用程序说。&lt;/p&gt;

&lt;p&gt;“很高兴认识你。”船长一边说，一边在她身上贴上了一张标有姓名的标签。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx3.sinaimg.cn/large/ad5fbf65gy1g4nwygks8xj20qo0hs0zy.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes 使用标签作为“名牌”来标识事物。它可以根据这些标签进行查询。标签是开放性的：你可以用他们来表示角色、稳定性或其他重要的属性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g4nwyt7gtqj20qo0hsdso.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;船长建议应用程序把她的容器搬到船上的一个船舱中。Phippy 很高兴地把她的容器搬到 Kube 船长巨轮的船舱内。Phippy 觉得这里像家一样。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g4nwzc5uqej20qo0hswlp.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在 Kubernetes 中，Pod 代表一个可运行的工作单元。通常，你会在 Pod 中运行一个容器。但是对于一些容器紧密耦合的情况，你可以选择在同一个 Pod 中运行多个容器。Kubernetes 负责将你的 Pod 和网络以及 Kubernetes 的其余环境相连。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g4nwztzwfsj20qo0hsqdi.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Phippy 有一些不同寻常的兴趣，她很喜欢遗传学和绵羊。所以她问船长：“如果我想克隆我自己，是否可以根据需求克隆任意次数呢？”&lt;/p&gt;

&lt;p&gt;“这很容易。”船长说。船长把 Phippy 介绍给了 Replication Controller。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws4.sinaimg.cn/large/ad5fbf65gy1g4nx08r8toj20qo0hsdnl.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Replication Controller 提供一种管理任意数量 Pod 的方法。一个 Replication Controller 包含一个 Pod 模板，该模板可以被复制任意次数。通过 Replication Controller，Kubernetes 将管理 Pod 的生命周期，包括伸缩、滚动更新和监控。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g4nx0phj56j20qo0hsnb3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;无数个日夜，小应用程序在她的船舱中与她的复制品相处十分愉快。但与自己为伍并没有所说的那么好……即使你拥有 N 个自己的克隆体。
Kube 船长慈祥地笑了笑：“我正好有一样东西。”
他刚开口，在 Phippy 的 Replication Controller 和船的其他部分之间打开了一条隧道。Kube 船长笑着说：“即使你的复制品来了又去，这条隧道始终会留在这里，你可以通过它发现其他 Pod，其他 Pod 也可以发现你！”&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws2.sinaimg.cn/large/ad5fbf65gy1g4nx160hjkj20qo0hsjyp.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;服务告知 Kubernetes 环境的其余部分（包括其他 Pod 和 Replication Controller）你的应用程序包含了哪些服务，当 Pod 来来往往，服务的 IP 地址和端口始终保持不变。其他应用程序可以通过 Kurbenetes 服务发现找到你的服务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g4nx1kbahcj20qo0hsk2e.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;多亏了这些服务，Phippy 开始探索船的其他部分。不久之后，Phippy 遇到了 Goldie。他们成了最好的朋友。有一天，Goldie 做了一件不同寻常的事。她送给 Phippy 一件礼物。Phippy 看了礼物一眼，悲伤的泪水夺眶而出。
“你为什么这么伤心呢？”Goldie 问道。
“我喜欢这个礼物，但我没有地方可以放它！”Phippy 抽噎道。
但 Goldie 知道该怎么做。“为什么不把它放入卷中呢？”&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws4.sinaimg.cn/large/ad5fbf65gy1g4nx2ibi95j20qo0hsdnp.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;卷表示容器可以访问和存储信息的位置。对于应用程序，卷显示为本地文件系统的一部分。但卷可以由本地存储、Ceph、Gluster、持久性块存储，以及其他存储后端支持。
Phippy 喜欢在 Kube 船长的船上生活，她很享受来自新朋友的陪伴（Goldie 的每个克隆人都同样令人愉悦）。但是，当她回想起在可怕的主机度过的日子，她想知道她是否也可以拥有一点自己的隐私。
“这听起来像是你所需要的，”Kube 船长说，“这是一个命名空间。”&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws2.sinaimg.cn/large/ad5fbf65gy1g4nx2nyz4uj20qo0hs10l.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;命名空间是 Kubernetes 内部的分组机制。服务、Pod、Replication Controller 和卷可以在命名空间内部轻松协作，但命名空间提供了与集群其他部分一定程度的隔离。
Phippy 与她的新朋友一起乘坐 Kube 船长的巨轮航行于大海之上。她经历了许多伟大的冒险，但最重要的是，Phippy 找到了自己的家。
所以 Phippy 从此过上了幸福的生活。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws2.sinaimg.cn/large/ad5fbf65gy1g4nx34vepkj21120shwek.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>由一封邮件看 Mailing List 在开源项目中的重要性</title>
      <link>https://guoxudong.io/en/post/kubernetes-client-python/</link>
      <pubDate>Thu, 04 Jul 2019 09:16:41 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kubernetes-client-python/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;只要仔细找，想要的轮子总会有的。
&amp;mdash; 某不知名 DevOps 工程师&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;感谢 &lt;code&gt;kubernetes-dev&lt;/code&gt; 的 Mailing List ！早上在浏览邮件时发现了下面这封有趣的邮件：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx3.sinaimg.cn/large/ad5fbf65gy1g4nkmrb8scj21780q0afv.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;接触 Kubernetes 也有不短的时间了，也见证了 Kubernetes 干掉 Swarm 和 Mesos 成为容器编排领域的事实标准的过程。在享受 Kubernetes 及其生态圈带来的便利的同时也在为 Kubernetes 及 CNCF 项目进行贡献。而使用 &lt;a href=&#34;https://github.com/kubernetes/kubectl&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;kubectl&lt;/code&gt;&lt;/a&gt;、&lt;a href=&#34;https://github.com/rancher/rancher&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;rancher&lt;/code&gt;&lt;/a&gt; 甚至是 &lt;a href=&#34;https://github.com/IBM/kui&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;kui&lt;/code&gt;&lt;/a&gt; 这些 CLI 和 UI 工具对 Kubernetes 集群进行操作和观察。&lt;/p&gt;

&lt;p&gt;虽然上面这些工具为操作 Kubernetes 集群带来了极大的便利，但是归根到底还是一些开源项目，并不能满足我们的全部需求。所以我们只能根据我们自己的需求和 Kubernetes 的 api-server 进行定制，但是由于 Kubernetes 的 api-server 比较复杂，短时间内并不是那么好梳理的。&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-client-python&#34;&gt;kubernetes-client/python&lt;/h2&gt;

&lt;p&gt;由于我们自研的 DevOps 平台是使用 python 开发的，所以我也基于 python 语言开发了一套 Kubernetes Client ，但总的来说由于 Kubernetes 的功能实在太多，而我的开发实践并不是很多，开发出来的功能只是差强人意。&lt;/p&gt;

&lt;p&gt;而 &lt;a href=&#34;https://github.com/kubernetes-client/python&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;kubernetes-client/python&lt;/code&gt;&lt;/a&gt; 这个官方给出的轮子是真的香！&lt;/p&gt;

&lt;h3 id=&#34;安装方便&#34;&gt;安装方便&lt;/h3&gt;

&lt;p&gt;这个安装方式简单的令人发指，支持的 python 版本为 &lt;code&gt;2.7 | 3.4 | 3.5 | 3.6 | 3.7&lt;/code&gt; 并且和所有 python 依赖包一样，只需要使用 &lt;code&gt;pip&lt;/code&gt; 安装即可：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;简单示例&#34;&gt;简单示例&lt;/h3&gt;

&lt;p&gt;查看所有的 pod ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python
#encoding: utf-8
#Author: guoxudong
from kubernetes import client, config

# Configs can be set in Configuration class directly or using helper utility
config.load_kube_config()

v1 = client.CoreV1Api()
print(&amp;quot;Listing pods with their IPs:&amp;quot;)
ret = v1.list_pod_for_all_namespaces(watch=False)
for i in ret.items:
    print(&amp;quot;%s\t%s\t%s&amp;quot; % (i.status.pod_ip, i.metadata.namespace, i.metadata.name))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行查看结果：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Listing pods with their IPs:
172.22.1.126	kube-system	coredns-5975fdf55b-bqgkx
172.22.0.2	kube-system	coredns-5975fdf55b-vxbb4
10.16.16.13	kube-system	flexvolume-9ccf7
10.16.16.15	kube-system	flexvolume-h5xn2
10.16.16.14	kube-system	flexvolume-kvn5x
10.16.16.17	kube-system	flexvolume-mf4zv
10.16.16.14	kube-system	kube-proxy-worker-7lpfz
10.16.16.15	kube-system	kube-proxy-worker-9wd9s
10.16.16.17	kube-system	kube-proxy-worker-phbbj
10.16.16.13	kube-system	kube-proxy-worker-pst5d
172.22.1.9	kube-system	metrics-server-78b597d5bf-wdvqh
172.22.1.12	kube-system	nginx-ingress-controller-796ccc5d76-9jh5s
172.22.1.125	kube-system	nginx-ingress-controller-796ccc5d76-jwwwz
10.16.16.17	kube-system	terway-6mfs8
10.16.16.14	kube-system	terway-fz9ck
10.16.16.13	kube-system	terway-t9777
10.16.16.15	kube-system	terway-xbxlp
172.22.1.8	kube-system	tiller-deploy-5b5d8dd754-wpcrc
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;果然是一个好轮子，引入 kubeconfig 的方式及展示所有 namespace 的 pod 的方法封装的也十分简洁，是个非常漂亮的范例。建议可以看一下&lt;a href=&#34;https://github.com/kubernetes-client/python&#34; target=&#34;_blank&#34;&gt;源码&lt;/a&gt;，肯定会有收获的！&lt;/p&gt;

&lt;h3 id=&#34;支持版本&#34;&gt;支持版本&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;client-python&lt;/code&gt; 遵循 &lt;a href=&#34;https://semver.org/lang/zh-CN/&#34; target=&#34;_blank&#34;&gt;semver&lt;/a&gt; 规范，所以在 &lt;code&gt;client-python&lt;/code&gt; 的主要版本增加之前，代码将继续使用明确支持的 Kubernetes 集群版本。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Kubernetes 1.5&lt;/th&gt;
&lt;th&gt;Kubernetes 1.6&lt;/th&gt;
&lt;th&gt;Kubernetes 1.7&lt;/th&gt;
&lt;th&gt;Kubernetes 1.8&lt;/th&gt;
&lt;th&gt;Kubernetes 1.9&lt;/th&gt;
&lt;th&gt;Kubernetes 1.10&lt;/th&gt;
&lt;th&gt;Kubernetes 1.11&lt;/th&gt;
&lt;th&gt;Kubernetes 1.12&lt;/th&gt;
&lt;th&gt;Kubernetes 1.13&lt;/th&gt;
&lt;th&gt;Kubernetes 1.14&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;client-python 1.0&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 2.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 3.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 4.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 5.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 6.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 7.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 8.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 9.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 10.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python HEAD&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;mailing-list-的重要性&#34;&gt;Mailing List 的重要性&lt;/h2&gt;

&lt;p&gt;这次的收获很大程度得益于 &lt;code&gt;kubernetes-dev&lt;/code&gt; 的 Mailing List 也就是邮件列表。这种沟通方式在国内不是很流行，大家更喜欢使用 QQ 和微信这样的即时通讯软件进行交流，但是大多数著名开源项目都是主要使用 &lt;strong&gt;Mailing List&lt;/strong&gt; 进行交流，交流的数量甚至比在 GitHub issue 中还多，在与 Apache 、 CNCF 项目开源的贡献者和维护者交流中得知了使用 &lt;strong&gt;Mailing List&lt;/strong&gt; 主要考虑是一下几点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;这种异步的交流方式可以让更多关心该话题的开发人员一起加入到讨论中。&lt;/li&gt;
&lt;li&gt;mailing list 是永久保留的，如果你对某个话题感兴趣，可以随时回复邮件，关注这个话题的开发者都会收到邮件，无论这个话题是昨天提出的，还是去年提出的，有助于解决一些陈年老 BUG （俗称技术债）。&lt;/li&gt;
&lt;li&gt;即时通讯软件虽然很便利，但是问题很快会被评论顶掉，虽然诸如 slack 这样的工具解决了部分这方面的问题，但是还是不如 mailing list 好用。&lt;/li&gt;
&lt;li&gt;并不是所有地区的开发者都有高速的宽带，性能优秀的PC，在地球上很多地区还是只能使用拨号上网，网速只有几kb/s，他们甚至 GitHub issue 都无法使用。但是你不能剥夺他们参与开源项目的权利，而 mailing list 是一种很好的交流方式。&lt;/li&gt;
&lt;li&gt;通过 mailing list 可以很好掌握社区动态，效果明显好于 GitHub watch ，因为并不是项目的所有 commit 都是你关心的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;如果你有志于参与到开源运动，在享受开源软件带来便利的同事，还想为开源软件做出自己的贡献，那么 mailing list 是你进入社区最好的选择。在 mailing list 中和来自世界各地志同道合的开发者交流中提升自己的能力，创造更大的价值，迈出你参与开源运动的第一步。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Kustomize 帮你管理 kubernetes 应用（五）：配合 kubedog 完善 CI/CD 的最后一步</title>
      <link>https://guoxudong.io/en/post/kustomize-5/</link>
      <pubDate>Wed, 03 Jul 2019 15:20:31 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kustomize-5/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;在以往的 pipeline 中，使用 kubectl 进行部署 Deployment 后无法检查 Deployment 是否部署成功，只能通过使用命令/脚本来手动检查 Deployment 状态，而 kubedog 这个小工具完美解决了这个问题，完善了 CI/CD 流水线的最后一步。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;kubedog&#34;&gt;KubeDog&lt;/h2&gt;

&lt;p&gt;kubedog 是一个 lib 库和 CLI 小工具，允许在 CI/CD 部署 pipeline 中观察和跟踪 Kubernetes 资源。与 kustomize 配合，集成到 pipeline 之后，完美的解决了 CI/CD 的最后一步，完美的替代了之前不够灵活的脚本（好吧，其实我也开发了类似的小工具，但是有这么好用的轮子，拿来直接用何乐而不为呢？）。&lt;/p&gt;

&lt;p&gt;kubedog 提供了 lib 库和 CLI 小工具，这里由于是介绍 CI/CD 中的实践，所以只介绍其中的 &lt;code&gt;rollout track&lt;/code&gt; 功能。 lib 库的使用和 CLI 的 &lt;code&gt;follow&lt;/code&gt; 功能这里就不做介绍了，有兴趣的同学可以去 &lt;a href=&#34;https://github.com/flant/kubedog&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; 上查看该项目的各种使用方式。&lt;/p&gt;

&lt;h3 id=&#34;集成-kubedog&#34;&gt;集成 KubeDog&lt;/h3&gt;

&lt;p&gt;由于我司目前使用的是 &lt;a href=&#34;https://drone.io/&#34; target=&#34;_blank&#34;&gt;drone&lt;/a&gt; 进行 CI ，每个 step 都是由一个 docker 制作的插件组成。我制作了一个包含 &lt;code&gt;kubectl&lt;/code&gt; 、 &lt;code&gt;kustomize&lt;/code&gt; 和 &lt;code&gt;kubedog&lt;/code&gt; 的镜像。该镜像已上传 dockerhub ，需要的可以自行拉取使用 &lt;code&gt;guoxudongdocker/kubectl&lt;/code&gt; ,而该插件的使用也在 &lt;a href=&#34;https://github.com/sunny0826/kubectl-kustomize&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; 和 &lt;a href=&#34;https://cloud.docker.com/u/guoxudongdocker/repository/docker/guoxudongdocker/kubectl&#34; target=&#34;_blank&#34;&gt;DockerHub&lt;/a&gt; 上查看。&lt;/p&gt;

&lt;p&gt;而集成方式也比较简单，直接将 &lt;code&gt;kubectl&lt;/code&gt; 、 &lt;code&gt;kustomize&lt;/code&gt; 和 &lt;code&gt;kubedog&lt;/code&gt; 的可执行包下载到 &lt;code&gt;/usr/local/bin&lt;/code&gt; 并赋予执行权限即可，下面就是 &lt;code&gt;Dockerfile&lt;/code&gt; 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dockerfile&#34;&gt;FROM alpine

LABEL maintainer=&amp;quot;sunnydog0826@gmail.com&amp;quot;

ENV KUBE_LATEST_VERSION=&amp;quot;v1.14.1&amp;quot;

RUN apk add --update ca-certificates \
 &amp;amp;&amp;amp; apk add --update -t deps curl \
 &amp;amp;&amp;amp; curl -L https://storage.googleapis.com/kubernetes-release/release/${KUBE_LATEST_VERSION}/bin/linux/amd64/kubectl -o /usr/local/bin/kubectl \
 &amp;amp;&amp;amp; chmod +x /usr/local/bin/kubectl \
 &amp;amp;&amp;amp; curl -L https://github.com/kubernetes-sigs/kustomize/releases/download/v2.0.3/kustomize_2.0.3_linux_amd64 -o /usr/local/bin/kustomize \
 &amp;amp;&amp;amp; chmod +x /usr/local/bin/kustomize \
 &amp;amp;&amp;amp; curl -L https://dl.bintray.com/flant/kubedog/v0.2.0/kubedog-linux-amd64-v0.2.0 -o /usr/local/bin/kubedog \
 &amp;amp;&amp;amp; chmod +x /usr/local/bin/kubedog \
 &amp;amp;&amp;amp; apk del --purge deps \
 &amp;amp;&amp;amp; rm /var/cache/apk/*


WORKDIR /root
ENTRYPOINT [&amp;quot;kubectl&amp;quot;]
CMD [&amp;quot;help&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;kustomize-配合-kubedog-使用&#34;&gt;Kustomize 配合 KubeDog 使用&lt;/h2&gt;

&lt;p&gt;在镜像构建好之后就可以直接使用了，这里使用的是 DockerHub 的镜像仓库，这里建议将镜像同步到私有仓库，比如阿里云的容器镜像服务或者 Habor ，因为国内拉取 DockerHub 的镜像不太稳定，经常会拉取镜像失败或者访问超时，在 CI/CD 流水线中推荐使用更稳定镜像。&lt;/p&gt;

&lt;p&gt;以下是 &lt;code&gt;.drone.yml&lt;/code&gt; 示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kind: pipeline
name: {your-pipeline-name}

steps:
- name: Kubernetes 部署
  image: guoxudongdocker/kubectl
  volumes:
  - name: kube
    path: /root/.kube
  commands:
    - cd deploy/overlays/dev    # 这里使用 kustomize ,详细使用方法请见 https://github.com/kubernetes-sigs/kustomize
    - kustomize edit set image {your-docker-registry}:${DRONE_BUILD_NUMBER}
    - kubectl apply -k . &amp;amp;&amp;amp; kubedog rollout track deployment {your-deployment-name} -n {your-namespace} -t {your-tomeout}

...

volumes:
- name: kube
  host:
    path: /tmp/cache/.kube  # kubeconfig 挂载位置

trigger:
  branch:
  - master  # 触发 CI 的分支
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的配置可见，在该 step 中执行了如下几步：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;进入 patch 所在路径&lt;/li&gt;
&lt;li&gt;使用了 Kustomize 命令 &lt;code&gt;kustomize edit set image {your-docker-registry}:${DRONE_BUILD_NUMBER}&lt;/code&gt; 方式将前面 step 中构建好的镜像的 tag 插入到 patch 中&lt;/li&gt;
&lt;li&gt;使用 &lt;code&gt;kubectl apply -k .&lt;/code&gt; 进行 k8s 部署，要注意最后的那个 &lt;code&gt;.&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;使用 kubedog 跟踪 Deployment 部署状态&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;命令解析：&lt;code&gt;kubedog rollout track deployment {your-deployment-name} -n {your-namespace} -t {your-tomeout}&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;deployment {your-deployment-name} : Deployment 的名称&lt;/li&gt;
&lt;li&gt;-n {your-namespace} : Deployment 所在的 namespace&lt;/li&gt;
&lt;li&gt;-t {your-tomeout} : 超时时间，单位为秒，超时后会报错，这里请根据实际部署情况进行设置&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;从 Kubernetes release v1.14 版本开始，&lt;code&gt;kustomize&lt;/code&gt; 集成到 &lt;code&gt;kubectl&lt;/code&gt; 中，越来越多 k8S 周边的小工具出现。这些小工具的出现帮助了 Kubernetes 的使用者来拉平 Kubernetes 的使用曲线，同时也标志着 K8S 的成熟，越来越多的开发人员基于使用 K8S 的痛点开发相关工具。套用一句今年 KubeCon 的 Keynote 演讲上，阿里云智能容器平台负责人丁宇的话： &lt;strong&gt;Kubernetes 正当时，云原生未来可期&lt;/strong&gt; 。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>记一次使用 Kustomize 时遇到的愚蠢问题</title>
      <link>https://guoxudong.io/en/post/kustomize-err-1/</link>
      <pubDate>Wed, 03 Jul 2019 13:44:50 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kustomize-err-1/</guid>
      <description>

&lt;h2 id=&#34;现象&#34;&gt;现象&lt;/h2&gt;

&lt;p&gt;在日常 CI/CD 流程中，已经将 Kustomize 集成到 pipeline 中使用，但是在对一个项目进行 Kustomize 改造时，将单个 &lt;code&gt;deploy.yaml&lt;/code&gt; 拆分为了若干个 patch 以达到灵活 Kubernetes 部署的目的。但是在使用 &lt;code&gt;kubectl apply -k .&lt;/code&gt; 命令进行部署的时候遇到了 &lt;code&gt;error: failed to find an object with apps_v1_Deployment|myapp to apply the patch&lt;/code&gt; 的报错。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx3.sinaimg.cn/large/ad5fbf65gy1g4mm1m3vx9j21oe10y102.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;解决之路&#34;&gt;解决之路&lt;/h2&gt;

&lt;p&gt;由于之前的使用中没有遇到此类报错，看报错信息像是 &lt;code&gt;apiVersion&lt;/code&gt; 的问题，所以先检查了所有 patch 的 &lt;code&gt;apiVersion&lt;/code&gt; ，但是并没有找到有什么问题。&lt;/p&gt;

&lt;h3 id=&#34;google-搜索&#34;&gt;Google 搜索&lt;/h3&gt;

&lt;p&gt;对该报错进行了搜索，搜索到如下结果：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws4.sinaimg.cn/large/ad5fbf65gy1g4mmee8ctxj21900ns44c.jpg&#34; alt=&#34;image&#34; /&gt;
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g4mmgrdz0fj21ou1b6wro.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;？？？ 为何这个 issue 没有解决就被提出者关闭了？&lt;/p&gt;

&lt;h3 id=&#34;问题解决&#34;&gt;问题解决&lt;/h3&gt;

&lt;p&gt;在 Google 了一圈之后还是没有找到什么有营养的回答，问题又回到了原点&amp;hellip;只能对所有的 patch 的每个字符和每个配置逐一进行了检查。结果发现是 &lt;code&gt;name&lt;/code&gt; 的内容 base 与 overlays 不同&amp;hellip; base 中是 &lt;code&gt;name:myapp&lt;/code&gt; ，而 overlays 中是 &lt;code&gt;name:my-app&lt;/code&gt; &amp;hellip;&lt;/p&gt;

&lt;p&gt;好吧，issue 关的是有道理的&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx1.sinaimg.cn/large/ad5fbf65gy1g4mmuqm6n2j2098048a9z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Kustomize 帮你管理 kubernetes 应用（四）：简述核心配置 kustomization.yaml</title>
      <link>https://guoxudong.io/en/post/kustomize-4/</link>
      <pubDate>Thu, 23 May 2019 12:50:12 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kustomize-4/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;在前面的文章中已经介绍了 kustomize 是什么，以及如何开始使用和如何简单的在 CI/CD 中使用，本篇文章将会介绍 kustomize 的核心文件 &lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/blob/master/docs/zh/kustomization.yaml&#34; target=&#34;_blank&#34;&gt;kustomization.yaml&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;另外，博主已经向 kustomize 贡献了中文文档，已被官方采纳，现在在 kustomize 中的 &lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/tree/master/docs/zh&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;docs/zh&lt;/code&gt;&lt;/a&gt; 目录中就可看到，翻译的不好的地方欢迎指正。同时也在 GitHub 上新建了一个 名为 &lt;a href=&#34;https://github.com/sunny0826/kustomize-lab&#34; target=&#34;_blank&#34;&gt;kustomize-lab&lt;/a&gt; 的 repo 用于演示 kustomize 的各种用法及技巧，本文中介绍的内容也会同步更新到该 repo 中，欢迎 fork、star、PR。&lt;/p&gt;

&lt;h2 id=&#34;kustomization-yaml-的作用&#34;&gt;&lt;code&gt;kustomization.yaml&lt;/code&gt; 的作用&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Kustomize 允许用户以一个应用描述文件 （YAML 文件）为基础（Base YAML），然后通过 Overlay 的方式生成最终部署应用所需的描述文件。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;有前面的文章&lt;a href=&#34;../kustomize-2&#34;&gt;《使用 Kustomize 帮你管理 kubernetes 应用（二）： Kustomize 的使用方法》&lt;/a&gt;中已经介绍了，每个 &lt;code&gt;base&lt;/code&gt; 或 &lt;code&gt;overlays&lt;/code&gt; 中都必须要有一个 &lt;code&gt;kustomization.yaml&lt;/code&gt;，这里我们看一下官方示例 &lt;code&gt;helloWorld&lt;/code&gt; 中的 &lt;code&gt;kustomization.yaml&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-YAML&#34;&gt;commonLabels:
  app: hello

resources:
- deployment.yaml
- service.yaml
- configMap.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到该项目中包含3个 resources ， &lt;code&gt;deployment.yaml&lt;/code&gt;、&lt;code&gt;service.yaml&lt;/code&gt; 、 &lt;code&gt;configMap.yaml&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.
└── helloWorld
    ├── configMap.yaml
    ├── deployment.yaml
    ├── kustomization.yaml
    └── service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;直接执行命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kustomize build helloWorld
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就可以看到结果了：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-YAML&#34;&gt;apiVersion: v1
data:
  altGreeting: Good Morning!
  enableRisky: &amp;quot;false&amp;quot;
kind: ConfigMap
metadata:
  labels:
    app: hello
  name: the-map
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: hello
  name: the-service
spec:
  ports:
  - port: 8666
    protocol: TCP
    targetPort: 8080
  selector:
    app: hello
    deployment: hello
  type: LoadBalancer
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hello
  name: the-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: hello
  template:
    metadata:
      labels:
        app: hello
        deployment: hello
    spec:
      containers:
      - command:
        - /hello
        - --port=8080
        - --enableRiskyFeature=$(ENABLE_RISKY)
        env:
        - name: ALT_GREETING
          valueFrom:
            configMapKeyRef:
              key: altGreeting
              name: the-map
        - name: ENABLE_RISKY
          valueFrom:
            configMapKeyRef:
              key: enableRisky
              name: the-map
        image: monopole/hello:1
        name: the-container
        ports:
        - containerPort: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的结果可以看大 kustomize 通过 &lt;code&gt;kustomization.yaml&lt;/code&gt; 将3个 resources 进行了处理，给三个 resources 添加了共同的 labels &lt;code&gt;app: hello&lt;/code&gt; 。这个示例展示了 &lt;code&gt;kustomization.yaml&lt;/code&gt; 的作用：&lt;strong&gt;将不同的 resources 进行整合，同时为他们加上相同的配置&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&#34;进阶使用&#34;&gt;进阶使用&lt;/h2&gt;

&lt;p&gt;上面只不过是一个简单的示例，下面将结合实际情况分享一些比较实用的用法&lt;/p&gt;

&lt;h3 id=&#34;根据环境生成不同配置&#34;&gt;根据环境生成不同配置&lt;/h3&gt;

&lt;p&gt;在实际的使用中，使用最多的就是为不同的环境配置不同的 &lt;code&gt;deploy.yaml&lt;/code&gt;，而使用 kustomize 可以把配置拆分为多个小的 patch ，然后通过 kustomize 来进行组合。而根据环境的不同，每个 patch 都可能不同，包括分配的资源、访问的方式、部署的节点都可以自由的定制。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.
├── flask-env
│   ├── README.md
│   ├── base
│   │   ├── deployment.yaml
│   │   ├── kustomization.yaml
│   │   └── service.yaml
│   └── overlays
│       ├── dev
│       │   ├── healthcheck_patch.yaml
│       │   ├── kustomization.yaml
│       │   └── memorylimit_patch.yaml
│       └── prod
│           ├── healthcheck_patch.yaml
│           ├── kustomization.yaml
│           └── memorylimit_patch.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里可以看到配置分为了 &lt;code&gt;base&lt;/code&gt; 和 &lt;code&gt;overlays&lt;/code&gt;， &lt;code&gt;overlays&lt;/code&gt; 则是继承了 &lt;code&gt;base&lt;/code&gt; 的配置，同时添加了诸如 healthcheck 和 memorylimit 等不同的配置，那么我们分别看一下 &lt;code&gt;base&lt;/code&gt; 和 &lt;code&gt;overlays&lt;/code&gt; 中 &lt;code&gt;kustomization.yaml&lt;/code&gt; 的内容&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;base&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-YAML&#34;&gt;commonLabels:
app: test-cicd

resources:
- service.yaml
- deployment.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;base&lt;/code&gt; 中的 &lt;code&gt;kustomization.yaml&lt;/code&gt; 中定义了一些基础配置&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;overlays&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-YAML&#34;&gt;bases:
- ../../base
patchesStrategicMerge:
- healthcheck_patch.yaml
- memorylimit_patch.yaml
namespace: devops-dev
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;overlays&lt;/code&gt; 中的 &lt;code&gt;kustomization.yaml&lt;/code&gt; 则是基于 &lt;code&gt;base&lt;/code&gt; 新增了一些个性化的配置，来达到生成不同环境的目的。&lt;/p&gt;

&lt;p&gt;执行命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kustomize build flask-env/overlays/dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-YAML&#34;&gt;apiVersion: v1
kind: Service
metadata:
  labels:
    app: test-cicd
  name: test-cicd
  namespace: devops-dev
spec:
  ports:
  - name: http
    port: 80
    targetPort: 80
  selector:
    app: test-cicd
  type: ClusterIP
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: test-cicd
  name: test-cicd
  namespace: devops-dev
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test-cicd
  template:
    metadata:
      labels:
        app: test-cicd
        version: 0.0.3
    spec:
      containers:
      - env:
        - name: ENV
          value: dev
        image: guoxudongdocker/flask-python:latest
        imagePullPolicy: Always
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 20
        name: test-cicd
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 20
        resources:
          limits:
            cpu: 300m
            memory: 500Mi
          requests:
            cpu: 300m
            memory: 500Mi
        volumeMounts:
        - mountPath: /etc/localtime
          name: host-time
      imagePullSecrets:
      - name: registry-pull-secret
      volumes:
      - hostPath:
          path: /etc/localtime
        name: host-time
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到包括 &lt;code&gt;replicas&lt;/code&gt;、&lt;code&gt;limits&lt;/code&gt;、&lt;code&gt;requests&lt;/code&gt;、&lt;code&gt;env&lt;/code&gt; 等 dev 中个性的配置都已经出现在了生成的 yaml 中。由于篇幅有限，这里没有把所有的配置有罗列出来，需要的可以去 &lt;a href=&#34;https://github.com/sunny0826/kustomize-lab&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; 上自取。&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;上面所有的 &lt;code&gt;kustomize build dir/&lt;/code&gt; 都可以使用 &lt;code&gt;kubectl apply -k dir/&lt;/code&gt; 实现，但是需要 &lt;code&gt;v14.0&lt;/code&gt; 版以上的 &lt;code&gt;kubectl&lt;/code&gt;，也就是说，其实我们在集成到 CI/CD 中的时候，甚至都不需要用来 &lt;code&gt;kustomize&lt;/code&gt; 命令集，有 &lt;code&gt;kubectl&lt;/code&gt; 就够了。&lt;/p&gt;

&lt;p&gt;由于篇幅有限，这里没法吧所有 &lt;code&gt;kustomization.yaml&lt;/code&gt; 的用途都罗列出来，不过可以在官方文档中找到我提交的中文翻译版 &lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/blob/master/docs/zh/kustomization.yaml&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;kustomization.yaml&lt;/code&gt;&lt;/a&gt;，可以直接去官方 GitHub 查看。同时 &lt;a href=&#34;https://github.com/sunny0826/kustomize-lab&#34; target=&#34;_blank&#34;&gt;kustomize-lab&lt;/a&gt; 会持续更行，敬请关注。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Kustomize 帮你管理 kubernetes 应用（三）：将 Kustomize 应用于 CI/CD</title>
      <link>https://guoxudong.io/en/post/kustomize-3/</link>
      <pubDate>Mon, 06 May 2019 16:46:28 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kustomize-3/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;首先明确软件版本，我这里使用的是 &lt;code&gt;Jenkins ver. 2.121.3&lt;/code&gt; ，这个版本比较老，其上安装 Kubernetes 插件所使用 &lt;code&gt;kubectl&lt;/code&gt; 版本也比较老，&lt;strong&gt;无法使用&lt;/strong&gt; Kustomize 的 yaml 文件需要的 &lt;code&gt;apiVersion: apps/v1&lt;/code&gt; ，直接使用生成 &lt;code&gt;deploy.yaml&lt;/code&gt; 文件会报错，所以这里选择了自己构建一个包含 &lt;code&gt;kubectl&lt;/code&gt; 和 &lt;code&gt;kustomize&lt;/code&gt; 的镜像，在镜像中使用 Kustomize 生成所需 yaml 文件并在 Kubernetes 上部署。&lt;/p&gt;

&lt;h2 id=&#34;软件版本&#34;&gt;软件版本&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;软件&lt;/th&gt;
&lt;th&gt;版本&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Jenkins&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://jenkins.io/&#34; target=&#34;_blank&#34;&gt;2.121.3&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;kubectl&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34; target=&#34;_blank&#34;&gt;v1.14.1&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;kustomize&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/releases&#34; target=&#34;_blank&#34;&gt;v2.0.3&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;前期准备&#34;&gt;前期准备&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Jenkins ：本篇使用 Jenkins 演示 CI/CD ，安装 Jenkins 就不在赘述，可以使用多种方法安装 Jenkins ，详细方法见&lt;a href=&#34;https://jenkins.io&#34; target=&#34;_blank&#34;&gt;官网&lt;/a&gt;。同时。 CI/CD 的工具有很多，这里为了省事使用笔者现有的 Jenkins 进行演示，&lt;strong&gt;不推荐&lt;/strong&gt;使用同笔者一样的版本，请使用较新的版本；同时也可以使用其他 CI/CD 工具，这里推荐使用 &lt;a href=&#34;https://drone.io/&#34; target=&#34;_blank&#34;&gt;drone&lt;/a&gt;。如果有更好的方案，欢迎交流，可以在&lt;a href=&#34;https://blog.maoxianplay.com/contact/&#34; target=&#34;_blank&#34;&gt;关于&lt;/a&gt;中找到我的联系方式。&lt;/li&gt;

&lt;li&gt;&lt;pre&gt;&lt;code class=&#34;language-kubectl```&#34;&gt;- Web 应用：这里使用 flask 写了一个简单的 web 应用，用于演示，同样以上传 dockerhub [```guoxudongdocker/flask-python```](https://hub.docker.com/r/guoxudongdocker/flask-python)

## 目录结构

首先看一下目录结构，目录中包括 ```Dockerfile``` 、 ```Jenkinsfile``` 、 Kustomize 要使用的 ```deploy``` 目录以及 web 应用目录。

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bush&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;.
├── Dockerfile
├── Jenkinsfile
├── app
│   ├── main.py
│   └── uwsgi.ini
└── deploy
    ├── base
    │   ├── deployment.yaml
    │   ├── kustomization.yaml
    │   └── service.yaml
    └── overlays
        ├── dev
        │   ├── healthcheck_patch.yaml
        │   ├── kustomization.yaml
        │   └── memorylimit_patch.yaml
        └── prod
            ├── healthcheck_patch.yaml
            ├── kustomization.yaml
            └── memorylimit_patch.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
这里可以看到 overlays 总共有两个子目录 `dev` 和 `prod` ，分别代表不同环境，在不同的环境中，应用不同的配置。

## Jenkins 配置

Jenkins 的配置相对简单，只需要新建一个 pipeline 类型的 job

![WX20190506-180159](https://wx4.sinaimg.cn/large/ad5fbf65gy1g2rr57oixbj20tn0ogq6v.jpg)

增加参数化构建，**注**：参数化构建需要安装 Jenkins 插件

![WX20190506-180918](https://ws4.sinaimg.cn/large/ad5fbf65gy1g2rrcb5ic9j21470q7mz8.jpg)

然后配置代码仓库即可

![WX20190507-094958](https://ws3.sinaimg.cn/large/ad5fbf65gy1g2sij1xlb2j214w0nw0uw.jpg)

## Pipeline 

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;groovy
podTemplate(label: &amp;lsquo;jnlp-slave&amp;rsquo;, cloud: &amp;lsquo;kubernetes&amp;rsquo;,
  containers: [
    containerTemplate(
        name: &amp;lsquo;jnlp&amp;rsquo;,
        image: &amp;lsquo;guoxudongdocker/jenkins-slave&amp;rsquo;,
        alwaysPullImage: true
    ),
    containerTemplate(name: &amp;lsquo;kubectl&amp;rsquo;, image: &amp;lsquo;guoxudongdocker/kubectl:v1.14.1&amp;rsquo;, command: &amp;lsquo;cat&amp;rsquo;, ttyEnabled: true),
  ],
  nodeSelector:&amp;lsquo;ci=jenkins&amp;rsquo;,
  volumes: [
    hostPathVolume(mountPath: &amp;lsquo;/var/run/docker.sock&amp;rsquo;, hostPath: &amp;lsquo;/var/run/docker.sock&amp;rsquo;),
    hostPathVolume(mountPath: &amp;lsquo;/usr/bin/docker&amp;rsquo;, hostPath: &amp;lsquo;/usr/bin/docker&amp;rsquo;),
    hostPathVolume(mountPath: &amp;lsquo;/usr/local/jdk&amp;rsquo;, hostPath: &amp;lsquo;/usr/local/jdk&amp;rsquo;),
    hostPathVolume(mountPath: &amp;lsquo;/usr/local/maven&amp;rsquo;, hostPath: &amp;lsquo;/usr/local/maven&amp;rsquo;),
    secretVolume(mountPath: &amp;lsquo;/home/jenkins/.kube&amp;rsquo;, secretName: &amp;lsquo;devops-ctl&amp;rsquo;),
  ],
)
{
    node(&amp;ldquo;jnlp-slave&amp;rdquo;){
        stage(&amp;lsquo;Git Checkout&amp;rsquo;){
            git branch: &amp;lsquo;${branch}&amp;rsquo;, url: &amp;lsquo;&lt;a href=&#34;https://github.com/sunny0826/flask-python.git&#39;&#34; target=&#34;_blank&#34;&gt;https://github.com/sunny0826/flask-python.git&#39;&lt;/a&gt;
        }
        stage(&amp;lsquo;Build and Push Image&amp;rsquo;){
            withCredentials([usernamePassword(credentialsId: &amp;lsquo;docker-register&amp;rsquo;, passwordVariable: &amp;lsquo;dockerPassword&amp;rsquo;, usernameVariable: &amp;lsquo;dockerUser&amp;rsquo;)]) {
                sh &amp;ldquo;&amp;rsquo;
                docker login -u ${dockerUser} -p ${dockerPassword}
                docker build -t guoxudongdocker/flask-python:${Tag} .
                docker push guoxudongdocker/flask-python:${Tag}
                &amp;ldquo;&amp;rsquo;
            }
        }
        stage(&amp;lsquo;Deploy to K8s&amp;rsquo;){
            if (&amp;lsquo;true&amp;rsquo; == &amp;ldquo;${deploy}&amp;rdquo;) {
                container(&amp;lsquo;kubectl&amp;rsquo;) {
                    sh &amp;ldquo;&amp;rsquo;
                    cd deploy/base
                    kustomize edit set image guoxudongdocker/flask-python:${Tag}
                    &amp;ldquo;&amp;rsquo;
                    echo &amp;ldquo;部署到 Kubernetes&amp;rdquo;
                    if (&amp;lsquo;prod&amp;rsquo; == &amp;ldquo;${ENV}&amp;rdquo;) {
                        sh &amp;ldquo;&amp;rsquo;
                        # kustomize build deploy/overlays/prod | kubectl apply -f -
                        kubectl applt -k deploy/overlays/prod
                        &amp;ldquo;&amp;rsquo;
                    }else {
                        sh &amp;ldquo;&amp;rsquo;
                        # kustomize build deploy/overlays/dev | kubectl apply -f -
                        kubectl applt -k deploy/overlays/dev
                        &amp;ldquo;&amp;rsquo;
                    }	
                }
            }else{
                echo &amp;ldquo;跳过Deploy to K8s&amp;rdquo;
            }&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
这里要注意几点：

- 拉取 git 中的代码需要在 jenkins 中配置凭据。
- 笔者的 jenkins 部署在 Kubernetes 上，要操作集群的话，需要将 kubeconfig 以 Secret 的形式挂载到 jenkins 所在 namespace。
- `jenkins-slave` 需要 Java 环境运行，所以要将宿主机的 `jdk` 挂载到 `jenkins-slave` 中。
- 同样的，宿主机中需要事先安装 `docker`。
- `docker-register` 为 dockerhub 的登录凭证，需要在 jenkins 中添加相应的凭证。

## 演示

image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---

### 开始构建

这里选择环境、分支，填入版本即可开始构建，**注意：**这里的版本将已 tag 的形式标记 docker 镜像。

![WX20190507-095142](https://ws2.sinaimg.cn/large/ad5fbf65gy1g2sikst7tuj20ob0evabw.jpg)

这里就可以看到构建成功了

![WX20190507-103721](https://ws2.sinaimg.cn/large/ad5fbf65ly1g2sjw9w22ej20v80km0w3.jpg)

### 查看结果

这里为了方便（其实就是懒），我就不给这个服务添加 ingress 来从外部访问了，这里使用 [KT](https://yq.aliyun.com/articles/690519) 打通本地和 k8s 集群网络来进行调试。

&amp;gt;为了简化在Kubernetes下进行联调测试的复杂度，云效在SSH隧道网络的基础上并结合Kubernetes特性构建了一款面向开发者的辅助工具kt

这里看到这个服务正常启动了

![WX20190507-104154](https://ws2.sinaimg.cn/large/ad5fbf65ly1g2sk11dnzxj20av027jrn.jpg)

### 发布新版本

更新 web 服务并提交

![WX20190507-104936](https://ws4.sinaimg.cn/large/ad5fbf65gy1g2sk94v1c5j209702vwej.jpg)


按照上面步骤在 jenkins 中重新构建，当然也可以配置钩子，每次代码提交后自动构建

### 查看查看新版本

同上面一样，在构建成功后查看服务是否更新

![WX20190507-105539](https://wx4.sinaimg.cn/large/ad5fbf65gy1g2skfczaz4j20by01smx7.jpg)

可以看到，版本已经更新了

### 发布生产环境

这里模拟一下发布生产环境，假设生产环境是在 `devops-prod` 的 namespace 中，这里只做演示之用，真正的生产环境中，可能存在不止一个 k8s 集群，这时需要修改 Jenkinsfile 中的 `secretVolume` 来挂载不同 k8s 的 kubeconfig 来达到发布到不同集群的目的。当然，一般发布生产环境只需选择测试通过的镜像来发布即可，不需要在进行构建打包。

![WX20190507-110730](https://ws3.sinaimg.cn/large/ad5fbf65gy1g2skrnbjyuj20fc0bjmxp.jpg)

### 查看生产版本

![WX20190507-110850](https://ws1.sinaimg.cn/large/ad5fbf65ly1g2skt3rp4yj20aq010glj.jpg)

### 总结

上面的这些步骤简单的演示了使用 jenkins 进行 CI/CD 的流程，流程十分简单，这里仅供参考

## Kustomize 的作用

那么， Kustomize 在整个流程中又扮演了一个什么角色呢？

### 更新镜像

在 `jenkinsfile` 中可以看到， kustomize 更新了基础配置的镜像版本，这里我们之前一直是使用 `sed -i &amp;quot;s/#Tag/${Tag}/g&amp;quot; deploy.yaml` 来进行替换了，但是不同环境存在比较多的差异，需要替换的越来越多，导致 Jekninsfile 也越来越臃肿和难以维护。 kustomize 解决了这个问题。

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
kustomize edit set image guoxudongdocker/flask-python:${Tag}&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
### 环境区分

上面也提到了，不同的环境我们存在这许多差异，虽然看上去大致类似，但是很多细节都需要修改。这时 kustomize 就起到了很大的作用，不同环境相同的配置都放在 `base` 中，而差异就可以在 `overlays` 中实现。

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
.
├── base
│   ├── deployment.yaml
│   ├── kustomization.yaml
│   └── service.yaml
└── overlays
    ├── dev
    │   ├── healthcheck_patch.yaml
    │   ├── kustomization.yaml
    │   └── memorylimit_patch.yaml
    └── prod
        ├── healthcheck_patch.yaml
        ├── kustomization.yaml
        └── memorylimit_patch.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;可以看到， `base` 中维护了项目共同的基础配置，如果有镜像版本等基础配置需要修改，可以使用 `kustomize edit set image ...` 来直接修改基础配置，而真正不同环境，或者不同使用情况的配置则在 `overlays` 中 以 patch 的形式添加配置。这里我的配置是 prod 环境部署的副本为2，同时给到的资源也更多，详情可以在 [Github](https://github.com/sunny0826/flask-python) 上查看。

### 与 kubectl 的集成

在 jenkinsfile 中可以看到

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash&lt;/p&gt;

&lt;h1 id=&#34;kustomize-build-deploy-overlays-dev-kubectl-apply-f&#34;&gt;kustomize build deploy/overlays/dev | kubectl apply -f -&lt;/h1&gt;

&lt;p&gt;kubectl apply -k deploy/overlays/dev
```&lt;/p&gt;

&lt;p&gt;这两条命令的执行效果是一样的，在 &lt;code&gt;kubectl v1.14.0&lt;/code&gt; 以上的版本中，已经集成了 kustomize ，可以直接使用 &lt;code&gt;kubectl&lt;/code&gt; 进行部署。&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;这里只是对 kustomize 在 CI/CD 中简单应用的展示，只是一种比较简单和基础的使用，真正的 CI 流程要比这个复杂的多，这里只是为了演示 kustomize 的使用而临时搭建的。而 kustomize 还有很多黑科技的用法，将会在后续的文章中介绍。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>4月29日 云栖社区分享PPT -- 阿里云容器服务的优势与调优</title>
      <link>https://guoxudong.io/en/post/aliyun-share/</link>
      <pubDate>Tue, 30 Apr 2019 18:46:24 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/aliyun-share/</guid>
      <description>&lt;p&gt;该PPT 为 2019年4月26日 在云栖社区分享使用，这里留作展示和记录，下载地址可以参考下方链接。&lt;/p&gt;

&lt;iframe src=&#34;https://guoxudong.io/aliyun-share/index.html&#34; style=&#34;width: 100%;height:600px;&#34; frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;由于图片资源位于 GitHub 上，国内访问可能会有些慢，建议下载观看。&lt;/p&gt;

&lt;p&gt;PPT 下载地址：&lt;a href=&#34;https://yq.aliyun.com/articles/700084&#34; target=&#34;_blank&#34;&gt;https://yq.aliyun.com/articles/700084&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>8分钟入门 K8S</title>
      <link>https://guoxudong.io/en/post/an-8-minute-introduction-to-k8s/</link>
      <pubDate>Tue, 30 Apr 2019 13:38:12 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/an-8-minute-introduction-to-k8s/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;读完 &lt;a href=&#34;https://www.amazon.com/Kubernetes-Running-Dive-Future-Infrastructure/dp/1491935677&#34; target=&#34;_blank&#34;&gt;Kubernetes: Up and Running&lt;/a&gt; 后，我写下了这篇文章。旨在为那些认为文章 &lt;a href=&#34;https://blog.maoxianplay.com/posts/cant/&#34; target=&#34;_blank&#34;&gt;TL;DR&lt;/a&gt; 的人进行一些总结，这同时也是一种强迫自己检查所阅读内容的好方法。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;基于 Google &lt;a href=&#34;https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/&#34; target=&#34;_blank&#34;&gt;Borg&lt;/a&gt; 的开源系统 K8S( Kubernetes ) 是一个非常强大的容器编排调度系统。 其整个生态系统，包括：工具，模块，附加组件等，都是使用 Golang 语言编写的，这使得 K8S 及其周边生态系统基本上是面向 API 对象、运行速度非常快的二进制文件的集合，并且这些二进制文件都有很好的文档记录，易于编写和构建应用程序。&lt;/p&gt;

&lt;p&gt;在深入了解之前，我想先介绍一下 K8S 的竞争对手：ECS 、 Nomad 和 Mesos 。ECS 是 AWS 自己的业务编排解决方案，而最近 AWS 上也引入了一个托管的 K8S 系统 &amp;ndash; EKS 。两者都提供 &lt;a href=&#34;https://aws.amazon.com/fargate/&#34; target=&#34;_blank&#34;&gt;FARGATE&lt;/a&gt; ，允许用户运行其应用并忽略其运行物理资源。&lt;/p&gt;

&lt;p&gt;K8S 作为一个开源系统，在采用量上毫无疑问是最大赢家，同时它也可以以托管形式在三个主要云提供商上提供服务。然而，它比其他系统更加复杂。K8S 可以处理几乎任何类型的容器化工作负载，但这并不意味着每个人都需要它。用户也可以选择其他解决方案，例如，单独部署在 AWS 上的互联网产品可以在生产环境很好的使用 ECS 而非 K8S。&lt;/p&gt;

&lt;p&gt;话虽如此，k8s也有其神奇之处 &amp;ndash; 它可以在任何地方部署，同时拥有一个活跃的社区和数百个核心开发人员，以及其广泛生态系统中的数千个其他开源贡献者。它快速、新颖、模块化和面向 API ，使其成为对于构建插件和服务非常友好的系统。&lt;/p&gt;

&lt;h2 id=&#34;话不多说-这里把-k8s-分为的11个部分介绍&#34;&gt;话不多说，这里把 K8S 分为的11个部分介绍&lt;/h2&gt;

&lt;h3 id=&#34;1-pods&#34;&gt;1. Pods&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Pods&lt;/strong&gt; 是 K8S 中创建或部署的最小基本单位。一个 pod 可以由多个容器组成，这些容器将形成一个部署在单个节点上的单元。一个 pod 包含一个容器之间共享的 IP。在微服务中， pod 将是执行某些后端工作或提供传入请求的微服务的单个实例。&lt;/p&gt;

&lt;h3 id=&#34;2-nodes&#34;&gt;2. Nodes&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Node&lt;/strong&gt; 就是服务器。它们是 K8S 部署其 pod 的“裸机”（也可以是虚拟机）。Nodes 为 K8S 提供可用的群集资源，以保持数据，运行作业，维护工作负载和创建网络路由。&lt;/p&gt;

&lt;h3 id=&#34;3-labels-annotations&#34;&gt;3. Labels &amp;amp; Annotations&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Labels&lt;/strong&gt; 是 K8S 及其终端用户过滤和筛选系统中类似资源的方式，也是一个资源需要“访问”或与另一个资源关联的粘合剂。例如：一个 Service 想要开放 Deployment 的端口。无论是监控，记录，日志，测试，任何k8s资源都应添加 Labels 以供进一步检查。例如： &lt;code&gt;app=worker&lt;/code&gt; ，一个给系统中所有工作 pod 的标签，稍后可以使用 &lt;code&gt;kubectl&lt;/code&gt; 工具或 k8s api 使用 &lt;code&gt;--selector&lt;/code&gt; 字段进行选择。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Annotations&lt;/strong&gt; 与 Labels 非常类似，但是它通常以字符串的形式用于存储元数据，但他不能用于标识和选择对象，通常也不会被 Kubernetes 直接使用，其主要目的是方便工具或用户的阅读和查找等。&lt;/p&gt;

&lt;h3 id=&#34;4-服务发现&#34;&gt;4. 服务发现&lt;/h3&gt;

&lt;p&gt;作为编排调度器，控制不同工作负载的资源，K8S 管理 pods 、jobs 和其他任何需要网络通信的物理资源。 K8S 使用 &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/components/#etcd&#34; target=&#34;_blank&#34;&gt;etcd&lt;/a&gt; 管理这些。 etcd 是 K8S 的“内部”数据库， Master 节点使用它来知道一切资源都在哪里。K8S 还为您的服务提供了实时的 “服务发现” - 所有 Pod 都使用的自定义 DNS 服务（CoreDNS），您可以通过解析其他服务的名称来获取其 IP 地址和端口。它不需要任何设置，在 K8S 集群中“开箱即用”。&lt;/p&gt;

&lt;h3 id=&#34;5-replicasets&#34;&gt;5. ReplicaSets&lt;/h3&gt;

&lt;p&gt;虽然 pod 运行任务，但通常单个实例是不够的。出于对冗余和负载处理的考虑，需要进行复制容器，即“弹性缩放”。K8S 使用 &lt;strong&gt;ReplicaSet&lt;/strong&gt; 来实现伸缩扩展。根据副本的数量来表示系统的期望状态，并且在任何给定时刻保持系统的当前状态。&lt;/p&gt;

&lt;p&gt;这也是配置自动扩展的地方，在系统负载高时创建新的副本，以及在不再需要这些资源来支持运行工作负载时减少扩展。简单的讲就是：少则增加，多增删除。&lt;/p&gt;

&lt;h3 id=&#34;6-daemonsets&#34;&gt;6. DaemonSets&lt;/h3&gt;

&lt;p&gt;有时，某些应用程序在每个节点上只需要一个实例。最好的例子就是像 &lt;a href=&#34;https://www.elastic.co/products/beats/filebeat&#34; target=&#34;_blank&#34;&gt;FileBeat&lt;/a&gt; 这样的日志采集组件。为了让 agent 从节点上收集日志，它需要运行在所有节点上，但只需要一个实例即可。为了创建满足上面需求的的工作负载，K8S 使用 &lt;strong&gt;DaemonSets&lt;/strong&gt; 来完成这个工作。&lt;/p&gt;

&lt;h3 id=&#34;7-statefulsets&#34;&gt;7. StatefulSets&lt;/h3&gt;

&lt;p&gt;虽然大多数微服务都是无状态的应用程序，但是还是有一部分并不是。有状态的工作负载需要由某种可靠的磁盘卷来支持。虽然应用程序容器本身可以是不变的，并且可以用更新的版本或更健康的实例来替换它们，但是即使使用其他副本也是需要持久化的数据。为此，&lt;strong&gt;StatefulSets&lt;/strong&gt; 允许部署整个生命周期内需要运行在同一节点的应用程序。它还保留了它的 “名称” ; 容器内的 &lt;code&gt;hostname&lt;/code&gt; 和整个集群中服务发现的名称。一个包含3个 ZooKeeper 的 StatefulSet 可以命名为 &lt;code&gt;zk-1&lt;/code&gt; ，&lt;code&gt;zk-2&lt;/code&gt; 和 &lt;code&gt;zk-3&lt;/code&gt; 还可以扩展为包含其他成员，如 &lt;code&gt;zk-4&lt;/code&gt; ， &lt;code&gt;zk-5&lt;/code&gt; 等&amp;hellip; StatefulSets 还需要管理 PVC 。&lt;/p&gt;

&lt;h3 id=&#34;8-jobs&#34;&gt;8. Jobs&lt;/h3&gt;

&lt;p&gt;K8S 核心团队考察了绝大多数需要使用编排系统的应用程序。虽然大多数应用程序需要持续的正常运行时间来处理服务请求，例如 Web 服务，但有时也需要运行批量任务并在任务完成后进行清理。如果您愿意，可以使用小型无服务器环境。而在 K8S 中实现这一功能，可以使用 &lt;strong&gt;Job&lt;/strong&gt; 资源。Jobs 正是听起来的那样，一个工作负载容器来完成特定的工作，并在成功后被销毁。一个很好的例子是设置一组 worker ，从要处理和存储的队列中读取任务。一旦队列为空，直到下一批准备好进行处理，都不再需要启动 worker。&lt;/p&gt;

&lt;h3 id=&#34;9-configmaps-secrets&#34;&gt;9. ConfigMaps &amp;amp; Secrets&lt;/h3&gt;

&lt;p&gt;如果您还不熟悉 &lt;a href=&#34;https://12factor.net/&#34; target=&#34;_blank&#34;&gt;Twelve-Factor App manifest&lt;/a&gt; 《&lt;a href=&#34;../12-factor&#34;&gt;十二要素应用&lt;/a&gt;》 ，可以点击链接了解一下。现代应用程序的一个关键概念是无环境，可通过注入的环境变量进行配置。应用程序应完全与其所在位置无关。&lt;strong&gt;ConfigMaps&lt;/strong&gt; 在 K8S 中实现这一重要概念。其本质上是环境变量的 key-value 列表，这些变量被传递给正在运行的工作负载以确定不同的 runtime 行为。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Secrets&lt;/strong&gt; 与 &lt;strong&gt;ConfigMaps&lt;/strong&gt; 类似，通过加密的方式防止密钥、密码、证书等敏感信息泄漏。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;我个人认为在任何系统上使用密码的最佳选择是 Hashicorp 的 Vault 。请务必阅读我去年写的关于它的&lt;a href=&#34;https://medium.com/prodopsio/security-for-dummies-protecting-application-secrets-made-easy-5ef3f8b748f7&#34; target=&#34;_blank&#34;&gt;文章&lt;/a&gt;，关于 Vault 可以为你的产品提供的功能，以及我的一位同事写的另一篇更具技术性的[文章](&lt;a href=&#34;https://medium.com/prodopsio/taking-your-hashicorp-vault-to-the-next-level-8549e7988b24）。&#34; target=&#34;_blank&#34;&gt;https://medium.com/prodopsio/taking-your-hashicorp-vault-to-the-next-level-8549e7988b24）。&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;10-deployments&#34;&gt;10. Deployments&lt;/h3&gt;

&lt;p&gt;为了使新版本快速替换原有的应用程序，我们希望将构建、测试和发布在一块来实现 &lt;a href=&#34;https://www.ibm.com/developerworks/community/blogs/beingagile/entry/short_feedback_loops_everywhere?lang=en&#34; target=&#34;_blank&#34;&gt;short feedback loops&lt;/a&gt; 。K8S 使用 Deployments 来不断部署新软件，Deployments 是一组用来描述特定运行工作负载的元数据。例如：发布新版本，bug 修复，甚至是回滚（这是k8s的另一个内部选项）。&lt;/p&gt;

&lt;p&gt;在 K8S 中部署软件有两个主要&lt;strong&gt;策略&lt;/strong&gt;：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Replacement&lt;/strong&gt;：将使用新副本替换您的整个工作负载，整个过程需要强制停机。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;RollingUpdate&lt;/strong&gt;：k8s通过两种特定配置来实现使用新的 Pods 实例滚动更新：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;MaxAvailable&lt;/code&gt; ： 该设置表示在部署新版本时可用的工作负载的百分比（或确切数量），100％表示“我有2个容器，保持2个存活并在整个部署期间正常提供服务”。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MaxSurge&lt;/code&gt; ： 该设置表示升级期间总 Pod 数最多可以超出期望的百分比（或数量），100％表示“我有 X 个容器，再部署 X 个容器，然后开始推出旧容器”。&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;11-storage&#34;&gt;11. Storage&lt;/h3&gt;

&lt;p&gt;K8S 在存储上添加了一层抽象，工作负载可以为不同的任务请求特定的存储，甚至可以管理持续的时间可以超过某个pod的生命周期。为了简短起见，我想向您介绍我最近发布的关于k8s存储的&lt;a href=&#34;https://medium.com/prodopsio/k8s-will-not-solve-your-storage-problems-5bda2e6180b5&#34; target=&#34;_blank&#34;&gt;文章&lt;/a&gt;，特别是为什么它不能完全解决数据库部署等数据持久性要求。&lt;/p&gt;

&lt;h2 id=&#34;概念性理解&#34;&gt;概念性理解&lt;/h2&gt;

&lt;p&gt;K8S 是根据一些指导方向设计和开发的，考虑到社区的性质，每个特征、概念和想法都被内置于系统中。此外，终端用户会以某种方式使用该系统，作为一个开源和免费的系统，不属于任何人，你可以用它做任何你想要做的事。&lt;/p&gt;

&lt;p&gt;面向 API ：系统中的每个部分都以一种可通过记录良好且可操作的 API 进行交互的方式进行构建。核心开发人员确保作为终端用户的您可以进行更改，查询和更新，用来提供更好的用户体验。&lt;/p&gt;

&lt;p&gt;工具友好 ： 作为上面一点的衍生，K8S 是热衷于在其 API 周围创建工具的。它将自身做为一个原始平台，以可定制的方式构建，以供其他人使用，并进一步开发用于不同的工具。有些已经变得非常有名并被广泛使用，如 Spinnaker ，Istio 和许多其他功工具。&lt;/p&gt;

&lt;p&gt;声明性状态 ： 鼓励用户使用具有声明性描述的系统而不是命令式描述。这意味着系统的状态和组件最好被描述为在某种版本控制（如 git ）中管理的代码，而不会因为某一处手动更改也对整体有影响。这样，k8s更容易&lt;a href=&#34;https://en.wikipedia.org/wiki/Disaster_recovery&#34; target=&#34;_blank&#34;&gt;灾难恢复&lt;/a&gt; ，易于在团队之间分享和传递。&lt;/p&gt;

&lt;h2 id=&#34;最后&#34;&gt;最后&lt;/h2&gt;

&lt;p&gt;本文试图将重点放在 K8S 的介绍和主要概念上，当然，K8S 还有其他非常重要的领域，比如物理系统构建模块，如 &lt;code&gt;kubelet&lt;/code&gt;， &lt;code&gt;kube-proxy&lt;/code&gt; ， &lt;code&gt;api-server&lt;/code&gt; 和终端操作工具：&lt;code&gt;kubectl&lt;/code&gt;。我将在下一篇文章中讨论以及介绍这些很酷的功能。&lt;/p&gt;

&lt;p&gt;原文地址： &lt;a href=&#34;https://medium.com/prodopsio/an-8-minute-introduction-to-k8s-94fda1fa5184&#34; target=&#34;_blank&#34;&gt;https://medium.com/prodopsio/an-8-minute-introduction-to-k8s-94fda1fa5184&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>阿里云容器服务新建集群优化方案(更新版)</title>
      <link>https://guoxudong.io/en/post/aliyun-k8s-perfect/</link>
      <pubDate>Thu, 25 Apr 2019 22:26:06 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/aliyun-k8s-perfect/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;选择阿里云的&lt;code&gt;容器服务&lt;/code&gt;，主要原因是公司主要业务基本都在运行在阿里云上。相较自建 kubernetes 集群，容器服务的优势在于部署相对简单，与阿里云 VPC 完美兼容，网络的配置相对简单，而如果使用 &lt;code&gt;kubeadm&lt;/code&gt; 安装部署 kubernetes 集群，除了众所周知的科学上网的问题，还有一系列的问题，包括 &lt;code&gt;etcd&lt;/code&gt; 、 &lt;code&gt;Scheduler&lt;/code&gt; 和 &lt;code&gt;Controller-Manager&lt;/code&gt; 的高可用问题等。并且如果使用托管版的阿里云 kubernetes 容器服务，还会省掉3台 master 节点的钱，并且可能将 master 节点的运维问题丢给阿里云解决，并且其提供的 master 节点性能肯定会比自购的配置好，这点是阿里云容器服务的研发小哥在来我司交流时专门强调的。&lt;/p&gt;

&lt;h2 id=&#34;问题&#34;&gt;问题&lt;/h2&gt;

&lt;p&gt;前面吹了阿里云容器服务的优势，那这里就说说在实践中遇到的容器服务的问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在新建集群的时候需要选择相应的 VPC 并选择 &lt;code&gt;Pod&lt;/code&gt; 和 &lt;code&gt;Service&lt;/code&gt; 所在的网段，这两个网段不能和 Node 节点存在于同一网段，但是如果您在阿里云中存在不止一个 VPC （VPC的网段可以是 10.0.0.0/8，172.16-31.0.0/12-16，192.168.0.0/16 ），如果网段设置不对的话，就可能会使原本存在该网段的 ECS 失联，需要删除集群重新创建。如果删除失败的话，还需要手动删除路由表中的记录（&lt;strong&gt;别问我是怎么知道的&lt;/strong&gt;）。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在使用容器服务创建集群后，会创建2个 SLB （之前是3个），一个是 SLB 是在 VPC 上并且绑定一个弹性IP（需要在创建的时候手动勾选创建弹性IP）用于 API Server，一个是经典网络的 SLB 使用提供给 Ingress 使用。但是这两个外网IP创建后的规格都是默认最大带宽、按流量收费，这个并不符合我们的要求，需要手动修改，&lt;del&gt;然而这个修改都会在第二天才能生效&lt;/del&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;容器服务创建集群后，Node 节点的名称会使&lt;code&gt;{region-id}.{ECS-id}&lt;/code&gt;的形式，这个命名方式在集群监控，使用 &lt;code&gt;kubectl&lt;/code&gt; 操作集群方面就显得比较反人类了，每次都要去查 &lt;code&gt;ECS id&lt;/code&gt; 才能确定是哪个节点，而这个 Node 节点名称是不能修改的！&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;网段问题解决&#34;&gt;网段问题解决&lt;/h2&gt;

&lt;p&gt;这个比较好解决，甚至可以说不用解决，只要把网段规划好，不要出现网段冲突就好&lt;/p&gt;

&lt;h2 id=&#34;node-节点名称无法修改问题解决&#34;&gt;Node 节点名称无法修改问题解决&lt;/h2&gt;

&lt;p&gt;这个功能之前已有人在阿里聆听平台提出这个问题了，咨询了容器服务的研发小哥，得到的反馈是该功能已经在灰度测试了，相信很快就可以上线了。&lt;/p&gt;

&lt;h2 id=&#34;创建-slb-规格问题解决&#34;&gt;创建 SLB 规格问题解决&lt;/h2&gt;

&lt;p&gt;相较之前自动创建3个 SLB 的方式，目前的版本只会自动创建2个并且有一个是 VPC 内网+弹性IP的方式，已经进行了优化，但是 ingress 绑定的 SLB 还是经典网络类型，无法接入云防火墙并且规格也是不合适的。这里给出解决方案：&lt;/p&gt;

&lt;h3 id=&#34;方法一-使用-kubectl-配置&#34;&gt;方法一：使用 &lt;code&gt;kubectl&lt;/code&gt; 配置&lt;/h3&gt;

&lt;h4 id=&#34;1-创建新的-slb&#34;&gt;1. 创建新的 SLB&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;这里需要创建一个新的 SLB 用来代替自动创建的不符合要求的 SLB。这里可以先私网 SLB 先不绑定弹性IP。&lt;strong&gt;&lt;em&gt;这里要注意的事，新建的 SLB 需要与 k8s集群处于同一 VPC 内，否则在后续会绑定失败&lt;/em&gt;&lt;/strong&gt;。
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1ma5lxgvdj21ws0s6qa5.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;li&gt;查看新购买 SLB 的 ID
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1ma8zuq1gj20sa0hoq4b.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;2-在创建集群后重新绑定-ingress-controller-的-service&#34;&gt;2. 在创建集群后重新绑定 &lt;code&gt;ingress-controller&lt;/code&gt; 的 &lt;code&gt;Service&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;首先需要使用 &lt;code&gt;kubectl&lt;/code&gt; 或者直接在阿里云控制台操作，创建新的 &lt;code&gt;nginx-ingress-svc&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# nginx ingress service
apiVersion: v1
kind: Service
metadata:
name: nginx-ingress-lb-{new-name}
namespace: kube-system
labels:
    app: nginx-ingress-lb-{new-name}
annotations:
    # set loadbalancer to the specified slb id
    service.beta.kubernetes.io/alicloud-loadbalancer-id: {SLB-ID}
    # set loadbalancer address type to intranet if using private slb instance
    #service.beta.kubernetes.io/alicloud-loadbalancer-address-type: intranet
    service.beta.kubernetes.io/alicloud-loadbalancer-force-override-listeners: &#39;true&#39;
    #service.beta.kubernetes.io/alicloud-loadbalancer-backend-label: node-role.kubernetes.io/ingress=true
spec:
type: LoadBalancer
# do not route traffic to other nodes
# and reserve client ip for upstream
externalTrafficPolicy: &amp;quot;Local&amp;quot;
ports:
- port: 80
    name: http
    targetPort: 80
- port: 443
    name: https
    targetPort: 443
selector:
    # select app=ingress-nginx pods
    app: ingress-nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建成功后，可以进到 SLB 页面查看，可以看到 &lt;code&gt;80&lt;/code&gt; 和 &lt;code&gt;443&lt;/code&gt; 端口的监听已经被添加了
    &lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1maej57c1j21ru0rwq8b.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-绑定符合要求的弹性ip&#34;&gt;3. 绑定符合要求的弹性IP&lt;/h4&gt;

&lt;p&gt;确定 SLB 创建成功并且已经成功监听后，这里就可以为 SLB 绑定符合您需求的弹性IP了，这里我们绑定一个按宽带计费2M的弹性IP&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1mak2r0p3j207k07mq33.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;4-验证连通性&#34;&gt;4. 验证连通性&lt;/h4&gt;

&lt;p&gt;到上面这步，我们的 ingress 入口 SLB 已经创建完成，这里我们验证一下是否联通。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在k8s集群中部署一个 &lt;code&gt;nginx&lt;/code&gt; ，直接在阿里云容器服务控制台操作即可
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1mant7ec6j21s40qegpr.jpg&#34; alt=&#34;image&#34; /&gt;
这里创建 ingress 路由，&lt;strong&gt;注意：这里的域名需要解析到刚才创建的 SLB 绑定的弹性IP&lt;/strong&gt;
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1maqf7gdjj21ns0kymz8.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;访问该域名，显示 &lt;code&gt;nginx&lt;/code&gt; 欢迎页，则证明修改成功
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1mat8srhnj21ak0hmact.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;方法二-使用阿里云容器服务控制台配置&#34;&gt;方法二： 使用阿里云容器服务控制台配置&lt;/h3&gt;

&lt;h4 id=&#34;1-阿里云容器控制台创建新-service&#34;&gt;1. 阿里云容器控制台创建新 &lt;code&gt;service&lt;/code&gt;&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;在阿里云容器服务控制台：&lt;code&gt;路由与负载均衡&lt;/code&gt; &amp;ndash;&amp;gt; &lt;code&gt;服务&lt;/code&gt; 点击&lt;code&gt;创建&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;选择 &lt;code&gt;kube-system&lt;/code&gt; 命名空间&lt;/li&gt;
&lt;li&gt;类型选中&lt;code&gt;负载均衡&lt;/code&gt; - &lt;code&gt;内网访问&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;关联 &lt;code&gt;nginx-ingress-controller&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;并添加端口映射&lt;/li&gt;
&lt;li&gt;点击创建&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g2g4fwfgevj20i50hsgmp.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;2-进入负载均衡查看-slb-是否创建&#34;&gt;2. 进入负载均衡查看 SLB 是否创建&lt;/h4&gt;

&lt;p&gt;可见 SLB 已经成功创建&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g2g4pb1d45j215303c74r.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-绑定符合要求的弹性ip-1&#34;&gt;3. 绑定符合要求的弹性IP&lt;/h4&gt;

&lt;p&gt;同方法一&lt;/p&gt;

&lt;h4 id=&#34;4-验证连通性-1&#34;&gt;4.验证连通性&lt;/h4&gt;

&lt;p&gt;同方法一&lt;/p&gt;

&lt;h3 id=&#34;后续操作&#34;&gt;后续操作&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;在确定新的 SLB 创建成功后，就可以将容器服务自动创建的 SLB 释放了&lt;/li&gt;
&lt;li&gt;删除 &lt;code&gt;kube-system&lt;/code&gt; 中原本绑定的 &lt;code&gt;Service&lt;/code&gt; &lt;strong&gt;（目前版本已经可以关联删除绑定的 SLB 了，不用分开操作）&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;这里别忘了，自动创建给API Server 的SLB还是按流量付费的，记得降配&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;上面的这些问题和解决方案都属于临时方案，已在阿里的聆听平台提出了上面的问题，相信很快就会有所改进。总的来说，阿里云容器服务在提供优质的 kubernetes 功能，并且只收 ECS 的钱，对于想学习 kubernetes 又没有太多资金的同学也比较友好，直接买按量付费实例，测试完释放即可，不用购买 master 节点，十分良心！&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>困难的 Kubernetes</title>
      <link>https://guoxudong.io/en/post/kubernetes-is-har/</link>
      <pubDate>Wed, 24 Apr 2019 10:18:46 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kubernetes-is-har/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;虽然 Kubernetes 赢得了容器之站，但是其仍然很难使用并且时长引起事故。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我想我应该给这篇文章做一点序言。 &lt;a href=&#34;https://kubernetes.io/&#34; target=&#34;_blank&#34;&gt;kubernetes&lt;/a&gt; 为许多应用程序提供新的 runtime ，如果使用得当，它可以成为一个强大的工具，并且可以将您冲复杂的开发生命周期中解放出来。然而在过去的几年里，我看到很多人和公司都会搭建他们的 Kubernetes ，但常常只是处于测试阶段，从未进入到生产。&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-是如何运作的&#34;&gt;Kubernetes 是如何运作的？&lt;/h2&gt;

&lt;p&gt;粗略的讲， Kubernetes 或者 K8S 看起来十分简单。您运行的 Kubernetes 节点至少被分为两类：Master 和 Workers。Master 节点通常不运行任何真实的工作负载，那是 Workers 节点的工作。&lt;/p&gt;

&lt;p&gt;Kubernetes 的 Master 节点包含一个名叫 API server 的组件，其提供的 API 可以通过 &lt;code&gt;kubectl&lt;/code&gt; 调用。此外还包括一个 scheduler ，负责调度容器，决定容器运行在哪个节点。最后一个组件是 controller-manager ，它实际上是一组多个控制器，负责处理节点中断、复制、加入 services 和 pods ，并且处理授权相关内容。所有的数据都存储在 etcd 中，这是一个可信赖的分布式键值存储服务（包含一些非常酷的功能）。总而言之，Master 节点负责管理集群，这里没什么特别大的惊喜。&lt;/p&gt;

&lt;p&gt;另一方面， 真实的工作负载运行在 Worker 节点上。为此，它还包括许多组件。首先，Worker 节点上会运行 &lt;strong&gt;&lt;em&gt;kubelet&lt;/em&gt;&lt;/strong&gt; ，它是与该节点上的容器一起运行的 API ，负责与管控组件沟通，并按照管控组件指示管理 Worker 节点。另一个组件就是 &lt;strong&gt;&lt;em&gt;kube-proxy&lt;/em&gt;&lt;/strong&gt; ，其负责转发网络连接，根据您的配置运行容器。可能还有其他东西，如 &lt;strong&gt;&lt;em&gt;kube-dns&lt;/em&gt;&lt;/strong&gt; 或 &lt;strong&gt;&lt;em&gt;gVisor&lt;/em&gt;&lt;/strong&gt;。您还需要集成某种 &lt;strong&gt;&lt;em&gt;overlay network&lt;/em&gt;&lt;/strong&gt; 或底层网络设置，以便 Kubernetes 可以管理您的 pod 之间的网络。&lt;/p&gt;

&lt;p&gt;如果您想要一个更完整的概述，建议去看 Kelsey Hightowers 的 &lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way&#34; target=&#34;_blank&#34;&gt;Kubernetes  -  The Hard Way&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;生产就绪的-kubernetes&#34;&gt;生产就绪的 Kubernetes&lt;/h2&gt;

&lt;p&gt;到目前为止，这听起来并不太糟糕。只是安装几个程序、配置、证书等。不要误会我的意思，这仍然是一个学习曲线，但这也不是系统管理员不能处理的问题。&lt;/p&gt;

&lt;p&gt;然而，简单地手动安装 Kubernetes 并不代表其已经完全准备就绪，所以让我们谈谈让这个东西运行起来所需的步骤。&lt;/p&gt;

&lt;p&gt;首先，&lt;strong&gt;安装&lt;/strong&gt;。如果您想要某种自动安装，无论是使用 Ansible ， Terraform 还是其他工具。&lt;a href=&#34;https://github.com/kubernetes/kops&#34; target=&#34;_blank&#34;&gt;kops&lt;/a&gt; 可以帮助您解决这个问题，但是使用 kops 意味着您将不知道它是如何设置的，并且当您以后想要调试某些东西时可能会引起一些其他问题。应对此自动化进行测试，并定期进行检查。&lt;/p&gt;

&lt;p&gt;其次，您需要&lt;strong&gt;监控&lt;/strong&gt;您的 Kubernetes 安装。所以您需要 &lt;a href=&#34;https://prometheus.io/&#34; target=&#34;_blank&#34;&gt;Prometheus&lt;/a&gt; 、 &lt;a href=&#34;https://grafana.com/&#34; target=&#34;_blank&#34;&gt;Grafana&lt;/a&gt; 等工具。您是在 Kubernetes 里面运行它吗？ 如果您的 Kubernetes 有问题，那么您的监控是否会也会挂掉？ 或者您单独运行它？ 如果是，那么您在哪里运行它？&lt;/p&gt;

&lt;p&gt;另外值得注意的是&lt;strong&gt;备份&lt;/strong&gt;。如果您的 Master 崩溃，数据无法恢复并且您需要重新配置系统上的所有 pod ，您会怎么做？您是否测试了再次运行 CI 系统中所有作业所需的时间？您有灾难恢复计划吗？&lt;/p&gt;

&lt;p&gt;现在，既然我们在谈论 CI 系统，那么您需要为您的镜像运行 Docker 镜像仓库。当然，您可以再次在 Kubernetes 上做，但如果 Kubernetes 崩溃&amp;hellip;&amp;hellip;您知道这个后果。当然，CI 系统与运行版本控制系统都有这个问题。理想情况下，这些系统是与生产环境隔离的，以便在系统出现问题时，至少可以访问 git ，来进行重新部署等操作。&lt;/p&gt;

&lt;h2 id=&#34;数据存储&#34;&gt;数据存储&lt;/h2&gt;

&lt;p&gt;最后，我们来谈谈最重要的部分：存储。Kubernetes 本身并不提供存储解决方案。当然，您可以将存储挂载到主机安装目录，但这既不推荐也不简单。&lt;/p&gt;

&lt;p&gt;基本上需要在 Kubernetes 下使用某种存储。例如，&lt;a href=&#34;https://rook.io/&#34; target=&#34;_blank&#34;&gt;rook&lt;/a&gt; 使得运行 &lt;a href=&#34;https://ceph.com/&#34; target=&#34;_blank&#34;&gt;Ceph&lt;/a&gt; 作为底层块存储需求的变得相对简单，但我对 Ceph 的体验是它还有有很多地方需要调整，所以您绝不是只需点击下一步就能走出困境。&lt;/p&gt;

&lt;h2 id=&#34;调试&#34;&gt;调试&lt;/h2&gt;

&lt;p&gt;在与开发人员谈论 Kubernetes 时，一种常见的回答经常出现：在使用 Kubernetes 时，人们常常在调试应用程序时遇到问题。即使是一个例如容器未能启动的简单问题，也会引起混乱。&lt;/p&gt;

&lt;p&gt;当然，这是一个教育问题。在过去的几十年中，开发人员已经学会了调试的“经典”步骤：在 &lt;code&gt;/vat/log/&lt;/code&gt; 中查看日志等。但是对于容器，我们甚至不知道容器运行在哪个服务器上，因此它呈现出了一种范式转换。&lt;/p&gt;

&lt;h2 id=&#34;问题-复杂&#34;&gt;问题：复杂&lt;/h2&gt;

&lt;p&gt;您可能已经注意到我正在跳过共有云提供商给您的东西，即使它不是一个完整的托管 Kubernetes。当然，如果您使用托管的 Kubernetes 解决方案，这很好，除了调试之外，您不需要处理上面这些问题。&lt;/p&gt;

&lt;p&gt;Kubernetes 拥有许多可以移动组件，但 Kubernetes 本身也并不能提供完整的解决方案。例如，&lt;a href=&#34;https://www.openshift.com/&#34; target=&#34;_blank&#34;&gt;RedHat OpenShift&lt;/a&gt; 可以，但它需要花钱，并且仍然需要添加自己的东西。&lt;/p&gt;

&lt;p&gt;现在Kubernetes正处于 &lt;a href=&#34;https://www.gartner.com/en/research/methodologies/gartner-hype-cycle&#34; target=&#34;_blank&#34;&gt;Gartner hype cycle&lt;/a&gt; 的顶峰，每个人都想要它，但很少有人真正理解它。在接下来的几年里，不少公司必须意识到 Kubernetes 并不是银弹，而如何正确有效地使用它才是关键。&lt;/p&gt;

&lt;p&gt;我认为，如果您有能力将 Ops 团队专门用于为开发人员来维护底层平台，那么运行自己的 Kubernetes 是值得的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;本文作者：&lt;a href=&#34;https://pasztor.at/&#34; target=&#34;_blank&#34;&gt;Janos Pasztor&lt;/a&gt;  2018-12-04&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;原文地址：&lt;a href=&#34;https://pasztor.at/blog/kubernetes-is-hard&#34; target=&#34;_blank&#34;&gt;https://pasztor.at/blog/kubernetes-is-hard&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Kustomize 帮你管理 kubernetes 应用（二）： Kustomize 的使用方法</title>
      <link>https://guoxudong.io/en/post/kustomize-2/</link>
      <pubDate>Fri, 19 Apr 2019 16:05:02 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kustomize-2/</guid>
      <description>

&lt;p&gt;本文介绍使用和维护 Kustomize 的方法及步骤。&lt;/p&gt;

&lt;h2 id=&#34;定制配置&#34;&gt;定制配置&lt;/h2&gt;

&lt;p&gt;在这个工作流方式中，所有的配置文件（ YAML 资源）都为用户所有，存在于私有 repo 中。其他人是无法使用的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g2813d1ia7j20qo0f0dgk.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;创建一个目录用于版本控制&lt;/p&gt;

&lt;p&gt;我们希望将一个名为 &lt;strong&gt;&lt;em&gt;ldap&lt;/em&gt;&lt;/strong&gt; 的 Kubernetes 集群应用的配置保存在自己的 repo 中。
这里使用 &lt;code&gt;git&lt;/code&gt; 进行版本控制。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git init ~/ldap
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建一个 &lt;code&gt;base&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p ~/ldap/base
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个目录中创建并提交 &lt;code&gt;kustomization.yaml&lt;/code&gt; 文件和一组资源，例如 &lt;code&gt;deployment.yaml&lt;/code&gt; &lt;code&gt;service.yaml&lt;/code&gt; 等。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建 &lt;code&gt;overlays&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p ~/ldap/overlays/staging
mkdir -p ~/ldap/overlays/production
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个目录都需要一个 &lt;code&gt;kustomization.yaml&lt;/code&gt; 文件以及一个或多个 &lt;code&gt;patch&lt;/code&gt; ，例如 &lt;code&gt;healthcheck_patch.yaml&lt;/code&gt; &lt;code&gt;memorylimit_patch.yaml&lt;/code&gt; 等。。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-staging```&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;production&lt;code&gt;目录则可能会在&lt;/code&gt;deployment``` 中增加在副本数。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;生成 &lt;code&gt;variants&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;运行 &lt;code&gt;kustomize&lt;/code&gt; ，将生成的配置用于 kubernetes 应用部署&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kustomize build ~/ldap/overlays/staging | kubectl apply -f -
kustomize build ~/ldap/overlays/production | kubectl apply -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 kubernetes 1.14 版本， &lt;code&gt;kustomize&lt;/code&gt; 已经集成到 &lt;code&gt;kubectl&lt;/code&gt; 命令中，成为了其一个子命令，可使用 &lt;code&gt;kubectl&lt;/code&gt; 来进行部署&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -k ~/ldap/overlays/staging
kubectl apply -k ~/ldap/overlays/production
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;使用现成的配置&#34;&gt;使用现成的配置&lt;/h2&gt;

&lt;p&gt;在这个工作流方式中，可从别人的 repo 中 fork kustomize 配置，并根据自己的需求来配置。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g281xyfebej20qo0f0dgr.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;通过 fork/modify/rebase 等方式获得配置&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;将其克隆为你自己的 &lt;code&gt;base&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;在这个 &lt;code&gt;bash&lt;/code&gt; 目录维护在一个 repo 中，在这个例子使用 &lt;code&gt;ladp&lt;/code&gt; 的 repo&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir ~/ldap
git clone https://github.com/$USER/ldap ~/ldap/base
cd ~/ldap/base
git remote add upstream git@github.com:$USER/ldap
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建 &lt;code&gt;overlays&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;如上面的案例一样，创建并完善 &lt;code&gt;overlays&lt;/code&gt; 目录中的内容&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p ~/ldap/overlays/staging
mkdir -p ~/ldap/overlays/production
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用户可以将 &lt;code&gt;overlays&lt;/code&gt; 维护在不同的 repo 中&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;生成 &lt;code&gt;variants&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kustomize build ~/ldap/overlays/staging | kubectl apply -f -
kustomize build ~/ldap/overlays/production | kubectl apply -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 kubernetes 1.14 版本， &lt;code&gt;kustomize&lt;/code&gt; 已经集成到 &lt;code&gt;kubectl&lt;/code&gt; 命令中，成为了其一个子命令，可使用 &lt;code&gt;kubectl&lt;/code&gt; 来进行部署&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -k ~/ldap/overlays/staging
kubectl apply -k ~/ldap/overlays/production
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;（可选）更新 &lt;code&gt;base&lt;/code&gt;
用户可以定期从上游 repo 中 &lt;code&gt;rebase&lt;/code&gt; 他们的 &lt;code&gt;base&lt;/code&gt; 以保证及时更新&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ~/ldap/base
git fetch upstream
git rebase upstream/master
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/blob/master/docs/workflows.md&#34; target=&#34;_blank&#34;&gt;kustomize workflows - github.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Rancher 2.2.1 解决工作负载监控为空问题</title>
      <link>https://guoxudong.io/en/post/rancher-prometheus-fix-question/</link>
      <pubDate>Thu, 18 Apr 2019 17:46:08 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/rancher-prometheus-fix-question/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;Rancher 2.2.X 版本于3月底正式GA，新版本处理其他部分的优化以外，最大亮点莫过于本身集成了 Prometheus ，可以通过 Rancher 自带 UI 或者 Grafana 查看集群的实时监控，对所有监控进行了一次聚合，不用再和之前一样，每个集群都要安装一个 Prometheus 用于监控，而告警部分也可使用 Rancher 自带的通知组件进行告警。通知方式目前支持 Slack 、 邮件、 PagerDuty 、 Webhook 、 企业微信，由于我司办公使用钉钉，所以我们使用了 Webhook 的方式，告警触发后通知我们的消息服务，然后消息服务将其发送到钉钉进行告警。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx2.sinaimg.cn/large/ad5fbf65gy1g26xsh6omvj20rk0ilta6.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;问题&#34;&gt;问题&lt;/h2&gt;

&lt;p&gt;Rancher 集成 Prometheus 后，监控方面变的十分强大，不用再徘徊于多个集群的 Grafana ，直接在 Rancher 上即可查看，非常方便&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx2.sinaimg.cn/large/ad5fbf65gy1g26xuv2frnj212b0onn1h.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;但是在使用的时候，我发现了一个问题：就是在查看 工作负载和 Pod 的时候会显示 &lt;strong&gt;&lt;em&gt;没有足够的数据绘制图表&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g26xzvi2cpj20po057q31.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;进入 Grafana 查看会发现，其实监控参数是存在的，但是没有采集到值，所以并没有展示出来。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g26y4j4s3yj21f50m9wqj.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;解决&#34;&gt;解决&lt;/h2&gt;

&lt;p&gt;在检查了配置后并没有找到原因，只好去 GitHub 上提一个 issue 来询问一下开发者或者其他用户有无遇到这个问题。&lt;/p&gt;

&lt;p&gt;Rancher 官方的开发者还是十分负责的， GitHub 上用户名为 &lt;a href=&#34;https://github.com/loganhz&#34; target=&#34;_blank&#34;&gt;Logan&lt;/a&gt; 的官方小哥来我指导解决这个问题。&lt;/p&gt;

&lt;p&gt;小哥发现我是导入的集群，要我进入 Prometheus 查看，发现 &lt;code&gt;cattle-prometheus/exporter-kube-state-cluster-monitoring&lt;/code&gt; 果然没有起来&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g26yb1p4eoj21db0am782.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;解决这个问题，需要在集群监控配置中添加一个高级选项，插入值为：&lt;code&gt;exporter-kubelets.https=false&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g26ycq6amfj221q0uggp8.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;点击保存，问题就解决了！&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g26yheqwp7j213e0g3di5.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;使用 Rancher 有半年，从2.0版本一直用到2.2版本，而18年分别在云栖大会和 KubeCon 上听了 Rancher 创始人梁胜博士的演讲。而从这一个小问题上就可以看到 Rancher 官方对每一个用户都是十分重视的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kustomize: 无需模板定制你的 kubernetes 配置</title>
      <link>https://guoxudong.io/en/post/introducing-kustomize-template-free-configuration-customization-for-kubernetes/</link>
      <pubDate>Mon, 15 Apr 2019 17:23:21 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/introducing-kustomize-template-free-configuration-customization-for-kubernetes/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;作者：Jeff Regan (Google), Phil Wittrock (Google) 2018-05-29&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果你在运行 kubernetes 集群，你可能会拷贝一些包含 kubernetes API 对象的 YAML 文件，并且根据你的需求来修改这些文件，通过这些 YAML 文件来定义你的 kubernetes 配置。&lt;/p&gt;

&lt;p&gt;但是这种方法存在很难找到配置的源头并对其进行改进。今天 Google 宣布推出 &lt;strong&gt;Kustomize&lt;/strong&gt; ，一个作为 &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-cli&#34; target=&#34;_blank&#34;&gt;SIG-CLI&lt;/a&gt; 子项目的命令行工具。这个工具提供了一个全新的、纯粹的声明式的方法来定制 kubernetes 配置，遵循并利用我们熟悉且精心设计的 Kubernetes API。&lt;/p&gt;

&lt;p&gt;有这样一个常见的场景，在互联网上可以看到别人的 CMS（content management system，内容管理系统）的 kubernetes 配置，这个配置是一组包括 Kubernetes API 对象的 YAML 描述文件。然后，在您自己公司的某个角落，您找到一个你非常了解的数据库，希望用它来该 CMS 的数据。&lt;/p&gt;

&lt;p&gt;你希望同时使用它们，此外，你希望自定义配置文件以便你的资源实例在集群中显示，并通过添加一个标签来区分在同一集群中做同样事情的其他资源。同时也希望为其配置适当的 CPU 、内存和副本数。&lt;/p&gt;

&lt;p&gt;此外，你还想要配置整个配置的多种变化：一个专门用于测试和实验的小服务实例（就计算资源而言），或更大的用于对外提供服务的生产级别的服务实例。同时，其他的团队也希望拥有他们自己的服务实例。&lt;/p&gt;

&lt;h2 id=&#34;定制就是复用&#34;&gt;定制就是复用&lt;/h2&gt;

&lt;p&gt;kubernetes 的配置并不是代码（是使用 YAML 描述的 API 对象，严格来说应该是数据），但是配置的生命周期与代码的生命周期有许多相似之处。&lt;/p&gt;

&lt;p&gt;你需要在版本控制中保留配置。所有者的配置不必与使用者的配置相同。配置可以作为整体的一部分。而用户希望为在不同的情况下复用这些配置。&lt;/p&gt;

&lt;p&gt;与代码复用相同，一种复用配置的方法是简单的全部拷贝并进行自定义。像代码一样，切断与源代码的联系使得从改进变的十分困难。许多团队和环境都使用这种方法，每个团队和环境都拥有自己的配置，这使得简单的升级变得十分棘手。&lt;/p&gt;

&lt;p&gt;另一种复用方法是将源代码抽象为参数化模板。使用一个通过执行脚本来替换所需参数的模板处理工具生成配置，通过为同一模板设置不同的值来达到复用的目的。而这种方式面临的问题是模板和参数文件并不在 kubernetes API 资源的规范中，这种方式必定是一种包装了 kubernetes API 的新东西、新语言。虽然这种方式很强大，但是也带来了学习成本和安装工具的成本。不同的团队需要不同的更改，因此几乎所有可以包含在 YAML 文件中的规范都会需要抽象成参数。&lt;/p&gt;

&lt;h2 id=&#34;自定义配置的新选择&#34;&gt;自定义配置的新选择&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;kustomize&lt;/strong&gt; 中工具的声明与规范是由名为 &lt;code&gt;kustomization.yaml&lt;/code&gt; 的文件定义。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;kustomize&lt;/strong&gt; 将会读取声明文件和 Kubernetes API 资源文件，将其组合然后将完整的资源进行标准化的输出。输出的文本可以被其他工具进一步处理，或者直接通过 &lt;strong&gt;kubectl&lt;/strong&gt; 应用于集群。&lt;/p&gt;

&lt;p&gt;例如，如果 &lt;code&gt;kustomization.yaml&lt;/code&gt; 文件包括：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;commonLabels:
  app: hello
resources:
- deployment.yaml
- configMap.yaml
- service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;确保这三个文件与 &lt;code&gt;kustomization.yaml&lt;/code&gt; 位于同一目录下，然后运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kustomize build
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将创建包含三个资源的 YAML 流，其中 &lt;code&gt;app: hello&lt;/code&gt; 为每个资源共同的标签。&lt;/p&gt;

&lt;p&gt;同样的，你可以使用 &lt;strong&gt;&lt;em&gt;commonAnnotations&lt;/em&gt;&lt;/strong&gt; 字段给所有资源添加注释， &lt;strong&gt;&lt;em&gt;namePrefix&lt;/em&gt;&lt;/strong&gt; 字段为所有的资源添加共同的前缀名。这些琐碎而有常见的定制只是一个开始。&lt;/p&gt;

&lt;p&gt;一个更常见的例子是，你需要为一组相同资源设置不同的参数。例如：开发、演示和生产的参数。&lt;/p&gt;

&lt;p&gt;为此，&lt;strong&gt;Kustomize&lt;/strong&gt; 允许用户以一个应用描述文件 （YAML 文件）为基础（Base YAML），然后通过 Overlay 的方式生成最终部署应用所需的描述文件。两者都是由 kustomization 文件表示。基础（Base）声明了共享的内容（资源和常见的资源配置），Overlay 则声明了差异。&lt;/p&gt;

&lt;p&gt;这里是一个目录树，用于管理集群应用程序的 &lt;strong&gt;&lt;em&gt;演示&lt;/em&gt;&lt;/strong&gt; 和 &lt;strong&gt;&lt;em&gt;生产&lt;/em&gt;&lt;/strong&gt; 配置参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;someapp/
├── base/
│   ├── kustomization.yaml
│   ├── deployment.yaml
│   ├── configMap.yaml
│   └── service.yaml
└── overlays/
    ├── production/
    │   └── kustomization.yaml
    │   ├── replica_count.yaml
    └── staging/
        ├── kustomization.yaml
        └── cpu_count.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;someapp/base/kustomization.yaml&lt;/code&gt; 文件指定了公共资源和常见自定义配置（例如，它们一些相同的标签，名称前缀和注释）。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;someapp/overlays/production/kustomization.yaml&lt;/code&gt; 文件的内容可能是：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;commonLabels:
  env: production
bases:
- ../../base
patches:
- replica_count.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 kustomization 指定了一个 &lt;strong&gt;&lt;em&gt;patch&lt;/em&gt;&lt;/strong&gt; 文件 &lt;code&gt;replica_count.yaml&lt;/code&gt; ，其内容可能是：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: the-deployment
spec:
  replicas: 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;patch&lt;/em&gt;&lt;/strong&gt; 是部分的资源声明，在这个例子中是 Deployment 的补丁 &lt;code&gt;someapp/base/deployment.yaml&lt;/code&gt; ，仅修改了副本数用以处理生产流量。&lt;/p&gt;

&lt;p&gt;该补丁不仅仅是一个无上下文 {parameter name，value} 元组。其作为部分 deployment spec，可以通过验证，即使与其余配置隔离读取，也具有明确的上下文和用途。&lt;/p&gt;

&lt;p&gt;要为生产环境创建资源，请运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kustomize build someapp/overlays/production
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行结果将作为一组完整资源打印到标准输出，并准备应用于集群。可以用类似的命令定义演示环境的配置。&lt;/p&gt;

&lt;h2 id=&#34;综上所述&#34;&gt;综上所述&lt;/h2&gt;

&lt;p&gt;使用 &lt;strong&gt;kustomize&lt;/strong&gt; ，您可以仅使用 Kubernetes API 资源文件就可以管理任意数量的 Kubernetes 定制配置。kustomize 的每个产物都是纯 YAML 的，每个都可以进行验证和运行的。&lt;strong&gt;kustomize&lt;/strong&gt; 鼓励通过 fork/modify/rebase 这样的&lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/blob/master/docs/workflows.md&#34; target=&#34;_blank&#34;&gt;工作流&lt;/a&gt;来管理海量的应用描述文件。&lt;/p&gt;

&lt;p&gt;尝试&lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/tree/master/examples/helloWorld&#34; target=&#34;_blank&#34;&gt;hello world&lt;/a&gt;示例，开始使用 &lt;strong&gt;kustomize&lt;/strong&gt; 吧！有关的反馈与讨论，可以通过加入&lt;a href=&#34;https://groups.google.com/forum/#!forum/kustomize&#34; target=&#34;_blank&#34;&gt;邮件列表&lt;/a&gt;或提 &lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/issues/new&#34; target=&#34;_blank&#34;&gt;issue&lt;/a&gt;，欢迎提交PR。&lt;/p&gt;

&lt;h2 id=&#34;译者按&#34;&gt;译者按&lt;/h2&gt;

&lt;p&gt;随着 kubernetes 1.14 的发布，kustomize 被集成到 &lt;code&gt;kubectl&lt;/code&gt; 中，用户可以利用 &lt;code&gt;kubectl apply -k dir/&lt;/code&gt; 将指定目录的 &lt;code&gt;kustomization.yaml&lt;/code&gt; 提交到集群中。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;原文链接&lt;/strong&gt; &lt;a href=&#34;https://kubernetes.io/blog/2018/05/29/introducing-kustomize-template-free-configuration-customization-for-kubernetes/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.io/blog/2018/05/29/introducing-kustomize-template-free-configuration-customization-for-kubernetes/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Kustomize 帮你管理 kubernetes 应用（一）：什么是 Kustomize ？</title>
      <link>https://guoxudong.io/en/post/kustomize-1/</link>
      <pubDate>Mon, 15 Apr 2019 13:32:59 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kustomize-1/</guid>
      <description>

&lt;h2 id=&#34;初识-kustomize&#34;&gt;初识 Kustomize&lt;/h2&gt;

&lt;p&gt;第一次听说 Kustomize 其实是在 kubernetes 1.14 发布时候，它被集成到 &lt;code&gt;kubectl&lt;/code&gt; 中，成为了一个子命令，但也只是扫了一眼，并没有深究。真正让我注意到它，并主动开始了解其功能和使用方法的，是张磊大神在云栖社区发表的一篇文章&lt;a href=&#34;https://yq.aliyun.com/articles/697883&#34; target=&#34;_blank&#34;&gt;《从Kubernetes 1.14 发布，看技术社区演进方向》&lt;/a&gt;，他在文中是这么说的：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Kustomize 允许用户以一个应用描述文件 （YAML 文件）为基础（Base YAML），然后通过 Overlay 的方式生成最终部署应用所需的描述文件，而不是像 Helm 那样只提供应用描述文件模板，然后通过字符替换（Templating）的方式来进行定制化。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这不正我在苦苦寻找的东西嘛！自从公司确定了应用容器化的方案，至今已有半年多了，这期间我们的服务一个接一个的实现了容器化，部署到了 kubernetes 集群中。kubernetes 集群也有原先了1个测试集群，几个节点，发展到了如今的多个集群，几十个节点。而在推进容器化的过程中，每个服务都对对应多个应用描述文件（ YAML 文件），而根据环境的不同，又配置了多套的应用描述文件。随着服务越部越多，应用描述文件更是呈爆炸式的增长。&lt;/p&gt;

&lt;p&gt;感谢 devops 文化，它是我不需要为每个应用去写 YAML 文件，各个应用的开发组承担了这一工作，我只需要为他们提供基础模板即可。但应用上线后出现的 OOM 、服务无法拉起等 YAML 文件配置有误导致的问题接踵而至，使得我必须要深入各个服务，为他们配置符合他们配置。虽然也使用了 &lt;code&gt;helm&lt;/code&gt; ，但是其只提供应用描述文件模板，在不同环境拉起一整套服务会节省很多时间，而像我们这种在指定环境快速迭代的服务，并不会减少很多时间。针对这种情况，我已经计划要自己开发一套更符合我们工作这种场景的应用管理服务，集成在我们自己的 devops 平台中。&lt;/p&gt;

&lt;p&gt;这时 Kustomize 出现了，我明锐的感觉到 Kustomize 可能就是解决我现阶段问题的一剂良药。&lt;/p&gt;

&lt;h2 id=&#34;什么是-kustomize&#34;&gt;什么是 Kustomize ？&lt;/h2&gt;

&lt;blockquote&gt;
&lt;h4 id=&#34;kubernetes-native-configuration-management&#34;&gt;Kubernetes native configuration management&lt;/h4&gt;

&lt;p&gt;Kustomize introduces a template-free way to customize application configuration that simplifies the use of off-the-shelf applications. Now, built into &lt;code&gt;kubectl&lt;/code&gt; as &lt;code&gt;apply -k&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kustomize&lt;/code&gt;  允许用户以一个应用描述文件 （YAML 文件）为基础（Base YAML），然后通过 Overlay 的方式生成最终部署应用所需的描述文件。而其他用户可以完全不受影响的使用任何一个 Base YAML 或者任何一层生成出来的 YAML 。这使得每一个用户都可以通过类似fork/modify/rebase 这样 Git 风格的流程来管理海量的应用描述文件。这种 PATCH 的思想跟 Docker 镜像是非常相似的，它可以规避“字符替换”对应用描述文件的入侵，也不需要用户学习额外的 DSL 语法（比如 Lua）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;而其成为 &lt;code&gt;kubectl&lt;/code&gt; 子命令则代表这 &lt;code&gt;kubectl&lt;/code&gt; 本身的插件机制的成熟，未来可能有更多的工具命令集成到 &lt;code&gt;kubectl&lt;/code&gt; 中。拿张磊大神的这张图不难看出，在 kubernetes 原生应用管理系统中，应用描述文件在整个应用管理体系中占据核心位置，通过应用描述文件可以组合和编排多种 kubernetes API 资源，kubernetes 通过控制器来保证集群中的资源与应用状态与描述文件完全一致。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g23cqlrodkj21bq0r8znk.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kustomize 不像 Helm 那样需要一整套独立的体系来完成管理应用，而是完全采用 kubernetes 的设计理念来完成管理应用的目的。同时使用起来也更加的得心应手。&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize - kustomize.io&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://yq.aliyun.com/articles/697883&#34; target=&#34;_blank&#34;&gt;从Kubernetes 1.14 发布，看技术社区演进方向 - yq.aliyun.com&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>单节点版rancher升级指南</title>
      <link>https://guoxudong.io/en/post/rancher-update-2.2.1/</link>
      <pubDate>Sun, 31 Mar 2019 11:15:35 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/rancher-update-2.2.1/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;Rancher 不仅可以在任何云提供商的任何地方部署 Kubernetes 集群，而且还将它们集中在集中式的身份验证和访问控制之下。由于它与资源的运行位置无关，因此您可以轻松地在不同的环境部署你的 kubernetes 集群并操作他们。 Rancher 不是将部署几个独立的 Kubernetes 集群，而是将它们统一为一个单独的托管Kubernetes Cloud。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;目前我们使用的是 rancher 2.1.1版本，在去年 rancher 发布 &lt;code&gt;v2.1.*&lt;/code&gt; 版本的时候做过一次升级，当时遇到了很多问题，虽然都一一解决，但是并没有有效的记录下来，这里在升级 &lt;code&gt;v2.2.*&lt;/code&gt; 版本的时候做一个记录以便在今后升级的时候的提供参考作用。&lt;/p&gt;

&lt;h2 id=&#34;升级前的准备&#34;&gt;升级前的准备&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;首先查看当前 rancher 版本，记下这个版本号后面需要使用。查看方式就是登陆 rancher 在左下角就可以看到当前版本号，我们这里使用的&lt;code&gt;v2.1.1&lt;/code&gt;版本。&lt;/li&gt;
&lt;li&gt;打开官方文档，这里推荐对照官方文档进行升级，一般官方文档都会及时更新并提供最佳升级方法，而一般的博客会因为其写作时间、使用版本、部署环境的不同有所偏差。官方文档： &lt;a href=&#34;https://www.cnrancher.com/docs/rancher/v2.x/cn/upgrades/single-node-upgrade/&#34; target=&#34;_blank&#34;&gt;https://www.cnrancher.com/docs/rancher/v2.x/cn/upgrades/single-node-upgrade/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;升级&#34;&gt;升级&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;首先获取正在运行的 rancher 容器 ID,由以下命令可知 &lt;code&gt;RANCHER_CONTAINER_ID&lt;/code&gt; 为 &lt;code&gt;83167cb60134&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker ps
CONTAINER ID        IMAGE                    COMMAND             CREATED             STATUS              
PORTS                                       NAMES
83167cb60134        rancher/rancher:latest   &amp;quot;entrypoint.sh&amp;quot;     4 months ago        Up 4 months         0.0.0.0:80-&amp;gt;80/tcp, 0.0.0.0:443-&amp;gt;443/tcp   priceless_newton
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;停止该容器&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker stop {RANCHER_CONTAINER_ID}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建正在运行的 Rancher Server 容器的数据卷容器，将在升级中使用，这里命名为 &lt;code&gt;rancher-data&lt;/code&gt; 容器。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;替换{RANCHER_CONTAINER_ID}为上一步中的容器ID。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;替换{RANCHER_CONTAINER_TAG}为你当前正在运行的Rancher版本，如上面的先决条件中所述。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker create --volumes-from {RANCHER_CONTAINER_ID} --name rancher-data rancher/rancher:{RANCHER_CONTAINER_TAG}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;备份 &lt;code&gt;rancher-data&lt;/code&gt; 数据卷容器&lt;/p&gt;

&lt;p&gt;如果升级失败，可以通过此备份还原Rancher Server，容器命名:rancher-data-snapshot-&lt;CURRENT_VERSION&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;替换{RANCHER_CONTAINER_ID}为上一步中的容器ID。&lt;/li&gt;
&lt;li&gt;替换{CURRENT_VERSION}为当前安装的Rancher版本的标记。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;替换{RANCHER_CONTAINER_TAG}为当前正在运行的Rancher版本，如先决条件中所述 。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker create --volumes-from {RANCHER_CONTAINER_ID} --name rancher-data-snapshot-{CURRENT_VERSION} rancher/rancher:{RANCHER_CONTAINER_TAG}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;拉取Rancher的最新镜像,这里确保有外网，可能拉取到新的镜像，如果没有外网，这里就需要将镜像上传到私有镜像仓库，将拉取地址设置为私有镜像仓库即可&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker pull rancher/rancher:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;通过 &lt;code&gt;rancher-data&lt;/code&gt; 数据卷容器启动新的 Rancher Server 容器。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;这里要注意到，我们这是使用的是独立容器+外部七层负载均衡，是通过阿里云SLB进行SSL证书认证，需要在启动的时候增加&lt;code&gt;--no-cacerts&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d --volumes-from rancher-data --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher:latest --no-cacerts
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;升级过程会需要一定时间，不要在升级过程中终止升级，强制终止可能会导致数据库迁移错误。&lt;/p&gt;

&lt;p&gt;升级 Rancher Server后， server 容器中的数据会保存到 &lt;code&gt;rancher-data&lt;/code&gt; 容器中，以便将来升级。&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;删除旧版本 Rancher Server 容器&lt;/p&gt;

&lt;p&gt;如果你只是停止以前的Rancher Server容器(并且不删除它),则旧版本容器可能随着主机重启后自动运行，导致容器端口冲突。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;升级成功&lt;/p&gt;

&lt;p&gt;访问 rancher 可以看到右下角版本已经完成更新。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g1lzcmucn6j20ck03qt8p.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>自动合并Kubeconfig，实现多k8s集群切换</title>
      <link>https://guoxudong.io/en/post/merge-kubeconfig/</link>
      <pubDate>Sun, 17 Mar 2019 10:45:02 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/merge-kubeconfig/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;随着微服务和容器化的深入人心，以及kubernetes已经成为容器编排领域的事实标准，越来越多的公司将自己的服务迁移到kubernetes集群中。而随着kubernetes集群的增加，集群管理的问题就凸显出来，不同的环境存在不同的集群，不同的业务线不同的集群，甚至有些开发人员都有自己的集群。诚然，如果集群是使用公有云如阿里云或华为云的容器服务，可以登录其控制台进行集群管理；或者使用rancher这用的多集群管理工具进行统一的管理。但是在想操作&lt;code&gt;istio&lt;/code&gt;特有的容器资源，或者想使用&lt;code&gt;istioctl&lt;/code&gt;的时候，或者像我一样就是想使用&lt;code&gt;kubectl&lt;/code&gt;命令的同学，这个时候多集群的切换就显的十分重要了。&lt;/p&gt;

&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-kubectl```命令行工具通过```kubeconfig```文件的配置来选择集群以及集群的API&#34;&gt;
## 原理
使用```kubeconfig```文件，您可以组织您的群集，用户和名称空间。 还可以定义上下文以快速轻松地在群集和名称空间之间切换。

### 上下文(Context) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kubeconfig```文件中的上下文元素用于以方便的名称对访问参数进行分组。 每个上下文有三个参数：集群，命名空间和用户。 默认情况下，kubectl命令行工具使用当前上下文中的参数与集群进行通信。可以使用下面的命令设置上下文：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;配置内容&#34;&gt;配置内容&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;kubectl config view
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;如果设置了&lt;code&gt;--kubeconfig&lt;/code&gt;标志，则只使用指定的文件。该标志只允许有一个实例。&lt;/li&gt;
&lt;li&gt;如果环境变量&lt;code&gt;KUBECONFIG&lt;/code&gt;存在，那么就使用该环境变量&lt;code&gt;KUBECONFIG&lt;/code&gt;里面的值，如果不存在该环境变量&lt;code&gt;KUBECONFIG&lt;/code&gt;，那么默认就是使用&lt;code&gt;$HOME/.kube/config&lt;/code&gt;文件。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;kubeconfig-内容&#34;&gt;&lt;code&gt;kubeconfig&lt;/code&gt;内容&lt;/h3&gt;

&lt;p&gt;从下面kubeconfig文件的配置来看集群、用户、上下文、当前上下文的关系就比较明显了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Config
preferences: {}

clusters:
- cluster:
name: {cluster-name}

users:
- name: {user-name}

contexts:
- context:
    cluster: {cluster-name}
    user: {user-name}
name: {context-name}

current-context: {context-name}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;为何要自动合并&#34;&gt;为何要自动合并&lt;/h2&gt;

&lt;p&gt;在日常的工作中，如果我们需要操作多个集群，会得到多个kubeconfig配置文件。一般的kubeconfig文件都是yaml格式的，但是也有少部分的集群kubeconfig时已json文件的形式给出的（比如华为云的=。=），比如我们公司再阿里云、华为云和自建环境上均存在kubernetes集群，平时操作要在多集群之间切换，这也就催生了我写这个工具（其实就是一个脚本）的动机。&lt;/p&gt;

&lt;h2 id=&#34;自动合并生成kubeconfig&#34;&gt;自动合并生成kubeconfig&lt;/h2&gt;

&lt;p&gt;众所周知，yaml是一种直观的能够被电脑识别的数据序列化格式，是一个可读性高并且容易被人类阅读的语言和json相比（没有格式化之前）可读性更强。而我这个工具并不是很关心kubeconfig的格式，只要将想要合并的kubeconfig放入指定文件即可。&lt;/p&gt;

&lt;p&gt;GitHub：&lt;a href=&#34;https://github.com/sunny0826/mergeKubeConfig&#34; target=&#34;_blank&#34;&gt;https://github.com/sunny0826/mergeKubeConfig&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;适用环境&#34;&gt;适用环境&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;需要在终端使用命令行管理多集群&lt;/li&gt;
&lt;li&gt;kubernetes集群中安装了istio，需要使用&lt;code&gt;istioctl&lt;/code&gt;命令，但是集群节点并没有安装&lt;code&gt;istioctl&lt;/code&gt;，需要在本地终端操作&lt;/li&gt;
&lt;li&gt;不愿频繁编辑.kube目录中的config文件的同学&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;准备工作&#34;&gt;准备工作&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Python环境：2.7或者3均可&lt;/li&gt;
&lt;li&gt;需要依赖包：&lt;code&gt;PyYAML&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;开始使用&#34;&gt;开始使用&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;安装依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install PyYAML
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;运行脚本&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;默认运行方式，kubeconfig文件放入&lt;code&gt;configfile&lt;/code&gt;文件,注意删掉作为示例的两个文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python merge.py
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;自定义kubeconfig文件目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python merge.py -d {custom-dir}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;运行后操作&#34;&gt;运行后操作&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;将生成的config文件放入.kube目录中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp config ~/.kube
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看所有的可使用的kubernetes集群角色&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config get-contexts
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;更多关于kubernetes配置文件操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config --help
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;切换kubernetes配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context {your-contexts}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;在使用kubernetes初期，在多集群之间我一直是频繁的切换&lt;code&gt;.kube/config&lt;/code&gt;文件来达到切换操作集群的目的。这也导致了我的&lt;code&gt;.kube&lt;/code&gt;目录中存在这多个类似于&lt;code&gt;al_test_config.bak&lt;/code&gt;、&lt;code&gt;al_prod_config.bak&lt;/code&gt;、&lt;code&gt;hw_test_config.bak&lt;/code&gt;的文件，本地环境已经自建环境，在集群切换的时候十分头疼。而后来使用&lt;code&gt;--kubeconfig&lt;/code&gt;来进行切换集群，虽然比之前的方法要方便很多，但是并不十分优雅。这个简单的小工具一举解决了我的文件，对于我这个&lt;code&gt;kubectl&lt;/code&gt;重度依赖者来说十分重要。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>解决kubernetes中ingress-nginx配置问题</title>
      <link>https://guoxudong.io/en/post/k8s-ingress-config/</link>
      <pubDate>Wed, 06 Mar 2019 14:42:05 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/k8s-ingress-config/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;随着公司容器化的深入，越来越多的服务陆续迁移到kubernetes集群中，有些问题在测试环境并未凸显，但是在生产环境中这些问题就显得格外的扎眼。这里就对实践中kubernetes集群中的7层负载均衡器ingress遇到的问题进行总结。&lt;/p&gt;

&lt;h2 id=&#34;http-s-负载均衡器-ingress&#34;&gt;HTTP(S)负载均衡器-ingress&lt;/h2&gt;

&lt;p&gt;Ingress是kubernetes API的标准资源类型之一，其本质就是一组基于DNS名称(host)或URL路径把请求转发至指定的Service资源的规则，&lt;strong&gt;用于将集群外的请求流量转发至集群内部完成服务发布&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;Ingress控制器(Ingress Controller)可以由任何具有反向代理(HTTP/HTTPS)功能的服务程序实现，如Nginx、Envoy、HAProxy、Vulcand和Traefik等。Ingress控制器本身也作为Pod对象与被代理的运行为Pod资源的应用运行于同一网络中。我们在这里选择了NGINX Ingress Controller，由于对NGINX的配置较为熟悉，同时我们使用的kubernetes是阿里云的容器服务，构建集群的时候，容器服务会自带NGINX Ingress Controller。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx2.sinaimg.cn/large/ad5fbf65ly1g0t3yj7wecj20w50doab9.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;根据实际情况ingress调优&#34;&gt;根据实际情况Ingress调优&lt;/h2&gt;

&lt;h3 id=&#34;1-解决400-request-header-or-cookie-too-large问题&#34;&gt;1. 解决400 Request Header Or Cookie Too Large问题&lt;/h3&gt;

&lt;p&gt;image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;h4 id=&#34;现象&#34;&gt;现象&lt;/h4&gt;

&lt;p&gt;微信小程序需要调用后端接口，需要在header中传一段很长的token参数，直接使用浏览器访问该端口可以访问通，但是在加上token访问之后，会报“400 Request Header Or Cookie Too Large”&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;html&amp;gt;
    &amp;lt;head&amp;gt;
        &amp;lt;title&amp;gt;400 Request Header Or Cookie Too Large&amp;lt;/title&amp;gt;
    &amp;lt;/head&amp;gt;
    &amp;lt;body&amp;gt;
        &amp;lt;center&amp;gt;
            &amp;lt;h1&amp;gt;400 Bad Request&amp;lt;/h1&amp;gt;
        &amp;lt;/center&amp;gt;
        &amp;lt;center&amp;gt;Request Header Or Cookie Too Large&amp;lt;/center&amp;gt;
        &amp;lt;hr&amp;gt;
        &amp;lt;center&amp;gt;nginx/1.15.6&amp;lt;/center&amp;gt;
    &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;问题定位&#34;&gt;问题定位&lt;/h4&gt;

&lt;p&gt;直接修改Service使用nodeport的形式访问，则没有报错，初步定位需要在ingress中nginx配置客户端的请求头，进入Ingress Controller的Pod查询配置，果然是请求头空间不足。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat nginx.conf | grep client_header_buffer_size
    client_header_buffer_size       1k;
$ cat nginx.conf | grep large_client_header_buffers
    large_client_header_buffers     4 8k;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;解决方法&#34;&gt;解决方法&lt;/h4&gt;

&lt;p&gt;在ingress中添加注释&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;nginx.ingress.kubernetes.io/server-snippet: client_header_buffer_size 2046k;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false-1&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Server snippet&lt;/strong&gt;&lt;br&gt;Using the annotation &lt;code&gt;nginx.ingress.kubernetes.io/server-snippet&lt;/code&gt; it is possible to add custom configuration in the server configuration block.
&lt;br&gt;该注释是将自定义配置加入nginx的server配置中&lt;/p&gt;

&lt;p&gt;image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false-2&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;h3 id=&#34;2-解决请求超时问题&#34;&gt;2. 解决请求超时问题&lt;/h3&gt;

&lt;h4 id=&#34;现象-1&#34;&gt;现象&lt;/h4&gt;

&lt;p&gt;有一个数据导出功能，需要将大量数据进行处理，然后以Excel格式返回，在导出一个大约3W条数据的时候，出现访问超时情况。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws2.sinaimg.cn/mw690/ad5fbf65ly1g0ubdwwzo5j21b30bjaat.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;解决方法-1&#34;&gt;解决方法&lt;/h4&gt;

&lt;p&gt;调整proxy_read_timeout，连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理
在ingress中添加注释&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;nginx.ingress.kubernetes.io/proxy-read-timeout: 600
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;这里需要注意的事该注释的value需要时number类型，不能加s，否则将不生效&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;3-增加白名单&#34;&gt;3. 增加白名单&lt;/h3&gt;

&lt;h4 id=&#34;现象-2&#34;&gt;现象&lt;/h4&gt;

&lt;p&gt;在实际的使用中，会有一部分应用需要设置只可以在办公场地的网络使用，之前使用阿里云 SLB 的时候可以针对端口进行访问控制，但是现在走 ingress ，都是从80 or 443端口进，所以需要在 ingress 设置&lt;/p&gt;

&lt;h4 id=&#34;解决方法-2&#34;&gt;解决方法&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Whitelist source range&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You can specify allowed client IP source ranges through the nginx.ingress.kubernetes.io/whitelist-source-range annotation. The value is a comma separated list of CIDRs, e.g. 10.0.0.0/24,172.10.0.1.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在 ingress 里配置 &lt;code&gt;nginx.ingress.kubernetes.io/whitelist-source-range&lt;/code&gt; ，如有多个ip段，用逗号分隔即可&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;nginx.ingress.kubernetes.io/whitelist-source-range: 10.0.0.0/24
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想全局适用，可以在阿里云 SLB 里操作，也可以将该配置加入到 &lt;code&gt;NGINX ConfigMap&lt;/code&gt; 中。&lt;/p&gt;

&lt;p&gt;image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false-3&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;p&gt;根据工作中遇到的实际问题，持续更新中&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;使用NGINX ingress controller的好处就是对于nginx配置相对比较熟悉，性能也不差。相关nginx配置的对应的ingress可以在 &lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/&lt;/a&gt; 上查到。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pod质量服务类别(QoS)</title>
      <link>https://guoxudong.io/en/post/k8s-qos/</link>
      <pubDate>Mon, 04 Mar 2019 19:18:13 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/k8s-qos/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;根据Pod对象的requests和limits属性，kubernetes将Pod对象归类到BestEffort、Burstable和Guaranteed三个服务质量（Quality of Service，QoS）类别。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Guaranteed

&lt;ul&gt;
&lt;li&gt;cpu:requests=limits&lt;/li&gt;
&lt;li&gt;memory:requests=limits&lt;/li&gt;
&lt;li&gt;这类Pod具有最高优先级&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Burstable

&lt;ul&gt;
&lt;li&gt;至少一个容器设置了cpu或内存资源的requests&lt;/li&gt;
&lt;li&gt;这类Pod具有中等优先级&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;BestEffort

&lt;ul&gt;
&lt;li&gt;未有任何一个容器设置requests或limits属性&lt;/li&gt;
&lt;li&gt;这类Pod具有最低优先级&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/ad5fbf65ly1g0rv2ipzqkj20hx0edmx8.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;同级别优先级的Pod资源在OOM时，与自身的requests属性相比，其内存占用比例最大的Pod对象将被首先杀死。如上图同属Burstable类别的Pod A将先于Pod B被杀死，虽然其内存用量小，但与自身的requests值相比，它的占用比例95%要大于Pod B的80%。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>阿里云日志服务采集k8s日志并实现livetail功能</title>
      <link>https://guoxudong.io/en/post/dashboard-k8s/</link>
      <pubDate>Thu, 14 Feb 2019 14:07:06 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/dashboard-k8s/</guid>
      <description>

&lt;h1 id=&#34;前言&#34;&gt;前言&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;目前的项目日志都是通过Logtail直接采集，投递到OSS持久化，同时可以通过阿里云日志服务、devops自建平台进行查看（虽然大部分人是直接登录ECS查看=。=），
在开始进行容器化之后，同样遇到日志的问题，目前的解决方案是阿里云日志服务持久化和展现格式化后的日志、使用rancher查看实时日志，
但是之前由于rancher平台出现一些问题，导致不能及时查看日志的情况，在这个背景下对阿里云日志服务采集k8s日志和livetail进行搭建并调研此方案是否可行。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;简介-转自阿里云官方文档&#34;&gt;简介（转自阿里云官方文档）&lt;/h1&gt;

&lt;p&gt;日志服务（Log Service，简称 LOG）是针对日志类数据的一站式服务，在阿里巴巴集团经历大量大数据场景锤炼而成。您无需开发就能快捷完成日志数据采集、消费、投递以及查询分析等功能，提升运维、运营效率，建立 DT 时代海量日志处理能力。&lt;/p&gt;

&lt;h1 id=&#34;kubernetes日志采集组件安装&#34;&gt;kubernetes日志采集组件安装&lt;/h1&gt;

&lt;h2 id=&#34;安装logtail&#34;&gt;安装Logtail&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;进入阿里云容器服务找到集群id
&lt;img src=&#34;https://guoxudong.io/images/source/log_ser.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;通过ssh登录master节点，或者任意安装了kubectl并配置了该集群kubeconfig的服务器&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;执行命令，将${your_k8s_cluster_id}替换为集群id&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget http://logtail-release-cn-hangzhou.oss-cn-hangzhou.aliyuncs.com/kubernetes/alicloud-log-k8s-install.sh -O alicloud-log-k8s-install.sh; chmod 744 ./alicloud-log-k8s-install.sh; sh ./alicloud-log-k8s-install.sh ${your_k8s_cluster_id}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Project k8s-log-${your_k8s_cluster_id}下会自动创建名为config-operation-log的Logstore，用于存储alibaba-log-controller的运行日志。请勿删除此Logstore，否则无法为alibaba-log-controller排查问题。&lt;/li&gt;
&lt;li&gt;若您需要将日志采集到已有的Project，请执行安装命令sh ./alicloud-log-k8s-install.sh${your_k8s_cluster_id} ${your_project_name} ，并确保日志服务Project和您的Kubernetes集群在同一地域。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;该条命令其实就是执行了一个shell脚本，使用helm安装了采集kubernetes集群日志的组件&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-vim&#34;&gt;#!/bin/bash

if [ $# -eq 0 ] ; then
    echo &amp;quot;[Invalid Param], use sudo ./install-k8s-log.sh {your-k8s-cluster-id}&amp;quot;
    exit 1
fi
    
clusterName=$(echo $1 | tr &#39;[A-Z]&#39; &#39;[a-z]&#39;)
curl --connect-timeout 5  http://100.100.100.200/latest/meta-data/region-id
    
if [ $? != 0 ]; then
    echo &amp;quot;[FAIL] ECS meta server connect fail, only support alibaba cloud k8s service&amp;quot;
    exit 1
fi
    
regionId=`curl http://100.100.100.200/latest/meta-data/region-id`
aliuid=`curl http://100.100.100.200/latest/meta-data/owner-account-id`
    
helmPackageUrl=&amp;quot;http://logtail-release-$regionId.oss-$regionId.aliyuncs.com/kubernetes/alibaba-cloud-log.tgz&amp;quot;
wget $helmPackageUrl -O alibaba-cloud-log.tgz
if [ $? != 0 ]; then
    echo &amp;quot;[FAIL] download alibaba-cloud-log.tgz from $helmPackageUrl failed&amp;quot;
    exit 1
fi
    
project=&amp;quot;k8s-log-&amp;quot;$clusterName
if [ $# -ge 2 ]; then
    project=$2
fi
    
echo [INFO] your k8s is using project : $project
    
helm install alibaba-cloud-log.tgz --name alibaba-log-controller \
    --set ProjectName=$project \
    --set RegionId=$regionId \
    --set InstallParam=$regionId \
    --set MachineGroupId=&amp;quot;k8s-group-&amp;quot;$clusterName \
    --set Endpoint=$regionId&amp;quot;-intranet.log.aliyuncs.com&amp;quot; \
    --set AlibabaCloudUserId=&amp;quot;:&amp;quot;$aliuid \
    --set LogtailImage.Repository=&amp;quot;registry.$regionId.aliyuncs.com/log-service/logtail&amp;quot; \
    --set ControllerImage.Repository=&amp;quot;registry.$regionId.aliyuncs.com/log-service/alibabacloud-log-controller&amp;quot;
    
installRst=$?
    
if [ $installRst -eq 0 ]; then
    echo &amp;quot;[SUCCESS] install helm package : alibaba-log-controller success.&amp;quot;
    exit 0
else
    echo &amp;quot;[FAIL] install helm package failed, errno &amp;quot; $installRst
    exit 0
fi
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;命令执行后，会在kubernetes集群中的每个节点运行一个日志采集的pod：logatail-ds，该pod位于kube-system&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://guoxudong.io/images/source/log_detail.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装完成后，可使用以下命令来查看pod状态，若状态全部成功后，则表示安装完成&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm status alibaba-log-controller
LAST DEPLOYED: Thu Nov 22 15:09:35 2018
NAMESPACE: default
STATUS: DEPLOYED
    
RESOURCES:
==&amp;gt; v1/ServiceAccount
NAME                    SECRETS  AGE
alibaba-log-controller  1        6d
    
==&amp;gt; v1beta1/CustomResourceDefinition
NAME                                   AGE
aliyunlogconfigs.log.alibabacloud.com  6d
    
==&amp;gt; v1beta1/ClusterRole
alibaba-log-controller  6d
    
==&amp;gt; v1beta1/ClusterRoleBinding
NAME                    AGE
alibaba-log-controller  6d
    
==&amp;gt; v1beta1/DaemonSet
NAME        DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE SELECTOR  AGE
logtail-ds  16       16       16     16          16         &amp;lt;none&amp;gt;         6d
    
==&amp;gt; v1beta1/Deployment
NAME                    DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
alibaba-log-controller  1        1        1           1          6d
    
==&amp;gt; v1/Pod(related)
NAME                                     READY  STATUS   RESTARTS  AGE
logtail-ds-2fqs4                         1/1    Running  0         6d
logtail-ds-4bz7w                         1/1    Running  1         6d
logtail-ds-6vg88                         1/1    Running  0         6d
logtail-ds-7tp6v                         1/1    Running  0         6d
logtail-ds-9575c                         1/1    Running  0         6d
logtail-ds-bgq84                         1/1    Running  0         6d
logtail-ds-kdlhr                         1/1    Running  0         6d
logtail-ds-lknxw                         1/1    Running  0         6d
logtail-ds-pdxfk                         1/1    Running  0         6d
logtail-ds-pf4dz                         1/1    Running  0         6d
logtail-ds-rzsnw                         1/1    Running  0         6d
logtail-ds-sqhbv                         1/1    Running  0         6d
logtail-ds-vvtwn                         1/1    Running  0         6d
logtail-ds-wwmhg                         1/1    Running  0         6d
logtail-ds-xbp4j                         1/1    Running  0         6d
logtail-ds-zpld9                         1/1    Running  0         6d
alibaba-log-controller-85f8fbb498-nzhc8  1/1    Running  0         6d
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;配置日志组件展示&#34;&gt;配置日志组件展示&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在集群内安装好日志组件后，登录阿里云日志服务控制台，就会发现有一个新的project，名称为k8s-log-{集群id}
&lt;img src=&#34;https://guoxudong.io/images/source/log_src_de.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建Logstore
&lt;img src=&#34;https://guoxudong.io/images/source/log-1.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据导入
&lt;img src=&#34;https://guoxudong.io/images/source/log-2.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;选择数据类型中选择docker标准输出
&lt;img src=&#34;https://guoxudong.io/images/source/log-3.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据源配置，这里可以使用默认的
&lt;img src=&#34;https://guoxudong.io/images/source/log-4.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;选择数据源
&lt;img src=&#34;https://guoxudong.io/images/source/log-5.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置好之后等待1-2分钟，日志就会进来了
&lt;img src=&#34;https://guoxudong.io/images/source/log-6.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;为了快速查询和过滤，需要配置索引
&lt;img src=&#34;https://guoxudong.io/images/source/log-7.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;添加容器名称、命名空间、pod名称作为索引（后续使用livetail需要）
&lt;img src=&#34;https://guoxudong.io/images/source/log-8.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;这样就完成了一个k8s集群日志采集和展示的基本流程了&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;livetail功能使用&#34;&gt;livetail功能使用&lt;/h1&gt;

&lt;h2 id=&#34;背景介绍&#34;&gt;背景介绍&lt;/h2&gt;

&lt;p&gt;在线上运维的场景中，往往需要对日志队列中进入的数据进行实时监控，从最新的日志数据中提取出关键的信息进而快速地分析出异常原因。在传统的运维方式中，如果需要对日志文件进行实时监控，需要到服务器上对日志文件执行命令tail -f，如果实时监控的日志信息不够直观，可以加上grep或者grep -v进行关键词过滤。日志服务在控制台提供了日志数据实时监控的交互功能LiveTail，针对线上日志进行实时监控分析，减轻运维压力。&lt;/p&gt;

&lt;h2 id=&#34;使用方法&#34;&gt;使用方法&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;这里选择来源类型为kubernetes，命名空间、pod名称、容器名称为上一步新建的3个索引的内容，过滤关键字的功劳与tail命令后加的grep命令是一样的，用于关键词过滤
&lt;img src=&#34;https://guoxudong.io/images/source/log-9.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;点击开启livetail，这时就有实时日志展示出来了
&lt;img src=&#34;https://guoxudong.io/images/source/log-10.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;以上就是阿里云livetail日志服务功能&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>kubernetes中pod同步时区问题</title>
      <link>https://guoxudong.io/en/post/pod-timezone/</link>
      <pubDate>Wed, 30 Jan 2019 20:18:13 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/pod-timezone/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;新版监控大屏于18年最后一天正式上线，之后陆续进行了几次优化和修改，最近发现一个比较大的bug，就是监控显示的时间轴不对，显示的就是和目前的时间相差8小时，这就引出了docker中的时区问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;问题的原因&#34;&gt;问题的原因&lt;/h1&gt;

&lt;p&gt;默认的情况，在K8S里启动一个容器，该容器的设置的时区是UTC0，但是对用户而言，主机环境并不在UTC0。我们在UTC8。如果不把容器的时区和主机主机设置为一致，则在查找日志等时候将非常不方便，也容易造成误解。但是K8S以及Docker容器没有一个简便的设置/开关在系统层面做配置。都需要我们从单个容器入手做设置，具体有两个方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;直接修改镜像的时间设置，好处是应用部署时无需做特殊设置，但是需要手动构建Docker镜像。&lt;/li&gt;
&lt;li&gt;部署应用时，单独读取主机的“/etc/localtime”文件，即创建pod时同步时区，无需修改镜像，但是每个应用都要单独设置。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;问题的解决&#34;&gt;问题的解决&lt;/h1&gt;

&lt;p&gt;这里我们选择第二种方法，即修改部署应用的yaml文件，创建pod时同步时区&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
    name: myweb
spec:
    replicas: 2
    template:
        metadata:
        labels:
            app: myweb
        spec:
        containers:
        - name: myweb
            image: nginx:apline
            ports:
            - containerPort: 80
        #挂载到pod中
            volumeMounts:
            - name: host-time
            mountPath: /etc/localtime    
        #需要被挂载的宿主机的时区文件
        volumes:
        - name: host-time
            hostPath:
            path: /etc/localtime
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;效果对比&#34;&gt;效果对比&lt;/h1&gt;

&lt;h2 id=&#34;修改时区前&#34;&gt;修改时区前&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://guoxudong.io/images/source/time-1.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;修改时区后&#34;&gt;修改时区后&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://guoxudong.io/images/source/time-2.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>为ingress配置SSL证书，实现HTTPS访问</title>
      <link>https://guoxudong.io/en/post/https-ingress/</link>
      <pubDate>Sat, 29 Dec 2018 21:28:13 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/https-ingress/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;devops平台率先在公司内使用kubernetes集群提供后端服务，但是由于之前一直处于探索阶段，所以使用的事http的方式提供后端服务，但是在开发统一入口后，出现了访问HTTPS页面的跨域问题，由此引出了后端服务配置SSL证书的问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;使用rancher配置ssl证书&#34;&gt;使用rancher配置SSL证书&lt;/h1&gt;

&lt;h2 id=&#34;下载ssl证书文件&#34;&gt;下载SSL证书文件&lt;/h2&gt;

&lt;p&gt;首先需要获得SSL证书文件，可以直接在阿里云SSL证书管理控制台下载&lt;/p&gt;

&lt;p&gt;选中需要下载证书，选择下载nginx证书
&lt;img src=&#34;https://guoxudong.io/images/source/zhengshu.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;将证书上传项目&#34;&gt;将证书上传项目&lt;/h2&gt;

&lt;p&gt;打开rancher，选择要使用证书的项目，点击资源中的证书&lt;/p&gt;

&lt;h2 id=&#34;将证书上传项目-1&#34;&gt;将证书上传项目&lt;/h2&gt;

&lt;p&gt;打开rancher，选择要使用证书的项目，点击资源中的证书
&lt;img src=&#34;https://guoxudong.io/images/source/https-1.png&#34; alt=&#34;image&#34; /&gt;
添加证书，点击从文件上传
&lt;img src=&#34;https://guoxudong.io/images/source/https-2.png&#34; alt=&#34;image&#34; /&gt;
上传证书文件中的秘钥和证书，点击保存即可&lt;/p&gt;

&lt;h1 id=&#34;使用yaml上传证书&#34;&gt;使用yaml上传证书&lt;/h1&gt;

&lt;p&gt;这个证书的原理其实是在相应的命名空间创建了一个包含证书信息的secrets&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
data:
    tls.crt: {私钥}
    tls.key: {证书}
kind: Secret
metadata:
    name: keking-cn
    namespace: devops-plat
type: kubernetes.io/tls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在kubernetes上运行该yaml即可&lt;/p&gt;

&lt;h1 id=&#34;rancher中证书绑定&#34;&gt;rancher中证书绑定&lt;/h1&gt;

&lt;p&gt;选中需要绑定证书的ingress，点击编辑，选中证书，保存即可（由于ingress-controller中没有绑定默认证书，所以这里不能选中默认）
&lt;img src=&#34;https://guoxudong.io/images/source/https-3.png&#34; alt=&#34;image&#34; /&gt;
保存完毕，证书即可生效&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes删除一直处于Terminating状态的namespace</title>
      <link>https://guoxudong.io/en/post/k8s-d-n/</link>
      <pubDate>Fri, 16 Nov 2018 18:18:13 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/k8s-d-n/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;近期由于公司需要将部署在ucloud上的rancher迁移到阿里云上，所以需要将原有Rancher依赖的namespace（cattle-system）删除，但在删除中出现了删除的namespace一直处于Terminating状态的情况&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://guoxudong.io/images/source/d-n-1.png&#34; alt=&#34;imgage&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;解决方案&#34;&gt;解决方案&lt;/h1&gt;

&lt;p&gt;运行命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl edit namespaces cattle-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到namespaces的yaml配置：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://guoxudong.io/images/source/d-n-2.png&#34; alt=&#34;imgage&#34; /&gt;&lt;/p&gt;

&lt;p&gt;将finalizer的value删除，这里将其设置为[]&lt;/p&gt;

&lt;p&gt;保存即可看到该namespace已被删除&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://guoxudong.io/images/source/d-n-3.png&#34; alt=&#34;imgage&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>kubernetes集群概述</title>
      <link>https://guoxudong.io/en/post/k8s-topo/</link>
      <pubDate>Wed, 03 Oct 2018 12:18:13 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/k8s-topo/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;随着2017年AWS，Azure和阿里云相继在其原有容器服务上新增了对kubernetes的支持，而Docker官网也在同年10月宣布同时支持Swarm好kubernetes容器编排系统。kubernetes俨然已成为容器编排领域事实上的标准，而2018年更是各大公司相继将服务迁移到kubernetes上，而kubernetes则以惊人更新速度，保持着每个季度发布一个大版本的速度高速发展着。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;kubernetes特征&#34;&gt;kubernetes特征&lt;/h1&gt;

&lt;p&gt;kubernetes是一种在一组主机上运行和协同容器化应用程序的系统，旨在提供可预测性、可拓展性与高可用性的方法来完全管理容器化应用和服务的生命周期平台。用户可以定义应用程序的运行方式，以及与其他应用程序或外部世界交互的途径，并能实现服务的扩容和缩容，执行平滑滚动更新，以及在不同版本的应用程序之间调度流量以测试功能或回滚有问题的部署。kubernetes提供了接口和可组合帆软平台原语，使得用户能够以高度的灵活性和可靠性定义及管理应用程序。&lt;/p&gt;

&lt;h1 id=&#34;kubernetes组件及网络通信&#34;&gt;kubernetes组件及网络通信&lt;/h1&gt;

&lt;p&gt;kubernetes集群的客户端可以分为两类：API Server客户端和应用程序（运行为Pod中的容器）客户端。
&lt;img src=&#34;https://guoxudong.io/images/source/kubernetes-topo.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一类客户端通常包含用户和Pod对象两种，它们通过API Server访问kubernetes集群完成管理任务，例如，管理集群上的各种资源对象。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;第二类客户端一般也包含人类用户和Pod对象两种，它们的访问目标是Pod上运行于容器中的应用程序提供的各种具体的服务，如redis或nginx等，不过，这些访问请求通常要经由Service或Ingress资源对象进行。另外，第二类客户端的访问目标对象的操作要经由第一类客户端创建和配置完成后才进行。&lt;/p&gt;

&lt;p&gt;访问API Server时，人类用户一般借助于命令行工具kubectl或图形UI（例如kubernetes dashboard）进行，也通过编程接口进行访问，包括REST API。访问Pod中的应用时，其访问方式要取决于Pod中的应用程序，例如，对于运行Nginx容器的Pod来说，其最常用工具就是浏览器。&lt;/p&gt;

&lt;p&gt;管理员（开发人员或运维人员）使用kubernetes集群的常见操作包括通过控制器创建Pod，在Pod的基础上创建Service供第二类客户端访问，更新Pod中的应用版本（更新和回滚）以及对应用规模进行扩容或缩容等，另外还有集群附件管理、存储卷管理、网络及网络策略管理、资源管理和安全管理等。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>centos7.2 安装k8s v1.11.0</title>
      <link>https://guoxudong.io/en/post/install-k8s/</link>
      <pubDate>Tue, 14 Aug 2018 20:07:03 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/install-k8s/</guid>
      <description>

&lt;h1 id=&#34;前言&#34;&gt;前言&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;最近由于公司业务发展到了瓶颈，原有的技术架构已经逐渐无法满足业务开发和测试的需求，出现了应用测试环境搭建复杂，有许多套（真的很多很多）应用环境，应用在持续集成/持续交付也遇到了很大的困难，经过讨论研究决定对应用和微服务进行容器化，这就是我首次直面docker和k8s的契机.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;kubernetes-介绍&#34;&gt;Kubernetes 介绍&lt;/h1&gt;

&lt;p&gt;Kubernetes 是 Google 团队发起的开源项目，它的目标是管理跨多个主机的容器，提供基本的部署，维护以及运用伸缩，主要实现语言为
Go 语言。
Kubernetes的特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;易学：轻量级，简单，容易理解&lt;/li&gt;
&lt;li&gt;便携：支持公有云，私有云，混合云，以及多种云平台&lt;/li&gt;
&lt;li&gt;可拓展：模块化，可插拔，支持钩子，可任意组合&lt;/li&gt;
&lt;li&gt;自修复：自动重调度，自动重启，自动复制&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;准备工作&#34;&gt;准备工作&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;注：以下操作都是在root权限下执行的&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;安装docker-ce，这里使用docker-ce-17.09.0.c版本，安装方法见&lt;a href=&#34;https://guoxudong.io/2018/install-docker&#34;&gt;之前的教程&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装Kubeadm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#安装 Kubeadm 首先我们要配置好阿里云的国内源，执行如下命令：
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
EOF

#之后，执行以下命令来重建yum缓存：
yum -y install epel-releaseyum
clean all
yum makecache
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来需要安装指定版本的Kubeadm（这里要安装指定版本，因为后续依赖的镜像由于有墙无法拉取，这里我们只有指定版本的镜像），注意：&lt;strong&gt;这里是安装指定版本的Kubeadm，k8s的版本更新之快完全超出你的想象！&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;yum -y install kubelet-1.11.0-0
yum -y install kubeadm-1.11.0-0
yum -y install kubectl-1.11.0-0
yum -y install kubernetes-cni
    
#执行命令启动Kubeadm服务：
systemctl enable kubelet &amp;amp;&amp;amp; systemctl start kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置 Kubeadm 所用到的镜像
这里是重中之重，因为在国内的原因，无法访问到 Google 的镜像库，所以我们需要执行以下脚本来从 Docker Hub 仓库中获取相同的镜像，并且更改 TAG 让其变成与 Google 拉去镜像一致。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;新建一个 Shell 脚本，填入以下代码之后保存&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-vim&#34;&gt;#docker.sh
#!/bin/bash
images=(kube-proxy-amd64:v1.11.0 kube-scheduler-amd64:v1.11.0 kube-controller-manager-amd64:v1.11.0 kube-apiserver-amd64:v1.11.0 etcd-amd64:3.2.18 coredns:1.1.3 pause-amd64:3.1 kubernetes-dashboard-amd64:v1.8.3 k8s-dns-sidecar-amd64:1.14.9 k8s-dns-kube-dns-amd64:1.14.9 k8s-dns-dnsmasq-nanny-amd64:1.14.9 )
for imageName in ${images[@]} ; do
docker pull keveon/$imageName
docker tag keveon/$imageName k8s.gcr.io/$imageName
docker rmi keveon/$imageName
done
# 个人新加的一句，V 1.11.0 必加
docker tag da86e6ba6ca1 k8s.gcr.io/pause:3.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;保存后使用chmod命令赋予脚本执行权限&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;chmod -R 777 ./docker.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;执行脚本拉取镜像&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sh docker.sh
#这里就开始了漫长的拉取镜像之路
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;关闭掉swap&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo swapoff -a
#要永久禁掉swap分区，打开如下文件注释掉swap那一行
# sudo vi /etc/stab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;关闭SELinux的&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 临时禁用selinux
# 永久关闭 修改/etc/sysconfig/selinux文件设置
sed -i &#39;s/SELINUX=permissive/SELINUX=disabled/&#39; /etc/sysconfig/selinux
# 这里按回车，下面是第二条命令
setenforce 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;关闭防火墙&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;systemctl disable firewalld.service &amp;amp;&amp;amp; systemctl stop firewalld.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置转发参数&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 配置转发相关参数，否则可能会出错
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
vm.swappiness=0
EOF
# 这里按回车，下面是第二条命令
sysctl --system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;这里就完成了k8s集群搭建的准备工作，集群搭建的话以上操作结束后将操作完的系统制作成系统镜像，方便集群搭建&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;正式安装&#34;&gt;正式安装&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;以下的操作都只在主节点上进行：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;初始化镜像&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubeadm init --kubernetes-version=v1.11.0 --pod-network-cidr=10.10.0.0/16  #这里填写集群所在网段
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;之后的输出会是这样：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;I0712 10:46:30.938979   13461 feature_gate.go:230] feature gates: &amp;amp;{map[]}
[init] using Kubernetes version: v1.11.0
[preflight] running pre-flight checks
I0712 10:46:30.961005   13461 kernel_validator.go:81] Validating kernel version
I0712 10:46:30.961061   13461 kernel_validator.go:96] Validating kernel config
    [WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 18.03.1-ce. Max validated version: 17.03
    [WARNING Hostname]: hostname &amp;quot;g2-apigateway&amp;quot; could not be reached
    [WARNING Hostname]: hostname &amp;quot;g2-apigateway&amp;quot; lookup g2-apigateway on 100.100.2.138:53: no such host
[preflight/images] Pulling images required for setting up a Kubernetes cluster
[preflight/images] This might take a minute or two, depending on the speed of your internet connection
[preflight/images] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;
[kubelet] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;
[kubelet] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;
[preflight] Activating the kubelet service
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [g2-apigateway kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.16.8.62]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Generated etcd/ca certificate and key.
[certificates] Generated etcd/server certificate and key.
[certificates] etcd/server serving cert is signed for DNS names [g2-apigateway localhost] and IPs [127.0.0.1 ::1]
[certificates] Generated etcd/peer certificate and key.
[certificates] etcd/peer serving cert is signed for DNS names [g2-apigateway localhost] and IPs [172.16.8.62 127.0.0.1 ::1]
[certificates] Generated etcd/healthcheck-client certificate and key.
[certificates] Generated apiserver-etcd-client certificate and key.
[certificates] valid certificates and keys now exist in &amp;quot;/etc/kubernetes/pki&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/admin.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/kubelet.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/controller-manager.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/scheduler.conf&amp;quot;
[controlplane] wrote Static Pod manifest for component kube-apiserver to &amp;quot;/etc/kubernetes/manifests/kube-apiserver.yaml&amp;quot;
[controlplane] wrote Static Pod manifest for component kube-controller-manager to &amp;quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&amp;quot;
[controlplane] wrote Static Pod manifest for component kube-scheduler to &amp;quot;/etc/kubernetes/manifests/kube-scheduler.yaml&amp;quot;
[etcd] Wrote Static Pod manifest for a local etcd instance to &amp;quot;/etc/kubernetes/manifests/etcd.yaml&amp;quot;
[init] waiting for the kubelet to boot up the control plane as Static Pods from directory &amp;quot;/etc/kubernetes/manifests&amp;quot;
[init] this might take a minute or longer if the control plane images have to be pulled
[apiclient] All control plane components are healthy after 41.001672 seconds
[uploadconfig] storing the configuration used in ConfigMap &amp;quot;kubeadm-config&amp;quot; in the &amp;quot;kube-system&amp;quot; Namespace
[kubelet] Creating a ConfigMap &amp;quot;kubelet-config-1.11&amp;quot; in namespace kube-system with the configuration for the kubelets in the cluster
[markmaster] Marking the node g2-apigateway as master by adding the label &amp;quot;node-role.kubernetes.io/master=&#39;&#39;&amp;quot;
[markmaster] Marking the node g2-apigateway as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[patchnode] Uploading the CRI Socket information &amp;quot;/var/run/dockershim.sock&amp;quot; to the Node API object &amp;quot;g2-apigateway&amp;quot; as an annotation
[bootstraptoken] using token: o337m9.ceq32wg9g2gro7gx
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the &amp;quot;cluster-info&amp;quot; ConfigMap in the &amp;quot;kube-public&amp;quot; namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

kubeadm join 10.10.207.253:6443 --token t69z6h.lr2etdbg9mfx5r15 --discovery-token-ca-cert-hash sha256:90e3a748c0eb4cb7058f3d0ee8870ee5d746214ab0589b5e841fd5d68fec8f00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;这里注意最后一行：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;kubeadm join 10.10.207.253:6443 --token t69z6h.lr2etdbg9mfx5r15 --discovery-token-ca-cert-hash sha256:90e3a748c0eb4cb7058f3d0ee8870ee5d746214ab0589b5e841fd5d68fec8f00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;证明集群主节点安装成功，这里要记得保存这条命令，以便之后各个节点加入集群&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;配置kubetl认证信息&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export KUBECONFIG=/etc/kubernetes/admin.conf
# 如果你想持久化的话，直接执行以下命令【推荐】
echo &amp;quot;export KUBECONFIG=/etc/kubernetes/admin.conf&amp;quot; &amp;gt;&amp;gt; ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;安装flanel网络&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p /etc/cni/net.d/

cat &amp;lt;&amp;lt;EOF&amp;gt; /etc/cni/net.d/10-flannel.conf
{
&amp;quot;name&amp;quot;: &amp;quot;cbr0&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;flannel&amp;quot;,
&amp;quot;delegate&amp;quot;: {
&amp;quot;isDefaultGateway&amp;quot;: true
}
}
EOF

mkdir /usr/share/oci-umount/oci-umount.d -p

mkdir /run/flannel/

cat &amp;lt;&amp;lt;EOF&amp;gt; /run/flannel/subnet.env
FLANNEL_NETWORK=10.244.0.0/16
FLANNEL_SUBNET=10.244.1.0/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=true
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;最后需要新建一个flannel.yml文件：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
name: flannel
rules:
- apiGroups:
    - &amp;quot;&amp;quot;
    resources:
    - pods
    verbs:
    - get
- apiGroups:
    - &amp;quot;&amp;quot;
    resources:
    - nodes
    verbs:
    - list
    - watch
- apiGroups:
    - &amp;quot;&amp;quot;
    resources:
    - nodes/status
    verbs:
    - patch
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
name: flannel
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: flannel
subjects:
- kind: ServiceAccount
name: flannel
namespace: kube-system
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: v1
kind: ServiceAccount
metadata:
name: flannel
namespace: kube-system
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
kind: ConfigMap
apiVersion: v1
metadata:
name: kube-flannel-cfg
namespace: kube-system
labels:
    tier: node
    app: flannel
data:
cni-conf.json: |
    {
    &amp;quot;name&amp;quot;: &amp;quot;cbr0&amp;quot;,
    &amp;quot;type&amp;quot;: &amp;quot;flannel&amp;quot;,
    &amp;quot;delegate&amp;quot;: {
        &amp;quot;isDefaultGateway&amp;quot;: true
    }
    }
net-conf.json: |
    {
    &amp;quot;Network&amp;quot;: &amp;quot;10.10.0.0/16&amp;quot;,    #这里换成集群所在的网段
    &amp;quot;Backend&amp;quot;: {
        &amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot;
    }
    }
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
name: kube-flannel-ds
namespace: kube-system
labels:
    tier: node
    app: flannel
spec:
template:
    metadata:
    labels:
        tier: node
        app: flannel
    spec:
    hostNetwork: true
    nodeSelector:
        beta.kubernetes.io/arch: amd64
    tolerations:
    - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
    serviceAccountName: flannel
    initContainers:
    - name: install-cni
        image: quay.io/coreos/flannel:v0.9.1-amd64
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conf
        volumeMounts:
        - name: cni
        mountPath: /etc/cni/net.d
        - name: flannel-cfg
        mountPath: /etc/kube-flannel/
    containers:
    - name: kube-flannel
        image: quay.io/coreos/flannel:v0.9.1-amd64
        command: [ &amp;quot;/opt/bin/flanneld&amp;quot;, &amp;quot;--ip-masq&amp;quot;, &amp;quot;--kube-subnet-mgr&amp;quot; ]
        securityContext:
        privileged: true
        env:
        - name: POD_NAME
        valueFrom:
            fieldRef:
            fieldPath: metadata.name
        - name: POD_NAMESPACE
        valueFrom:
            fieldRef:
            fieldPath: metadata.namespace
        volumeMounts:
        - name: run
        mountPath: /run
        - name: flannel-cfg
        mountPath: /etc/kube-flannel/
    volumes:
        - name: run
        hostPath:
            path: /run
        - name: cni
        hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
        configMap:
            name: kube-flannel-cfg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;执行：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create -f ./flannel.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认情况下，master节点不参与工作负载，但如果希望安装出一个all-in-one的k8s环境，则可以执行以下命令：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;让master节点成为一个node节点：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl taint nodes --all node-role.kubernetes.io/master-
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;查看节点信息：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;会看到如下的输出：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NAME            STATUS     ROLES     AGE       VERSION
k8s-master      Ready      master    18h       v1.11.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;以下是节点配置&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在配置好主节点之后，就可以配置集群的其他节点了，这里建议直接安装之前做好准备工作的系统镜像
进入节点机器之后，直接执行之前保存好的命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubeadm join 10.10.207.253:6443 --token t69z6h.lr2etdbg9mfx5r15 --discovery-token-ca-cert-hash sha256:90e3a748c0eb4cb7058f3d0ee8870ee5d746214ab0589b5e841fd5d68fec8f00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行完后会看到：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;[preflight] running pre-flight checks
        [WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs_wrr ip_vs_sh ip_vs ip_vs_rr] or no builtin kernel ipvs support: map[ip_vs_rr:{} ip_vs_wrr:{} ip_vs_sh:{} nf_conntrack_ipv4:{} ip_vs:{}]
you can solve this problem with following methods:
1. Run &#39;modprobe -- &#39; to load missing kernel modules;
2. Provide the missing builtin kernel ipvs support

I0725 09:59:27.929247   10196 kernel_validator.go:81] Validating kernel version
I0725 09:59:27.929356   10196 kernel_validator.go:96] Validating kernel config
[discovery] Trying to connect to API Server &amp;quot;10.10.207.253:6443&amp;quot;
[discovery] Created cluster-info discovery client, requesting info from &amp;quot;https://10.10.207.253:6443&amp;quot;
[discovery] Requesting info from &amp;quot;https://10.10.207.253:6443&amp;quot; again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &amp;quot;10.10.207.253:6443&amp;quot;
[discovery] Successfully established connection with API Server &amp;quot;10.10.207.253:6443&amp;quot;
[kubelet] Downloading configuration for the kubelet from the &amp;quot;kubelet-config-1.11&amp;quot; ConfigMap in the kube-system namespace
[kubelet] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;
[kubelet] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;
[preflight] Activating the kubelet service
[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...
[patchnode] Uploading the CRI Socket information &amp;quot;/var/run/dockershim.sock&amp;quot; to the Node API object &amp;quot;k8s-node1&amp;quot; as an annotation

This node has joined the cluster:
* Certificate signing request was sent to master and a response
was received.
* The Kubelet was informed of the new secure connection details.

Run &#39;kubectl get nodes&#39; on the master to see this node join the cluster.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;这里就表示执行完毕了，可以去主节点执行命令：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;可以看到节点已加入集群：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NAME        STATUS    ROLES     AGE       VERSION
k8s-master  Ready     master    20h       v1.11.0
k8s-node1   Ready     &amp;lt;none&amp;gt;    20h       v1.11.0
k8s-node2   Ready     &amp;lt;none&amp;gt;    20h       v1.11.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这期间可能需要等待一段时间，状态才会全部变为ready&lt;/p&gt;

&lt;h1 id=&#34;kubernetes-dashboard安装&#34;&gt;kubernetes-dashboard安装&lt;/h1&gt;

&lt;p&gt;详见：&lt;a href=&#34;https://guoxudong.io/2018/dashboard-k8s&#34;&gt;kubernetes安装dashboard&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;采坑指南&#34;&gt;采坑指南&lt;/h1&gt;

&lt;p&gt;有时会出现master节点一直处于notready的状态，这里可能是没有启动flannel，只需要按照上面的教程配置好flannel，然后执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create -f ./flannel.yml
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
