<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GuoXD Blog</title>
    <link>https://guoxudong.io/en/authors/guoxudong/</link>
    <description>Recent content on GuoXD Blog</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&lt;a rel=&#39;license&#39; href=&#39;http://creativecommons.org/licenses/by-nc/4.0/&#39; target=&#39;_blank&#39;&gt;知识共享署名-非商业性使用 4.0 国际许可协议&lt;/a&gt;</copyright>
    <lastBuildDate>Mon, 09 Dec 2019 10:07:46 +0800</lastBuildDate>
    
	    <atom:link href="https://guoxudong.io/en/authors/guoxudong/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Kubecm：管理你的 kubeconfig</title>
      <link>https://guoxudong.io/en/post/kubecm/</link>
      <pubDate>Mon, 09 Dec 2019 10:07:46 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kubecm/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;该项目脱胎于 &lt;a href=&#34;https://github.com/sunny0826/mergeKubeConfig&#34; target=&#34;_blank&#34;&gt;mergeKubeConfig&lt;/a&gt; 项目，最早写该项目的目的是在一堆杂乱无章的 kubeconfig 中自由的切换。随着需要操作的 Kubernetes 集群越来越多，在不同的集群之间切换也越来越麻烦，而操作 Kubernetes 集群的本质不过是通过 &lt;code&gt;kubeconfig&lt;/code&gt; 访问 Kubernetes 集群的 API Server，以操作 Kubernetes 的各种资源，而 &lt;code&gt;kubeconfig&lt;/code&gt; 不过是一个 yaml 文件，用来保存访问集群的密钥，最早的 &lt;a href=&#34;https://github.com/sunny0826/mergeKubeConfig&#34; target=&#34;_blank&#34;&gt;mergeKubeConfig&lt;/a&gt; 不过是一个操作 yaml 文件的 Python 脚本。而随着 golang 学习的深入，也就动了重写这个项目的念头，就这样 &lt;a href=&#34;https://github.com/sunny0826/kubecm&#34; target=&#34;_blank&#34;&gt;kubecm&lt;/a&gt; 诞生了。&lt;/p&gt;

&lt;h2 id=&#34;kubecm&#34;&gt;kubecm&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/sunny0826/kubecm&#34; target=&#34;_blank&#34;&gt;kubecm&lt;/a&gt; 由 golang 编写，支持 &lt;code&gt;Mac&lt;/code&gt; &lt;code&gt;Linux&lt;/code&gt; 和 &lt;code&gt;windows&lt;/code&gt; 平台，&lt;code&gt;delete&lt;/code&gt; &lt;code&gt;rename&lt;/code&gt; &lt;code&gt;switch&lt;/code&gt; 提供比较实用的交互式的操作，目前的功能包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;add ：添加新的 &lt;code&gt;kubeconfig&lt;/code&gt; 到 &lt;code&gt;$HOME/.kube/config&lt;/code&gt; 中&lt;/li&gt;
&lt;li&gt;completion ：命令行自动补全功能&lt;/li&gt;
&lt;li&gt;delete：删除已有的 &lt;code&gt;context&lt;/code&gt; ，提供交互式和指定删除两种方式&lt;/li&gt;
&lt;li&gt;merge：将指定目录中的 &lt;code&gt;kubeconfig&lt;/code&gt; 合并为一个 &lt;code&gt;kubeconfig&lt;/code&gt; 文件&lt;/li&gt;
&lt;li&gt;rename：重名指定的 &lt;code&gt;context&lt;/code&gt;，提供交互式和指定重命名两种方式&lt;/li&gt;
&lt;li&gt;switch：交互式切换 &lt;code&gt;context&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/sunny0826/kubecm&#34; target=&#34;_blank&#34;&gt;kubecm&lt;/a&gt; 支持 &lt;code&gt;Mac&lt;/code&gt; &lt;code&gt;Linux&lt;/code&gt; 和 &lt;code&gt;windows&lt;/code&gt; 平台，安装方式也比较简单：&lt;/p&gt;

&lt;h4 id=&#34;macos&#34;&gt;MacOS&lt;/h4&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    使用 &lt;code&gt;brew&lt;/code&gt; 或者直接下载二进制可执行文件
  &lt;/div&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install sunny0826/tap/kubecm
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;linux&#34;&gt;Linux&lt;/h4&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    下载二进制可执行文件
  &lt;/div&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# linux x86_64
curl -Lo kubecm.tar.gz https://github.com/sunny0826/kubecm/releases/download/v${VERSION}/kubecm_${VERSION}_Linux_x86_64.tar.gz
tar -zxvf kubecm.tar.gz kubecm
cd kubecm
sudo mv kubecm /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;windows&#34;&gt;Windows&lt;/h4&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    下载二进制可执行文件，并将文件移动到 &lt;code&gt;$PATH&lt;/code&gt; 中即可
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;命令行自动补全&#34;&gt;命令行自动补全&lt;/h2&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;a href=&#34;https://github.com/sunny0826/kubecm&#34; target=&#34;_blank&#34;&gt;kubecm&lt;/a&gt; 提供了和 &lt;a href=&#34;https://github.com/kubernetes/kubectl&#34; target=&#34;_blank&#34;&gt;kubectl&lt;/a&gt; 一样的 completion 命令行自动补全功能（支持 bash/zsh）
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;以 &lt;code&gt;zsh&lt;/code&gt; 为例，在 &lt;code&gt;$HOME/.zshrc&lt;/code&gt; 中添加&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-vim&#34;&gt;source &amp;lt;(kubecm completion zsh)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后使用 &lt;code&gt;source&lt;/code&gt; 命令，使其生效&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source $HOME/.zshrc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后，在输入 &lt;code&gt;kubecm&lt;/code&gt; 后按 &lt;kbd&gt;tab&lt;/kbd&gt; 键，就可以看到命令行自动补全的内容&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva2.sinaimg.cn/large/ad5fbf65gy1g9qa0yy3bvj21co0f2hdt.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;操作-kubeconfig&#34;&gt;操作 kubeconfig&lt;/h2&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;a href=&#34;https://github.com/sunny0826/kubecm&#34; target=&#34;_blank&#34;&gt;kubecm&lt;/a&gt; 可以实现 &lt;code&gt;kubeconfig&lt;/code&gt; 的查看、添加、删除、合并、重命名和切换
  &lt;/div&gt;
&lt;/div&gt;

&lt;h4 id=&#34;查看&#34;&gt;查看&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 查看 $HOME/.kube/config 中所有的 context
kubecm
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;添加&#34;&gt;添加&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 添加 example.yaml 到 $HOME/.kube/config.yaml，该方式不会覆盖源 kubeconfig，只会在当前目录中生成一个 config.yaml 文件
kubecm add -f example.yaml

# 功能同上，但是会将 example.yaml 中的 context 命名为 test
kubecm add -f example.yaml -n test

# 添加 -c 会覆盖源 kubeconfig
kubecm add -f example.yaml -c
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;删除&#34;&gt;删除&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 交互式删除
kubecm delete
# 删除指定 context
kubecm delete my-context
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;合并&#34;&gt;合并&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 合并 test 目录中的 kubeconfig,该方式不会覆盖源 kubeconfig，只会在当前目录中生成一个 config.yaml 文件
kubecm merge -f test 

# 添加 -c 会覆盖源 kubeconfig
kubecm merge -f test -c
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;重命名&#34;&gt;重命名&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 交互式重命名
kubecm rename
# 将 dev 重命名为 test
kubecm rename -o dev -n test
# 重命名 current-context 为 dev
kubecm rename -n dev -c
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;效果展示&#34;&gt;效果展示&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;Interaction.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/sunny0826/kubecm&#34; target=&#34;_blank&#34;&gt;kubecm&lt;/a&gt; 项目的初衷为学习 golang 并熟悉 client-go 的使用，随着使用的深入，断断续续增加了不少功能，开发出了一个看上去还算正规的项目。总的来说都是根据自己的喜好来开发的业余项目，欢迎各位通过 &lt;a href=&#34;https://github.com/sunny0826/kubecm/issues/new&#34; target=&#34;_blank&#34;&gt;ISSUE&lt;/a&gt; 来进行交流和讨论。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>小工具介绍：KubeWatch</title>
      <link>https://guoxudong.io/en/post/kubewatch/</link>
      <pubDate>Wed, 04 Dec 2019 17:09:51 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kubewatch/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;这次要介绍一个 Kubernetes 资源观测工具，实时监控 Kubernetes 集群中各种资源的新建、更新和删除，并实时通知到各种协作软件/聊天软件，目前支持的通知渠道有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;slack&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hipchat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mattermost&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;flock&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;webhook&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我这边开发了钉钉的通知渠道，但是在上游 &lt;a href=&#34;https://github.com/bitnami-labs/kubewatch/issues/198&#34; target=&#34;_blank&#34;&gt;ISSUE#198&lt;/a&gt; 中提出的贡献请求并没有得到回应，所以这边只能 fork 了代码，然后自己进行了开发，以支持钉钉通知。&lt;/p&gt;

&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;

&lt;p&gt;这里推荐使用 helm 进行安装，快速部署&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm install kubewatch stable/kubewatch \
--set rbac.create=true \
--set slack.channel=&#39;#YOUR_CHANNEL&#39; \
--set slack.token=&#39;xoxb-YOUR_TOKEN&#39; \
--set resourcesToWatch.pod=true \
--set resourcesToWatch.daemonset=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想使用钉钉通知，则可以在 &lt;a href=&#34;https://github.com/sunny0826/kubewatch-chat&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; 上拉取我的代码，代码中包含 helm chart 包，可直接进行安装&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/sunny0826/kubewatch-chat.git
cd kubewatch-chat
helm install kubewatch kubewatch \
--set dingtalk.sign=&amp;quot;XXX&amp;quot; \
--set dingtalk.token=&amp;quot;XXXX-XXXX-XXXX&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;钉钉配置&#34;&gt;钉钉配置&lt;/h2&gt;

&lt;p&gt;在钉钉中创建 &lt;code&gt;智能群助手&lt;/code&gt; ，之后&lt;/p&gt;

&lt;h3 id=&#34;获取-token&#34;&gt;获取 token&lt;/h3&gt;

&lt;p&gt;复制的 webhook 中 &lt;code&gt;https://oapi.dingtalk.com/robot/send?access_token={YOUR_TOKEN}&lt;/code&gt;, &lt;code&gt;{YOUR_TOKEN}&lt;/code&gt; 就是要填入的 token。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva4.sinaimg.cn/large/ad5fbf65gy1g9ku2hvs16j20ep05smxk.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;安全设置&#34;&gt;安全设置&lt;/h2&gt;

&lt;p&gt;钉钉智能群助手在更新后新增了安全设置，提供三种验证方式 &lt;code&gt;自定义关键词&lt;/code&gt; &lt;code&gt;加签&lt;/code&gt; &lt;code&gt;IP地址（段）&lt;/code&gt;，这里推荐使用 &lt;code&gt;IP地址（段）的方式&lt;/code&gt;，直接将 Kubernetes 集群的出口 IP 填入设置即可。同时也提供了 &lt;code&gt;加签&lt;/code&gt; 的方式，拷贝秘钥，将其填入 &lt;code&gt;dingtalk.sign&lt;/code&gt; 中。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/ad5fbf65gy1g9ku6qjwy2j20fo077glw.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;项目配置&#34;&gt;项目配置&lt;/h2&gt;

&lt;p&gt;编辑 &lt;code&gt;kubewatch/value.yaml&lt;/code&gt; ，修改配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;## Global Docker image parameters
## Please, note that this will override the image parameters, including dependencies, configured to use the global value
## Current available global Docker image parameters: imageRegistry and imagePullSecrets
##
# global:
#   imageRegistry: myRegistryName
#   imagePullSecrets:
#     - myRegistryKeySecretName

slack:
  enabled: false
  channel: &amp;quot;&amp;quot;
  token: &amp;quot;xoxb&amp;quot;

hipchat:
  enabled: false
  # room: &amp;quot;&amp;quot;
  # token: &amp;quot;&amp;quot;
  # url: &amp;quot;&amp;quot;
mattermost:
  enabled: false
  # channel: &amp;quot;&amp;quot;
  # url: &amp;quot;&amp;quot;
  # username: &amp;quot;&amp;quot;
flock:
  enabled: false
  # url: &amp;quot;&amp;quot;
webhook:
  enabled: false
  # url: &amp;quot;&amp;quot;
dingtalk:
  enabled: true
  token: &amp;quot;&amp;quot;
  sign: &amp;quot;&amp;quot;

# namespace to watch, leave it empty for watching all.
namespaceToWatch: &amp;quot;&amp;quot;

# Resources to watch
resourcesToWatch:
  deployment: true
  replicationcontroller: false
  replicaset: false
  daemonset: false
  services: false
  pod: true
  job: false
  persistentvolume: false

image:
  registry: docker.io
#  repository: bitnami/kubewatch
  repository: guoxudongdocker/kubewatch-chart
#  tag: 0.0.4-debian-9-r405
  tag: latest
  pullPolicy: Always
  ## Optionally specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ##
  # pullSecrets:
  #   - myRegistryKeySecretName

## String to partially override kubewatch.fullname template (will maintain the release name)
##
# nameOverride:

## String to fully override kubewatch.fullname template
##
# fullnameOverride:

rbac:
  # If true, create &amp;amp; use RBAC resources
  #
  create: true

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

resources: {}
  # limits:
  #   cpu: 100m
  #   memory: 300Mi
  # requests:
  #   cpu: 100m
  #   memory: 300Mi

# Affinity for pod assignment
# Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
# affinity: {}

# Tolerations for pod assignment
# Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: []

# Node labels for pod assignment
# Ref: https://kubernetes.io/docs/user-guide/node-selection/
nodeSelector: {}

podAnnotations: {}
podLabels: {}
replicaCount: 1

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 &lt;code&gt;value.yaml&lt;/code&gt; 安装&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/sunny0826/kubewatch-chat.git
cd kubewatch-chat
helm install my-release -f kubewatch/values.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;slack-配置&#34;&gt;Slack 配置&lt;/h2&gt;

&lt;p&gt;Slack 为 kubewatch 默认的通知软件，这里就不简介 Slack 的安装和注册，直接从创建 APP 开始&lt;/p&gt;

&lt;h3 id=&#34;创建一个-app&#34;&gt;创建一个 APP&lt;/h3&gt;

&lt;p&gt;进去创建 &lt;a href=&#34;https://api.slack.com/apps&#34; target=&#34;_blank&#34;&gt;APP 页面&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/ad5fbf65gy1g9kum3x5npj21h40p6tdx.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;选择 &lt;code&gt;App Name&lt;/code&gt; 和 &lt;code&gt;Development Slack Workspace&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/ad5fbf65gy1g9kupp0av1j210c0uejvj.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;添加-bot-用户&#34;&gt;添加 Bot 用户&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax3.sinaimg.cn/large/ad5fbf65gy1g9kuszmgggj21n4156gu2.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;添加-app-到-workspace&#34;&gt;添加 App 到 Workspace&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://tva3.sinaimg.cn/large/ad5fbf65gy1g9kuyzwzetj21qu0wmq9n.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;获取-bot-token&#34;&gt;获取 Bot-token&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax3.sinaimg.cn/large/ad5fbf65gy1g9kv06dva8j21s60uajxf.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;通知效果&#34;&gt;通知效果&lt;/h2&gt;

&lt;p&gt;在 Slack 中，&lt;code&gt;创建&lt;/code&gt; &lt;code&gt;更新&lt;/code&gt; &lt;code&gt;删除&lt;/code&gt; 分别以绿、黄和红色代表&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax1.sinaimg.cn/large/ad5fbf65gy1g9kv23nvmoj213c0mewj4.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在钉钉中，我进行了汉化&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax3.sinaimg.cn/large/ad5fbf65gy1g9kv5fppglj20dd08zdgs.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax4.sinaimg.cn/large/ad5fbf65gy1g9kv5uuxn4j20ea08fgmk.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;对于 kubewatch 我们这里主要用作监控各种 CronJob 的定时触发状态，已经 ConfigMap 和 Secrets 的状态变化，同时也观察 HPA 触发的弹性伸缩的状态，可以实时观测到业务高峰的到来，是一个不错的小工具。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Velero 进行集群备份与迁移</title>
      <link>https://guoxudong.io/en/post/aliyun-velero/</link>
      <pubDate>Wed, 13 Nov 2019 09:13:22 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/aliyun-velero/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;在近日的一个风和日丽的下午，正在快乐的写 bug 时，突然间钉钉就被 call 爆了，原来是 k8s 测试集群的一个 namespace 突然不见了。这个 namespace 里面有 60 多个服务，瞬间全部没有了……虽然得益于我们的 CI/CD 系统，这些服务很快都重新部署并正常运行了，但是如果在生产环境，那后果就是不可想象的了。在排查这个问题发生的原因的同时，集群资源的灾备和恢复功能就提上日程了，这时 Velero 就出现了。&lt;/p&gt;

&lt;h2 id=&#34;velero&#34;&gt;Velero&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/vmware-tanzu/velero&#34; target=&#34;_blank&#34;&gt;Velero&lt;/a&gt; 是 VMWare 开源的 k8s 集群备份、迁移工具。可以帮助我们完成 k8s 的例行备份工作，以便在出现上面问题的时候可以快速进行恢复。同时也提供了集群迁移功能，可以将 k8s 资源迁移到其他 k8s 集群的功能。Velero 将集群资源保存在对象存储中，默认情况下可以使用 &lt;a href=&#34;https://velero.io/docs/v1.1.0/aws-config&#34; target=&#34;_blank&#34;&gt;AWS&lt;/a&gt;、&lt;a href=&#34;https://velero.io/docs/v1.1.0/azure-config&#34; target=&#34;_blank&#34;&gt;Azure&lt;/a&gt;、&lt;a href=&#34;https://velero.io/docs/v1.1.0/gcp-config&#34; target=&#34;_blank&#34;&gt;GCP&lt;/a&gt; 的对象存储，同时也给出了插件功能用来拓展其他平台的存储，这里我们用到的就是阿里云的对象存储 OSS，阿里云也提供了 Velero 的插件，用于将备份存储到 OSS 中。下面我就介绍一下如何在阿里云容器服务 ACK 使用 Velero 完成备份和迁移。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Velero 地址：&lt;a href=&#34;https://github.com/vmware-tanzu/velero&#34; target=&#34;_blank&#34;&gt;https://github.com/vmware-tanzu/velero&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ACK 插件地址：&lt;a href=&#34;https://github.com/AliyunContainerService/velero-plugin&#34; target=&#34;_blank&#34;&gt;https://github.com/AliyunContainerService/velero-plugin&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;下载-velero-客户端&#34;&gt;下载 Velero 客户端&lt;/h3&gt;

&lt;p&gt;Velero 由客户端和服务端组成，服务器部署在目标 k8s 集群上，而客户端则是运行在本地的命令行工具。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;前往 &lt;a href=&#34;https://github.com/vmware-tanzu/velero/releases&#34; target=&#34;_blank&#34;&gt;Velero 的 Release 页面&lt;/a&gt; 下载客户端，直接在 GitHub 上下载即可&lt;/li&gt;
&lt;li&gt;解压 release 包&lt;/li&gt;
&lt;li&gt;将 release 包中的二进制文件 &lt;code&gt;velero&lt;/code&gt; 移动到 &lt;code&gt;$PATH&lt;/code&gt; 中的某个目录下&lt;/li&gt;
&lt;li&gt;执行 &lt;code&gt;velero -h&lt;/code&gt; 测试&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;创建-oss-bucket&#34;&gt;创建 OSS bucket&lt;/h3&gt;

&lt;p&gt;创建一个 OSS bucket 用于存储备份文件，这里也可以用已有的 bucket，之后会在 bucket 中创建 &lt;code&gt;backups&lt;/code&gt;、&lt;code&gt;metadata&lt;/code&gt;、&lt;code&gt;restores&lt;/code&gt;三个目录，这里建议在已有的 bucket 中创建一个子目录用于存储备份文件。&lt;/p&gt;

&lt;p&gt;创建 OSS 的时候一定要选对区域，要和 ACK 集群在同一个区域，存储类型和读写权限选择&lt;strong&gt;标准存储&lt;/strong&gt;和&lt;strong&gt;私有&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva3.sinaimg.cn/wap720/ad5fbf65gy1g8w7t8c4xbj21021d8thq.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;创建阿里云-ram-用户&#34;&gt;创建阿里云 RAM 用户&lt;/h3&gt;

&lt;p&gt;这里需要创建一个阿里云 RAM 的用户，用于操作 OSS 以及 ACK 资源。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;新建权限策略&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax4.sinaimg.cn/large/ad5fbf65gy1g8w80cjiv2j21uo18cag8.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;策略内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;Version&amp;quot;: &amp;quot;1&amp;quot;,
    &amp;quot;Statement&amp;quot;: [
        {
            &amp;quot;Action&amp;quot;: [
                &amp;quot;ecs:DescribeSnapshots&amp;quot;,
                &amp;quot;ecs:CreateSnapshot&amp;quot;,
                &amp;quot;ecs:DeleteSnapshot&amp;quot;,
                &amp;quot;ecs:DescribeDisks&amp;quot;,
                &amp;quot;ecs:CreateDisk&amp;quot;,
                &amp;quot;ecs:Addtags&amp;quot;,
                &amp;quot;oss:PutObject&amp;quot;,
                &amp;quot;oss:GetObject&amp;quot;,
                &amp;quot;oss:DeleteObject&amp;quot;,
                &amp;quot;oss:GetBucket&amp;quot;,
                &amp;quot;oss:ListObjects&amp;quot;
            ],
            &amp;quot;Resource&amp;quot;: [
                &amp;quot;*&amp;quot;
            ],
            &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;新建用户&lt;/p&gt;

&lt;p&gt;在新建用户的时候要选择 &lt;code&gt;编程访问&lt;/code&gt;，来获取 &lt;code&gt;AccessKeyID&lt;/code&gt; 和 &lt;code&gt;AccessKeySecret&lt;/code&gt;，这里请创建一个新用于用于备份，不要使用老用户的 AK 和 AS。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax2.sinaimg.cn/large/ad5fbf65gy1g8w8h4ek4uj21h40ue785.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;部署服务端&#34;&gt;部署服务端&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;拉取 &lt;a href=&#34;https://github.com/AliyunContainerService/velero-plugin&#34; target=&#34;_blank&#34;&gt;Velero 插件&lt;/a&gt; 到本地&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/AliyunContainerService/velero-plugin
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置修改&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;修改 &lt;code&gt;install/credentials-velero&lt;/code&gt; 文件，将新建用户中获得的 &lt;code&gt;AccessKeyID&lt;/code&gt; 和 &lt;code&gt;AccessKeySecret&lt;/code&gt; 填入，这里的 OSS EndPoint 为之前 OSS 的访问域名（&lt;strong&gt;注：这里需要选择外网访问的 EndPoint。&lt;/strong&gt;）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax2.sinaimg.cn/large/ad5fbf65gy1g8w8xd1sgzj21c20cm75z.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ALIBABA_CLOUD_ACCESS_KEY_ID=&amp;lt;ALIBABA_CLOUD_ACCESS_KEY_ID&amp;gt;
ALIBABA_CLOUD_ACCESS_KEY_SECRET=&amp;lt;ALIBABA_CLOUD_ACCESS_KEY_SECRET&amp;gt;
ALIBABA_CLOUD_OSS_ENDPOINT=&amp;lt;ALIBABA_CLOUD_OSS_ENDPOINT&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改 &lt;code&gt;install/01-velero.yaml&lt;/code&gt;，将 OSS 配置填入：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;---
apiVersion: velero.io/v1
kind: BackupStorageLocation
metadata:
labels:
    component: velero
name: default
namespace: velero
spec:
config: {}
objectStorage:
    bucket: &amp;lt;ALIBABA_CLOUD_OSS_BUCKET&amp;gt;  # OSS bucket 名称
    prefix: &amp;lt;OSS_PREFIX&amp;gt;    # bucket 子目录
provider: alibabacloud
---
apiVersion: velero.io/v1
kind: VolumeSnapshotLocation
metadata:
labels:
    component: velero
name: default
namespace: velero
spec:
config:
    region: &amp;lt;REGION&amp;gt;    # 地域，如果是华东2（上海），则为 cn-shanghai
provider: alibabacloud
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;k8s 部署 Velero 服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 新建 namespace
kubectl create namespace velero
# 部署 credentials-velero 的 secret
kubectl create secret generic cloud-credentials --namespace velero --from-file cloud=install/credentials-velero
# 部署 CRD
kubectl apply -f install/00-crds.yaml
# 部署 Velero
kubectl apply -f install/01-velero.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;测试 Velero 状态&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ velero version
Client:
    Version: v1.1.0
    Git commit: a357f21aec6b39a8244dd23e469cc4519f1fe608
Server:
    Version: v1.1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到 Velero 的客户端和服务端已经部署成功。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;服务端清理&lt;/p&gt;

&lt;p&gt;在完成测试或者需要重新安装时，执行如下命令进行清理：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl delete namespace/velero clusterrolebinding/velero
kubectl delete crds -l component=velero
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;备份测试&#34;&gt;备份测试&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;velero-plugin&lt;/code&gt; 项目中已经给出 &lt;code&gt;example&lt;/code&gt; 用于测试备份。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;部署测试服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -f examples/base.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对 &lt;code&gt;nginx-example&lt;/code&gt; 所在的 namespace 进行备份&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero backup create nginx-backup --include-namespaces nginx-example --wait
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;模拟 namespace 被误删&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl delete namespaces nginx-example
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用 Velero 进行恢复&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero restore create --from-backup nginx-backup --wait
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;集群迁移&#34;&gt;集群迁移&lt;/h3&gt;

&lt;p&gt;迁移方法同备份，在备份后切换集群，在新集群恢复备份即可。&lt;/p&gt;

&lt;h3 id=&#34;高级用法&#34;&gt;高级用法&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定时备份&lt;/p&gt;

&lt;p&gt;对集群资源进行定时备份，则可在发生意外的情况下，进行恢复（默认情况下，备份保留 30 天）。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 每日1点进行备份
velero create schedule &amp;lt;SCHEDULE NAME&amp;gt; --schedule=&amp;quot;0 1 * * *&amp;quot;
# 每日1点进行备份，备份保留48小时
velero create schedule &amp;lt;SCHEDULE NAME&amp;gt; --schedule=&amp;quot;0 1 * * *&amp;quot; --ttl 48h
# 每6小时进行一次备份
velero create schedule &amp;lt;SCHEDULE NAME&amp;gt; --schedule=&amp;quot;@every 6h&amp;quot;
# 每日对 web namespace 进行一次备份
velero create schedule &amp;lt;SCHEDULE NAME&amp;gt; --schedule=&amp;quot;@every 24h&amp;quot; --include-namespaces web
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;定时备份的名称为：&lt;code&gt;&amp;lt;SCHEDULE NAME&amp;gt;-&amp;lt;TIMESTAMP&amp;gt;&lt;/code&gt;，恢复命令为：&lt;code&gt;velero restore create --from-backup &amp;lt;SCHEDULE NAME&amp;gt;-&amp;lt;TIMESTAMP&amp;gt;&lt;/code&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;备份删除&lt;/p&gt;

&lt;p&gt;直接执行命令进行删除&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero delete backups &amp;lt;BACKUP_NAME&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;备份资源查看&lt;/p&gt;

&lt;p&gt;备份查看&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero backup get
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看定时备份&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero schedule get
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看可恢复备份&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero restore get
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;备份排除项目&lt;/p&gt;

&lt;p&gt;可为资源添加指定标签，添加标签的资源在备份的时候被排除。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 添加标签
kubectl label -n &amp;lt;ITEM_NAMESPACE&amp;gt; &amp;lt;RESOURCE&amp;gt;/&amp;lt;NAME&amp;gt; velero.io/exclude-from-backup=true
# 为 default namespace 添加标签
kubectl label -n default namespace/default velero.io/exclude-from-backup=true
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;问题汇总&#34;&gt;问题汇总&lt;/h3&gt;

&lt;h4 id=&#34;时区问题&#34;&gt;时区问题&lt;/h4&gt;

&lt;p&gt;进行定时备份时，发现备份使用的事 UTC 时间，并不是本地时间，经过排查后发现是 &lt;code&gt;velero&lt;/code&gt; 镜像的时区问题，在调整后就会正常定时备份了，这里我重新调整了时区，直接调整镜像就好，修改 &lt;code&gt;install/01-velero.yaml&lt;/code&gt; 文件，将镜像替换为 &lt;code&gt;registry-vpc.cn-shanghai.aliyuncs.com/keking/velero:latest&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: velero
  namespace: velero
spec:
  replicas: 1
  selector:
    matchLabels:
      deploy: velero
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: &amp;quot;8085&amp;quot;
        prometheus.io/scrape: &amp;quot;true&amp;quot;
      labels:
        component: velero
        deploy: velero
    spec:
      serviceAccountName: velero
      containers:
      - name: velero
        # sync from gcr.io/heptio-images/velero:latest
        image: registry-vpc.cn-shanghai.aliyuncs.com/keking/velero:latest   # 修复时区后的镜像
        imagePullPolicy: IfNotPresent
        command:
          - /velero
        args:
          - server
          - --default-volume-snapshot-locations=alibabacloud:default
        env:
          - name: VELERO_SCRATCH_DIR
            value: /scratch
          - name: ALIBABA_CLOUD_CREDENTIALS_FILE
            value: /credentials/cloud
        volumeMounts:
          - mountPath: /plugins
            name: plugins
          - mountPath: /scratch
            name: scratch
          - mountPath: /credentials
            name: cloud-credentials
      initContainers:
      - image: registry.cn-hangzhou.aliyuncs.com/acs/velero-plugin-alibabacloud:v1.2
        imagePullPolicy: IfNotPresent
        name: velero-plugin-alibabacloud
        volumeMounts:
        - mountPath: /target
          name: plugins
      volumes:
        - emptyDir: {}
          name: plugins
        - emptyDir: {}
          name: scratch
        - name: cloud-credentials
          secret:
            secretName: cloud-credentials

&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;版本问题&#34;&gt;版本问题&lt;/h4&gt;

&lt;p&gt;截止发稿时，Velero 已经发布了 v1.2.0 版本，目前 ACK 的 Velero 的插件还未升级。&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;近日正好有 k8s 集群服务迁移服务的需求，使用 Velero 完成了服务的迁移，同时也每日进行集群资源备份，其能力可以满足容器服务的灾备和迁移场景，实测可用，现已运行在所有的 k8s 集群。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Grafana 展示阿里云监控指标</title>
      <link>https://guoxudong.io/en/post/aliyun-cms-grafana/</link>
      <pubDate>Thu, 07 Nov 2019 11:08:36 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/aliyun-cms-grafana/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;对于阿里云用户来说，阿里云监控是一个很不错的产品，首先它在配额内使用是免费的！免费的！免费的！重要的事情说三遍。他的功能类似于 zabbix，但是比 zabbix 提供了更多的监控项，基本上在云上使用的资源都可以通过云监控来实时监控。而它提供的开箱即用方式，天然集成云资源，并提供多种告警方式，免去了监控与告警系统搭建与维护的繁琐，并且减少了资源的消耗，比购买 ECS 自己搭建 zabbix 要少消耗很多资源。同时阿里云监控和阿里云其他服务一样，也提供了比较完整的 OpenApi 以及各种语言的 sdk，可以基于阿里云的 OpenApi 将其与自己的系统集成。我们之前也是这么做的，但是随着监控项的增加，以及经常需要在办公场地监控投屏的专项监控页，光凭我们的运维开发工程师使用 vue 写速度明显跟不上，而且页面的美观程度也差很多。&lt;/p&gt;

&lt;h3 id=&#34;手写前端-vs-grafana&#34;&gt;手写前端 VS Grafana&lt;/h3&gt;

&lt;p&gt;手写前端虽然可定制化程度更高，但是需要消耗大量精力进行调试，对于运维人员，哪怕是运维开发也是吃不消的（前端小哥哥和小姐姐是不会来帮你的，下图就是我去年拿 vue 写的伪 Grafana 展示页面，花费了大约一周时间在调整这些前端元素）。
&lt;img src=&#34;https://tva4.sinaimg.cn/large/ad5fbf65gy1g8pfrw1licj22ye1gg4qp.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Grafana 则标准化程度很高，展示也更加符合大众审美，某些定制化需求可以通过自定义 DataSource 或者 AJAX 插件的 iframe 模式完成。开发后端 DataSource 肯定就没有前端调整 css 那么痛苦和耗时了，整体配置开发一个这样的页面可能只消耗一人天就能完成。而在新产品上线时，构建一个专项监控展示页面速度就更快了，几分钟内就能完成。
&lt;img src=&#34;https://tva4.sinaimg.cn/large/ad5fbf65gy1g8pfvp0keej22yc1g2khm.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;关于阿里云监控&#34;&gt;关于阿里云监控&lt;/h2&gt;

&lt;p&gt;云监控（CloudMonitor）是一项针对阿里云资源和互联网应用进行监控的服务。&lt;/p&gt;

&lt;p&gt;云监控为云上用户提供开箱即用的企业级开放型一站式监控解决方案。涵盖 IT 设施基础监控，外网网络质量拨测监控，基于事件、自定义指标、日志的业务监控。为您全方位提供更高效、更全面、更省钱的监控服务。通过提供跨产品、跨地域的应用分组管理模型和报警模板，帮助您快速构建支持几十种云产品、管理数万实例的高效监控报警管理体系。通过提供 Dashboard，帮助您快速构建自定义业务监控大盘。使用云监控，不但可以帮助您提升您的系统服务可用时长，还可以降低企业 IT 运维监控成本。&lt;/p&gt;

&lt;p&gt;云监控服务可用于收集获取阿里云资源的监控指标或用户自定义的监控指标，探测服务可用性，以及针对指标设置警报。使您全面了解阿里云上的资源使用情况、业务的运行状况和健康度，并及时收到异常报警做出反应，保证应用程序顺畅运行。&lt;/p&gt;

&lt;h2 id=&#34;关于-grafana&#34;&gt;关于 Grafana&lt;/h2&gt;

&lt;p&gt;Grafana 是一个跨平台的开源的度量分析和可视化工具，可以通过将采集的数据查询然后可视化的展示，并及时通知。由于云监控的 Grafana 还没有支持告警，所以我们这里只用了 Grafana 的可视化功能，而告警本身就是云监控自带的，所以也不需要依赖 Grafana 来实现。而我们的 Prometheus 也使用了 Grafana 进行数据可视化，所以有现成的 Grafana-Server 使用。&lt;/p&gt;

&lt;h2 id=&#34;阿里云监控对接-grafana&#34;&gt;阿里云监控对接 Grafana&lt;/h2&gt;

&lt;p&gt;首先 Grafana 服务的部署方式这里就不做介绍了，请使用较新版本的 Grafana，最好是 5.5.0+。后文中也有我开源的基于阿里云云监控的 Grafana 的 helm chart，可以使用 helm 安装，并会直接导入云监控的指标，这个会在后文中介绍。&lt;/p&gt;

&lt;h3 id=&#34;安装阿里云监控插件&#34;&gt;安装阿里云监控插件&lt;/h3&gt;

&lt;p&gt;进入插件目录进行安装&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /var/lib/grafana/plugins/
git clone https://github.com/aliyun/aliyun-cms-grafana.git 
service grafana-server restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是使用 docker 或者部署在 k8s 集群，这里也可以使用环境变量在 Grafana 部署的时候进行安装&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;...
spec:
  containers:
  - env:
    - name: GF_INSTALL_PLUGINS  # 多个插件请使用,隔开
      value: grafana-simple-json-datasource,https://github.com/aliyun/aliyun-cms-grafana/archive/master.zip;aliyun-cms-grafana
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;您也可以下载 aliyun-cms-grafana.zip 插件解压后，上传服务器的 Grafana 的 plugins 目录下，重启 grafana-server 即可。&lt;/p&gt;

&lt;h3 id=&#34;配置云监控-datasource&#34;&gt;配置云监控 DataSource&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Grafana 启动后，进入 &lt;code&gt;Configuration&lt;/code&gt; 页面，选择 &lt;code&gt;DataSource&lt;/code&gt; Tab 页，单击右上方的&lt;code&gt;Add data source&lt;/code&gt;，添加数据源。&lt;/li&gt;
&lt;li&gt;选中&lt;code&gt;CMS Grafana Service&lt;/code&gt;，单击&lt;code&gt;select&lt;/code&gt;。
&lt;img src=&#34;https://tvax2.sinaimg.cn/large/ad5fbf65gy1g8ph0ukr0pj21nm0jk76m.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;li&gt;填写配置项，URL 根据云监控所在地域填写，并且填写阿里云账号的 accessKeyId 和 accessSecret，完成后单击&lt;code&gt;Save&amp;amp;Test&lt;/code&gt;。
&lt;img src=&#34;https://tvax3.sinaimg.cn/large/ad5fbf65gy1g8ph4bg2bij218m194n9f.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;创建-dashboard&#34;&gt;创建 Dashboard&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;单击 &lt;code&gt;Create&lt;/code&gt; -&amp;gt; &lt;code&gt;Dashboard&lt;/code&gt; -&amp;gt; &lt;code&gt;Add Query&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;配置图标，数据源选择之前添加的 &lt;code&gt;CMS Grafana Service&lt;/code&gt;，然后文档中的配置项填入指标即可（这里要注意的是，云监控 API 给返回的只有实例 ID，并没有自定义的实例名称，这里需要手动将其填入 &lt;code&gt;Y - column describe&lt;/code&gt; 中；而且只支持输入单个 Dimension，若输入多个，默认选第一个，由于这些问题才有了后续我开发的 &lt;code&gt;cms-grafana-builder&lt;/code&gt; 的动机）。
&lt;img src=&#34;https://tva4.sinaimg.cn/large/ad5fbf65gy1g8phck0irbj22ye13in79.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;li&gt;配置参考 &lt;a href=&#34;https://help.aliyun.com/document_detail/28619.html&#34; target=&#34;_blank&#34;&gt;云产品监控项&lt;/a&gt;，
&lt;img src=&#34;https://tva2.sinaimg.cn/large/ad5fbf65gy1g8phg832uvj21a40vo793.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;使用-helm-chart-的方式部署-grafana&#34;&gt;使用 helm chart 的方式部署 Grafana&lt;/h2&gt;

&lt;p&gt;项目地址：&lt;a href=&#34;https://github.com/sunny0826/cms-grafana-builder&#34; target=&#34;_blank&#34;&gt;https://github.com/sunny0826/cms-grafana-builder&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;cms-grafana-builder&#34;&gt;cms-grafana-builder&lt;/h3&gt;

&lt;p&gt;由于上文中的问题，我们需要手动选择每个实例 ID 到 Dimension 中，并且还要讲该实例的名称键入 &lt;code&gt;Y - column describe&lt;/code&gt; 中，十分的繁琐，根本不可能大批量的输入。&lt;/p&gt;

&lt;p&gt;这就是我开发这个 Grafana 指标参数生成器的原因，起初只是一个 python 脚本，用来将我们要监控的指标组装成一个 Grafana 可以使用 json 文件，之后结合 Grafana 的容器化部署方法，将其做成了一个 helm chart。可以在启动的时候自动将需要的参数生成，并且每日会对所有指标进行更新，这样就不用每次新购或者释放掉资源后还需要再跑一遍脚本。&lt;/p&gt;

&lt;h3 id=&#34;部署&#34;&gt;部署&lt;/h3&gt;

&lt;p&gt;只需要将项目拉取下来运行 &lt;code&gt;helm install&lt;/code&gt; 命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm install my-release kk-grafana-cms \
--namespace {your_namespace} \
--set access_key_id={your_access_key_id} \
--set access_secret={your_access_secret} \
--set region_id={your_aliyun_region_id} \
--set password={admin_password}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多详情见 &lt;a href=&#34;https://github.com/sunny0826/cms-grafana-builder&#34; target=&#34;_blank&#34;&gt;github README&lt;/a&gt;，欢迎提 issue 交流。&lt;/p&gt;

&lt;h3 id=&#34;指标选择&#34;&gt;指标选择&lt;/h3&gt;

&lt;p&gt;在部署成功后，可修改 ConfigMap：&lt;code&gt;grafana-cms-metric&lt;/code&gt;，然后修改对应的监控指标项。&lt;/p&gt;

&lt;h3 id=&#34;效果&#34;&gt;效果&lt;/h3&gt;

&lt;p&gt;ECS:
&lt;img src=&#34;https://tvax1.sinaimg.cn/large/ad5fbf65gy1g8pi9toh3dj21gv0pldyf.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;RDS:
&lt;img src=&#34;https://tva2.sinaimg.cn/large/ad5fbf65gy1g8pi9o91ejj21h80q316p.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;EIP:
&lt;img src=&#34;https://tva4.sinaimg.cn/large/ad5fbf65gy1g8pi9i9if3j21h70q3aif.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Redis:
&lt;img src=&#34;https://tvax1.sinaimg.cn/large/ad5fbf65gy1g8pi8ss733j21h30pz7b6.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;为了满足公司需求，后续还开发 DataSource 定制部分，用于公司监控大屏的展示，这部分是另一个项目，不在这个项目里，就不细说了，之后有机会总结后再进行分享。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>阿里云产品夜谈-容器服务交流</title>
      <link>https://guoxudong.io/en/post/aliyun-product-meetup/</link>
      <pubDate>Mon, 30 Sep 2019 09:32:35 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/aliyun-product-meetup/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://tvax4.sinaimg.cn/large/ad5fbf65gy1g7hb4iwdpvj213i0vs4qq.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;9月25日晚受邀来到阿里云飞天园区参加阿里云MVP产品夜谈，在会上遇到了容器服务团队的负责人易立，并就容器服务进行了交流。此次参加夜谈的除了来自全球各地的阿里云MVP，还有来自安全团队、容器团队、AIoT 团队、大数据团队、数据库团队、人工智能团队、中间件团队、搜索引擎&amp;amp;智能推荐团队的负责人&amp;amp;产品经理。各个参会的MVP可以根据自己的研究方向或者感兴趣的方向选择，直接与团队负责人面对面交流，获取阿里云产品的最新信息，并提出使用意见，促进产品的发展。由于主要从事云原生&amp;amp;容器方面的工作，我选择了容器团队，与阿里云容器服务团队负责人易立就容器服务进行交流，本文记录了部分交流内容。&lt;/p&gt;

&lt;h2 id=&#34;容器服务交流&#34;&gt;容器服务交流&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://tva3.sinaimg.cn/large/ad5fbf65gy1g7hdbw7rwij21zk13ax6s.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;关于集群版本-集群升级&#34;&gt;关于集群版本&amp;amp;集群升级&lt;/h3&gt;

&lt;p&gt;众所周知，Kubernetes 以非常稳定的3个月发布一个版本速度在高速迭代这，Kubernetes v1.16.0 也即将 release ，但是目前 ACK 的 Kubernetes 版本依旧为 v1.12.6-aliyun.1 已落后官方4个大版本。得到的回复是新版本 1.14 已经上线，下周就可以升级了，1.14 版本之前已经上线，只不过一直在灰度测试，下周（2019年9月29日）就全面放开升级了。截止写这篇文章的时候，我们的多个 Kubernetes 集群已成功升级到 v1.14.6-aliyun.1 ，虽然在升级的时候出现了一点小问题，但是最后还是顺利解决了。&lt;/p&gt;

&lt;p&gt;然后就是集群升级的问题，集群升级的时候会建议对所有节点打快照，确保节点安全，但是如果在节点升级当中失败，就会出现一半为新版本节点，一半为旧版本节点的问题。我们的一个节点升级失败，就出现了上述问题，最后还是将该节点容器驱散，并将该节点移出集群才解决了升级问题。希望集群升级提供整体状态保存&amp;amp;回退功能，确保如果升级失败（或者出现新旧版本不兼容问题）的时候可以安全回退到之前版本。&lt;/p&gt;

&lt;h3 id=&#34;关于容器服务前端展示&#34;&gt;关于容器服务前端展示&lt;/h3&gt;

&lt;p&gt;ACK 的 WEB 界面相对简陋，一直以来都是对 Kubernetes Dashboard 进行了简单的包装，和其他公有云相比确实不如。不过这也不是容器服务独有的问题，阿里云你产品众多，大部分都有这样的问题。与易立交流得知，容器服务团队目前主要的任务还是确保 Kubernetes 集群的安全稳定运行，他们在安全和可用性上花费的大量精力，貌似并没有拿到什么前端开发资源。我注意到像费用中心、日志服务等产品都有了新版页面，这里希望能容器服务页面也能尽快改版，提高页面操作的便捷和美观。&lt;/p&gt;

&lt;h3 id=&#34;关于授权管理&#34;&gt;关于授权管理&lt;/h3&gt;

&lt;p&gt;一直以来容器服务都有授权管理功能，后来都基于RAM重新做了授权管理功能。但是RAM权限管理策略十分复杂，配置起来也很麻烦，不同的策略结构和语法学习起来非常困难。在配置和管理起来非常困难，我们只能把所有权限收回，每项权限都要根据需求提工单来进行配置，还时长会出现配置不生效的问题。而且这个问题一提出，就引起了大家的共鸣，后了解得知，为了安全合规的要求，操作便捷和安全合规没法兼顾。这里希望授权管理上能在确保合规的同时，能提升RAM操作的便捷性。&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax2.sinaimg.cn/large/ad5fbf65gy1g7hdrlln6vj21rm0ycwov.jpg&#34; alt=&#34;image&#34; /&gt;
关于容器服务的交流主要是以上几点，其他的还包括监控、存储和 CI/CD 方面进行了交流，同时也获得了不少建议。当面给阿里云提需求的机会并不多，我也是抓住机会，把日常使用 ACK 的问题汇总之后一股脑的丢了出去。有类似需求的同学可以在&lt;a href=&#34;https://connect.aliyun.com&#34; target=&#34;_blank&#34;&gt;阿里云的聆听平台&lt;/a&gt;上给阿里云提交建议，以我的经验，合理的需求会很快审核通过并排期开发，换句话说就是“人人都可以是阿里云的产品经理”。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>当码农遇见公益</title>
      <link>https://guoxudong.io/en/post/alibaba-public-welfare/</link>
      <pubDate>Sun, 29 Sep 2019 09:52:11 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/alibaba-public-welfare/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;本次参加云栖大会，除了受到阿里云开发者社区的邀请，同时也受到了阿里码上公益团队的邀请，于9月25日下午参加了&lt;strong&gt;阿里巴巴技术公益专场&lt;/strong&gt;。说来惭愧，作为一个码农关注并加入码上公益已半年有余，但是除了在平台上以自己的经验来给出各种建议外，并没有贡献什么实质的代码，这也可能是因为我专职运维开发，在devops和效能提升上有很多自己的见解，但对于前端UI和各种官网的构建并不是很在行。&lt;/p&gt;

&lt;h2 id=&#34;阿里巴巴与公益&#34;&gt;阿里巴巴与公益&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax4.sinaimg.cn/large/ad5fbf65gy1g7gdc2nfg5j21z819i4qv.jpg&#34; alt=&#34;image&#34; /&gt;
一直以来我对于公益的理解还是停留在很浅的阶段，而且很长一段时间以来，公益还是停留在有限的圈子内，像我这样的技术人与公益完全就是两个世界。而阿里巴巴推出的码上公益则改变了这一点，这个平台让我了解到，原来公益除了捐助之外还能以自己的技术能力做出更大的贡献。此次技术公益专场，阿里发布了技术公益基金，我也是第一次见到这么多的阿里合伙人，可见公益在阿里巴巴集团内部的重要。阿里巴巴不只在集团内部推进公益，而且还为我们这样的普通人提供了像码上公益、蚂蚁森林、一书等平台和产品，让越来越多的人参与到公益事业中，让更多的人平等的享有技术红利。&lt;/p&gt;

&lt;h2 id=&#34;让代码更有温度&#34;&gt;让代码更有温度&lt;/h2&gt;

&lt;p&gt;参加码上公益的初衷只不过是想为公益事业做一些力所能及的事情，尤其是使用代码这种方式，在公益事业上展示我们技术人的才华。也符合与我的价值观：&lt;strong&gt;技术让世界更美好&lt;/strong&gt;。我们技术人可以通过一种比捐助更有温度的方式：代码，来让我们的世界越来越美好。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/ad5fbf65gy1g7g8ywqxu5j21z01b87wp.jpg&#34; alt=&#34;image&#34; /&gt;
在公益专场中有幸结识了 Michael HERMANN 老师，作为一个德国人，他却操这一口流利的中文，十几年如一日的在中国偏远的乡村中为那里的孩子带去教育，同时也为孩子们带去了希望。在专场后的公益沟通会有幸与 Michael 老师一桌，这是一位可爱的老人，十分关注中国偏远地区孩子们的教育问题，在他的身上我看到了一个作为公益人的坚持，在他眼里国籍、文化、地域都没有差别，有的只是希望这些生活在偏远地区的孩子都享有受教育的权利，十分值得敬佩。同时还结识了许多志同道合的码农朋友，大家都希望用自己温暖的代码为公益事业做出自己的贡献。&lt;/p&gt;

&lt;h2 id=&#34;我们只有一个地球&#34;&gt;我们只有一个地球&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax2.sinaimg.cn/large/ad5fbf65gy1g7ge105htyj21z419mqvb.jpg&#34; alt=&#34;image&#34; /&gt;
之后《用现代科技助力中国虎豹保护》的主题分享，让我了解到了，环境的保护也是公益中很重要的一部分。而科技的发展，让从事动物保护的人员可以做的更仔细更完善，对于AI、大数据的应用，使得对于野生动物的保护更精准。技术赋能野生动物保护，可以更好的保护野生动物，同时也让动物保护人员的工作更轻松，让这些常年在大山密林中保护野生动物的科学家不再那么辛苦，同时更好的保护野生动物。&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;之后南都公益基金会理事长徐永光分享了《互联网带来的时代改变》，了解到了互联网公益并不只是刚刚兴起，而是一直在努力；而联合国世界粮食计划署驻华代表屈四喜则带来了《为了“零饥饿”目标》。感谢阿里巴巴提供了这个机会，让我这样的技术人可以参与到公益事业当中，为公益事业贡献出自己的力量。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>玩转 Drone CI</title>
      <link>https://guoxudong.io/en/post/drone-optimize/</link>
      <pubDate>Wed, 11 Sep 2019 13:53:09 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/drone-optimize/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;使用 drone CI 已有小半年，在将原有项目的 CI 系统从 jenkins 向 drone 迁移的时候，也陆陆续续遇到了一些问题。在这段时间，也完成了使用官方插件到插件定制的转变，使得 drone CI 流程更贴合我们 devops 开发流程。通过这篇文章总结一下目前我们对 drone 进行的一些定制化开发以及使用技巧，由于 drone 官方的文档不是很详细，所以也希望通过这种方法来和其他使用 drone 的用户分享和交流使用经验。&lt;/p&gt;

&lt;h2 id=&#34;并行构建&#34;&gt;并行构建&lt;/h2&gt;

&lt;p&gt;在默认情况下，drone 会按照步骤执行，但是有时会遇到前后端在同一个 repo 的情况，这时使用并行构建就可以省去很多的构建时间。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;构建流程：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在下面的示例里会展示一个如下流程：repo 中包含一个由 Java 写的服务以及一个 vue 前端项目，maven 构建和 npm 构建同时进行，maven 构建成功后会镜像 docker 镜像构建并上传镜像仓库，docker 构建成功后会镜像 k8s 部署，部署成功后会进行 vue 项目前端发布，在 k8s 部署成功并且前端发布成功后，进行钉钉构建成功同时，否则进行钉钉构建失败通知。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;前端构建 ————————————          前端发布
                    \      /        \
                     \    /       钉钉通知
                      \  /          /
后端构建 —— 镜像构建 —— k8s部署 ——————

&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;.drone.yml&lt;/code&gt; 配置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kind: &amp;quot;pipeline&amp;quot;
name: &amp;quot;default&amp;quot;
steps:
  - name: &amp;quot;Maven编译&amp;quot;
    image: &amp;quot;guoxudongdocker/drone-maven&amp;quot;
    commands:
      - &amp;quot;mvn clean install&amp;quot;
    depends_on: [ &amp;quot;clone&amp;quot; ]
  - name: &amp;quot;构建镜像&amp;quot;
    image: &amp;quot;guoxudongdocker/drone-docker&amp;quot;
    settings:
      username:
        from_secret: &amp;quot;docker_user&amp;quot;
      password:
        from_secret: &amp;quot;docker_pass&amp;quot;
      dockerfile: &amp;quot;Dockerfile&amp;quot;
      repo: &amp;quot;registry-vpc.cn-shanghai.aliyuncs.com/guoxudong/test&amp;quot;
      registry: &amp;quot;registry-vpc.cn-shanghai.aliyuncs.com&amp;quot;
      tags: &amp;quot;${DRONE_BUILD_NUMBER}&amp;quot;
    depends_on: [ &amp;quot;Maven编译&amp;quot; ]
  - name: &amp;quot;Kubernetes 部署&amp;quot;
    image: &amp;quot;guoxudongdocker/kubectl&amp;quot;
    settings:
      config: &amp;quot;deploy/overlays/uat&amp;quot;
      timeout: 300
      check: false
    depends_on: [ &amp;quot;构建镜像&amp;quot; ]
  - name: &amp;quot;前端构建&amp;quot;
    image: &amp;quot;guoxudongdocker/node-drone&amp;quot;
    commands:
      - &amp;quot;npm install&amp;quot;
      - &amp;quot;npm run build&amp;quot;
    depends_on: [ &amp;quot;clone&amp;quot; ]
  - name: &amp;quot;前端上传&amp;quot;
    image: &amp;quot;guoxudongdocker/node-drone&amp;quot;
    commands:
      - &amp;quot;do something&amp;quot;
    depends_on: [ &amp;quot;前端构建&amp;quot;,&amp;quot;Kubernetes 部署&amp;quot; ]
  - name: &amp;quot;钉钉通知&amp;quot;
    image: &amp;quot;guoxudongdocker/drone-dingtalk&amp;quot;
    settings:
      token:
        from_secret: &amp;quot;dingding&amp;quot;
      type: &amp;quot;markdown&amp;quot;
      message_color: true
      message_pic: true
      sha_link: true
    depends_on: [ &amp;quot;前端上传&amp;quot;,&amp;quot;Kubernetes 部署&amp;quot; ]
    when:
      status:
        - &amp;quot;failure&amp;quot;
        - &amp;quot;success&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;多子项目构建&#34;&gt;多子项目构建&lt;/h2&gt;

&lt;p&gt;在使用 drone 中遇到的最大问题就是，我们有很多项目都是在一个 repo 中有很多子项目，而每个子项目都是 k8s 中的一个服务，这时一个 &lt;code&gt;.drone.yml&lt;/code&gt; 文件很难把所有的服务都囊括。而又不想每个子项目拉一个分支管理，当前的模式就很不合适。&lt;/p&gt;

&lt;h3 id=&#34;插件开发&#34;&gt;插件开发&lt;/h3&gt;

&lt;p&gt;针对这个问题，我们对 drone 进行了定制化开发，会在每次提交代码后，对新提交的代码和老代码进行比较，筛选出做了修改的子项目，然后对有修改的子项目尽心 CI ，其余的子项目则不进行发布。&lt;/p&gt;

&lt;p&gt;而以上的方式仅适用于测试环境的快速迭代，生产环境则采用 tag 的模式，针对不同的子项目，打不同前缀的 tag ，比如子项目为 test1 ，则打 &lt;code&gt;test1-v0.0.1&lt;/code&gt; 的 tag，就会对该子项目进行生产发布。&lt;/p&gt;

&lt;h3 id=&#34;构建效果&#34;&gt;构建效果&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;有修改的子项目&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g6vm2ul2zfj21ky148jx0.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;无修改的子项目&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx1.sinaimg.cn/large/ad5fbf65gy1g6vm49on4kj21jk11iaf7.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;kubernetes-发布状态检查&#34;&gt;Kubernetes 发布状态检查&lt;/h2&gt;

&lt;p&gt;之前的 Kubernetes 发布只是将服务发布到 Kubernetes 集群，并不管服务是否正常启动。针对这个问题以及我们的 Kubernetes 应用管理模式，我们开发了 drone 的 Kubernetes 发布插件，该插件包括 &lt;code&gt;kubectl&lt;/code&gt; 、&lt;code&gt;kustomize&lt;/code&gt;、&lt;code&gt;kubedog&lt;/code&gt; ，来完善我们的 Kubernetes 发布 step 。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;.drone.yml&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;steps:
- name: Kubernetes 部署
  image: guoxudongdocker/kubectl
  volumes:
  - name: kube
    path: /root/.kube
  settings:
    check: false                 # 该参数为是否开启子模块检查
    config: deploy/overlays/uat  # 这里使用 kustomize ,详细使用方法请见 https://github.com/kubernetes-sigs/kustomize
    timeout: 300                 # kubedog 的检测超时
    name: {your-deployment-name} # 如果开启子模块检查则需要填入子模块名称

...

volumes:
- name: kube
  host:
    path: /tmp/cache/.kube  # kubeconfig 挂载位置

trigger:
  branch:
  - master  # 触发 CI 的分支
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用该插件会如果为测试构建，则会自动设置 docker 镜像 tag 为 &lt;code&gt;DRONE_BUILD_NUMBER&lt;/code&gt; ；如果为生产构建（git tag），则叫自动设置 docker 镜像 tag 为 &lt;code&gt;DRONE_TAG&lt;/code&gt; ，然后通过 &lt;code&gt;kubectl apply -k .&lt;/code&gt; 进行部署，同时使用 &lt;code&gt;kubedog&lt;/code&gt; 进行部署状态检查，如果服务正常启动则该 step 通过，如果超时或者部署报错则该 step 失败。&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;根据我们目前的开发模式，对 drone 插件进行了全方位的开发。由于 dockerhub 的镜像拉取经常超时，则将镜像推送到了我们自己的镜像仓库；对钉钉通知也进行了优化；同时也根据我们目前的开发语言进行了插件的开发，提供了基于 Java 、Python 以及 Node.js 的 drone 插件，基本可以满足我们现在的 CI 需求，但随着 drone 的深入使用，越来越多的问题将会暴露出来。后续将会不断解决遇到的问题，持续优化。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GitHub/Gitee 静态页托管页部署SSL证书</title>
      <link>https://guoxudong.io/en/post/aliyun-ssl/</link>
      <pubDate>Fri, 23 Aug 2019 09:36:55 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/aliyun-ssl/</guid>
      <description>

&lt;p&gt;本文档介绍了在 &lt;a href=&#34;https://pages.github.com/&#34; target=&#34;_blank&#34;&gt;Github&lt;/a&gt; / &lt;a href=&#34;https://gitee.com/help/articles/4136&#34; target=&#34;_blank&#34;&gt;Gitee&lt;/a&gt; 的静态页托管Pages服务部署SSL证书，配置HTTPS安全访问的操作说明。&lt;/p&gt;

&lt;h3 id=&#34;pages服务&#34;&gt;Pages服务&lt;/h3&gt;

&lt;p&gt;Github/Gitee的Pages是一个免费的静态网页托管服务，您可以使用Github或码云Pages托管博客、项目官网等静态网页。常见的静态站点生成器有：Hugo、Jekyll、Hexo等，可以用来生成静态站点。默认情况下，托管的站点使用 &lt;code&gt;github.io&lt;/code&gt; / &lt;code&gt;gitee.io&lt;/code&gt; 域名来访问站点，同时也支持自定义域名，并配置强制使用HTTPS。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;注意：如果要在 Gitee Pages 上配置自定义域名+HTTPS，则需要开启 Gitee Pages Pro 。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;github-pages-服务部署ssl证书&#34;&gt;Github Pages 服务部署SSL证书&lt;/h3&gt;

&lt;h4 id=&#34;前提条件&#34;&gt;前提条件&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;GitHub 仓库&lt;/li&gt;
&lt;li&gt;开启 GitHub Pages&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g69e503ukoj21ig0hwad9.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;证书签发&#34;&gt;证书签发&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;购买证书后点击申请&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g69ee2r500j22cc078t9z.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;证书申请&lt;/p&gt;

&lt;p&gt;如果该域名是由阿里云购买，则选择自动DNS验证，如果不是在阿里云购买的，可以选择手动验证。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx1.sinaimg.cn/bmiddle/ad5fbf65gy1g69egsu7fuj20ye0swwh3.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/bmiddle/ad5fbf65gy1g69eo1wls7j20ya0r0418.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;证书签发&lt;/p&gt;

&lt;p&gt;证书通过申请后，会收到证书签发的邮件。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/wap720/ad5fbf65gy1g69epoqw6uj21680cotaj.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;设置自定义域名&#34;&gt;设置自定义域名&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;解析域名&lt;/p&gt;

&lt;p&gt;在证书签发成功后，添加DNS解析，将绑定了SSL证书的域名解析到 &lt;code&gt;YourRepo.github.io&lt;/code&gt; 。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws2.sinaimg.cn/large/ad5fbf65gy1g69evivrvqj21mi07it9g.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置域名&lt;/p&gt;

&lt;p&gt;解析之后将域名添加到 &lt;code&gt;Custom domain&lt;/code&gt; 并且点击 &lt;code&gt;Save&lt;/code&gt; ，Github会自动验证，出现&lt;code&gt;Your site is published at https://YourDomainName.com/&lt;/code&gt;则证明解析成功。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g69esrcn2tj21a210wwk0.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;gitee-pages-pro-服务部署ssl证书&#34;&gt;Gitee Pages Pro 服务部署SSL证书&lt;/h3&gt;

&lt;h4 id=&#34;前提条件-1&#34;&gt;前提条件&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Gitee 仓库&lt;/li&gt;
&lt;li&gt;开启 Gitee Pages Pro&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Gitee 需要开启 Gitee Pages Pro 服务才支持自定义域名+HTTPS。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;证书签发-1&#34;&gt;证书签发&lt;/h4&gt;

&lt;p&gt;证书签发同 Github Pages。这里介绍非阿里云购买的域名，进行证书申请。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;购买证书流程如上&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;申请证书&lt;/p&gt;

&lt;p&gt;证书验证方式选择&lt;code&gt;手工DNS验证&lt;/code&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;拷贝验证信息&lt;/p&gt;

&lt;p&gt;拷贝验证信息内的&lt;code&gt;记录值&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/bmiddle/ad5fbf65gy1g69eo1wls7j20ya0r0418.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;验证解析&lt;/p&gt;

&lt;p&gt;进入购买域名所在网站进行DNS解析，这里以&lt;a href=&#34;https://www.name.com/zh-cn/&#34; target=&#34;_blank&#34;&gt;name.com&lt;/a&gt;为例：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws4.sinaimg.cn/large/ad5fbf65gy1g69fqad2euj221g0700tt.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;解析成功之后，返回阿里云SSL证书管理页面点击&lt;code&gt;验证&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;证书签发&lt;/p&gt;

&lt;p&gt;签发成功后会收到签发成功的邮件。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;设置自定义域名-1&#34;&gt;设置自定义域名&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;解析域名&lt;/p&gt;

&lt;p&gt;进入域名所在网站，添加DNS解析记录，将绑定了SSL证书的域名解析到&lt;code&gt;gitee.gitee.io&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws3.sinaimg.cn/large/ad5fbf65gy1g69fyy5it5j21z606mjs9.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置域名&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;域名添加到&lt;code&gt;自定义域名&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g69g11wx0qj21a60xiq7m.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置证书&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;证书下载，选择 nginx 类型。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws4.sinaimg.cn/bmiddle/ad5fbf65gy1g69g3pua7xj20ne0v0jus.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;gitee pages 配置证书，将证书文件与私钥文件贴入并提交。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws4.sinaimg.cn/large/ad5fbf65gy1g69g64n1btj21bs0yogq8.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;勾选&lt;code&gt;强制使用HTTPS&lt;/code&gt;，并保存。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;验证&#34;&gt;验证&lt;/h3&gt;

&lt;p&gt;在Github/Gitee配置成功之后，您可在浏览器中输入 &lt;a href=&#34;https://www.YourDomainName.com&#34; target=&#34;_blank&#34;&gt;https://www.YourDomainName.com&lt;/a&gt; 验证证书安装结果。可以正常访问静态托管站点，并且浏览器地址栏显示绿色的小锁标识说明证书安装成功。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>解决 Nginx-Ingress 重定向失败问题</title>
      <link>https://guoxudong.io/en/post/nginx-ingress-error/</link>
      <pubDate>Fri, 16 Aug 2019 11:15:37 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/nginx-ingress-error/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;最近对公司 Kubernetes 集群的 &lt;code&gt;nginx-ingress-controller&lt;/code&gt; 进行了升级，但是升级后却出现了大问题，之前所有采用 &lt;code&gt;nginx.ingress.kubernetes.io/rewrite-target: /&lt;/code&gt; 注释进行重定向的 Ingress 路由全部失效了，但是那些直接解析了域名，没有进行重定向的却没有发生这个问题。&lt;/p&gt;

&lt;h2 id=&#34;问题分析&#34;&gt;问题分析&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;首先检查对应服务健康状态，发现所有出问题的服务的状态均正常，同时受影响的之后 http 调用，而 RPC 调用却不受影响，这时问题就定位到了 ingress。&lt;/li&gt;
&lt;li&gt;然后检查 nginx-ingress-controller ，发现 nginx-ingress-controller 的状态也是正常的，路由也是正常的。&lt;/li&gt;
&lt;li&gt;最后发现受影响的只有添加了重定向策略的 ingress 。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;问题解决&#34;&gt;问题解决&lt;/h2&gt;

&lt;p&gt;问题已经定位，接下来就是着手解决问题，这时候值得注意的就是之前进行了什么变更：升级了 nginx-ingress-controller 版本！看来问题就出现在新版本上，那么就打开官方文档：&lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/examples/rewrite/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.github.io/ingress-nginx/examples/rewrite/&lt;/a&gt; 看一下吧。&lt;/p&gt;

&lt;h3 id=&#34;attention&#34;&gt;Attention&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Starting in Version 0.22.0, ingress definitions using the annotation &lt;code&gt;nginx.ingress.kubernetes.io/rewrite-target&lt;/code&gt; are not backwards compatible with previous versions. In Version 0.22.0 and beyond, any substrings within the request URI that need to be passed to the rewritten path must explicitly be defined in a &lt;a href=&#34;https://www.regular-expressions.info/refcapture.html&#34; target=&#34;_blank&#34;&gt;capture group&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;文档上给出了非常明显的警告⚠️：从 V0.22.0 版本开始将不再兼容之前的入口定义，再查看一下我的 nginx-ingress-controller 版本，果然问题出现来这里。&lt;/p&gt;

&lt;h3 id=&#34;note&#34;&gt;Note&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.regular-expressions.info/refcapture.html&#34; target=&#34;_blank&#34;&gt;Captured groups&lt;/a&gt; are saved in numbered placeholders, chronologically, in the form &lt;code&gt;$1&lt;/code&gt;, &lt;code&gt;$2&lt;/code&gt; &amp;hellip; &lt;code&gt;$n&lt;/code&gt;. These placeholders can be used as parameters in the &lt;code&gt;rewrite-target&lt;/code&gt; annotation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;示例&#34;&gt;示例&lt;/h3&gt;

&lt;p&gt;到这里问题已经解决了，在更新了 ingress 的配置之后，之前所有无法重定向的服务现在都已经可以正常访问了。修改见如下示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ echo &#39;
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  name: rewrite
  namespace: default
spec:
  rules:
  - host: rewrite.bar.com
    http:
      paths:
      - backend:
          serviceName: http-svc
          servicePort: 80
        path: /something(/|$)(.*)
&#39; | kubectl create -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;解决这个问题的实际时间虽然不长，但是着实让人出了一身冷汗，同时也给了我警示：变更有风险，升级需谨慎。在升级之前需要先浏览新版本的升级信息，同时需要制定完善的回滚策略，确保万无一失。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Golang 装逼指南 Ⅱ：在 Homwebrew 上发布 Golang 项目</title>
      <link>https://guoxudong.io/en/post/golang-to-homebrew/</link>
      <pubDate>Thu, 25 Jul 2019 16:27:57 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/golang-to-homebrew/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;经过上篇文章，我们已经可以在 GitHub 上构建一个看上去正经的 Golang 项目了，但这并不是结束。一个真正的用 Golang 构建的可执行文件是可以在全平台运行的，前文中我们使用 &lt;code&gt;goreleaser&lt;/code&gt; 发布了一个非常漂亮的 release 页面，并在 release 页面上提供多平台的可执行文件下载。但是如果只是拿着可执行文件到处拷贝执行，总归不够优雅，所以这里就介绍如何在 Homebrew 上发布自己的 Golang 应用，如何像各种牛逼的项目那样使用 &lt;code&gt;brew&lt;/code&gt; 一键安装自己的项目。&lt;/p&gt;

&lt;h2 id=&#34;homebrew&#34;&gt;Homebrew&lt;/h2&gt;

&lt;p&gt;对于使用 macOS 的用户来说，Homebrew 一点也不陌生，它类似于 CentOS 的 &lt;code&gt;yum&lt;/code&gt; 和 Ubuntu 的 &lt;code&gt;apt-get&lt;/code&gt; 。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://brew.sh/&#34; target=&#34;_blank&#34;&gt;Homebrew&lt;/a&gt; 是最初由 Max Howell 用 Ruby 写的 OS X 软件管理系统，其代码开源在 &lt;a href=&#34;https://github.com/Homebrew/brew/&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; 上。&lt;/p&gt;

&lt;p&gt;Homebrew 给自己贴了个一句话简介：The missing package manager for OS X。翻译过来成中文就是：macOS 缺失的软件包的管理器。名副其实地是，Homebrew 真的很好用。安装、升级、卸载等操作，在 Homebrew 的生态下，都只需要一条命令就可以了。并且 Homebrew 会自动为你解决软件包的依赖问题。&lt;/p&gt;

&lt;h2 id=&#34;发布步骤&#34;&gt;发布步骤&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;创建 Formula&lt;/li&gt;
&lt;li&gt;修改 rb 脚本&lt;/li&gt;
&lt;li&gt;本地测试&lt;/li&gt;
&lt;li&gt;创建 Tap&lt;/li&gt;
&lt;li&gt;实际安装&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;创建-formula&#34;&gt;创建 Formula&lt;/h3&gt;

&lt;p&gt;首先进入 release 页面，拷贝 macOS 的 Darwin 包地址&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws4.sinaimg.cn/large/ad5fbf65gy1g5c7b4mi5fj21lq0se4ck.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后通过命令在本地创建 Formula&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew create https://github.com/sunny0826/kubecm/releases/download/v0.0.1/kubecm_0.0.1_Darwin_x86_64.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过这个命令，&lt;code&gt;brew&lt;/code&gt; 会创建一个名为 &lt;code&gt;kubecm.rb&lt;/code&gt; 的文件在 &lt;code&gt;/usr/local/Homebrew/Library/Taps/homebrew/homebrew-core/Formula/&lt;/code&gt; 目录。&lt;/p&gt;

&lt;h3 id=&#34;修改-rb-脚本&#34;&gt;修改 rb 脚本&lt;/h3&gt;

&lt;p&gt;然后在该目录可以看到 &lt;code&gt;kubecm.rb&lt;/code&gt; 这个脚本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ruby&#34;&gt;# Documentation: https://github.com/Homebrew/brew/blob/master/share/doc/homebrew/Formula-Cookbook.md
#                http://www.rubydoc.info/github/Homebrew/brew/master/Formula
# PLEASE REMOVE ALL GENERATED COMMENTS BEFORE SUBMITTING YOUR PULL REQUEST!

class OtfccMac64 &amp;lt; Formula
  desc &amp;quot;&amp;quot;
  homepage &amp;quot;&amp;quot;
  url &amp;quot;https://github.com/sunny0826/kubecm/releases/download/v0.0.1/kubecm_0.0.1_Darwin_x86_64.tar.gz&amp;quot;
  version &amp;quot;0.0.1&amp;quot;
  sha256 &amp;quot;f393b14f9c42c890b8a60949b13a8f9b5c3f814daa8b18901656ccc3b866f646&amp;quot;

  # depends_on &amp;quot;cmake&amp;quot; =&amp;gt; :build
  depends_on :x11 # if your formula requires any X11/XQuartz components

  def install
    # ENV.deparallelize  # if your formula fails when building in parallel

    # Remove unrecognized options if warned by configure
    system &amp;quot;./configure&amp;quot;, &amp;quot;--disable-debug&amp;quot;,
                          &amp;quot;--disable-dependency-tracking&amp;quot;,
                          &amp;quot;--disable-silent-rules&amp;quot;,
                          &amp;quot;--prefix=#{prefix}&amp;quot;
    # system &amp;quot;cmake&amp;quot;, &amp;quot;.&amp;quot;, *std_cmake_args
    system &amp;quot;make&amp;quot;, &amp;quot;install&amp;quot; # if this fails, try separate make/make install steps
  end

  test do
    # `test do` will create, run in and delete a temporary directory.
    #
    # This test will fail and we won&#39;t accept that! It&#39;s enough to just replace
    # &amp;quot;false&amp;quot; with the main program this formula installs, but it&#39;d be nice if you
    # were more thorough. Run the test with `brew test otfcc-win32`. Options passed
    # to `brew install` such as `--HEAD` also need to be provided to `brew test`.
    #
    # The installed folder is not in the path, so use the entire path to any
    # executables being tested: `system &amp;quot;#{bin}/program&amp;quot;, &amp;quot;do&amp;quot;, &amp;quot;something&amp;quot;`.
    system &amp;quot;false&amp;quot;
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认提供的脚本并不适合我们，修改这个脚本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ruby&#34;&gt;class Kubecm &amp;lt; Formula
  desc &amp;quot;Merge multiple kubeconfig&amp;quot;
  homepage &amp;quot;https://github.com/sunny0826/kubecm&amp;quot;
  url &amp;quot;https://github.com/sunny0826/kubecm/releases/download/v0.0.1/kubecm_0.0.1_Darwin_x86_64.tar.gz&amp;quot;
  version &amp;quot;0.0.1&amp;quot;
  sha256 &amp;quot;8c2766e7720049ba0ce9e3d20b7511796a6ba224ce1386cd1d4ef8cc6e1315cd&amp;quot;
  # depends_on &amp;quot;cmake&amp;quot; =&amp;gt; :build

  def install
    bin.install &amp;quot;kubecm&amp;quot;
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分布填上 &lt;code&gt;desc&lt;/code&gt; 、 &lt;code&gt;homepage&lt;/code&gt; 、&lt;code&gt;url&lt;/code&gt; 等信息，由于这里下载的是 darwin 包，所以直接在 &lt;code&gt;install&lt;/code&gt; 中填上 &lt;code&gt;bin.install &amp;quot;kubecm&amp;quot;&lt;/code&gt; 即可。&lt;/p&gt;

&lt;h3 id=&#34;本地测试&#34;&gt;本地测试&lt;/h3&gt;

&lt;p&gt;保存脚本，然后使用 &lt;code&gt;brew install kubecm&lt;/code&gt; 进行测试，查看结果：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Updating Homebrew...
Fast-forwarded master to origin/master.
Fast-forwarded master to origin/master.
==&amp;gt; Auto-updated Homebrew!
Updated 2 taps (sunny0826/tap, homebrew/cask).
==&amp;gt; Updated Formulae
sunny0826/tap/kubecm

==&amp;gt; Downloading https://github.com/sunny0826/kubecm/releases/download/v0.0.1/kubecm_0.0.1_Darwin_x86_64.tar.gz
Already downloaded: /Users/guoxudong/Library/Caches/Homebrew/kubecm-86.64.tar.gz
🍺  /usr/local/Cellar/kubecm/86.64: 5 files, 5.4MB, built in 1 second
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到已经安装成功了！&lt;/p&gt;

&lt;h3 id=&#34;创建-tap&#34;&gt;创建 Tap&lt;/h3&gt;

&lt;p&gt;在本地测试成功之后，就可以把他发布了。这里需要在 GitHub 上创建一个名为 &lt;code&gt;homebrew-tap&lt;/code&gt; 的 repo 注意该 repo 需要以 &lt;code&gt;homebrew-&lt;/code&gt; 为前缀，像是&lt;a href=&#34;https://github.com/sunny0826/homebrew-tap&#34; target=&#34;_blank&#34;&gt;这样&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;然后将刚才的 &lt;code&gt;kubecm.rb&lt;/code&gt; 脚本上传到这个 repo ，然后就可以通过 &lt;code&gt;brew&lt;/code&gt; 的方式安装了。&lt;/p&gt;

&lt;h3 id=&#34;实际测试&#34;&gt;实际测试&lt;/h3&gt;

&lt;p&gt;发布好之后，就可以测试发布成功没有了。&lt;/p&gt;

&lt;p&gt;首先卸载之前使用本地脚本安装的应用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ brew uninstall kubecm
Uninstalling /usr/local/Cellar/kubecm/86.64... (5 files, 5.4MB)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后使用&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew tap sunny0826/tap &amp;amp;&amp;amp; brew install kubecm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install sunny0826/tap/kubecm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;来进行安装。&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;到这我们就成功的在 Homebrew 上发布了自己的 Golang 应用，本篇中的方法仅适合 Golang 开发的二进制可执行文件的发布，其他语言的发布需要在 &lt;code&gt;.rb&lt;/code&gt; 脚本上有所修改，更多内容请参考&lt;a href=&#34;https://docs.brew.sh/&#34; target=&#34;_blank&#34;&gt;官方文档&lt;/a&gt;。这里要再介绍一下我用 Golang 开发的另一个小工具 &lt;a href=&#34;https://github.com/sunny0826/kubecm&#34; target=&#34;_blank&#34;&gt;kubecm&lt;/a&gt; ，该项目之前我是使用 python 开发的，用于合并多个 kubeconfig 文件，本次重写新增了查看所有 kubeconfig 和 删除 kubeconfig 中 context 等功能，同时也在 Homebrew 上发布，欢迎拍砖。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Golang 装逼指南：在 GitHub 上构建一个看上去正规的 Golang 项目</title>
      <link>https://guoxudong.io/en/post/golang-project/</link>
      <pubDate>Fri, 19 Jul 2019 10:38:26 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/golang-project/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;接触 golang 时间很长，但是真正动手开始写 golang 也就是在最近。虽然写的不多，但是见过的 golang 项目可是不计其数，从 &lt;a href=&#34;https://github.com/kubernetes/kubernetes&#34; target=&#34;_blank&#34;&gt;Kubernetes&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/istio/istio&#34; target=&#34;_blank&#34;&gt;istio&lt;/a&gt; 到亲身参与的 &lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize&#34; target=&#34;_blank&#34;&gt;kustomize&lt;/a&gt; 再到 Kubernetes 生态圈的众多小工具，比如： &lt;a href=&#34;https://github.com/instrumenta/kubeval&#34; target=&#34;_blank&#34;&gt;kubeval&lt;/a&gt; 、 &lt;a href=&#34;https://github.com/flant/kubedog&#34; target=&#34;_blank&#34;&gt;kubedog&lt;/a&gt; 等。从项目使用者和贡献者的角度接触了各种形形色色的 golang 项目。作为一个开发人员，在享受各种开源项目带来便利的同时，也希望自己动手开发一个 golang 项目。以我阅项目无数的经验，那么肯定要构建一个看上去正规的 GitHub 项目。&lt;/p&gt;

&lt;h2 id=&#34;goland-设置&#34;&gt;GoLand 设置&lt;/h2&gt;

&lt;p&gt;Go 开发环境的安装网上教程很多，这里就不做介绍了。这里主要介绍一下在 GoLand 上开发环境的设置，这里的设置主要在 MacOS 上进行，其他系统可能有所不同。&lt;/p&gt;

&lt;h3 id=&#34;使用goland-ide-vgo&#34;&gt;使用Goland IDE vgo&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;vgo&lt;/code&gt; 是基于 Go Module 规范的包管理工具，同官方的 go mod 命令工具类似。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;开启 &lt;code&gt;vgo&lt;/code&gt;，&lt;code&gt;GoLand&lt;/code&gt;-&amp;gt;&lt;code&gt;Preferences&lt;/code&gt;-&amp;gt;&lt;code&gt;GO&lt;/code&gt;-&amp;gt;&lt;code&gt;Go Modules(vgo)&lt;/code&gt;
&lt;img src=&#34;https://ws2.sinaimg.cn/large/ad5fbf65gy1g556yudwh8j20s20jhgn4.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;手动修改 &lt;code&gt;go.mod&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;其中 latest 为最新版本，GoLand 会去下载最新依赖代码，下载成功后会修改 &lt;code&gt;go.mod&lt;/code&gt; 并且生成 &lt;code&gt;go.sum&lt;/code&gt; 依赖分析文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;module github.com/sunny0826/hamal

go 1.12

require (
    github.com/mitchellh/go-homedir latest
    github.com/spf13/cobra latest
    github.com/spf13/viper latest
)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;更新成功&lt;/p&gt;

&lt;p&gt;在更新成功后，会生成 &lt;code&gt;go.sum&lt;/code&gt; 文件并修改 &lt;code&gt;go.mod&lt;/code&gt; 文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;module github.com/sunny0826/hamal

go 1.12

require (
    github.com/mitchellh/go-homedir v1.1.0
    github.com/spf13/cobra v0.0.5
    github.com/spf13/viper v1.4.0
)

&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用快捷键 &lt;code&gt;⌥(option)+↩(return)&lt;/code&gt; 或者点击鼠标右键, 选择 &lt;code&gt;Sync packages of github.com/sunny0826/hamal&lt;/code&gt; 在 import 处导入依赖。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;配置代理&#34;&gt;配置代理&lt;/h3&gt;

&lt;p&gt;如果要选出 golang 最劝退一个原因，那么依赖下载难肯定得票最高！这个时候一个合适的梯子就很重要了，如果没有这个梯子，上面的这步就完全无法完成。这里主要介绍 GoLand 上的配置，Shadowsocks 的安装和配置就不做介绍了。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;GoLand&lt;/code&gt;-&amp;gt;&lt;code&gt;Preferences&lt;/code&gt;-&amp;gt;&lt;code&gt;Appearance &amp;amp; Behavior&lt;/code&gt;-&amp;gt;&lt;code&gt;System Settings&lt;/code&gt;-&amp;gt;&lt;code&gt;HTTP Proxy&lt;/code&gt; 这里设置好之后，别忘了点击 &lt;code&gt;Check connection&lt;/code&gt; 测试一下梯子搭成没有。
&lt;img src=&#34;https://ws4.sinaimg.cn/large/ad5fbf65gy1g557j6it07j20s20je40p.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;配置-go-fmt-goimports-和-golangci-lint&#34;&gt;配置 &lt;code&gt;go fmt&lt;/code&gt;、 &lt;code&gt;goimports&lt;/code&gt; 和 &lt;code&gt;golangci-lint&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;这三个工具都是 GoLand 自带的，设置起来十分简单:&lt;code&gt;GoLand&lt;/code&gt;-&amp;gt;&lt;code&gt;Preferences&lt;/code&gt;-&amp;gt;&lt;code&gt;Tools&lt;/code&gt;-&amp;gt;&lt;code&gt;File Watchers&lt;/code&gt;，点击添加即可。之后在写完代码之后就会自动触发这3个工具的自动检测，工具作用：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;go fmt&lt;/code&gt; : 统一的代码格式化工具。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;golangci-lint&lt;/code&gt; : 静态代码质量检测工具，用于包的质量分析。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;goimports&lt;/code&gt; : 自动 import 依赖包工具。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://ws3.sinaimg.cn/large/ad5fbf65gy1g557ps83gsj20s30njtbs.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;安装配置-golint&#34;&gt;安装配置 &lt;code&gt;golint&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;GoLand 没有自带 &lt;code&gt;golint&lt;/code&gt; 工具，需要手动安装：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p $GOPATH/src/golang.org/x/
cd $GOPATH/src/golang.org/x/
git clone https://github.com/golang/lint.git
git clone https://github.com/golang/tools.git
cd $GOPATH/src/golang.org/x/lint/golint
go install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装成功之后将会在 &lt;code&gt;$GOPATH/bin&lt;/code&gt; 目录下看到自动生成了 &lt;code&gt;golint&lt;/code&gt; 二进制工具文件。&lt;/p&gt;

&lt;p&gt;GoLand 配置 &lt;code&gt;golint&lt;/code&gt;，修改 &lt;code&gt;Name&lt;/code&gt;, &lt;code&gt;Program&lt;/code&gt;, &lt;code&gt;Arguments&lt;/code&gt; 三项配置，其中 &lt;code&gt;Arguments&lt;/code&gt; 需要加上 &lt;code&gt;-set_exit_status&lt;/code&gt; 参数，如图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g557z8a5jgj20ln0i0t9z.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;travis-ci-持续集成&#34;&gt;Travis CI 持续集成&lt;/h2&gt;

&lt;p&gt;在 Github 上装逼怎么能少的了 Travis CI ，直接登录 &lt;a href=&#34;https://travis-ci.org/&#34; target=&#34;_blank&#34;&gt;Travis CI&lt;/a&gt;，使用 GitHub 登录，然后选择需要使用 Travis CI 的项目，在项目根目录添加 &lt;code&gt;.travis.yml&lt;/code&gt; ，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;language: go

go:
  - 1.12.5

sudo: required

install:
  - echo &amp;quot;install&amp;quot;

script:
  - echo &amp;quot;script&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里只是一个示例，在每次 push 代码之后，都会触发 CI，具体语法可以参看&lt;a href=&#34;https://docs.travis-ci.com/&#34; target=&#34;_blank&#34;&gt;官方文档&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;装逼重点：&lt;/strong&gt; 你以为使用 Travis CI 就是为了持续集成吗？那就太天真了！使用 Travis CI 当然为了他的 Badges ，将 &lt;code&gt;RESULT&lt;/code&gt; 拷贝到你的 &lt;code&gt;README.md&lt;/code&gt; 里面就好了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws2.sinaimg.cn/large/ad5fbf65gy1g558xf6io4j22dk15an4t.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;go-report-card&#34;&gt;GO Report Card&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;又一装逼重点&lt;/strong&gt;：我们在 GoLand 上安装了 &lt;code&gt;golint&lt;/code&gt; 等工具进行代码质量检测，在撸码的时候就能进行代码检查，那么这个就是为了纯装逼了。&lt;a href=&#34;https://goreportcard.com/&#34; target=&#34;_blank&#34;&gt;GO Report Card&lt;/a&gt; 是一个 golang 代码检测网站，你只需把 Github 地址填上去即可。获取 Badges 的方法和 Travis CI 类似，将 MarkDown 中的内容拷贝到 &lt;code&gt;RERADME.md&lt;/code&gt; 中就好。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws3.sinaimg.cn/large/ad5fbf65gy1g559flsl3xj21t410ok1a.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;goreleaser&#34;&gt;GoReleaser&lt;/h2&gt;

&lt;p&gt;持续集成有了，代码检查也有了，再下面就是怎么发布一个漂亮的 release 了。如果还在手动发布 release ，那么就又掉 low 了。使用 GoReleaser 一行命令来发布一个漂亮的 release 吧。&lt;/p&gt;

&lt;p&gt;由于使用的的 MacOS ，这里使用 &lt;code&gt;brew&lt;/code&gt; 来安装：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install goreleaser
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在项目根目录生成 &lt;code&gt;.goreleaser.yml&lt;/code&gt; 配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;goreleaser init
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置好了以后要记得往 &lt;code&gt;.gitignore&lt;/code&gt; 加上 &lt;code&gt;dist&lt;/code&gt;，因为 goreleaser 会默认把编译编译好的文件输出到 &lt;code&gt;dist&lt;/code&gt; 目录中。&lt;/p&gt;

&lt;p&gt;goreleaser 配置好后，可以先编译测试一下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;goreleaser --skip-validate --skip-publish --snapshot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt; 首次使用 goreleaser 要配置 GITHUB_TOKEN ，可以在&lt;a href=&#34;https://github.com/settings/tokens/new&#34; target=&#34;_blank&#34;&gt;这里&lt;/a&gt;申请，申请好之后运行下面的命令配置&lt;code&gt;GITHUB_TOKEN&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export GITHUB_TOKEN=&amp;lt;YOUR_TOKEN&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;确保没有问题，那么就可以操作 git 和 goreleaser 来发布 release 了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git add .
git commit -m &amp;quot;add goreleaser&amp;quot;
git tag -a v0.0.3 -m &amp;quot;First release&amp;quot;
git push origin master
git push origin v0.0.3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;全部搞定后，一行命令起飞：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;goreleaser
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;goreleaser&lt;/code&gt; 配合 CI 食用，效果更佳，这里就不做介绍了。
&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g55a7t8bq4j20sq0liacm.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;badges-展示神器&#34;&gt;Badges 展示神器&lt;/h2&gt;

&lt;p&gt;这里介绍一个展示 Badges 的神器：&lt;a href=&#34;https://shields.io/&#34; target=&#34;_blank&#34;&gt;https://shields.io/&lt;/a&gt; 。这个网站提供各种各样的 Badges ，如果你愿意，完全可以把你的 GitHub README.md 填满，有兴趣的同学可以自取。
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g55aendhrwj22fg19igz0.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;到这里可以在 GitHub 上装逼的 golang 配置已经介绍的差不多了，其实还有 &lt;a href=&#34;https://codecov.io/&#34; target=&#34;_blank&#34;&gt;Codecov&lt;/a&gt;、&lt;a href=&#34;https://circleci.com/&#34; target=&#34;_blank&#34;&gt;CircleCI&lt;/a&gt; 等工具，这里就不做介绍了。这里要介绍的是我们的第一个 golang 项目 &lt;a href=&#34;https://github.com/sunny0826/hamal&#34; target=&#34;_blank&#34;&gt;Hamal&lt;/a&gt;，该项目是一个命令行工具，用来在不同的镜像仓库之间同步镜像。由于我司推行混合云，使用了阿里云与华为云，而在阿里云或华为云环境互相推镜像的时候时间都比较长，所以开发这个小工具用于在办公网络镜像同步，同时也可以用来将我在 dockerhub 上托管的镜像同步到我们的私有仓库，欢迎拍砖。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GitOps 与 ChatOps 的落地实践</title>
      <link>https://guoxudong.io/en/post/gitops-and-chatops/</link>
      <pubDate>Thu, 11 Jul 2019 09:24:17 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/gitops-and-chatops/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;说到 GitOps 和 ChatOps ，那就不得不谈到 DevOps 。 DevOps 作为一种文化，旨在促进开发、测试和运维人员之间的沟通与协作。而促进合作的方式，往往是使用一系列工具，完成这三个角色的相互协作。这带来的好处也是显而易见的：更快的交付速度和更低的人力成本。获益于 DevOps 和公有云，一个近百人的研发团队，可以只配备一到两个专职运维人员，降低的成本不言而喻。既然 DevOps 是一种文化，那么在不同的团队则会有不同的实践，而无论实践如何，其最终目的都是一样的：最大化的实现自动化，释放更多的人力资源，创建更大价值。&lt;/p&gt;

&lt;p&gt;而 GitOps 和 ChatOps ，则是 DevOps 的两种实践。这两种实践分别通过使用 &lt;code&gt;版本控制软件 Git&lt;/code&gt; 和&lt;code&gt;实时聊天软件&lt;/code&gt;来达到提升交付速度和研发效率的目的。&lt;/p&gt;

&lt;h2 id=&#34;gitops&#34;&gt;GitOps&lt;/h2&gt;

&lt;p&gt;GitOps 是一种实现持续交付的模型，它的核心思想是将应用系统的声明性基础架构和应用程序存放在 Git 的版本控制库中。&lt;/p&gt;

&lt;p&gt;将 Git 作为交付流水线的核心，每个开发人员都可以提交拉取请求（Pull Request）并使用 Gi​​t 来加速和简化 Kubernetes 的应用程序部署和运维任务。通过使用像 Git 这样的简单熟悉工具，开发人员可以更高效地将注意力集中在创建新功能而不是运维相关任务上。&lt;/p&gt;

&lt;p&gt;通过应用 GitOps ，应用系统的基础架构和应用程序代码可以快速查找来源——基础架构和应用程序代码都存放在 gitlab 、或者 github 等版本控制系统上。这使开发团队可以提高开发和部署速度并提高应用系统可靠性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g4vpmjzylfj20qy09tq4b.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;将 GitOps 应用在持续交付流水线上，有诸多优势和特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;安全的云原生 CI/CD 管道模型&lt;/li&gt;
&lt;li&gt;更快的平均部署时间和平均恢复时间&lt;/li&gt;
&lt;li&gt;稳定且可重现的回滚（例如，根据Git恢复/回滚/ fork）&lt;/li&gt;
&lt;li&gt;与监控和可视化工具相结合，对已经部署的应用进行全方位的监控&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在我看来 GitOps 的最大优势就是通过完善的 git 分支管理来达到管理所有 CI/CD 管道流水线的目的，不同的环境可以对应不同分支，在该环境出现问题时候，可以直接查找对应分支代码，达到快速排查问题的目的。而对于 Git 的熟悉，更是省去学习使用一般 DevOps 工具所需的学习成本和配置时间，开发人员可以无任何培训直接上手使用，进一步降低了时间与人力成本。&lt;/p&gt;

&lt;h2 id=&#34;chatops&#34;&gt;ChatOps&lt;/h2&gt;

&lt;p&gt;ChatOps 以聊天室（聊天群），即实时聊天软件为中心，通过一系列的机器人去对接后台的各种服务，开发&amp;amp;测试&amp;amp;运维人员只需要在聊天窗口中与机器人对话，即可与后台服务进行交互，整个工作的展开就像是使唤一个智能助手那样简单自然。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx1.sinaimg.cn/large/ad5fbf65gy1g4vr2yialfj20rp0bbmyd.jpg&#34; alt=&#34;ChatOps&#34; /&gt;&lt;/p&gt;

&lt;p&gt;ChatOps 带来了很多好处：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;公开透明。所有的工作消息都在同一个聊天平台中沉淀并公开给所有相关成员，消除沟通壁垒，工作历史有迹可循，团队合作更加顺畅。&lt;/li&gt;
&lt;li&gt;上下文共享。减少因工作台切换等对消息的截断，保证消息的完整性，让工作承接有序，各角色，各工具都成为完成工作流中的一环，打造真正流畅的工作体验。&lt;/li&gt;
&lt;li&gt;移动友好。只需要在前台与预设好的机器人对话即可完成与后台工具、系统的交互，在移动环境下无需再与众多复杂的工具直接对接，大大提升移动办公的可行性。&lt;/li&gt;
&lt;li&gt;DevOps 文化打造。用与机器人对话这种简单的方式降低 DevOps 的接受门槛，让这种自动化办公的理念更容易的扩展到团队的每一个角落。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于 ChatOps 的理解最早要源于在 GitHub 上参与开源项目的一些经历，在向 Kubernetes 相关项目提交 PR 时，会有一个名叫 &lt;code&gt;k8s-ci-robot&lt;/code&gt; 的小机器人来自动为该 RP 打上标签，并且根据你提交 PR 时的 comment 信息来为你分配 Reviewers，如果没有填的话，则会自动为你分配 Reviewers 等功能。同时可以在 comment 中输入命令，还可以进行其他的操作，详见：&lt;a href=&#34;https://prow.k8s.io/command-help&#34; target=&#34;_blank&#34;&gt;命令列表&lt;/a&gt;。而其实这个机器人的后端就是名为 &lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/prow#bots-home&#34; target=&#34;_blank&#34;&gt;Prow&lt;/a&gt; 的由 Google 发起的适应云原生 CI/CD 开源项目，有兴趣的话推荐阅读：&lt;a href=&#34;https://www.servicemesher.com/blog/prow-quick-start-guide/&#34; target=&#34;_blank&#34;&gt;Prow 快速入门向导&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;而一篇名为：&lt;a href=&#34;https://wanqu.co/b/7/%E6%B9%BE%E5%8C%BA%E6%97%A5%E6%8A%A5%E6%98%AF%E5%A6%82%E4%BD%95%E8%BF%90%E4%BD%9C%E7%9A%84/&#34; target=&#34;_blank&#34;&gt;《湾区日报是如何运作的？》&lt;/a&gt; 文章更是让我坚定信心开始开发自己 ChatOps 系统。该文章介绍作者是怎么运营一个名叫湾区日报的个人博客，这个博客通过11个渠道（网站，iOS app，Android app、微博，微信，Twitter，Chrome 浏览器推送、Facebook、邮件订阅、RSS、Telegram）推荐给读者，而这个11个渠道的发布都是通过 slack 和作者开发的小机器人完成。在我还在为使用脚本可以在多渠道发布个人技术博客而沾沾自喜的时候，人家早在多年前就开始使用 ChatOps 模式向多渠道使用多格式自动推送文章了。这也坚定了我开发我们自己的 ChatOps 系统的决心。&lt;/p&gt;

&lt;h2 id=&#34;gitops-chatops-的实践&#34;&gt;GitOps &amp;amp; ChatOps 的实践&lt;/h2&gt;

&lt;h3 id=&#34;使用-drone-实现-gitops&#34;&gt;使用 Drone 实现 GitOps&lt;/h3&gt;

&lt;p&gt;DevOps 文化早已在我司落地，这也是为什么我们有将近百人的研发团队，却只有两个专职运维的原因。CI/CD 方面我们之前使用的是 jenkins ， jenkins 是一个十分强大的工具，但是随着公司的发展，项目也越来越多，粗略统计了一下我们在 jenkins 中有几百个 Job ，虽然所有项目都使用 Jenkinsfile 的方式将 pipeline 持久化到了 gitlab 中，但是所有的 Job 配置，包括参数化构建配置，SCM 配置等都是保存在 jenkins 上，一旦有失，几百个 Job &amp;hellip;哭都没有地方哭去（别问我是怎么知道的）。&lt;/p&gt;

&lt;p&gt;经过调研我们选择了 &lt;a href=&#34;https://drone.io/&#34; target=&#34;_blank&#34;&gt;drone CI&lt;/a&gt; 进行 GitOps ，通过自己开发不同功能的插件，完善了我们的整个 CI/CD 流水线。而插件的开发也并不是从头开始，而是直接 fork 现有的插件进行定制化的二次开发，有兴趣的可以到我的 &lt;a href=&#34;https://github.com/sunny0826/drone-dingtalk-message&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;   和 &lt;a href=&#34;https://cloud.docker.com/repository/list&#34; target=&#34;_blank&#34;&gt;DockerHub&lt;/a&gt; 上查看。&lt;/p&gt;

&lt;p&gt;将项目配置进行了分离，配置使用单独的 git 仓库维护，同时整合了镜像安全扫描，钉钉通知等功能。
&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g4vvfow9w9j21k810243r.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;由于 drone CI 的配置文件 &lt;code&gt;.drone.yml&lt;/code&gt; 需要统一规范，所以我们在自己的 DevOps 平台开发了 Drone 配置页面，帮助开发自主配置。我们提供了 &lt;code&gt;Java&lt;/code&gt; 、 &lt;code&gt;Node&lt;/code&gt; 、 &lt;code&gt;Python&lt;/code&gt; 三种配置模板，并且由于 DevOps 平台已与 GitLab 集成，可以直接将生成的 &lt;code&gt;.drone.yml&lt;/code&gt; 文件插入到相应 git 项目中。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx3.sinaimg.cn/large/ad5fbf65gy1g4vvoqggfoj22lk16aagu.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;同时也提供了钉钉构建通知，在构建成功后会发送到相应的开发群组中，如果需希望自动发布的话，也可点击通知中的连接自行发布。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g4vvrigyvlj20ec0bh40u.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;自研平台配合钉钉-outgoing-功能实现-chatops&#34;&gt;自研平台配合钉钉 Outgoing 功能实现 ChatOps&lt;/h3&gt;

&lt;p&gt;前面的构建通知机器人使用的是钉钉的&lt;a href=&#34;https://open-doc.dingtalk.com/microapp/serverapi2/qf2nxq&#34; target=&#34;_blank&#34;&gt;自定义机器人&lt;/a&gt;，将构建信息推送到各个项目群中。而钉钉机器人的 Outgoing 功能，则可用来实现 ChatOps 的功能（&lt;strong&gt;注意：钉钉的 Outgoing 功能目前还处于灰度测试阶段，想要使用的需要联系官方管理员开启该功能&lt;/strong&gt;）。&lt;/p&gt;

&lt;p&gt;由于我司专职运维人员只有两位，管理着整个团队全部的基础设施。但是随着开发团队的扩张，运维人员每天要处理大量的咨询类工作，而这类工作有着重复性强和技术性弱的特点，对于运维人员的技术水平毫无提升，那么这类工作交给机器人岂不是更好。得益于我们 DevOps 平台完善的 API ，小助手机器人的开发并不困难。&lt;/p&gt;

&lt;p&gt;小助手机器人的诞生，极大的提高了咨询类工作的效率，同时也释放了运维人员的工作时间，运维人员可以将更多精力投注到更有技术含量的事情上。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g4vwfdgd2xj20iu0ajwgc.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;小助手机器人还有运维版本，功能包括：批量操作虚拟机、重启服务、DNS 解析、Kubernetes 信息检测&amp;amp;操作等功能，由于还是测试版本，这里就不做详细介绍了。&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;上文中简要的介绍了 GitOps 和 ChatOps 在我司的落地实践，从决定落地 GitOps 和 ChatOps 至今不过短短的2个月。得益于我司浓厚的 DevOps 文化氛围，让我可以在极短的时间内将 GitOps 和 ChatOps 落地实践。但毕竟实践的时间还短，很多需求还在收集和调研中，后续的开发还在持续进行。欢迎对 GitOps 和 ChatOps 感兴趣的同学一起交流，共同提升。&lt;/p&gt;

&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.weave.works/technologies/gitops/&#34; target=&#34;_blank&#34;&gt;GitOps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://bearyinnovative.com/salon-chatops/&#34; target=&#34;_blank&#34;&gt;DevOps 理念升级，ChatOps 概述及实践经验&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>阿里云 ACK 挂载 NAS 数据卷</title>
      <link>https://guoxudong.io/en/post/nas-k8s/</link>
      <pubDate>Mon, 08 Jul 2019 15:09:56 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/nas-k8s/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;今天接到一个将 NAS 数据卷挂载到 Kubernetes 集群的需求，需要将一个 NAS 数据卷挂载到集群中。这一很简单的操作由于好久没有操作了，去翻看了一下官方文档，发现官方文档还在停留在去年7月份&amp;hellip;为了防止之后还有相似情况的发生，这里将所有操作做一个简单记录。&lt;/p&gt;

&lt;h2 id=&#34;购买存储包-创建文件系统&#34;&gt;购买存储包（创建文件系统）&lt;/h2&gt;

&lt;p&gt;在挂载 NAS 之前，首先要先购买 NAS 文件存储，这里推荐购买存储包，100G 的 SSD 急速型一年只需1400多，而容量型只要279，对于我这种只有少量 NAS 存储需求的人来说是是靠谱的，因为我只需要5G的左右的存储空间，SSD 急速型 NAS 一年只要18块，完美。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g4sglwrx0gj22wa09gae4.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;选择想要创建 NAS 所在 VPC 和 区域&lt;/p&gt;

&lt;h2 id=&#34;添加挂载点&#34;&gt;添加挂载点&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;点击添加挂载点
&lt;img src=&#34;https://wx3.sinaimg.cn/large/ad5fbf65gy1g4sgp0dos2j22ky0iowkr.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;选择 VPC 网络、交换机和权限组
&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g4sgpwqrgoj20xu0vowib.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;linux-挂载-nas-数据卷&#34;&gt;Linux 挂载 NAS 数据卷&lt;/h2&gt;

&lt;p&gt;在挂载点创建成功后，就可以将 NAS 数据卷挂载到 Linux 系统，这里以 CentOS 为例：&lt;/p&gt;

&lt;h3 id=&#34;安装-nfs-客户端&#34;&gt;安装 NFS 客户端&lt;/h3&gt;

&lt;p&gt;如果 Linux 系统要挂载 NAS ，首先需要安装 NFS 客户端&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo yum install nfs-utils
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;挂载-nfs-文件系统&#34;&gt;挂载 NFS 文件系统&lt;/h3&gt;

&lt;p&gt;这里阿里云早就进行了优化，点击创建的文件系统，页面上就可以 copy 挂载命令。页面提供了挂载地址的 copy 和挂载命令的 copy 功能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g4sh2i33wnj22w40yyn55.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;挂载命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo mount -t nfs -o vers=4,minorversion=0,noresvport xxxxx.cn-shanghai.nas.aliyuncs.com:/ /mnt
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;查看挂载结果&#34;&gt;查看挂载结果&lt;/h3&gt;

&lt;p&gt;直接在挂载数据卷所在服务上执行命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;df -h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就可以看到结果：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g4sh6xwyt8j20lj0850tq.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-集群挂载-nas-数据卷&#34;&gt;Kubernetes 集群挂载 NAS 数据卷&lt;/h2&gt;

&lt;p&gt;K8S 的持久数据卷挂载大同小异，流程都是：&lt;strong&gt;创建PV&lt;/strong&gt; -&amp;gt; &lt;strong&gt;创建PVC&lt;/strong&gt; -&amp;gt; &lt;strong&gt;使用PVC&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;下面就简单介绍在阿里云上的操作：&lt;/p&gt;

&lt;h3 id=&#34;创建存储卷-pv&#34;&gt;创建存储卷（PV）&lt;/h3&gt;

&lt;p&gt;首先要创建存储卷，选择 &lt;strong&gt;容器服务&lt;/strong&gt; -&amp;gt; &lt;strong&gt;存储卷&lt;/strong&gt; -&amp;gt; &lt;strong&gt;创建&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这里要注意的是：&lt;strong&gt;挂载点域名使用上面面的挂载地址&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g4shuiiyyqj20hc0hp0tz.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;创建存储声明-pvc&#34;&gt;创建存储声明（PVC）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;选择 NAS&lt;/strong&gt; -&amp;gt; &lt;strong&gt;已有存储卷&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;选择刚才创建的存储卷&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws4.sinaimg.cn/large/ad5fbf65gy1g4shv5vs1kj20hx0bvt9g.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;使用pvc&#34;&gt;使用PVC&lt;/h3&gt;

&lt;p&gt;使用的方法这里就不做详细介绍了，相关文章也比较多，这里就只记录 Deployment 中使用的 yaml 片段：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;...
volumeMounts:
- mountPath: /data      # 挂载路径
    name: volume-nas-test
...
volumes:
- name: volume-nas-test
persistentVolumeClaim:
    claimName: nas-test     # PVC 名称
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;这里只是做一个简单的记录，仅适用于阿里云 ACK 容器服务，同时也是 ACK 的一个简单应用。由于不经常对数据卷进行操作，这里做简单的记录，防止以后使用还要再看一遍文档。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>语义化版本控制规范（SemVer）</title>
      <link>https://guoxudong.io/en/post/semver/</link>
      <pubDate>Sat, 06 Jul 2019 09:40:42 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/semver/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;原文地址：&lt;a href=&#34;https://semver.org/lang/zh-CN/&#34; target=&#34;_blank&#34;&gt;https://semver.org/lang/zh-CN/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;语义化版本 2.0.0&lt;/p&gt;

&lt;p&gt;提供多种语言，语义化版本控制的规范是由 Gravatars 创办者兼 GitHub 共同创办者 &lt;a href=&#34;http://tom.preston-werner.com/&#34; target=&#34;_blank&#34;&gt;Tom Preston-Werner&lt;/a&gt; 所建立。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;摘要&#34;&gt;摘要&lt;/h2&gt;

&lt;p&gt;版本格式：主版本号.次版本号.修订号，版本号递增规则如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;主版本号：当你做了不兼容的 API 修改，&lt;/li&gt;
&lt;li&gt;次版本号：当你做了向下兼容的功能性新增，&lt;/li&gt;
&lt;li&gt;修订号：当你做了向下兼容的问题修正。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;先行版本号及版本编译元数据可以加到“主版本号.次版本号.修订号”的后面，作为延伸。&lt;/p&gt;

&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;

&lt;p&gt;在软件管理的领域里存在着被称作“依赖地狱”的死亡之谷，系统规模越大，加入的包越多，你就越有可能在未来的某一天发现自己已深陷绝望之中。&lt;/p&gt;

&lt;p&gt;在依赖高的系统中发布新版本包可能很快会成为噩梦。如果依赖关系过高，可能面临版本控制被锁死的风险（必须对每一个依赖包改版才能完成某次升级）。而如果依赖关系过于松散，又将无法避免版本的混乱（假设兼容于未来的多个版本已超出了合理数量）。当你专案的进展因为版本依赖被锁死或版本混乱变得不够简便和可靠，就意味着你正处于依赖地狱之中。&lt;/p&gt;

&lt;p&gt;作为这个问题的解决方案之一，我提议用一组简单的规则及条件来约束版本号的配置和增长。这些规则是根据（但不局限于）已经被各种封闭、开放源码软件所广泛使用的惯例所设计。为了让这套理论运作，你必须先有定义好的公共 API 。这可以透过文件定义或代码强制要求来实现。无论如何，这套 API 的清楚明了是十分重要的。一旦你定义了公共 API，你就可以透过修改相应的版本号来向大家说明你的修改。考虑使用这样的版本号格式：X.Y.Z （主版本号.次版本号.修订号）修复问题但不影响API 时，递增修订号；API 保持向下兼容的新增及修改时，递增次版本号；进行不向下兼容的修改时，递增主版本号。&lt;/p&gt;

&lt;p&gt;我称这套系统为“语义化的版本控制”，在这套约定下，版本号及其更新方式包含了相邻版本间的底层代码和修改内容的信息。&lt;/p&gt;

&lt;h2 id=&#34;语义化版本控制规范-semver&#34;&gt;语义化版本控制规范（SemVer）&lt;/h2&gt;

&lt;p&gt;以下关键词 MUST、MUST NOT、REQUIRED、SHALL、SHALL NOT、SHOULD、SHOULD NOT、 RECOMMENDED、MAY、OPTIONAL 依照 RFC 2119 的叙述解读。（译注：为了保持语句顺畅， 以下文件遇到的关键词将依照整句语义进行翻译，在此先不进行个别翻译。）&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;使用语义化版本控制的软件必须（MUST）定义公共 API。该 API 可以在代码中被定义或出现于严谨的文件内。无论何种形式都应该力求精确且完整。&lt;/li&gt;
&lt;li&gt;标准的版本号必须（MUST）采用 X.Y.Z 的格式，其中 X、Y 和 Z 为非负的整数，且禁止（MUST NOT）在数字前方补零。X 是主版本号、Y 是次版本号、而 Z 为修订号。每个元素必须（MUST）以数值来递增。例如：1.9.1 -&amp;gt; 1.10.0 -&amp;gt; 1.11.0。&lt;/li&gt;
&lt;li&gt;标记版本号的软件发行后，禁止（MUST NOT）改变该版本软件的内容。任何修改都必须（MUST）以新版本发行。&lt;/li&gt;
&lt;li&gt;主版本号为零（0.y.z）的软件处于开发初始阶段，一切都可能随时被改变。这样的公共 API 不应该被视为稳定版。&lt;/li&gt;
&lt;li&gt;1.0.0 的版本号用于界定公共 API 的形成。这一版本之后所有的版本号更新都基于公共 API 及其修改内容。&lt;/li&gt;
&lt;li&gt;修订号 Z（x.y.Z | x &amp;gt; 0）必须（MUST）在只做了向下兼容的修正时才递增。这里的修正指的是针对不正确结果而进行的内部修改。&lt;/li&gt;
&lt;li&gt;次版本号 Y（x.Y.z | x &amp;gt; 0）必须（MUST）在有向下兼容的新功能出现时递增。在任何公共 API 的功能被标记为弃用时也必须（MUST）递增。也可以（MAY）在内部程序有大量新功能或改进被加入时递增，其中可以（MAY）包括修订级别的改变。每当次版本号递增时，修订号必须（MUST）归零。&lt;/li&gt;
&lt;li&gt;主版本号 X（X.y.z | X &amp;gt; 0）必须（MUST）在有任何不兼容的修改被加入公共 API 时递增。其中可以（MAY）包括次版本号及修订级别的改变。每当主版本号递增时，次版本号和修订号必须（MUST）归零。&lt;/li&gt;
&lt;li&gt;先行版本号可以（MAY）被标注在修订版之后，先加上一个连接号再加上一连串以句点分隔的标识符来修饰。标识符必须（MUST）由 ASCII 字母数字和连接号 [0-9A-Za-z-] 组成，且禁止（MUST NOT）留白。数字型的标识符禁止（MUST NOT）在前方补零。先行版的优先级低于相关联的标准版本。被标上先行版本号则表示这个版本并非稳定而且可能无法满足预期的兼容性需求。范例：1.0.0-alpha、1.0.0-alpha.1、1.0.0-0.3.7、1.0.0-x.7.z.92。&lt;/li&gt;
&lt;li&gt;版本编译元数据可以（MAY）被标注在修订版或先行版本号之后，先加上一个加号再加上一连串以句点分隔的标识符来修饰。标识符必须（MUST）由 ASCII 字母数字和连接号 [0-9A-Za-z-] 组成，且禁止（MUST NOT）留白。当判断版本的优先层级时，版本编译元数据可（SHOULD）被忽略。因此当两个版本只有在版本编译元数据有差别时，属于相同的优先层级。范例：1.0.0-alpha+001、1.0.0+20130313144700、1.0.0-beta+exp.sha.5114f85。&lt;/li&gt;
&lt;li&gt;版本的优先层级指的是不同版本在排序时如何比较。判断优先层级时，必须（MUST）把版本依序拆分为主版本号、次版本号、修订号及先行版本号后进行比较（版本编译元数据不在这份比较的列表中）。由左到右依序比较每个标识符，第一个差异值用来决定优先层级：主版本号、次版本号及修订号以数值比较，例如：1.0.0 &amp;lt; 2.0.0 &amp;lt; 2.1.0 &amp;lt; 2.1.1。当主版本号、次版本号及修订号都相同时，改以优先层级比较低的先行版本号决定。例如：1.0.0-alpha &amp;lt; 1.0.0。有相同主版本号、次版本号及修订号的两个先行版本号，其优先层级必须（MUST）透过由左到右的每个被句点分隔的标识符来比较，直到找到一个差异值后决定：只有数字的标识符以数值高低比较，有字母或连接号时则逐字以 ASCII 的排序来比较。数字的标识符比非数字的标识符优先层级低。若开头的标识符都相同时，栏位比较多的先行版本号优先层级比较高。范例：1.0.0-alpha &amp;lt; 1.0.0-alpha.1 &amp;lt; 1.0.0-alpha.beta &amp;lt; 1.0.0-beta &amp;lt; 1.0.0-beta.2 &amp;lt; 1.0.0-beta.11 &amp;lt; 1.0.0-rc.1 &amp;lt; 1.0.0。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;为什么要使用语义化的版本控制&#34;&gt;为什么要使用语义化的版本控制？&lt;/h2&gt;

&lt;p&gt;这并不是一个新的或者革命性的想法。实际上，你可能已经在做一些近似的事情了。问题在于只是“近似”还不够。如果没有某个正式的规范可循，版本号对于依赖的管理并无实质意义。将上述的想法命名并给予清楚的定义，让你对软件使用者传达意向变得容易。一旦这些意向变得清楚，弹性（但又不会太弹性）的依赖规范就能达成。&lt;/p&gt;

&lt;p&gt;举个简单的例子就可以展示语义化的版本控制如何让依赖地狱成为过去。假设有个名为“救火车”的函式库，它需要另一个名为“梯子”并已经有使用语义化版本控制的包。当救火车创建时，梯子的版本号为 3.1.0。因为救火车使用了一些版本 3.1.0 所新增的功能， 你可以放心地指定依赖于梯子的版本号大等于 3.1.0 但小于 4.0.0。这样，当梯子版本 3.1.1 和 3.2.0 发布时，你可以将直接它们纳入你的包管理系统，因为它们能与原有依赖的软件兼容。&lt;/p&gt;

&lt;p&gt;作为一位负责任的开发者，你理当确保每次包升级的运作与版本号的表述一致。现实世界是复杂的，我们除了提高警觉外能做的不多。你所能做的就是让语义化的版本控制为你提供一个健全的方式来发行以及升级包，而无需推出新的依赖包，节省你的时间及烦恼。&lt;/p&gt;

&lt;p&gt;如果你对此认同，希望立即开始使用语义化版本控制，你只需声明你的函式库正在使用它并遵循这些规则就可以了。请在你的 README 文件中保留此页连结，让别人也知道这些规则并从中受益。&lt;/p&gt;

&lt;h2 id=&#34;faq&#34;&gt;FAQ&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;在 0.y.z 初始开发阶段，我该如何进行版本控制？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;最简单的做法是以 0.1.0 作为你的初始化开发版本，并在后续的每次发行时递增次版本号。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;如何判断发布 1.0.0 版本的时机？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;当你的软件被用于正式环境，它应该已经达到了 1.0.0 版。如果你已经有个稳定的 API 被使用者依赖，也会是 1.0.0 版。如果你很担心向下兼容的问题，也应该算是 1.0.0 版了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;这不会阻碍快速开发和迭代吗？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;主版本号为零的时候就是为了做快速开发。如果你每天都在改变 API，那么你应该仍在主版本号为零的阶段（0.y.z），或是正在下个主版本的独立开发分支中。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;对于公共 API，若即使是最小但不向下兼容的改变都需要产生新的主版本号，岂不是很快就达到 42.0.0 版？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这是开发的责任感和前瞻性的问题。不兼容的改变不应该轻易被加入到有许多依赖代码的软件中。升级所付出的代价可能是巨大的。要递增主版本号来发行不兼容的改版，意味着你必须为这些改变所带来的影响深思熟虑，并且评估所涉及的成本及效益比。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;为整个公共 API 写文件太费事了！&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;为供他人使用的软件编写适当的文件，是你作为一名专业开发者应尽的职责。保持专案高效一个非常重要的部份是掌控软件的复杂度，如果没有人知道如何使用你的软件或不知道哪些函数的调用是可靠的，要掌控复杂度会是困难的。长远来看，使用语义化版本控制以及对于公共 API 有良好规范的坚持，可以让每个人及每件事都运行顺畅。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;万一不小心把一个不兼容的改版当成了次版本号发行了该怎么办？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;一旦发现自己破坏了语义化版本控制的规范，就要修正这个问题，并发行一个新的次版本号来更正这个问题并且恢复向下兼容。即使是这种情况，也不能去修改已发行的版本。可以的话，将有问题的版本号记录到文件中，告诉使用者问题所在，让他们能够意识到这是有问题的版本。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;如果我更新了自己的依赖但没有改变公共 API 该怎么办？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;由于没有影响到公共 API，这可以被认定是兼容的。若某个软件和你的包有共同依赖，则它会有自己的依赖规范，作者也会告知可能的冲突。要判断改版是属于修订等级或是次版等级，是依据你更新的依赖关系是为了修复问题或是加入新功能。对于后者，我经常会预期伴随着更多的代码，这显然会是一个次版本号级别的递增。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;如果我变更了公共 API 但无意中未遵循版本号的改动怎么办呢？（意即在修订等级的发布中，误将重大且不兼容的改变加到代码之中）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;自行做最佳的判断。如果你有庞大的使用者群在依照公共 API 的意图而变更行为后会大受影响，那么最好做一次主版本的发布，即使严格来说这个修复仅是修订等级的发布。记住， 语义化的版本控制就是透过版本号的改变来传达意义。若这些改变对你的使用者是重要的，那就透过版本号来向他们说明。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;我该如何处理即将弃用的功能？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;弃用现存的功能是软件开发中的家常便饭，也通常是向前发展所必须的。当你弃用部份公共 API 时，你应该做两件事：（1）更新你的文件让使用者知道这个改变，（2）在适当的时机将弃用的功能透过新的次版本号发布。在新的主版本完全移除弃用功能前，至少要有一个次版本包含这个弃用信息，这样使用者才能平顺地转移到新版 API。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;语义化版本对于版本的字串长度是否有限制呢？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;没有，请自行做适当的判断。举例来说，长到 255 个字元的版本已过度夸张。再者，特定的系统对于字串长度可能会有他们自己的限制。&lt;/p&gt;

&lt;h2 id=&#34;关于&#34;&gt;关于&lt;/h2&gt;

&lt;p&gt;语义化版本控制的规范是由 Gravatars 创办者兼 GitHub 共同创办者 &lt;a href=&#34;http://tom.preston-werner.com/&#34; target=&#34;_blank&#34;&gt;Tom Preston-Werner&lt;/a&gt; 所建立。&lt;/p&gt;

&lt;p&gt;如果您有任何建议，请到 &lt;a href=&#34;https://github.com/mojombo/semver/issues&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; 上提出您的问题。&lt;/p&gt;

&lt;h2 id=&#34;许可证&#34;&gt;许可证&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://creativecommons.org/licenses/by/3.0/&#34; target=&#34;_blank&#34;&gt;知识共享 署名 3.0 (CC BY 3.0)&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>由一封邮件看 Mailing List 在开源项目中的重要性</title>
      <link>https://guoxudong.io/en/post/kubernetes-client-python/</link>
      <pubDate>Thu, 04 Jul 2019 09:16:41 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kubernetes-client-python/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;只要仔细找，想要的轮子总会有的。
&amp;mdash; 某不知名 DevOps 工程师&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;感谢 &lt;code&gt;kubernetes-dev&lt;/code&gt; 的 Mailing List ！早上在浏览邮件时发现了下面这封有趣的邮件：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx3.sinaimg.cn/large/ad5fbf65gy1g4nkmrb8scj21780q0afv.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;接触 Kubernetes 也有不短的时间了，也见证了 Kubernetes 干掉 Swarm 和 Mesos 成为容器编排领域的事实标准的过程。在享受 Kubernetes 及其生态圈带来的便利的同时也在为 Kubernetes 及 CNCF 项目进行贡献。而使用 &lt;a href=&#34;https://github.com/kubernetes/kubectl&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;kubectl&lt;/code&gt;&lt;/a&gt;、&lt;a href=&#34;https://github.com/rancher/rancher&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;rancher&lt;/code&gt;&lt;/a&gt; 甚至是 &lt;a href=&#34;https://github.com/IBM/kui&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;kui&lt;/code&gt;&lt;/a&gt; 这些 CLI 和 UI 工具对 Kubernetes 集群进行操作和观察。&lt;/p&gt;

&lt;p&gt;虽然上面这些工具为操作 Kubernetes 集群带来了极大的便利，但是归根到底还是一些开源项目，并不能满足我们的全部需求。所以我们只能根据我们自己的需求和 Kubernetes 的 api-server 进行定制，但是由于 Kubernetes 的 api-server 比较复杂，短时间内并不是那么好梳理的。&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-client-python&#34;&gt;kubernetes-client/python&lt;/h2&gt;

&lt;p&gt;由于我们自研的 DevOps 平台是使用 python 开发的，所以我也基于 python 语言开发了一套 Kubernetes Client ，但总的来说由于 Kubernetes 的功能实在太多，而我的开发实践并不是很多，开发出来的功能只是差强人意。&lt;/p&gt;

&lt;p&gt;而 &lt;a href=&#34;https://github.com/kubernetes-client/python&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;kubernetes-client/python&lt;/code&gt;&lt;/a&gt; 这个官方给出的轮子是真的香！&lt;/p&gt;

&lt;h3 id=&#34;安装方便&#34;&gt;安装方便&lt;/h3&gt;

&lt;p&gt;这个安装方式简单的令人发指，支持的 python 版本为 &lt;code&gt;2.7 | 3.4 | 3.5 | 3.6 | 3.7&lt;/code&gt; 并且和所有 python 依赖包一样，只需要使用 &lt;code&gt;pip&lt;/code&gt; 安装即可：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;简单示例&#34;&gt;简单示例&lt;/h3&gt;

&lt;p&gt;查看所有的 pod ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python
#encoding: utf-8
#Author: guoxudong
from kubernetes import client, config

# Configs can be set in Configuration class directly or using helper utility
config.load_kube_config()

v1 = client.CoreV1Api()
print(&amp;quot;Listing pods with their IPs:&amp;quot;)
ret = v1.list_pod_for_all_namespaces(watch=False)
for i in ret.items:
    print(&amp;quot;%s\t%s\t%s&amp;quot; % (i.status.pod_ip, i.metadata.namespace, i.metadata.name))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行查看结果：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Listing pods with their IPs:
172.22.1.126	kube-system	coredns-5975fdf55b-bqgkx
172.22.0.2	kube-system	coredns-5975fdf55b-vxbb4
10.16.16.13	kube-system	flexvolume-9ccf7
10.16.16.15	kube-system	flexvolume-h5xn2
10.16.16.14	kube-system	flexvolume-kvn5x
10.16.16.17	kube-system	flexvolume-mf4zv
10.16.16.14	kube-system	kube-proxy-worker-7lpfz
10.16.16.15	kube-system	kube-proxy-worker-9wd9s
10.16.16.17	kube-system	kube-proxy-worker-phbbj
10.16.16.13	kube-system	kube-proxy-worker-pst5d
172.22.1.9	kube-system	metrics-server-78b597d5bf-wdvqh
172.22.1.12	kube-system	nginx-ingress-controller-796ccc5d76-9jh5s
172.22.1.125	kube-system	nginx-ingress-controller-796ccc5d76-jwwwz
10.16.16.17	kube-system	terway-6mfs8
10.16.16.14	kube-system	terway-fz9ck
10.16.16.13	kube-system	terway-t9777
10.16.16.15	kube-system	terway-xbxlp
172.22.1.8	kube-system	tiller-deploy-5b5d8dd754-wpcrc
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;果然是一个好轮子，引入 kubeconfig 的方式及展示所有 namespace 的 pod 的方法封装的也十分简洁，是个非常漂亮的范例。建议可以看一下&lt;a href=&#34;https://github.com/kubernetes-client/python&#34; target=&#34;_blank&#34;&gt;源码&lt;/a&gt;，肯定会有收获的！&lt;/p&gt;

&lt;h3 id=&#34;支持版本&#34;&gt;支持版本&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;client-python&lt;/code&gt; 遵循 &lt;a href=&#34;https://semver.org/lang/zh-CN/&#34; target=&#34;_blank&#34;&gt;semver&lt;/a&gt; 规范，所以在 &lt;code&gt;client-python&lt;/code&gt; 的主要版本增加之前，代码将继续使用明确支持的 Kubernetes 集群版本。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Kubernetes 1.5&lt;/th&gt;
&lt;th&gt;Kubernetes 1.6&lt;/th&gt;
&lt;th&gt;Kubernetes 1.7&lt;/th&gt;
&lt;th&gt;Kubernetes 1.8&lt;/th&gt;
&lt;th&gt;Kubernetes 1.9&lt;/th&gt;
&lt;th&gt;Kubernetes 1.10&lt;/th&gt;
&lt;th&gt;Kubernetes 1.11&lt;/th&gt;
&lt;th&gt;Kubernetes 1.12&lt;/th&gt;
&lt;th&gt;Kubernetes 1.13&lt;/th&gt;
&lt;th&gt;Kubernetes 1.14&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;client-python 1.0&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 2.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 3.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 4.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 5.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 6.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 7.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 8.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 9.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python 10.0&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;client-python HEAD&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;mailing-list-的重要性&#34;&gt;Mailing List 的重要性&lt;/h2&gt;

&lt;p&gt;这次的收获很大程度得益于 &lt;code&gt;kubernetes-dev&lt;/code&gt; 的 Mailing List 也就是邮件列表。这种沟通方式在国内不是很流行，大家更喜欢使用 QQ 和微信这样的即时通讯软件进行交流，但是大多数著名开源项目都是主要使用 &lt;strong&gt;Mailing List&lt;/strong&gt; 进行交流，交流的数量甚至比在 GitHub issue 中还多，在与 Apache 、 CNCF 项目开源的贡献者和维护者交流中得知了使用 &lt;strong&gt;Mailing List&lt;/strong&gt; 主要考虑是一下几点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;这种异步的交流方式可以让更多关心该话题的开发人员一起加入到讨论中。&lt;/li&gt;
&lt;li&gt;mailing list 是永久保留的，如果你对某个话题感兴趣，可以随时回复邮件，关注这个话题的开发者都会收到邮件，无论这个话题是昨天提出的，还是去年提出的，有助于解决一些陈年老 BUG （俗称技术债）。&lt;/li&gt;
&lt;li&gt;即时通讯软件虽然很便利，但是问题很快会被评论顶掉，虽然诸如 slack 这样的工具解决了部分这方面的问题，但是还是不如 mailing list 好用。&lt;/li&gt;
&lt;li&gt;并不是所有地区的开发者都有高速的宽带，性能优秀的PC，在地球上很多地区还是只能使用拨号上网，网速只有几kb/s，他们甚至 GitHub issue 都无法使用。但是你不能剥夺他们参与开源项目的权利，而 mailing list 是一种很好的交流方式。&lt;/li&gt;
&lt;li&gt;通过 mailing list 可以很好掌握社区动态，效果明显好于 GitHub watch ，因为并不是项目的所有 commit 都是你关心的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;如果你有志于参与到开源运动，在享受开源软件带来便利的同事，还想为开源软件做出自己的贡献，那么 mailing list 是你进入社区最好的选择。在 mailing list 中和来自世界各地志同道合的开发者交流中提升自己的能力，创造更大的价值，迈出你参与开源运动的第一步。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Kustomize 帮你管理 kubernetes 应用（五）：配合 kubedog 完善 CI/CD 的最后一步</title>
      <link>https://guoxudong.io/en/post/kustomize-5/</link>
      <pubDate>Wed, 03 Jul 2019 15:20:31 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kustomize-5/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;在以往的 pipeline 中，使用 kubectl 进行部署 Deployment 后无法检查 Deployment 是否部署成功，只能通过使用命令/脚本来手动检查 Deployment 状态，而 kubedog 这个小工具完美解决了这个问题，完善了 CI/CD 流水线的最后一步。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;kubedog&#34;&gt;KubeDog&lt;/h2&gt;

&lt;p&gt;kubedog 是一个 lib 库和 CLI 小工具，允许在 CI/CD 部署 pipeline 中观察和跟踪 Kubernetes 资源。与 kustomize 配合，集成到 pipeline 之后，完美的解决了 CI/CD 的最后一步，完美的替代了之前不够灵活的脚本（好吧，其实我也开发了类似的小工具，但是有这么好用的轮子，拿来直接用何乐而不为呢？）。&lt;/p&gt;

&lt;p&gt;kubedog 提供了 lib 库和 CLI 小工具，这里由于是介绍 CI/CD 中的实践，所以只介绍其中的 &lt;code&gt;rollout track&lt;/code&gt; 功能。 lib 库的使用和 CLI 的 &lt;code&gt;follow&lt;/code&gt; 功能这里就不做介绍了，有兴趣的同学可以去 &lt;a href=&#34;https://github.com/flant/kubedog&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; 上查看该项目的各种使用方式。&lt;/p&gt;

&lt;h3 id=&#34;集成-kubedog&#34;&gt;集成 KubeDog&lt;/h3&gt;

&lt;p&gt;由于我司目前使用的是 &lt;a href=&#34;https://drone.io/&#34; target=&#34;_blank&#34;&gt;drone&lt;/a&gt; 进行 CI ，每个 step 都是由一个 docker 制作的插件组成。我制作了一个包含 &lt;code&gt;kubectl&lt;/code&gt; 、 &lt;code&gt;kustomize&lt;/code&gt; 和 &lt;code&gt;kubedog&lt;/code&gt; 的镜像。该镜像已上传 dockerhub ，需要的可以自行拉取使用 &lt;code&gt;guoxudongdocker/kubectl&lt;/code&gt; ,而该插件的使用也在 &lt;a href=&#34;https://github.com/sunny0826/kubectl-kustomize&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; 和 &lt;a href=&#34;https://cloud.docker.com/u/guoxudongdocker/repository/docker/guoxudongdocker/kubectl&#34; target=&#34;_blank&#34;&gt;DockerHub&lt;/a&gt; 上查看。&lt;/p&gt;

&lt;p&gt;而集成方式也比较简单，直接将 &lt;code&gt;kubectl&lt;/code&gt; 、 &lt;code&gt;kustomize&lt;/code&gt; 和 &lt;code&gt;kubedog&lt;/code&gt; 的可执行包下载到 &lt;code&gt;/usr/local/bin&lt;/code&gt; 并赋予执行权限即可，下面就是 &lt;code&gt;Dockerfile&lt;/code&gt; 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dockerfile&#34;&gt;FROM alpine

LABEL maintainer=&amp;quot;sunnydog0826@gmail.com&amp;quot;

ENV KUBE_LATEST_VERSION=&amp;quot;v1.14.1&amp;quot;

RUN apk add --update ca-certificates \
 &amp;amp;&amp;amp; apk add --update -t deps curl \
 &amp;amp;&amp;amp; curl -L https://storage.googleapis.com/kubernetes-release/release/${KUBE_LATEST_VERSION}/bin/linux/amd64/kubectl -o /usr/local/bin/kubectl \
 &amp;amp;&amp;amp; chmod +x /usr/local/bin/kubectl \
 &amp;amp;&amp;amp; curl -L https://github.com/kubernetes-sigs/kustomize/releases/download/v2.0.3/kustomize_2.0.3_linux_amd64 -o /usr/local/bin/kustomize \
 &amp;amp;&amp;amp; chmod +x /usr/local/bin/kustomize \
 &amp;amp;&amp;amp; curl -L https://dl.bintray.com/flant/kubedog/v0.2.0/kubedog-linux-amd64-v0.2.0 -o /usr/local/bin/kubedog \
 &amp;amp;&amp;amp; chmod +x /usr/local/bin/kubedog \
 &amp;amp;&amp;amp; apk del --purge deps \
 &amp;amp;&amp;amp; rm /var/cache/apk/*


WORKDIR /root
ENTRYPOINT [&amp;quot;kubectl&amp;quot;]
CMD [&amp;quot;help&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;kustomize-配合-kubedog-使用&#34;&gt;Kustomize 配合 KubeDog 使用&lt;/h2&gt;

&lt;p&gt;在镜像构建好之后就可以直接使用了，这里使用的是 DockerHub 的镜像仓库，这里建议将镜像同步到私有仓库，比如阿里云的容器镜像服务或者 Habor ，因为国内拉取 DockerHub 的镜像不太稳定，经常会拉取镜像失败或者访问超时，在 CI/CD 流水线中推荐使用更稳定镜像。&lt;/p&gt;

&lt;p&gt;以下是 &lt;code&gt;.drone.yml&lt;/code&gt; 示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kind: pipeline
name: {your-pipeline-name}

steps:
- name: Kubernetes 部署
  image: guoxudongdocker/kubectl
  volumes:
  - name: kube
    path: /root/.kube
  commands:
    - cd deploy/overlays/dev    # 这里使用 kustomize ,详细使用方法请见 https://github.com/kubernetes-sigs/kustomize
    - kustomize edit set image {your-docker-registry}:${DRONE_BUILD_NUMBER}
    - kubectl apply -k . &amp;amp;&amp;amp; kubedog rollout track deployment {your-deployment-name} -n {your-namespace} -t {your-tomeout}

...

volumes:
- name: kube
  host:
    path: /tmp/cache/.kube  # kubeconfig 挂载位置

trigger:
  branch:
  - master  # 触发 CI 的分支
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的配置可见，在该 step 中执行了如下几步：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;进入 patch 所在路径&lt;/li&gt;
&lt;li&gt;使用了 Kustomize 命令 &lt;code&gt;kustomize edit set image {your-docker-registry}:${DRONE_BUILD_NUMBER}&lt;/code&gt; 方式将前面 step 中构建好的镜像的 tag 插入到 patch 中&lt;/li&gt;
&lt;li&gt;使用 &lt;code&gt;kubectl apply -k .&lt;/code&gt; 进行 k8s 部署，要注意最后的那个 &lt;code&gt;.&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;使用 kubedog 跟踪 Deployment 部署状态&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;命令解析：&lt;code&gt;kubedog rollout track deployment {your-deployment-name} -n {your-namespace} -t {your-tomeout}&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;deployment {your-deployment-name} : Deployment 的名称&lt;/li&gt;
&lt;li&gt;-n {your-namespace} : Deployment 所在的 namespace&lt;/li&gt;
&lt;li&gt;-t {your-tomeout} : 超时时间，单位为秒，超时后会报错，这里请根据实际部署情况进行设置&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;从 Kubernetes release v1.14 版本开始，&lt;code&gt;kustomize&lt;/code&gt; 集成到 &lt;code&gt;kubectl&lt;/code&gt; 中，越来越多 k8S 周边的小工具出现。这些小工具的出现帮助了 Kubernetes 的使用者来拉平 Kubernetes 的使用曲线，同时也标志着 K8S 的成熟，越来越多的开发人员基于使用 K8S 的痛点开发相关工具。套用一句今年 KubeCon 的 Keynote 演讲上，阿里云智能容器平台负责人丁宇的话： &lt;strong&gt;Kubernetes 正当时，云原生未来可期&lt;/strong&gt; 。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>记一次使用 Kustomize 时遇到的愚蠢问题</title>
      <link>https://guoxudong.io/en/post/kustomize-err-1/</link>
      <pubDate>Wed, 03 Jul 2019 13:44:50 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kustomize-err-1/</guid>
      <description>

&lt;h2 id=&#34;现象&#34;&gt;现象&lt;/h2&gt;

&lt;p&gt;在日常 CI/CD 流程中，已经将 Kustomize 集成到 pipeline 中使用，但是在对一个项目进行 Kustomize 改造时，将单个 &lt;code&gt;deploy.yaml&lt;/code&gt; 拆分为了若干个 patch 以达到灵活 Kubernetes 部署的目的。但是在使用 &lt;code&gt;kubectl apply -k .&lt;/code&gt; 命令进行部署的时候遇到了 &lt;code&gt;error: failed to find an object with apps_v1_Deployment|myapp to apply the patch&lt;/code&gt; 的报错。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx3.sinaimg.cn/large/ad5fbf65gy1g4mm1m3vx9j21oe10y102.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;解决之路&#34;&gt;解决之路&lt;/h2&gt;

&lt;p&gt;由于之前的使用中没有遇到此类报错，看报错信息像是 &lt;code&gt;apiVersion&lt;/code&gt; 的问题，所以先检查了所有 patch 的 &lt;code&gt;apiVersion&lt;/code&gt; ，但是并没有找到有什么问题。&lt;/p&gt;

&lt;h3 id=&#34;google-搜索&#34;&gt;Google 搜索&lt;/h3&gt;

&lt;p&gt;对该报错进行了搜索，搜索到如下结果：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws4.sinaimg.cn/large/ad5fbf65gy1g4mmee8ctxj21900ns44c.jpg&#34; alt=&#34;image&#34; /&gt;
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g4mmgrdz0fj21ou1b6wro.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;？？？ 为何这个 issue 没有解决就被提出者关闭了？&lt;/p&gt;

&lt;h3 id=&#34;问题解决&#34;&gt;问题解决&lt;/h3&gt;

&lt;p&gt;在 Google 了一圈之后还是没有找到什么有营养的回答，问题又回到了原点&amp;hellip;只能对所有的 patch 的每个字符和每个配置逐一进行了检查。结果发现是 &lt;code&gt;name&lt;/code&gt; 的内容 base 与 overlays 不同&amp;hellip; base 中是 &lt;code&gt;name:myapp&lt;/code&gt; ，而 overlays 中是 &lt;code&gt;name:my-app&lt;/code&gt; &amp;hellip;&lt;/p&gt;

&lt;p&gt;好吧，issue 关的是有道理的&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx1.sinaimg.cn/large/ad5fbf65gy1g4mmuqm6n2j2098048a9z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubecon 2019 见闻：云原生未来可期</title>
      <link>https://guoxudong.io/en/post/kubecon-2019/</link>
      <pubDate>Tue, 02 Jul 2019 10:18:18 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kubecon-2019/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;2019年6月24-26日，KubeCon + CloudNativeCon + Open Source Summit大会在上海世博中心举行。本次大会规模空前，预计有超过40个国家，3500多名云原生、开源领域的开发者参加，门票更是早早售罄。作为一名云原生应用的使用者与开发者，我也报名参与了这次大会。&lt;/p&gt;

&lt;p&gt;6月的上海已经入梅，潮湿的空气对于已经在上海生活好多年的我还是会感受到不适，但是这些也无法阻碍 KubeCon 带给我的兴奋与激动，况且这次是在家门口举行，3站地铁就能到达了世博中心。&lt;/p&gt;

&lt;p&gt;参与本次大会不仅仅是因为可以接触到最新的 Kubernetes &amp;amp; Cloud Native 实践，更是因为可以与很多神交已久的朋友会面，同时也可以与很多业界大牛面对面的交流，获取宝贵的经验。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws2.sinaimg.cn/large/ad5fbf65gy1g4lbp87794j22bc1jknpe.jpg&#34; alt=&#34;48124131751_3fc63103d5_o&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;国内云原生开源力量强劲&#34;&gt;国内云原生开源力量强劲&lt;/h2&gt;

&lt;p&gt;会议第一天，先后参加了蚂蚁金服组织的 SOFAStack Workshop 和由 CNCF, VMware, 阿里云和 PingCAP 联合主办的中国原创CNCF项目社区沙龙，同时在午休时候短暂的旁听了华为主办的 Apache ServiceComb Meetup 。&lt;/p&gt;

&lt;p&gt;作为 &lt;a href=&#34;http://servicemesher.com&#34; target=&#34;_blank&#34;&gt;ServiceMesher&lt;/a&gt; 社区的一员，与会议的组织者在社区中都很熟悉了，虽然没有注册 SOFAStack Workshop 会议，但也很自然的混进去了。作为金融级分布式框架 SOFAStack ，是用于快速构建金融级分布式架构的一套中间件，也是建立于蚂蚁金服海量金融场景锤炼出来的最佳实践。而此次的 Workshop 更是经过了精心的准备，准备了充足的材料，并在蚂蚁同学的帮助了使用 SOFAStack 实践了 &lt;strong&gt;使用 SOFAStack 快速构建微服务&lt;/strong&gt; 、 &lt;strong&gt;SOFABoot 动态模块实践&lt;/strong&gt; 、 &lt;strong&gt;使用 Seata 保障支付一致性&lt;/strong&gt; 、 &lt;strong&gt;基于 Serverless 轻松构建云上应用&lt;/strong&gt; 和 &lt;strong&gt;使用 CloudMesh 轻松实践 Service Mesh&lt;/strong&gt; 五个demo。在 Service Mesh 方面，蚂蚁的同学异常的活跃，是 Istio 中文文档翻译和 &lt;a href=&#34;http://servicemesher.com&#34; target=&#34;_blank&#34;&gt;ServiceMesher&lt;/a&gt; 社区的主要组织者。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws4.sinaimg.cn/large/ad5fbf65gy1g4lcu8mpmsj21480tokjl.jpg&#34; alt=&#34;image&#34; /&gt;
&lt;center&gt;《未来架构-从服务化到云原生》作者敖小剑老师现场签名&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;午饭后短暂旁听了华为主办的 Apache ServiceComb Meetup ，虽然时间短暂，只有不到1个小时，但是对于国内运作开源项目，尤其是 Apache 项目有个更深的理解，同时让我联想到了之前在 &lt;a href=&#34;https://developer.aliyun.com/article/704943&#34; target=&#34;_blank&#34;&gt;《从开源小白到 Apache Member ，阿里工程师的成长笔记》&lt;/a&gt; 阿里巴巴技术专家望陶成为的文章，中国的开源软件和开发者在开源领域起到越来越重要的作用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx1.sinaimg.cn/large/ad5fbf65gy1g4lcxc767gj23402c07wi.jpg&#34; alt=&#34;IMG_2470&#34; /&gt;&lt;/p&gt;

&lt;p&gt;下午参加了由 CNCF, VMware, 阿里云和 PingCAP 联合主办的中国原创CNCF项目社区沙龙，聆听了 &lt;a href=&#34;https://github.com/goharbor/harbor&#34; target=&#34;_blank&#34;&gt;Harbor&lt;/a&gt; 、 &lt;a href=&#34;https://github.com/dragonflyoss/Dragonfly&#34; target=&#34;_blank&#34;&gt;Dragonfly&lt;/a&gt; 、 &lt;a href=&#34;https://github.com/tikv/tikv&#34; target=&#34;_blank&#34;&gt;TiKV&lt;/a&gt; 三个中国原创 CNCF 项目的分享，同时 李响、Dan Kohn 等大佬也在会上发言表达了对国内 CNCF 开源项目的肯定及期待。而在会上也结识了阿里云容器镜像服务的开发小哥，作为阿里云的资深用户与开发小哥进行了交流，了解一些容器镜像服务方面的新功能，同时也反应了在使用中遇到的问题，总的来说收获颇丰。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws3.sinaimg.cn/large/ad5fbf65gy1g4lfret268j23402c0kjm.jpg&#34; alt=&#34;IMG_2472&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;现场惊现-linux-及-git-创始人-linus-torvalds&#34;&gt;现场惊现 Linux 及 Git 创始人 Linus Torvalds&lt;/h2&gt;

&lt;p&gt;大会第二天的 Keynote 一直是 Linux 基金会宣传其理念，愿景以及赞助商进行市场宣传的重要活动。而当天最让人激动的就是 Linus 在 Keynote 后的一次嘉宾谈话，毫不夸张的说，我的工作就是 Linus 给的。而为了看到活的 Linus ，很多人一大早就在 red hall 的门前排起了长队，由于没有看好座位的分布，我只是找到了一个比较偏的位置，但是还是可以看的比较清楚。 Linus 本人还是十分幽默的，在谈话中提到了 Linux 5.1-rc6 的 release 计划，同时还询问现成有多少人是从事内核开发的，不过现场举手的同学并不多。&lt;/p&gt;

&lt;p&gt;Keynote 上还提到了中国在整个云原生运动中的巨大贡献，中国的 K8s contributors 已经在全球所有贡献者中排名第二，超过 10% 的 CNCF 会员来自中国，26%的 Kubernetes 的认证供应商来自中国，同时也公布了蚂蚁金服作为黄金会员加入 CNCF。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g4lghq7jo9j20xc0m87o8.jpg&#34; alt=&#34;48125038821_66fcf00e96_o&#34; /&gt;&lt;/p&gt;

&lt;p&gt;平均每小时一个的分组会议，我的日程安排的满满的，但就这样还是出现了由于到晚了无法进入分组会议室的情况，注意这里不是因为到晚了不让进，而是进去都没有站的地方！可见人气之旺！让我有了像是上学时候穿梭在教学楼，赶人气高的选修课的错觉。&lt;/p&gt;

&lt;p&gt;而在赞助商展示区也有不少的收获，与rancher、kong、jenkins等开源软件的开发者进行了交流，同时也获得了不少小礼品。最大的收获就是在阿里云展台与张磊大神的合影。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g4lh0fv4x5j23402c0qv7.jpg&#34; alt=&#34;IMG_2536&#34; /&gt;
&lt;center&gt;与张磊大神的合影&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;servicemesher-社区的壮大&#34;&gt;ServiceMesher 社区的壮大&lt;/h2&gt;

&lt;p&gt;平时都是在网上与社区的朋友们进行交流，讨论技术，交流经验。而 KubeCon 就变成了一场网友面基大会，见到了很多有过交流和帮助过我的朋友，包括 Jimmy 、 小剑 、秀龙老哥&amp;hellip;等等，同时也认识了不少新朋友。与去年11月的 KubeCon 相比，社区的朋友越来越多，在短短半年内 Service Mesh 相关书籍已经出版了4本，而作者都是社区成员，可见社区的活跃。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx3.sinaimg.cn/large/ad5fbf65gy1g4k6mh797pj21900u07it.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;写在最后&#34;&gt;写在最后&lt;/h2&gt;

&lt;p&gt;正如 KubeCon 第三天 Keynote 上，阿里云智能容器平台负责人丁宇的话：&lt;strong&gt;Kubernetes 正当时，云原生未来可期&lt;/strong&gt; 。在 KubeCon 上看到云原生及开源软件的发展速度迅猛，各大厂商也都在最近几年组建了自己的开源团队，在使用开源软件获取便利的同时也在回馈社区，这也是让竞争对手共同为一款开源软件进行贡献的原因。相信随着开源运动在国内的深入，将会出现越来越多中国原创的开源项目，也会有更多的开发者加入到开源项目中，在贡献的同时提升自己的技术水平。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Kustomize 帮你管理 kubernetes 应用（四）：简述核心配置 kustomization.yaml</title>
      <link>https://guoxudong.io/en/post/kustomize-4/</link>
      <pubDate>Thu, 23 May 2019 12:50:12 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kustomize-4/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;在前面的文章中已经介绍了 kustomize 是什么，以及如何开始使用和如何简单的在 CI/CD 中使用，本篇文章将会介绍 kustomize 的核心文件 &lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/blob/master/docs/zh/kustomization.yaml&#34; target=&#34;_blank&#34;&gt;kustomization.yaml&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;另外，博主已经向 kustomize 贡献了中文文档，已被官方采纳，现在在 kustomize 中的 &lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/tree/master/docs/zh&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;docs/zh&lt;/code&gt;&lt;/a&gt; 目录中就可看到，翻译的不好的地方欢迎指正。同时也在 GitHub 上新建了一个 名为 &lt;a href=&#34;https://github.com/sunny0826/kustomize-lab&#34; target=&#34;_blank&#34;&gt;kustomize-lab&lt;/a&gt; 的 repo 用于演示 kustomize 的各种用法及技巧，本文中介绍的内容也会同步更新到该 repo 中，欢迎 fork、star、PR。&lt;/p&gt;

&lt;h2 id=&#34;kustomization-yaml-的作用&#34;&gt;&lt;code&gt;kustomization.yaml&lt;/code&gt; 的作用&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Kustomize 允许用户以一个应用描述文件 （YAML 文件）为基础（Base YAML），然后通过 Overlay 的方式生成最终部署应用所需的描述文件。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;有前面的文章&lt;a href=&#34;../kustomize-2&#34;&gt;《使用 Kustomize 帮你管理 kubernetes 应用（二）： Kustomize 的使用方法》&lt;/a&gt;中已经介绍了，每个 &lt;code&gt;base&lt;/code&gt; 或 &lt;code&gt;overlays&lt;/code&gt; 中都必须要有一个 &lt;code&gt;kustomization.yaml&lt;/code&gt;，这里我们看一下官方示例 &lt;code&gt;helloWorld&lt;/code&gt; 中的 &lt;code&gt;kustomization.yaml&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-YAML&#34;&gt;commonLabels:
  app: hello

resources:
- deployment.yaml
- service.yaml
- configMap.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到该项目中包含3个 resources ， &lt;code&gt;deployment.yaml&lt;/code&gt;、&lt;code&gt;service.yaml&lt;/code&gt; 、 &lt;code&gt;configMap.yaml&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.
└── helloWorld
    ├── configMap.yaml
    ├── deployment.yaml
    ├── kustomization.yaml
    └── service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;直接执行命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kustomize build helloWorld
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就可以看到结果了：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-YAML&#34;&gt;apiVersion: v1
data:
  altGreeting: Good Morning!
  enableRisky: &amp;quot;false&amp;quot;
kind: ConfigMap
metadata:
  labels:
    app: hello
  name: the-map
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: hello
  name: the-service
spec:
  ports:
  - port: 8666
    protocol: TCP
    targetPort: 8080
  selector:
    app: hello
    deployment: hello
  type: LoadBalancer
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hello
  name: the-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: hello
  template:
    metadata:
      labels:
        app: hello
        deployment: hello
    spec:
      containers:
      - command:
        - /hello
        - --port=8080
        - --enableRiskyFeature=$(ENABLE_RISKY)
        env:
        - name: ALT_GREETING
          valueFrom:
            configMapKeyRef:
              key: altGreeting
              name: the-map
        - name: ENABLE_RISKY
          valueFrom:
            configMapKeyRef:
              key: enableRisky
              name: the-map
        image: monopole/hello:1
        name: the-container
        ports:
        - containerPort: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的结果可以看大 kustomize 通过 &lt;code&gt;kustomization.yaml&lt;/code&gt; 将3个 resources 进行了处理，给三个 resources 添加了共同的 labels &lt;code&gt;app: hello&lt;/code&gt; 。这个示例展示了 &lt;code&gt;kustomization.yaml&lt;/code&gt; 的作用：&lt;strong&gt;将不同的 resources 进行整合，同时为他们加上相同的配置&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&#34;进阶使用&#34;&gt;进阶使用&lt;/h2&gt;

&lt;p&gt;上面只不过是一个简单的示例，下面将结合实际情况分享一些比较实用的用法&lt;/p&gt;

&lt;h3 id=&#34;根据环境生成不同配置&#34;&gt;根据环境生成不同配置&lt;/h3&gt;

&lt;p&gt;在实际的使用中，使用最多的就是为不同的环境配置不同的 &lt;code&gt;deploy.yaml&lt;/code&gt;，而使用 kustomize 可以把配置拆分为多个小的 patch ，然后通过 kustomize 来进行组合。而根据环境的不同，每个 patch 都可能不同，包括分配的资源、访问的方式、部署的节点都可以自由的定制。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.
├── flask-env
│   ├── README.md
│   ├── base
│   │   ├── deployment.yaml
│   │   ├── kustomization.yaml
│   │   └── service.yaml
│   └── overlays
│       ├── dev
│       │   ├── healthcheck_patch.yaml
│       │   ├── kustomization.yaml
│       │   └── memorylimit_patch.yaml
│       └── prod
│           ├── healthcheck_patch.yaml
│           ├── kustomization.yaml
│           └── memorylimit_patch.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里可以看到配置分为了 &lt;code&gt;base&lt;/code&gt; 和 &lt;code&gt;overlays&lt;/code&gt;， &lt;code&gt;overlays&lt;/code&gt; 则是继承了 &lt;code&gt;base&lt;/code&gt; 的配置，同时添加了诸如 healthcheck 和 memorylimit 等不同的配置，那么我们分别看一下 &lt;code&gt;base&lt;/code&gt; 和 &lt;code&gt;overlays&lt;/code&gt; 中 &lt;code&gt;kustomization.yaml&lt;/code&gt; 的内容&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;base&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-YAML&#34;&gt;commonLabels:
app: test-cicd

resources:
- service.yaml
- deployment.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;base&lt;/code&gt; 中的 &lt;code&gt;kustomization.yaml&lt;/code&gt; 中定义了一些基础配置&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;overlays&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-YAML&#34;&gt;bases:
- ../../base
patchesStrategicMerge:
- healthcheck_patch.yaml
- memorylimit_patch.yaml
namespace: devops-dev
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;overlays&lt;/code&gt; 中的 &lt;code&gt;kustomization.yaml&lt;/code&gt; 则是基于 &lt;code&gt;base&lt;/code&gt; 新增了一些个性化的配置，来达到生成不同环境的目的。&lt;/p&gt;

&lt;p&gt;执行命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kustomize build flask-env/overlays/dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-YAML&#34;&gt;apiVersion: v1
kind: Service
metadata:
  labels:
    app: test-cicd
  name: test-cicd
  namespace: devops-dev
spec:
  ports:
  - name: http
    port: 80
    targetPort: 80
  selector:
    app: test-cicd
  type: ClusterIP
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: test-cicd
  name: test-cicd
  namespace: devops-dev
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test-cicd
  template:
    metadata:
      labels:
        app: test-cicd
        version: 0.0.3
    spec:
      containers:
      - env:
        - name: ENV
          value: dev
        image: guoxudongdocker/flask-python:latest
        imagePullPolicy: Always
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 20
        name: test-cicd
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 20
        resources:
          limits:
            cpu: 300m
            memory: 500Mi
          requests:
            cpu: 300m
            memory: 500Mi
        volumeMounts:
        - mountPath: /etc/localtime
          name: host-time
      imagePullSecrets:
      - name: registry-pull-secret
      volumes:
      - hostPath:
          path: /etc/localtime
        name: host-time
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到包括 &lt;code&gt;replicas&lt;/code&gt;、&lt;code&gt;limits&lt;/code&gt;、&lt;code&gt;requests&lt;/code&gt;、&lt;code&gt;env&lt;/code&gt; 等 dev 中个性的配置都已经出现在了生成的 yaml 中。由于篇幅有限，这里没有把所有的配置有罗列出来，需要的可以去 &lt;a href=&#34;https://github.com/sunny0826/kustomize-lab&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; 上自取。&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;上面所有的 &lt;code&gt;kustomize build dir/&lt;/code&gt; 都可以使用 &lt;code&gt;kubectl apply -k dir/&lt;/code&gt; 实现，但是需要 &lt;code&gt;v14.0&lt;/code&gt; 版以上的 &lt;code&gt;kubectl&lt;/code&gt;，也就是说，其实我们在集成到 CI/CD 中的时候，甚至都不需要用来 &lt;code&gt;kustomize&lt;/code&gt; 命令集，有 &lt;code&gt;kubectl&lt;/code&gt; 就够了。&lt;/p&gt;

&lt;p&gt;由于篇幅有限，这里没法吧所有 &lt;code&gt;kustomization.yaml&lt;/code&gt; 的用途都罗列出来，不过可以在官方文档中找到我提交的中文翻译版 &lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/blob/master/docs/zh/kustomization.yaml&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;kustomization.yaml&lt;/code&gt;&lt;/a&gt;，可以直接去官方 GitHub 查看。同时 &lt;a href=&#34;https://github.com/sunny0826/kustomize-lab&#34; target=&#34;_blank&#34;&gt;kustomize-lab&lt;/a&gt; 会持续更行，敬请关注。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>轻量快速的 CI 工具 Drone</title>
      <link>https://guoxudong.io/en/post/drone-ci/</link>
      <pubDate>Tue, 21 May 2019 08:59:00 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/drone-ci/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;公司之前一直在使用 Jenkins 作为 CI/CD 工具， Jenkins 非常强大，它完成了几乎所有 CI/CD 的工作，并且应用于整个团队有好长一段时间了。但是随着公司推荐数字化、智慧化，以及服务容器化的推进， Jenkins 的一些弊端也凸显了出来：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;重量级：&lt;/strong&gt; Jenkins 功能十分齐全，几乎可以做所有的事情。但是这也是他的一个弊端，过于重量级，有时候往往一个小的修改需要改动许多地方，升级\下载插件后需要进行重启等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;升级不易：&lt;/strong&gt; 在一些安全 Jenkins 相关的安全漏洞被公开后，我们会对 Jenkins 进行升级，但这也不是一件容易的事。之前就出现过升级\重启后，所有 job 丢失，虽然我们所有项目配置都是以 Jenkinsfile 的形式统一存储，但是每个 job 都需要重新重新创建，包括每个 job 的权限&amp;hellip;.._(´ཀ`」 ∠)_&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;权限控制复杂：&lt;/strong&gt; 这其实也是 Jenkins 的一大优势，可以精确控制每个用户的权限，但是需要花费更多时间去配置，时间长了也会出现权限混乱的问题。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;UI 界面：&lt;/strong&gt; 这个其实是吐槽最多的部分，虽然有诸如：Blue Ocean 这样的插件来展示 pipeline ，但是还是没有从根本改变它简陋的 UI 界面。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;那么为什么选择使用 Drone 呢？&lt;/p&gt;

&lt;p&gt;其实在 GitHub 上提交 PR 后，大部分开源项目都会使用 &lt;a href=&#34;http://travis-ci.org/&#34; target=&#34;_blank&#34;&gt;travis-ci&lt;/a&gt; 对提交的代码进行 CI 及检查，而如果是 Kubernetes 相关的项目，则会使用 &lt;a href=&#34;https://github.com/k8s-ci-robot&#34; target=&#34;_blank&#34;&gt;prow&lt;/a&gt; 进行 CI。但是 &lt;a href=&#34;http://travis-ci.org/&#34; target=&#34;_blank&#34;&gt;travis-ci&lt;/a&gt; 只能用于 GitHub ，在寻找类似项目的时候， Drone 进入了我的视野。&lt;/p&gt;

&lt;p&gt;大道至简。和 Jenkins 相比， Drone 就轻量的多了，从应用本身的安装部署到流水线的构建都简洁的多。由于是和源码管理系统相集成，所以 Drone 天生就省去了各种账户\权限的配置，直接与 gitlab 、 github 、 Bitbucket 这样的源码管理系统操作源代码的权限一致。正如它官网上写的那样：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Any Source Code Manager&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Drone integrates seamlessly with multiple source code management systems, including GitHub, GitHubEnterprise, Bitbucket, and GitLab.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Any Platform&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Drone natively supports multiple operating systems and architectures, including Linux x64, ARM, ARM64 and Windows x64.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Any Language&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Drone works with any language, database or service that runs inside a Docker container. Choose from thousands of public Docker images or provide your own.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Drone 天生支持任何源码管理工具、任何平台和任何语言。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;而写这篇文章的目的，并不是要吹捧这个工具有多么的好用，而是要总结在搭建 drone 和使用时候需要的各种坑，帮助读者绕过这些坑。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;声明&#34;&gt;声明&lt;/h2&gt;

&lt;p&gt;鉴于在使用 Drone CI 中，遇到的各种坑都和 Drone 的版本有关，这里首先声明我使用的 Drone 版本为&lt;code&gt;1.1&lt;/code&gt;，使用&lt;code&gt;0.8&lt;/code&gt;版本的同学请绕道。&lt;/p&gt;

&lt;h2 id=&#34;搭建-drone&#34;&gt;搭建 Drone&lt;/h2&gt;

&lt;p&gt;这里要说的就是在使用 drone 中遇到的第一个坑，在最初正准备搭建 drone 的时候 Google 了很多相关的 blog ，大部分 blog （包括某些 &lt;a href=&#34;https://medium.com/&#34; target=&#34;_blank&#34;&gt;medium.com&lt;/a&gt; 上面近期的英文 blog） 推荐的安装方式都是使用 &lt;code&gt;docker-compose&lt;/code&gt;，而无一例外的都失败了&amp;hellip;走投无路之下，我回到了&lt;a href=&#34;https://docs.drone.io/installation/&#34; target=&#34;_blank&#34;&gt;官网的文档&lt;/a&gt;，发现&lt;code&gt;1.0&lt;/code&gt;之后许多参数都发生了变化，并且官方推荐使用 docker 的方式运行 Drone。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;所以在使用任何开源软件之前都要去阅读它的文档，不要跟着一篇 blog 就开始了（包括我的），这样会少踩很多坑！！！&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这里以 gitlab 为例，展示网上版本启动参数和实际参数的不同：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;th&gt;各种blog&lt;/th&gt;
&lt;th&gt;官网文档&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;设置 Drone 的管理员&lt;/td&gt;
&lt;td&gt;DRONE_ADMIN=admin&lt;/td&gt;
&lt;td&gt;DRONE_USER_CREATE=username:admin,admin:true&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;设置GitLab的域名&lt;/td&gt;
&lt;td&gt;DRONE_GITLAB_URL&lt;/td&gt;
&lt;td&gt;DRONE_SERVER_HOST&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;GitLab的Application中的key&lt;/td&gt;
&lt;td&gt;DRONE_GITLAB_CLIENT&lt;/td&gt;
&lt;td&gt;DRONE_GITLAB_CLIENT_ID&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;GitLab的Application中的secret&lt;/td&gt;
&lt;td&gt;DRONE_GITLAB_SECRET&lt;/td&gt;
&lt;td&gt;DRONE_GITLAB_CLIENT_SECRET&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Drone 域名&lt;/td&gt;
&lt;td&gt;DRONE_HOST&lt;/td&gt;
&lt;td&gt;DRONE_GITLAB_SERVER&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;上面只是列举了部分官方文档和网上流产版本的不同，所以在使用之前一定要仔细阅读官方文档。下附运行 drone 的命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run \
  --volume=/var/run/docker.sock:/var/run/docker.sock \
  --volume=/var/lib/drone:/data \
  --env=DRONE_GIT_ALWAYS_AUTH=false \
  --env=DRONE_GITLAB_SERVER={your-gitlab-url} \  # gitlab 的 URL
  --env=DRONE_GITLAB_CLIENT_ID={your-gitlab-applications-id} \  #GitLab的Application中的id
  --env=DRONE_GITLAB_CLIENT_SECRET={your-gitlab-applicati-secret} \ #GitLab的Application中的secret
  --env=DRONE_SERVER_HOST={your-drone-url} \    # drone 的URl
  --env=DRONE_SERVER_PROTO=http \
  --env=DRONE_TLS_AUTOCERT=false \
  --env=DRONE_USER_CREATE=username:{your-admin-username},admin:true \   # Drone的管理员
  --publish=8000:80 \
  --publish=443:443 \
  --restart=always \
  --detach=true \
  --name=drone \
  drone/drone:1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于 &lt;code&gt;gitlab Application&lt;/code&gt; 的配置和 Drone 其他参数含义请参考&lt;a href=&#34;https://docs.drone.io/installation/gitlab/single-machine/&#34; target=&#34;_blank&#34;&gt;官方文档&lt;/a&gt;，这里只展示单节点办的运行方式。&lt;/p&gt;

&lt;h2 id=&#34;核心文件-drone-yml&#34;&gt;核心文件 &lt;code&gt;.drone.yml&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;要使用 Drone 只需在项目根创建一个 &lt;code&gt;.drone.yml&lt;/code&gt; 文件即可，这个是 Drone 构建脚本的配置文件，它随项目一块进行版本管理，开发者不需要额外再去维护一个配置脚本。其实现代 CI 程序都是这么做了，这个主要是相对于 Jekins 来说的。虽然 Jekins 也有插件支持，但毕竟还是需要配置。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;值得注意的事这个文件时 &lt;code&gt;.drone.yml&lt;/code&gt;，由于 Kubernetes 使用的多了，第一次创建了一个 &lt;code&gt;.drone.yaml&lt;/code&gt; 文件，导致怎么都获取不到配置&amp;hellip;_(´ཀ`」 ∠)_&amp;hellip; YAML 工程师石锤了&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这里放一个 Java 的 .drone.yml ，这个项目是 fork 别人的项目用作演示，记得要修改 &lt;code&gt;deployment.yaml&lt;/code&gt; 中的镜像仓库地址修改为自己的私有仓库。&lt;/p&gt;

&lt;p&gt;示例项目源码：&lt;a href=&#34;https://github.com/sunny0826/pipeline-example-maven&#34; target=&#34;_blank&#34;&gt;https://github.com/sunny0826/pipeline-example-maven&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-YAML&#34;&gt;kind: pipeline
name: pipeline-example-maven

steps:
- name: Maven编译
  image: maven:3-jdk-7
  volumes:
  - name: cache
    path: /root/.m2
  commands:
    - mvn clean install

- name: 构建镜像  
  image: plugins/docker
  volumes:
  - name: docker
    path: /var/run/docker.sock
  settings:
    username: 
      from_secret: docker_user
    password: 
      from_secret: docker_pass
    repo: {your-repo}
    registry: {your-registry}
    tags: ${DRONE_BUILD_NUMBER}

- name: Kubernetes 部署
  image: guoxudongdocker/kubectl:v1.14.1 
  volumes:
  - name: kube
    path: /root/.kube
  commands:
    - sed -i &amp;quot;s/#Tag/${DRONE_BUILD_NUMBER}/g&amp;quot; deployment.yaml
    - kubectl apply -f deployment.yaml

- name: 钉钉通知
  image: guoxudongdocker/drone-dingtalk 
  settings:
    token: 
      from_secret: dingding
    type: markdown
    message_color: true
    message_pic: true
    sha_link: true
  when:
    status: [failure, success]

volumes:
- name: cache
  host:
    path: /tmp/cache/.m2
- name: kube
  host:
    path: /tmp/cache/.kube/.test_kube
- name: docker
  host:
    path: /var/run/docker.sock

trigger:
  branch:
  - master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;值得注意的事：上面的这个 &lt;code&gt;.drone.yml&lt;/code&gt; 文件将本地的&lt;code&gt;.m2&lt;/code&gt;文件、kubeconfig文件、&lt;code&gt;docker.sock&lt;/code&gt; 文件挂载到 pipeline 中以实现 maven 打包缓存，k8s 部署、docker 缓存的作用，以提高 CI 速度。而是用挂载需要管理员在项目 settings 中勾选 &lt;code&gt;Trusted&lt;/code&gt; ，这个操作只能管理员进行，普通用户是看不到这个选项的。而管理员就是在docker运行时候 &lt;code&gt;--env=DRONE_USER_CREATE=username:{your-admin-username},admin:true&lt;/code&gt; 设置的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws3.sinaimg.cn/large/ad5fbf65gy1g38qvifxwij21d40tk76s.jpg&#34; alt=&#34;WX20190521-104717@2x&#34; /&gt;&lt;/p&gt;

&lt;p&gt;而上传镜像和钉钉同时需要在 settings 设置中添加 secret&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;docker_user：docker 仓库用户名&lt;/li&gt;
&lt;li&gt;docker_pass：docker 仓库密码&lt;/li&gt;
&lt;li&gt;dingding： 钉钉机器人 token&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;注意这里的钉钉 token 是 webhook 中 &lt;code&gt;https://oapi.dingtalk.com/robot/send?access_token=&lt;/code&gt; 后这部分
&lt;img src=&#34;https://ws2.sinaimg.cn/large/ad5fbf65gy1g38r1mkoztj20iy0ezgmg.jpg&#34; alt=&#34;WX20190521-105337&#34; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g38qxizsg1j21ia0tujtb.jpg&#34; alt=&#34;WX20190521-104942@2x&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;构建结果&#34;&gt;构建结果&lt;/h2&gt;

&lt;p&gt;添加 &lt;code&gt;.drone.yml&lt;/code&gt; 文件后，向 master 分支提交代码即可出发 CI 构建&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx3.sinaimg.cn/large/ad5fbf65gy1g38r68yb8pj21l40sawit.jpg&#34; alt=&#34;WX20190521-105809@2x&#34; /&gt;&lt;/p&gt;

&lt;p&gt;CI 结束后，会在钉钉机器人所在群收到通知&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws4.sinaimg.cn/large/ad5fbf65gy1g38r8cttcrj20e90bzacr.jpg&#34; alt=&#34;WX20190521-110009&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;插件支持&#34;&gt;插件支持&lt;/h2&gt;

&lt;p&gt;可以看到，每一步的镜像都是一个镜像，上面 pipeline 中的 Kubernetes 及钉钉通知插件就是我开发的，具体开发方法可以参考&lt;a href=&#34;https://docs.drone.io/&#34; target=&#34;_blank&#34;&gt;官方文档&lt;/a&gt;，而官方也提供了许多&lt;a href=&#34;http://plugins.drone.io/&#34; target=&#34;_blank&#34;&gt;官方插件&lt;/a&gt;。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;构建后部署：&lt;a href=&#34;http://plugins.drone.io/mactynow/drone-kubernetes/&#34; target=&#34;_blank&#34;&gt;Kubernetes&lt;/a&gt;、&lt;a href=&#34;http://plugins.drone.io/ipedrazas/drone-helm/&#34; target=&#34;_blank&#34;&gt;helm&lt;/a&gt;、&lt;a href=&#34;http://plugins.drone.io/appleboy/drone-scp/&#34; target=&#34;_blank&#34;&gt;scp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;构建后通知：&lt;a href=&#34;http://plugins.drone.io/lddsb/drone-dingtalk-message/&#34; target=&#34;_blank&#34;&gt;钉钉&lt;/a&gt; 、&lt;a href=&#34;http://plugins.drone.io/drillster/drone-email/&#34; target=&#34;_blank&#34;&gt;Email&lt;/a&gt;、&lt;a href=&#34;http://plugins.drone.io/drone-plugins/drone-slack/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;、&lt;a href=&#34;http://plugins.drone.io/lizheming/drone-wechat/&#34; target=&#34;_blank&#34;&gt;微信&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;Drone 整体用起来还是很方便的，搭建、上手速度都很快，但是官方文档给的不够详实，而网上充斥着各种各样0.8版本的的实例，但是其实官网早就发布了1.0版本，而官方并没有 &lt;code&gt;example&lt;/code&gt; 这样的示例项目，这样就又把本来降下来的学习曲线拉高了。许多坑都需要自己去趟，我在测试 drone 的时候，就构构建了上百次，不停的修改 &lt;code&gt;.drone.yml&lt;/code&gt; ， commit 信息看起来是很恐怖的。后续抽空会向官方贡献 &lt;code&gt;example&lt;/code&gt; 这样的 PR。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Kustomize 帮你管理 kubernetes 应用（三）：将 Kustomize 应用于 CI/CD</title>
      <link>https://guoxudong.io/en/post/kustomize-3/</link>
      <pubDate>Mon, 06 May 2019 16:46:28 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kustomize-3/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;首先明确软件版本，我这里使用的是 &lt;code&gt;Jenkins ver. 2.121.3&lt;/code&gt; ，这个版本比较老，其上安装 Kubernetes 插件所使用 &lt;code&gt;kubectl&lt;/code&gt; 版本也比较老，&lt;strong&gt;无法使用&lt;/strong&gt; Kustomize 的 yaml 文件需要的 &lt;code&gt;apiVersion: apps/v1&lt;/code&gt; ，直接使用生成 &lt;code&gt;deploy.yaml&lt;/code&gt; 文件会报错，所以这里选择了自己构建一个包含 &lt;code&gt;kubectl&lt;/code&gt; 和 &lt;code&gt;kustomize&lt;/code&gt; 的镜像，在镜像中使用 Kustomize 生成所需 yaml 文件并在 Kubernetes 上部署。&lt;/p&gt;

&lt;h2 id=&#34;软件版本&#34;&gt;软件版本&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;软件&lt;/th&gt;
&lt;th&gt;版本&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Jenkins&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://jenkins.io/&#34; target=&#34;_blank&#34;&gt;2.121.3&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;kubectl&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34; target=&#34;_blank&#34;&gt;v1.14.1&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;kustomize&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/releases&#34; target=&#34;_blank&#34;&gt;v2.0.3&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;前期准备&#34;&gt;前期准备&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Jenkins ：本篇使用 Jenkins 演示 CI/CD ，安装 Jenkins 就不在赘述，可以使用多种方法安装 Jenkins ，详细方法见&lt;a href=&#34;https://jenkins.io&#34; target=&#34;_blank&#34;&gt;官网&lt;/a&gt;。同时。 CI/CD 的工具有很多，这里为了省事使用笔者现有的 Jenkins 进行演示，&lt;strong&gt;不推荐&lt;/strong&gt;使用同笔者一样的版本，请使用较新的版本；同时也可以使用其他 CI/CD 工具，这里推荐使用 &lt;a href=&#34;https://drone.io/&#34; target=&#34;_blank&#34;&gt;drone&lt;/a&gt;。如果有更好的方案，欢迎交流，可以在&lt;a href=&#34;https://blog.maoxianplay.com/contact/&#34; target=&#34;_blank&#34;&gt;关于&lt;/a&gt;中找到我的联系方式。&lt;/li&gt;

&lt;li&gt;&lt;pre&gt;&lt;code class=&#34;language-kubectl```&#34;&gt;- Web 应用：这里使用 flask 写了一个简单的 web 应用，用于演示，同样以上传 dockerhub [```guoxudongdocker/flask-python```](https://hub.docker.com/r/guoxudongdocker/flask-python)

## 目录结构

首先看一下目录结构，目录中包括 ```Dockerfile``` 、 ```Jenkinsfile``` 、 Kustomize 要使用的 ```deploy``` 目录以及 web 应用目录。

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bush&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;.
├── Dockerfile
├── Jenkinsfile
├── app
│   ├── main.py
│   └── uwsgi.ini
└── deploy
    ├── base
    │   ├── deployment.yaml
    │   ├── kustomization.yaml
    │   └── service.yaml
    └── overlays
        ├── dev
        │   ├── healthcheck_patch.yaml
        │   ├── kustomization.yaml
        │   └── memorylimit_patch.yaml
        └── prod
            ├── healthcheck_patch.yaml
            ├── kustomization.yaml
            └── memorylimit_patch.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
这里可以看到 overlays 总共有两个子目录 `dev` 和 `prod` ，分别代表不同环境，在不同的环境中，应用不同的配置。

## Jenkins 配置

Jenkins 的配置相对简单，只需要新建一个 pipeline 类型的 job

![WX20190506-180159](https://wx4.sinaimg.cn/large/ad5fbf65gy1g2rr57oixbj20tn0ogq6v.jpg)

增加参数化构建，**注**：参数化构建需要安装 Jenkins 插件

![WX20190506-180918](https://ws4.sinaimg.cn/large/ad5fbf65gy1g2rrcb5ic9j21470q7mz8.jpg)

然后配置代码仓库即可

![WX20190507-094958](https://ws3.sinaimg.cn/large/ad5fbf65gy1g2sij1xlb2j214w0nw0uw.jpg)

## Pipeline 

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;groovy
podTemplate(label: &amp;lsquo;jnlp-slave&amp;rsquo;, cloud: &amp;lsquo;kubernetes&amp;rsquo;,
  containers: [
    containerTemplate(
        name: &amp;lsquo;jnlp&amp;rsquo;,
        image: &amp;lsquo;guoxudongdocker/jenkins-slave&amp;rsquo;,
        alwaysPullImage: true
    ),
    containerTemplate(name: &amp;lsquo;kubectl&amp;rsquo;, image: &amp;lsquo;guoxudongdocker/kubectl:v1.14.1&amp;rsquo;, command: &amp;lsquo;cat&amp;rsquo;, ttyEnabled: true),
  ],
  nodeSelector:&amp;lsquo;ci=jenkins&amp;rsquo;,
  volumes: [
    hostPathVolume(mountPath: &amp;lsquo;/var/run/docker.sock&amp;rsquo;, hostPath: &amp;lsquo;/var/run/docker.sock&amp;rsquo;),
    hostPathVolume(mountPath: &amp;lsquo;/usr/bin/docker&amp;rsquo;, hostPath: &amp;lsquo;/usr/bin/docker&amp;rsquo;),
    hostPathVolume(mountPath: &amp;lsquo;/usr/local/jdk&amp;rsquo;, hostPath: &amp;lsquo;/usr/local/jdk&amp;rsquo;),
    hostPathVolume(mountPath: &amp;lsquo;/usr/local/maven&amp;rsquo;, hostPath: &amp;lsquo;/usr/local/maven&amp;rsquo;),
    secretVolume(mountPath: &amp;lsquo;/home/jenkins/.kube&amp;rsquo;, secretName: &amp;lsquo;devops-ctl&amp;rsquo;),
  ],
)
{
    node(&amp;ldquo;jnlp-slave&amp;rdquo;){
        stage(&amp;lsquo;Git Checkout&amp;rsquo;){
            git branch: &amp;lsquo;${branch}&amp;rsquo;, url: &amp;lsquo;&lt;a href=&#34;https://github.com/sunny0826/flask-python.git&#39;&#34; target=&#34;_blank&#34;&gt;https://github.com/sunny0826/flask-python.git&#39;&lt;/a&gt;
        }
        stage(&amp;lsquo;Build and Push Image&amp;rsquo;){
            withCredentials([usernamePassword(credentialsId: &amp;lsquo;docker-register&amp;rsquo;, passwordVariable: &amp;lsquo;dockerPassword&amp;rsquo;, usernameVariable: &amp;lsquo;dockerUser&amp;rsquo;)]) {
                sh &amp;ldquo;&amp;rsquo;
                docker login -u ${dockerUser} -p ${dockerPassword}
                docker build -t guoxudongdocker/flask-python:${Tag} .
                docker push guoxudongdocker/flask-python:${Tag}
                &amp;ldquo;&amp;rsquo;
            }
        }
        stage(&amp;lsquo;Deploy to K8s&amp;rsquo;){
            if (&amp;lsquo;true&amp;rsquo; == &amp;ldquo;${deploy}&amp;rdquo;) {
                container(&amp;lsquo;kubectl&amp;rsquo;) {
                    sh &amp;ldquo;&amp;rsquo;
                    cd deploy/base
                    kustomize edit set image guoxudongdocker/flask-python:${Tag}
                    &amp;ldquo;&amp;rsquo;
                    echo &amp;ldquo;部署到 Kubernetes&amp;rdquo;
                    if (&amp;lsquo;prod&amp;rsquo; == &amp;ldquo;${ENV}&amp;rdquo;) {
                        sh &amp;ldquo;&amp;rsquo;
                        # kustomize build deploy/overlays/prod | kubectl apply -f -
                        kubectl applt -k deploy/overlays/prod
                        &amp;ldquo;&amp;rsquo;
                    }else {
                        sh &amp;ldquo;&amp;rsquo;
                        # kustomize build deploy/overlays/dev | kubectl apply -f -
                        kubectl applt -k deploy/overlays/dev
                        &amp;ldquo;&amp;rsquo;
                    }	
                }
            }else{
                echo &amp;ldquo;跳过Deploy to K8s&amp;rdquo;
            }&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
这里要注意几点：

- 拉取 git 中的代码需要在 jenkins 中配置凭据。
- 笔者的 jenkins 部署在 Kubernetes 上，要操作集群的话，需要将 kubeconfig 以 Secret 的形式挂载到 jenkins 所在 namespace。
- `jenkins-slave` 需要 Java 环境运行，所以要将宿主机的 `jdk` 挂载到 `jenkins-slave` 中。
- 同样的，宿主机中需要事先安装 `docker`。
- `docker-register` 为 dockerhub 的登录凭证，需要在 jenkins 中添加相应的凭证。

## 演示

image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---

### 开始构建

这里选择环境、分支，填入版本即可开始构建，**注意：**这里的版本将已 tag 的形式标记 docker 镜像。

![WX20190507-095142](https://ws2.sinaimg.cn/large/ad5fbf65gy1g2sikst7tuj20ob0evabw.jpg)

这里就可以看到构建成功了

![WX20190507-103721](https://ws2.sinaimg.cn/large/ad5fbf65ly1g2sjw9w22ej20v80km0w3.jpg)

### 查看结果

这里为了方便（其实就是懒），我就不给这个服务添加 ingress 来从外部访问了，这里使用 [KT](https://yq.aliyun.com/articles/690519) 打通本地和 k8s 集群网络来进行调试。

&amp;gt;为了简化在Kubernetes下进行联调测试的复杂度，云效在SSH隧道网络的基础上并结合Kubernetes特性构建了一款面向开发者的辅助工具kt

这里看到这个服务正常启动了

![WX20190507-104154](https://ws2.sinaimg.cn/large/ad5fbf65ly1g2sk11dnzxj20av027jrn.jpg)

### 发布新版本

更新 web 服务并提交

![WX20190507-104936](https://ws4.sinaimg.cn/large/ad5fbf65gy1g2sk94v1c5j209702vwej.jpg)


按照上面步骤在 jenkins 中重新构建，当然也可以配置钩子，每次代码提交后自动构建

### 查看查看新版本

同上面一样，在构建成功后查看服务是否更新

![WX20190507-105539](https://wx4.sinaimg.cn/large/ad5fbf65gy1g2skfczaz4j20by01smx7.jpg)

可以看到，版本已经更新了

### 发布生产环境

这里模拟一下发布生产环境，假设生产环境是在 `devops-prod` 的 namespace 中，这里只做演示之用，真正的生产环境中，可能存在不止一个 k8s 集群，这时需要修改 Jenkinsfile 中的 `secretVolume` 来挂载不同 k8s 的 kubeconfig 来达到发布到不同集群的目的。当然，一般发布生产环境只需选择测试通过的镜像来发布即可，不需要在进行构建打包。

![WX20190507-110730](https://ws3.sinaimg.cn/large/ad5fbf65gy1g2skrnbjyuj20fc0bjmxp.jpg)

### 查看生产版本

![WX20190507-110850](https://ws1.sinaimg.cn/large/ad5fbf65ly1g2skt3rp4yj20aq010glj.jpg)

### 总结

上面的这些步骤简单的演示了使用 jenkins 进行 CI/CD 的流程，流程十分简单，这里仅供参考

## Kustomize 的作用

那么， Kustomize 在整个流程中又扮演了一个什么角色呢？

### 更新镜像

在 `jenkinsfile` 中可以看到， kustomize 更新了基础配置的镜像版本，这里我们之前一直是使用 `sed -i &amp;quot;s/#Tag/${Tag}/g&amp;quot; deploy.yaml` 来进行替换了，但是不同环境存在比较多的差异，需要替换的越来越多，导致 Jekninsfile 也越来越臃肿和难以维护。 kustomize 解决了这个问题。

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
kustomize edit set image guoxudongdocker/flask-python:${Tag}&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
### 环境区分

上面也提到了，不同的环境我们存在这许多差异，虽然看上去大致类似，但是很多细节都需要修改。这时 kustomize 就起到了很大的作用，不同环境相同的配置都放在 `base` 中，而差异就可以在 `overlays` 中实现。

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
.
├── base
│   ├── deployment.yaml
│   ├── kustomization.yaml
│   └── service.yaml
└── overlays
    ├── dev
    │   ├── healthcheck_patch.yaml
    │   ├── kustomization.yaml
    │   └── memorylimit_patch.yaml
    └── prod
        ├── healthcheck_patch.yaml
        ├── kustomization.yaml
        └── memorylimit_patch.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;可以看到， `base` 中维护了项目共同的基础配置，如果有镜像版本等基础配置需要修改，可以使用 `kustomize edit set image ...` 来直接修改基础配置，而真正不同环境，或者不同使用情况的配置则在 `overlays` 中 以 patch 的形式添加配置。这里我的配置是 prod 环境部署的副本为2，同时给到的资源也更多，详情可以在 [Github](https://github.com/sunny0826/flask-python) 上查看。

### 与 kubectl 的集成

在 jenkinsfile 中可以看到

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash&lt;/p&gt;

&lt;h1 id=&#34;kustomize-build-deploy-overlays-dev-kubectl-apply-f&#34;&gt;kustomize build deploy/overlays/dev | kubectl apply -f -&lt;/h1&gt;

&lt;p&gt;kubectl apply -k deploy/overlays/dev
```&lt;/p&gt;

&lt;p&gt;这两条命令的执行效果是一样的，在 &lt;code&gt;kubectl v1.14.0&lt;/code&gt; 以上的版本中，已经集成了 kustomize ，可以直接使用 &lt;code&gt;kubectl&lt;/code&gt; 进行部署。&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;这里只是对 kustomize 在 CI/CD 中简单应用的展示，只是一种比较简单和基础的使用，真正的 CI 流程要比这个复杂的多，这里只是为了演示 kustomize 的使用而临时搭建的。而 kustomize 还有很多黑科技的用法，将会在后续的文章中介绍。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>4月29日 云栖社区分享PPT -- 阿里云容器服务的优势与调优</title>
      <link>https://guoxudong.io/en/post/aliyun-share/</link>
      <pubDate>Tue, 30 Apr 2019 18:46:24 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/aliyun-share/</guid>
      <description>&lt;p&gt;该PPT 为 2019年4月26日 在云栖社区分享使用，这里留作展示和记录，下载地址可以参考下方链接。&lt;/p&gt;

&lt;iframe src=&#34;https://guoxudong.io/aliyun-share/index.html&#34; style=&#34;width: 100%;height:600px;&#34; frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;由于图片资源位于 GitHub 上，国内访问可能会有些慢，建议下载观看。&lt;/p&gt;

&lt;p&gt;PPT 下载地址：&lt;a href=&#34;https://yq.aliyun.com/articles/700084&#34; target=&#34;_blank&#34;&gt;https://yq.aliyun.com/articles/700084&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>炫酷的终端软件 eDEX-UI</title>
      <link>https://guoxudong.io/en/post/edex-ui/</link>
      <pubDate>Mon, 29 Apr 2019 11:55:47 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/edex-ui/</guid>
      <description>

&lt;p&gt;马上就是五一假期了，而且今年的五一假期有4天！想必大家已经安排好是在家写代码还是出门去冒险了。不过在五一假期之前，我这里推荐一个好玩的又好用的软件给大家。&lt;/p&gt;

&lt;p&gt;想必大部分朋友和我一样在上周去看了复联4，其中钢铁侠战衣及设备各种炫酷又极具科技感的操作界面一定让你记忆犹新。很多朋友可能和我一样，都希望拥有一套这样的操作界面，这样不管是工作还是学习都会变得有趣而高效（主要是炫酷）。其实很早以前我就尝试写过，但是由于技术有限，写出来的工具都不是很符合我的要求，渐渐的也就都废弃了。而今天要介绍的这个软件，完全符合我的要求，高端大气上档次，并且还是开源的。&lt;/p&gt;

&lt;h2 id=&#34;edex-ui&#34;&gt;eDEX-UI&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/GitSquared/edex-ui&#34; target=&#34;_blank&#34;&gt;eDEX-UI&lt;/a&gt; 是一个全屏且跨平台、可定制的终端模拟器，具有先进的监控和触摸屏支持。它的外观类似科幻的计算机界面。在保持未来感的外观和感觉的同时，它努力保持一定的功能水平并可用于现实场景，其更大的目标是将科幻用户体验纳入主流。&lt;/p&gt;

&lt;h3 id=&#34;特性&#34;&gt;特性&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;功能齐全的终端仿真器，带有选项卡、颜色、模拟鼠标，并支持 curses 和类似 curses的应用程序。&lt;/li&gt;
&lt;li&gt;实时系统（CPU、RAM、进程）和网络（GeoIP、活动连接、传输速率）监控。&lt;/li&gt;
&lt;li&gt;完全支持触摸屏，包括屏幕键盘。&lt;/li&gt;
&lt;li&gt;具备跟随终端 CWD（当前工作目录）的目录查看器。&lt;/li&gt;
&lt;li&gt;包括主题、屏幕键盘布局、CSS 注入等在内的高级自定义。&lt;/li&gt;
&lt;li&gt;由才华横溢的声音设计师制作的可选音效，可实现最佳的好莱坞黑客氛围。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;显示&#34;&gt;显示&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://yqfile.alicdn.com/b959597643a41c4b83e697307877082124c360d4.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这里我使用了 &lt;code&gt;tron-disrupted&lt;/code&gt; 主题，还有多种主题可以选择&lt;/p&gt;

&lt;p&gt;可以看到这里的界面十分炫酷，可以为有些乏味的 shell 操作增添一抹乐趣&lt;/p&gt;

&lt;h3 id=&#34;配置&#34;&gt;配置&lt;/h3&gt;

&lt;p&gt;eDEX-UI 可以通过 &lt;code&gt;settings.json&lt;/code&gt; 文件进行配置，配置包括执行的 shell 类型、工作目录、键盘类型、主题等&lt;/p&gt;

&lt;p&gt;&lt;code&gt;settings.json&lt;/code&gt; 在 Mac 系统中，存放在 &lt;code&gt;/Users/guoxudong/Library/Application Support/eDEX-UI&lt;/code&gt; 中，默认的工作目录也是这个路径&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx3.sinaimg.cn/large/ad5fbf65gy1g2jflhunukj21h30tck0r.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这里可以看到我选择使用 &lt;code&gt;zsh&lt;/code&gt; 和 &lt;code&gt;tron-disrupted&lt;/code&gt; 主题，并将工作目录改为了我的用户空间&lt;/p&gt;

&lt;h2 id=&#34;局限&#34;&gt;局限&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;目前看来该软件的全平台支持是不错的，同时还支持触摸屏操作，但是目前还未测试在 pad 上使用，测试之后会在后续文章中补充&lt;/li&gt;
&lt;li&gt;CPU 占用过高，该软件 CPU 占用很高，如果是配置一般的电脑不建议让其作为终端常驻，偶尔拿出来玩玩即可&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>阿里云容器服务新建集群优化方案(更新版)</title>
      <link>https://guoxudong.io/en/post/aliyun-k8s-perfect/</link>
      <pubDate>Thu, 25 Apr 2019 22:26:06 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/aliyun-k8s-perfect/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;选择阿里云的&lt;code&gt;容器服务&lt;/code&gt;，主要原因是公司主要业务基本都在运行在阿里云上。相较自建 kubernetes 集群，容器服务的优势在于部署相对简单，与阿里云 VPC 完美兼容，网络的配置相对简单，而如果使用 &lt;code&gt;kubeadm&lt;/code&gt; 安装部署 kubernetes 集群，除了众所周知的科学上网的问题，还有一系列的问题，包括 &lt;code&gt;etcd&lt;/code&gt; 、 &lt;code&gt;Scheduler&lt;/code&gt; 和 &lt;code&gt;Controller-Manager&lt;/code&gt; 的高可用问题等。并且如果使用托管版的阿里云 kubernetes 容器服务，还会省掉3台 master 节点的钱，并且可能将 master 节点的运维问题丢给阿里云解决，并且其提供的 master 节点性能肯定会比自购的配置好，这点是阿里云容器服务的研发小哥在来我司交流时专门强调的。&lt;/p&gt;

&lt;h2 id=&#34;问题&#34;&gt;问题&lt;/h2&gt;

&lt;p&gt;前面吹了阿里云容器服务的优势，那这里就说说在实践中遇到的容器服务的问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在新建集群的时候需要选择相应的 VPC 并选择 &lt;code&gt;Pod&lt;/code&gt; 和 &lt;code&gt;Service&lt;/code&gt; 所在的网段，这两个网段不能和 Node 节点存在于同一网段，但是如果您在阿里云中存在不止一个 VPC （VPC的网段可以是 10.0.0.0/8，172.16-31.0.0/12-16，192.168.0.0/16 ），如果网段设置不对的话，就可能会使原本存在该网段的 ECS 失联，需要删除集群重新创建。如果删除失败的话，还需要手动删除路由表中的记录（&lt;strong&gt;别问我是怎么知道的&lt;/strong&gt;）。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在使用容器服务创建集群后，会创建2个 SLB （之前是3个），一个是 SLB 是在 VPC 上并且绑定一个弹性IP（需要在创建的时候手动勾选创建弹性IP）用于 API Server，一个是经典网络的 SLB 使用提供给 Ingress 使用。但是这两个外网IP创建后的规格都是默认最大带宽、按流量收费，这个并不符合我们的要求，需要手动修改，&lt;del&gt;然而这个修改都会在第二天才能生效&lt;/del&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;容器服务创建集群后，Node 节点的名称会使&lt;code&gt;{region-id}.{ECS-id}&lt;/code&gt;的形式，这个命名方式在集群监控，使用 &lt;code&gt;kubectl&lt;/code&gt; 操作集群方面就显得比较反人类了，每次都要去查 &lt;code&gt;ECS id&lt;/code&gt; 才能确定是哪个节点，而这个 Node 节点名称是不能修改的！&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;网段问题解决&#34;&gt;网段问题解决&lt;/h2&gt;

&lt;p&gt;这个比较好解决，甚至可以说不用解决，只要把网段规划好，不要出现网段冲突就好&lt;/p&gt;

&lt;h2 id=&#34;node-节点名称无法修改问题解决&#34;&gt;Node 节点名称无法修改问题解决&lt;/h2&gt;

&lt;p&gt;这个功能之前已有人在阿里聆听平台提出这个问题了，咨询了容器服务的研发小哥，得到的反馈是该功能已经在灰度测试了，相信很快就可以上线了。&lt;/p&gt;

&lt;h2 id=&#34;创建-slb-规格问题解决&#34;&gt;创建 SLB 规格问题解决&lt;/h2&gt;

&lt;p&gt;相较之前自动创建3个 SLB 的方式，目前的版本只会自动创建2个并且有一个是 VPC 内网+弹性IP的方式，已经进行了优化，但是 ingress 绑定的 SLB 还是经典网络类型，无法接入云防火墙并且规格也是不合适的。这里给出解决方案：&lt;/p&gt;

&lt;h3 id=&#34;方法一-使用-kubectl-配置&#34;&gt;方法一：使用 &lt;code&gt;kubectl&lt;/code&gt; 配置&lt;/h3&gt;

&lt;h4 id=&#34;1-创建新的-slb&#34;&gt;1. 创建新的 SLB&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;这里需要创建一个新的 SLB 用来代替自动创建的不符合要求的 SLB。这里可以先私网 SLB 先不绑定弹性IP。&lt;strong&gt;&lt;em&gt;这里要注意的事，新建的 SLB 需要与 k8s集群处于同一 VPC 内，否则在后续会绑定失败&lt;/em&gt;&lt;/strong&gt;。
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1ma5lxgvdj21ws0s6qa5.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;li&gt;查看新购买 SLB 的 ID
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1ma8zuq1gj20sa0hoq4b.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;2-在创建集群后重新绑定-ingress-controller-的-service&#34;&gt;2. 在创建集群后重新绑定 &lt;code&gt;ingress-controller&lt;/code&gt; 的 &lt;code&gt;Service&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;首先需要使用 &lt;code&gt;kubectl&lt;/code&gt; 或者直接在阿里云控制台操作，创建新的 &lt;code&gt;nginx-ingress-svc&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# nginx ingress service
apiVersion: v1
kind: Service
metadata:
name: nginx-ingress-lb-{new-name}
namespace: kube-system
labels:
    app: nginx-ingress-lb-{new-name}
annotations:
    # set loadbalancer to the specified slb id
    service.beta.kubernetes.io/alicloud-loadbalancer-id: {SLB-ID}
    # set loadbalancer address type to intranet if using private slb instance
    #service.beta.kubernetes.io/alicloud-loadbalancer-address-type: intranet
    service.beta.kubernetes.io/alicloud-loadbalancer-force-override-listeners: &#39;true&#39;
    #service.beta.kubernetes.io/alicloud-loadbalancer-backend-label: node-role.kubernetes.io/ingress=true
spec:
type: LoadBalancer
# do not route traffic to other nodes
# and reserve client ip for upstream
externalTrafficPolicy: &amp;quot;Local&amp;quot;
ports:
- port: 80
    name: http
    targetPort: 80
- port: 443
    name: https
    targetPort: 443
selector:
    # select app=ingress-nginx pods
    app: ingress-nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建成功后，可以进到 SLB 页面查看，可以看到 &lt;code&gt;80&lt;/code&gt; 和 &lt;code&gt;443&lt;/code&gt; 端口的监听已经被添加了
    &lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1maej57c1j21ru0rwq8b.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-绑定符合要求的弹性ip&#34;&gt;3. 绑定符合要求的弹性IP&lt;/h4&gt;

&lt;p&gt;确定 SLB 创建成功并且已经成功监听后，这里就可以为 SLB 绑定符合您需求的弹性IP了，这里我们绑定一个按宽带计费2M的弹性IP&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1mak2r0p3j207k07mq33.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;4-验证连通性&#34;&gt;4. 验证连通性&lt;/h4&gt;

&lt;p&gt;到上面这步，我们的 ingress 入口 SLB 已经创建完成，这里我们验证一下是否联通。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在k8s集群中部署一个 &lt;code&gt;nginx&lt;/code&gt; ，直接在阿里云容器服务控制台操作即可
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1mant7ec6j21s40qegpr.jpg&#34; alt=&#34;image&#34; /&gt;
这里创建 ingress 路由，&lt;strong&gt;注意：这里的域名需要解析到刚才创建的 SLB 绑定的弹性IP&lt;/strong&gt;
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1maqf7gdjj21ns0kymz8.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;访问该域名，显示 &lt;code&gt;nginx&lt;/code&gt; 欢迎页，则证明修改成功
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1mat8srhnj21ak0hmact.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;方法二-使用阿里云容器服务控制台配置&#34;&gt;方法二： 使用阿里云容器服务控制台配置&lt;/h3&gt;

&lt;h4 id=&#34;1-阿里云容器控制台创建新-service&#34;&gt;1. 阿里云容器控制台创建新 &lt;code&gt;service&lt;/code&gt;&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;在阿里云容器服务控制台：&lt;code&gt;路由与负载均衡&lt;/code&gt; &amp;ndash;&amp;gt; &lt;code&gt;服务&lt;/code&gt; 点击&lt;code&gt;创建&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;选择 &lt;code&gt;kube-system&lt;/code&gt; 命名空间&lt;/li&gt;
&lt;li&gt;类型选中&lt;code&gt;负载均衡&lt;/code&gt; - &lt;code&gt;内网访问&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;关联 &lt;code&gt;nginx-ingress-controller&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;并添加端口映射&lt;/li&gt;
&lt;li&gt;点击创建&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g2g4fwfgevj20i50hsgmp.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;2-进入负载均衡查看-slb-是否创建&#34;&gt;2. 进入负载均衡查看 SLB 是否创建&lt;/h4&gt;

&lt;p&gt;可见 SLB 已经成功创建&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g2g4pb1d45j215303c74r.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-绑定符合要求的弹性ip-1&#34;&gt;3. 绑定符合要求的弹性IP&lt;/h4&gt;

&lt;p&gt;同方法一&lt;/p&gt;

&lt;h4 id=&#34;4-验证连通性-1&#34;&gt;4.验证连通性&lt;/h4&gt;

&lt;p&gt;同方法一&lt;/p&gt;

&lt;h3 id=&#34;后续操作&#34;&gt;后续操作&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;在确定新的 SLB 创建成功后，就可以将容器服务自动创建的 SLB 释放了&lt;/li&gt;
&lt;li&gt;删除 &lt;code&gt;kube-system&lt;/code&gt; 中原本绑定的 &lt;code&gt;Service&lt;/code&gt; &lt;strong&gt;（目前版本已经可以关联删除绑定的 SLB 了，不用分开操作）&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;这里别忘了，自动创建给API Server 的SLB还是按流量付费的，记得降配&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;上面的这些问题和解决方案都属于临时方案，已在阿里的聆听平台提出了上面的问题，相信很快就会有所改进。总的来说，阿里云容器服务在提供优质的 kubernetes 功能，并且只收 ECS 的钱，对于想学习 kubernetes 又没有太多资金的同学也比较友好，直接买按量付费实例，测试完释放即可，不用购买 master 节点，十分良心！&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>困难的 Kubernetes</title>
      <link>https://guoxudong.io/en/post/kubernetes-is-har/</link>
      <pubDate>Wed, 24 Apr 2019 10:18:46 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kubernetes-is-har/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;虽然 Kubernetes 赢得了容器之站，但是其仍然很难使用并且时长引起事故。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我想我应该给这篇文章做一点序言。 &lt;a href=&#34;https://kubernetes.io/&#34; target=&#34;_blank&#34;&gt;kubernetes&lt;/a&gt; 为许多应用程序提供新的 runtime ，如果使用得当，它可以成为一个强大的工具，并且可以将您冲复杂的开发生命周期中解放出来。然而在过去的几年里，我看到很多人和公司都会搭建他们的 Kubernetes ，但常常只是处于测试阶段，从未进入到生产。&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-是如何运作的&#34;&gt;Kubernetes 是如何运作的？&lt;/h2&gt;

&lt;p&gt;粗略的讲， Kubernetes 或者 K8S 看起来十分简单。您运行的 Kubernetes 节点至少被分为两类：Master 和 Workers。Master 节点通常不运行任何真实的工作负载，那是 Workers 节点的工作。&lt;/p&gt;

&lt;p&gt;Kubernetes 的 Master 节点包含一个名叫 API server 的组件，其提供的 API 可以通过 &lt;code&gt;kubectl&lt;/code&gt; 调用。此外还包括一个 scheduler ，负责调度容器，决定容器运行在哪个节点。最后一个组件是 controller-manager ，它实际上是一组多个控制器，负责处理节点中断、复制、加入 services 和 pods ，并且处理授权相关内容。所有的数据都存储在 etcd 中，这是一个可信赖的分布式键值存储服务（包含一些非常酷的功能）。总而言之，Master 节点负责管理集群，这里没什么特别大的惊喜。&lt;/p&gt;

&lt;p&gt;另一方面， 真实的工作负载运行在 Worker 节点上。为此，它还包括许多组件。首先，Worker 节点上会运行 &lt;strong&gt;&lt;em&gt;kubelet&lt;/em&gt;&lt;/strong&gt; ，它是与该节点上的容器一起运行的 API ，负责与管控组件沟通，并按照管控组件指示管理 Worker 节点。另一个组件就是 &lt;strong&gt;&lt;em&gt;kube-proxy&lt;/em&gt;&lt;/strong&gt; ，其负责转发网络连接，根据您的配置运行容器。可能还有其他东西，如 &lt;strong&gt;&lt;em&gt;kube-dns&lt;/em&gt;&lt;/strong&gt; 或 &lt;strong&gt;&lt;em&gt;gVisor&lt;/em&gt;&lt;/strong&gt;。您还需要集成某种 &lt;strong&gt;&lt;em&gt;overlay network&lt;/em&gt;&lt;/strong&gt; 或底层网络设置，以便 Kubernetes 可以管理您的 pod 之间的网络。&lt;/p&gt;

&lt;p&gt;如果您想要一个更完整的概述，建议去看 Kelsey Hightowers 的 &lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way&#34; target=&#34;_blank&#34;&gt;Kubernetes  -  The Hard Way&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;生产就绪的-kubernetes&#34;&gt;生产就绪的 Kubernetes&lt;/h2&gt;

&lt;p&gt;到目前为止，这听起来并不太糟糕。只是安装几个程序、配置、证书等。不要误会我的意思，这仍然是一个学习曲线，但这也不是系统管理员不能处理的问题。&lt;/p&gt;

&lt;p&gt;然而，简单地手动安装 Kubernetes 并不代表其已经完全准备就绪，所以让我们谈谈让这个东西运行起来所需的步骤。&lt;/p&gt;

&lt;p&gt;首先，&lt;strong&gt;安装&lt;/strong&gt;。如果您想要某种自动安装，无论是使用 Ansible ， Terraform 还是其他工具。&lt;a href=&#34;https://github.com/kubernetes/kops&#34; target=&#34;_blank&#34;&gt;kops&lt;/a&gt; 可以帮助您解决这个问题，但是使用 kops 意味着您将不知道它是如何设置的，并且当您以后想要调试某些东西时可能会引起一些其他问题。应对此自动化进行测试，并定期进行检查。&lt;/p&gt;

&lt;p&gt;其次，您需要&lt;strong&gt;监控&lt;/strong&gt;您的 Kubernetes 安装。所以您需要 &lt;a href=&#34;https://prometheus.io/&#34; target=&#34;_blank&#34;&gt;Prometheus&lt;/a&gt; 、 &lt;a href=&#34;https://grafana.com/&#34; target=&#34;_blank&#34;&gt;Grafana&lt;/a&gt; 等工具。您是在 Kubernetes 里面运行它吗？ 如果您的 Kubernetes 有问题，那么您的监控是否会也会挂掉？ 或者您单独运行它？ 如果是，那么您在哪里运行它？&lt;/p&gt;

&lt;p&gt;另外值得注意的是&lt;strong&gt;备份&lt;/strong&gt;。如果您的 Master 崩溃，数据无法恢复并且您需要重新配置系统上的所有 pod ，您会怎么做？您是否测试了再次运行 CI 系统中所有作业所需的时间？您有灾难恢复计划吗？&lt;/p&gt;

&lt;p&gt;现在，既然我们在谈论 CI 系统，那么您需要为您的镜像运行 Docker 镜像仓库。当然，您可以再次在 Kubernetes 上做，但如果 Kubernetes 崩溃&amp;hellip;&amp;hellip;您知道这个后果。当然，CI 系统与运行版本控制系统都有这个问题。理想情况下，这些系统是与生产环境隔离的，以便在系统出现问题时，至少可以访问 git ，来进行重新部署等操作。&lt;/p&gt;

&lt;h2 id=&#34;数据存储&#34;&gt;数据存储&lt;/h2&gt;

&lt;p&gt;最后，我们来谈谈最重要的部分：存储。Kubernetes 本身并不提供存储解决方案。当然，您可以将存储挂载到主机安装目录，但这既不推荐也不简单。&lt;/p&gt;

&lt;p&gt;基本上需要在 Kubernetes 下使用某种存储。例如，&lt;a href=&#34;https://rook.io/&#34; target=&#34;_blank&#34;&gt;rook&lt;/a&gt; 使得运行 &lt;a href=&#34;https://ceph.com/&#34; target=&#34;_blank&#34;&gt;Ceph&lt;/a&gt; 作为底层块存储需求的变得相对简单，但我对 Ceph 的体验是它还有有很多地方需要调整，所以您绝不是只需点击下一步就能走出困境。&lt;/p&gt;

&lt;h2 id=&#34;调试&#34;&gt;调试&lt;/h2&gt;

&lt;p&gt;在与开发人员谈论 Kubernetes 时，一种常见的回答经常出现：在使用 Kubernetes 时，人们常常在调试应用程序时遇到问题。即使是一个例如容器未能启动的简单问题，也会引起混乱。&lt;/p&gt;

&lt;p&gt;当然，这是一个教育问题。在过去的几十年中，开发人员已经学会了调试的“经典”步骤：在 &lt;code&gt;/vat/log/&lt;/code&gt; 中查看日志等。但是对于容器，我们甚至不知道容器运行在哪个服务器上，因此它呈现出了一种范式转换。&lt;/p&gt;

&lt;h2 id=&#34;问题-复杂&#34;&gt;问题：复杂&lt;/h2&gt;

&lt;p&gt;您可能已经注意到我正在跳过共有云提供商给您的东西，即使它不是一个完整的托管 Kubernetes。当然，如果您使用托管的 Kubernetes 解决方案，这很好，除了调试之外，您不需要处理上面这些问题。&lt;/p&gt;

&lt;p&gt;Kubernetes 拥有许多可以移动组件，但 Kubernetes 本身也并不能提供完整的解决方案。例如，&lt;a href=&#34;https://www.openshift.com/&#34; target=&#34;_blank&#34;&gt;RedHat OpenShift&lt;/a&gt; 可以，但它需要花钱，并且仍然需要添加自己的东西。&lt;/p&gt;

&lt;p&gt;现在Kubernetes正处于 &lt;a href=&#34;https://www.gartner.com/en/research/methodologies/gartner-hype-cycle&#34; target=&#34;_blank&#34;&gt;Gartner hype cycle&lt;/a&gt; 的顶峰，每个人都想要它，但很少有人真正理解它。在接下来的几年里，不少公司必须意识到 Kubernetes 并不是银弹，而如何正确有效地使用它才是关键。&lt;/p&gt;

&lt;p&gt;我认为，如果您有能力将 Ops 团队专门用于为开发人员来维护底层平台，那么运行自己的 Kubernetes 是值得的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;本文作者：&lt;a href=&#34;https://pasztor.at/&#34; target=&#34;_blank&#34;&gt;Janos Pasztor&lt;/a&gt;  2018-12-04&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;原文地址：&lt;a href=&#34;https://pasztor.at/blog/kubernetes-is-hard&#34; target=&#34;_blank&#34;&gt;https://pasztor.at/blog/kubernetes-is-hard&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GitHub 黑话指南</title>
      <link>https://guoxudong.io/en/post/cant/</link>
      <pubDate>Mon, 22 Apr 2019 09:11:24 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/cant/</guid>
      <description>

&lt;p&gt;这里的&lt;strong&gt;黑话&lt;/strong&gt;，其实就是 GitHub 上的一些&lt;strong&gt;迷之缩写&lt;/strong&gt;，这些歪果老司机们在 GitHub 上肆无忌惮的使用着他们的“黑话”，让我们这些非英语母语国家的新司机在 code review 时经常是一脸懵逼 + 黑人问好&amp;hellip;&lt;/p&gt;

&lt;p&gt;image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://wx3.sinaimg.cn/thumb180/ad5fbf65gy1g2b5dhfuqqj206506cq2t.jpg&#34; alt=&#34;image&#34; /&gt;
&lt;img src=&#34;https://ws1.sinaimg.cn/thumb180/ad5fbf65gy1g2b5dvubm6j2069050a9x.jpg&#34; alt=&#34;image&#34; /&gt;
&lt;img src=&#34;https://ws1.sinaimg.cn/thumb180/ad5fbf65gy1g2b5fe5ahmj20c80bhdg8.jpg&#34; alt=&#34;image&#34; /&gt;
&lt;/center&gt;
image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false-1&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;p&gt;记住下面这些“&lt;strong&gt;黑话&lt;/strong&gt;”，以后我们也可以愉快的在 GitHub 上装逼了~&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PR&lt;/strong&gt;: Pull Request. 拉取请求，给其他项目提交代码，这个是最为常见的缩写。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WIP&lt;/strong&gt;: Work In Progress. 传说中提 PR 的最佳实践是，如果你有个改动很大的 PR，可以在写了一部分的情况下先提交，但是在标题里写上 WIP，以告诉项目维护者这个功能还未完成，方便维护者提前 review 部分提交的代码。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LGTM&lt;/strong&gt;: Looks Good To Me. &lt;strong&gt;&lt;em&gt;已阅&lt;/em&gt;&lt;/strong&gt; 代码已经过 review，可以合并。也可理解为 “OJBK”。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SGTM&lt;/strong&gt;: Sounds Good To Me. 同上，也是已经通过了 review 的意思。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PTAL&lt;/strong&gt;: Please Take A Look. 你来瞅瞅？用来提示别人来看一下。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TBR&lt;/strong&gt;: To Be Reviewed. 提示维护者进行 review。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Too Long; Didn&amp;rsquo;t Read. 太长懒得看。也有很多文档在做简略描述之前会写这么一句，第一次看到的反应是：这TM是啥？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TBD&lt;/strong&gt;: To Be Done(or Defined/Discussed/Decided/Determined). 根据语境不同意义有所区别，但一般都是还没搞定的意思。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://farer.org/2017/03/01/code-review-acronyms/&#34; target=&#34;_blank&#34;&gt;LGTM? 那些迷之缩写 - farer.org&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>什么的容器？Docker 工作原理及容器化简易指南</title>
      <link>https://guoxudong.io/en/post/what-are-containers-a-simple-guide-to-containerization-and-how-docker-works/</link>
      <pubDate>Sat, 20 Apr 2019 19:54:50 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/what-are-containers-a-simple-guide-to-containerization-and-how-docker-works/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Docker 非常棒！&lt;/strong&gt; 它使软件开发者无需担心配置和依赖性，在任何地方打包，发送和运行他们的应用程序。而在与 kubernetes 相结合后，它使应用集群部署和管理变得更方便。这使得 Docker 深受软件开发者的喜爱，越来越多的开发者开始使用 Docker。&lt;/p&gt;

&lt;p&gt;那么 Docker 到底是什么？&lt;/p&gt;

&lt;p&gt;它是构建、测试、部署和发布&lt;strong&gt;容器化&lt;/strong&gt;应用的&lt;strong&gt;平台&lt;/strong&gt;。称其为平台是因为 Docker 其实是一套用于管理与容器相关的所有事物的工具。作为 Docker 的核心，接下来我们将深入探讨容器。&lt;/p&gt;

&lt;h2 id=&#34;什么是容器&#34;&gt;什么是容器？&lt;/h2&gt;

&lt;p&gt;容器提供了在计算机上的隔离环境中安装和运行应用程序的方法。在容器内运行的应用程序仅可使用于为该容器分配的资源，例如：CPU，内存，磁盘，进程空间，用户，网络，卷等。在使用有限的容器资源的同时，并不与其他容器冲突。您可以将容器视为简易计算机上运行应用程序的隔离沙箱。&lt;/p&gt;

&lt;p&gt;这个概念听起来很熟悉，有些类似于虚拟机。但它们有一个关键的区别：容器使用的一种非常不同的，轻量的技术来实现资源隔离。容器利用了底层 Linux 内核的功能，而不是虚拟机采用的  &lt;a href=&#34;https://en.wikipedia.org/wiki/Hypervisor&#34; target=&#34;_blank&#34;&gt;hypervisor&lt;/a&gt; 的方法。换句话说，容器调用 Linux 命令来分配和隔离出一组资源，然后在此空间中运行您的应用程序。我们快速来看下两个这样的功能：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;namespaces&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;简单的讲就是，&lt;a href=&#34;http://man7.org/linux/man-pages/man7/namespaces.7.html&#34; target=&#34;_blank&#34;&gt;Linux namespace&lt;/a&gt; 允许用户在独立进程之间隔离 CPU 等资源。进程的访问权限及可见性仅限于其所在的 namespaces 。因此，用户无需担心在一个 namespace 内运行的进程与在另一个 namespace 内运行的进程冲突。甚至可以同一台机器上的不同容器中运行具有相同 PID 的进程。同样的，两个不同容器中的应用程序可以使用相同的端口。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;cgroups&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://man7.org/linux/man-pages/man7/cgroups.7.html&#34; target=&#34;_blank&#34;&gt;cgroups&lt;/a&gt; 允许对可用资源设置限制和约束。例如，您可以在一台拥有 16G 内存的计算机上创建一个 namespace ，限制其内部进程可用内存为 1GB。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;到这，您可能已经猜到 Docker 的工作原理了。当您请求 Docker 运行容器时，Docker 会在您的计算机上设置一个资源隔离的环境。然后 Docker 会将打包的应用程序和关联的文件复制到 namespace 内的文件系统中，此时环境的配置就完成了。之后 Docker 会执行您指定的命令运行应用程序。&lt;/p&gt;

&lt;p&gt;简而言之，Docker 通过使用 Linux namespace 和 cgroup（以及其他一些命令）来协调配置容器，将应用程序文件复制到为容器分配的磁盘，然后运行启动命令。Docker 还附带了许多其他用于管理容器的工具，例如：列出正在运行的容器，停止容器，发布容器镜像等许多其他工具。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g2a8h1rc6lj211a0rcjsu.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;与虚拟机相比，容器更轻量且速度更快，因为它利用了 Linux 底层操作系统在隔离的环境中运行。虚拟机的 hypervisor 创建了一个非常牢固的边界，以防止应用程序突破它，而&lt;a href=&#34;https://sysdig.com/blog/container-isolation-gone-wrong/&#34; target=&#34;_blank&#34;&gt;容器的边界不那么强大&lt;/a&gt;。另一个区别是，由于 namespace 和 cgroups 功能仅在 Linux 上可用，因此容器无法在其他操作系统上运行。此时您可能想知道 Docker 如何在 macOS 或 Windows 上运行？ Docker 实际上使用了一个技巧，并在非 Linux 操作系统上安装 Linux 虚拟机，然后在虚拟机内运行容器。&lt;/p&gt;

&lt;p&gt;让我们利用目前为止学到的所有内容，从头开始创建和运行 Docker 容器。如果你还没有将 Docker 安装在你的机器上，可以参考&lt;a href=&#34;https://docs.docker.com/install/&#34; target=&#34;_blank&#34;&gt;这里&lt;/a&gt;安装 Docker 。在这个示例中，我们将创建一个 Docker 容器，下载一个用 C语言 写的 Web 服务，编译并运行它，然后使用浏览器访问这个 Web 服务。&lt;/p&gt;

&lt;p&gt;我们将从所有 Docker 项目开始的地方：创建一个 &lt;code&gt;Dockerfile&lt;/code&gt; 开始。此文件描述了如何创建用于运行容器的 docker 镜像。既然我们还没有聊到镜像，那么让我们看一下&lt;a href=&#34;https://docs.docker.com/get-started/#images-and-containers&#34; target=&#34;_blank&#34;&gt;镜像的官方定义&lt;/a&gt;：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;镜像是一个可执行包，其包含运行应用程序所需的代码、运行时、库、环境变量和配置文件，容器是镜像的运行时实例。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;简单的讲，当你要求 Docker 运行一个容器时，你必须给它一个包含如下内容的镜像：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;包含应用程序及其所有依赖的&lt;strong&gt;文件系统快照&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;容器启动时的运行命令。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在 Docker 的世界，使用别人的镜像作为基础镜像来创建自己的镜像是十分普遍的。例如，官方 reds Docker 镜像就是基于 Debian 文件系统快照（&lt;a href=&#34;http://www.ethernetresearch.com/geekzone/building-linux-rootfs-from-scratch/&#34; target=&#34;_blank&#34;&gt;rootfs tarball&lt;/a&gt;），并安装在其上配置 Redis。&lt;/p&gt;

&lt;p&gt;在我们的示例中，我们选择 &lt;a href=&#34;https://hub.docker.com/_/alpine&#34; target=&#34;_blank&#34;&gt;Alpine Linux&lt;/a&gt; 为基础镜像。当您在 Docker 中看到 “alpine” 时，它通常意味着一个精简的基本镜像。 Alpine Linux 镜像大小只有约为5 MB！&lt;/p&gt;

&lt;p&gt;在您的计算机创建一个新目录（例如 &lt;code&gt;dockerprj&lt;/code&gt; ），然后新建一个 &lt;code&gt;Dockerfile&lt;/code&gt; 文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;umermansoor:dockerprj$ touch Dockerfile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将如下内容粘贴到 &lt;code&gt;Dockerfile&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-docker&#34;&gt;# Use Alpine Linux rootfs tarball to base our image on
FROM alpine:3.9 

# Set the working directory to be &#39;/home&#39;
WORKDIR &#39;/home&#39;

# Setup our application on container&#39;s file system
RUN wget http://www.cs.cmu.edu/afs/cs/academic/class/15213-s00/www/class28/tiny.c \
  &amp;amp;&amp;amp; apk add build-base \
  &amp;amp;&amp;amp; gcc tiny.c -o tiny \
  &amp;amp;&amp;amp; echo &#39;Hello World&#39; &amp;gt;&amp;gt; index.html

# Start the web server. This is container&#39;s entry point
CMD [&amp;quot;./tiny&amp;quot;, &amp;quot;8082&amp;quot;]

# Expose port 8082
EXPOSE 8082 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 &lt;code&gt;Dockerfile&lt;/code&gt; 包含创建镜像的内容说明。我们创建镜像基于 Alpine Linux（&lt;a href=&#34;http://www.ethernetresearch.com/geekzone/building-linux-rootfs-from-scratch/&#34; target=&#34;_blank&#34;&gt;rootfs tarball&lt;/a&gt;），并将工作目录设置为 &lt;code&gt;/home&lt;/code&gt; 。接下来下载，编译并创建了一个用C编写的简单 Web 服务器的可执行文件，然后指定在运行容器时要执行的命令，并将容器端口8082暴露给主机。&lt;/p&gt;

&lt;p&gt;现在，我们就可以构建镜像了。在 &lt;code&gt;Dockerfile&lt;/code&gt; 的同级目录运行 &lt;code&gt;docker build&lt;/code&gt; 命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;umermansoor:dockerprj$ docker build -t codeahoydocker .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果这个命令成功了，您将看到：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Successfully tagged codeahoydocker:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时我们的镜像就创建成功了，该镜像主要包括：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;文件系统快照（Alpine Linux 和 我们安装的 Web 服务）&lt;/li&gt;
&lt;li&gt;启动命令（&lt;code&gt;./tiny 8092&lt;/code&gt;）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g2aakgpe16j20zo0bqjt5.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;既然成功构建了镜像，那么我们可以使用如下命令运行容器。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;umermansoor:dockerprj$ docker run -p 8082:8082 codeahoydocker:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;让我们了解下这里发生了什么。&lt;/p&gt;

&lt;p&gt;通过 &lt;code&gt;docker run&lt;/code&gt; 命令，我们请求 Docker 基于 &lt;code&gt;codeahoydocker:latest&lt;/code&gt; 镜像创建和启动一个容器。&lt;code&gt;-p 8082:8082&lt;/code&gt; 将本地的8082端口映射到容器的8082端口（容器内的 Web 服务器正在监听8082端口上的连接）。打开你的浏览器并访问 localhost:8082/index.html 。你将可以看到 &lt;strong&gt;&lt;em&gt;Hello World&lt;/em&gt;&lt;/strong&gt; 信息。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g2aazadeamj20yo0rcq5e.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;最后我想补充一点，虽然 Docker 非常棒，而且对于大多数项目来说它是一个不错的选择，但我们并非处处都要使用它。在我的工作中，Docker 与 Kubernetes 结合使用，可以非常轻松地部署和管理后端微服务，我们不必为每个服务配置新的运行环境。另一方面，对于性能密集型应用程序，Docker 可能不是最佳选择。我经手的其中一个项目必须处理来自移动游戏客户端的 TCP 长连接（每台机器1000个），这时 Docker 网络出现了很多问题，导致无法将它用于该项目。&lt;/p&gt;

&lt;p&gt;希望上面这些内容有用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;这篇文章由 &lt;a href=&#34;https://www.linkedin.com/in/umansoor&#34; target=&#34;_blank&#34;&gt;Umer Mansoor&lt;/a&gt; 撰写，可以在 &lt;a href=&#34;https://www.facebook.com/codeahoy&#34; target=&#34;_blank&#34;&gt;Facebook&lt;/a&gt; 或 &lt;a href=&#34;https://twitter.com/codeahoy&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt; 上关注并留下评论。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;原文地址： &lt;a href=&#34;https://codeahoy.com/2019/04/12/what-are-containers-a-simple-guide-to-containerization-and-how-docker-works/&#34; target=&#34;_blank&#34;&gt;https://codeahoy.com/2019/04/12/what-are-containers-a-simple-guide-to-containerization-and-how-docker-works/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Kustomize 帮你管理 kubernetes 应用（二）： Kustomize 的使用方法</title>
      <link>https://guoxudong.io/en/post/kustomize-2/</link>
      <pubDate>Fri, 19 Apr 2019 16:05:02 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kustomize-2/</guid>
      <description>

&lt;p&gt;本文介绍使用和维护 Kustomize 的方法及步骤。&lt;/p&gt;

&lt;h2 id=&#34;定制配置&#34;&gt;定制配置&lt;/h2&gt;

&lt;p&gt;在这个工作流方式中，所有的配置文件（ YAML 资源）都为用户所有，存在于私有 repo 中。其他人是无法使用的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g2813d1ia7j20qo0f0dgk.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;创建一个目录用于版本控制&lt;/p&gt;

&lt;p&gt;我们希望将一个名为 &lt;strong&gt;&lt;em&gt;ldap&lt;/em&gt;&lt;/strong&gt; 的 Kubernetes 集群应用的配置保存在自己的 repo 中。
这里使用 &lt;code&gt;git&lt;/code&gt; 进行版本控制。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git init ~/ldap
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建一个 &lt;code&gt;base&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p ~/ldap/base
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个目录中创建并提交 &lt;code&gt;kustomization.yaml&lt;/code&gt; 文件和一组资源，例如 &lt;code&gt;deployment.yaml&lt;/code&gt; &lt;code&gt;service.yaml&lt;/code&gt; 等。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建 &lt;code&gt;overlays&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p ~/ldap/overlays/staging
mkdir -p ~/ldap/overlays/production
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个目录都需要一个 &lt;code&gt;kustomization.yaml&lt;/code&gt; 文件以及一个或多个 &lt;code&gt;patch&lt;/code&gt; ，例如 &lt;code&gt;healthcheck_patch.yaml&lt;/code&gt; &lt;code&gt;memorylimit_patch.yaml&lt;/code&gt; 等。。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-staging```&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;production&lt;code&gt;目录则可能会在&lt;/code&gt;deployment``` 中增加在副本数。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;生成 &lt;code&gt;variants&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;运行 &lt;code&gt;kustomize&lt;/code&gt; ，将生成的配置用于 kubernetes 应用部署&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kustomize build ~/ldap/overlays/staging | kubectl apply -f -
kustomize build ~/ldap/overlays/production | kubectl apply -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 kubernetes 1.14 版本， &lt;code&gt;kustomize&lt;/code&gt; 已经集成到 &lt;code&gt;kubectl&lt;/code&gt; 命令中，成为了其一个子命令，可使用 &lt;code&gt;kubectl&lt;/code&gt; 来进行部署&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -k ~/ldap/overlays/staging
kubectl apply -k ~/ldap/overlays/production
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;使用现成的配置&#34;&gt;使用现成的配置&lt;/h2&gt;

&lt;p&gt;在这个工作流方式中，可从别人的 repo 中 fork kustomize 配置，并根据自己的需求来配置。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g281xyfebej20qo0f0dgr.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;通过 fork/modify/rebase 等方式获得配置&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;将其克隆为你自己的 &lt;code&gt;base&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;在这个 &lt;code&gt;bash&lt;/code&gt; 目录维护在一个 repo 中，在这个例子使用 &lt;code&gt;ladp&lt;/code&gt; 的 repo&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir ~/ldap
git clone https://github.com/$USER/ldap ~/ldap/base
cd ~/ldap/base
git remote add upstream git@github.com:$USER/ldap
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建 &lt;code&gt;overlays&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;如上面的案例一样，创建并完善 &lt;code&gt;overlays&lt;/code&gt; 目录中的内容&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p ~/ldap/overlays/staging
mkdir -p ~/ldap/overlays/production
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用户可以将 &lt;code&gt;overlays&lt;/code&gt; 维护在不同的 repo 中&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;生成 &lt;code&gt;variants&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kustomize build ~/ldap/overlays/staging | kubectl apply -f -
kustomize build ~/ldap/overlays/production | kubectl apply -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 kubernetes 1.14 版本， &lt;code&gt;kustomize&lt;/code&gt; 已经集成到 &lt;code&gt;kubectl&lt;/code&gt; 命令中，成为了其一个子命令，可使用 &lt;code&gt;kubectl&lt;/code&gt; 来进行部署&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -k ~/ldap/overlays/staging
kubectl apply -k ~/ldap/overlays/production
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;（可选）更新 &lt;code&gt;base&lt;/code&gt;
用户可以定期从上游 repo 中 &lt;code&gt;rebase&lt;/code&gt; 他们的 &lt;code&gt;base&lt;/code&gt; 以保证及时更新&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ~/ldap/base
git fetch upstream
git rebase upstream/master
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/blob/master/docs/workflows.md&#34; target=&#34;_blank&#34;&gt;kustomize workflows - github.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Rancher 2.2.1 解决工作负载监控为空问题</title>
      <link>https://guoxudong.io/en/post/rancher-prometheus-fix-question/</link>
      <pubDate>Thu, 18 Apr 2019 17:46:08 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/rancher-prometheus-fix-question/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;Rancher 2.2.X 版本于3月底正式GA，新版本处理其他部分的优化以外，最大亮点莫过于本身集成了 Prometheus ，可以通过 Rancher 自带 UI 或者 Grafana 查看集群的实时监控，对所有监控进行了一次聚合，不用再和之前一样，每个集群都要安装一个 Prometheus 用于监控，而告警部分也可使用 Rancher 自带的通知组件进行告警。通知方式目前支持 Slack 、 邮件、 PagerDuty 、 Webhook 、 企业微信，由于我司办公使用钉钉，所以我们使用了 Webhook 的方式，告警触发后通知我们的消息服务，然后消息服务将其发送到钉钉进行告警。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx2.sinaimg.cn/large/ad5fbf65gy1g26xsh6omvj20rk0ilta6.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;问题&#34;&gt;问题&lt;/h2&gt;

&lt;p&gt;Rancher 集成 Prometheus 后，监控方面变的十分强大，不用再徘徊于多个集群的 Grafana ，直接在 Rancher 上即可查看，非常方便&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx2.sinaimg.cn/large/ad5fbf65gy1g26xuv2frnj212b0onn1h.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;但是在使用的时候，我发现了一个问题：就是在查看 工作负载和 Pod 的时候会显示 &lt;strong&gt;&lt;em&gt;没有足够的数据绘制图表&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g26xzvi2cpj20po057q31.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;进入 Grafana 查看会发现，其实监控参数是存在的，但是没有采集到值，所以并没有展示出来。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g26y4j4s3yj21f50m9wqj.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;解决&#34;&gt;解决&lt;/h2&gt;

&lt;p&gt;在检查了配置后并没有找到原因，只好去 GitHub 上提一个 issue 来询问一下开发者或者其他用户有无遇到这个问题。&lt;/p&gt;

&lt;p&gt;Rancher 官方的开发者还是十分负责的， GitHub 上用户名为 &lt;a href=&#34;https://github.com/loganhz&#34; target=&#34;_blank&#34;&gt;Logan&lt;/a&gt; 的官方小哥来我指导解决这个问题。&lt;/p&gt;

&lt;p&gt;小哥发现我是导入的集群，要我进入 Prometheus 查看，发现 &lt;code&gt;cattle-prometheus/exporter-kube-state-cluster-monitoring&lt;/code&gt; 果然没有起来&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g26yb1p4eoj21db0am782.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;解决这个问题，需要在集群监控配置中添加一个高级选项，插入值为：&lt;code&gt;exporter-kubelets.https=false&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g26ycq6amfj221q0uggp8.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;点击保存，问题就解决了！&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g26yheqwp7j213e0g3di5.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;使用 Rancher 有半年，从2.0版本一直用到2.2版本，而18年分别在云栖大会和 KubeCon 上听了 Rancher 创始人梁胜博士的演讲。而从这一个小问题上就可以看到 Rancher 官方对每一个用户都是十分重视的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kustomize: 无需模板定制你的 kubernetes 配置</title>
      <link>https://guoxudong.io/en/post/introducing-kustomize-template-free-configuration-customization-for-kubernetes/</link>
      <pubDate>Mon, 15 Apr 2019 17:23:21 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/introducing-kustomize-template-free-configuration-customization-for-kubernetes/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;作者：Jeff Regan (Google), Phil Wittrock (Google) 2018-05-29&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果你在运行 kubernetes 集群，你可能会拷贝一些包含 kubernetes API 对象的 YAML 文件，并且根据你的需求来修改这些文件，通过这些 YAML 文件来定义你的 kubernetes 配置。&lt;/p&gt;

&lt;p&gt;但是这种方法存在很难找到配置的源头并对其进行改进。今天 Google 宣布推出 &lt;strong&gt;Kustomize&lt;/strong&gt; ，一个作为 &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-cli&#34; target=&#34;_blank&#34;&gt;SIG-CLI&lt;/a&gt; 子项目的命令行工具。这个工具提供了一个全新的、纯粹的声明式的方法来定制 kubernetes 配置，遵循并利用我们熟悉且精心设计的 Kubernetes API。&lt;/p&gt;

&lt;p&gt;有这样一个常见的场景，在互联网上可以看到别人的 CMS（content management system，内容管理系统）的 kubernetes 配置，这个配置是一组包括 Kubernetes API 对象的 YAML 描述文件。然后，在您自己公司的某个角落，您找到一个你非常了解的数据库，希望用它来该 CMS 的数据。&lt;/p&gt;

&lt;p&gt;你希望同时使用它们，此外，你希望自定义配置文件以便你的资源实例在集群中显示，并通过添加一个标签来区分在同一集群中做同样事情的其他资源。同时也希望为其配置适当的 CPU 、内存和副本数。&lt;/p&gt;

&lt;p&gt;此外，你还想要配置整个配置的多种变化：一个专门用于测试和实验的小服务实例（就计算资源而言），或更大的用于对外提供服务的生产级别的服务实例。同时，其他的团队也希望拥有他们自己的服务实例。&lt;/p&gt;

&lt;h2 id=&#34;定制就是复用&#34;&gt;定制就是复用&lt;/h2&gt;

&lt;p&gt;kubernetes 的配置并不是代码（是使用 YAML 描述的 API 对象，严格来说应该是数据），但是配置的生命周期与代码的生命周期有许多相似之处。&lt;/p&gt;

&lt;p&gt;你需要在版本控制中保留配置。所有者的配置不必与使用者的配置相同。配置可以作为整体的一部分。而用户希望为在不同的情况下复用这些配置。&lt;/p&gt;

&lt;p&gt;与代码复用相同，一种复用配置的方法是简单的全部拷贝并进行自定义。像代码一样，切断与源代码的联系使得从改进变的十分困难。许多团队和环境都使用这种方法，每个团队和环境都拥有自己的配置，这使得简单的升级变得十分棘手。&lt;/p&gt;

&lt;p&gt;另一种复用方法是将源代码抽象为参数化模板。使用一个通过执行脚本来替换所需参数的模板处理工具生成配置，通过为同一模板设置不同的值来达到复用的目的。而这种方式面临的问题是模板和参数文件并不在 kubernetes API 资源的规范中，这种方式必定是一种包装了 kubernetes API 的新东西、新语言。虽然这种方式很强大，但是也带来了学习成本和安装工具的成本。不同的团队需要不同的更改，因此几乎所有可以包含在 YAML 文件中的规范都会需要抽象成参数。&lt;/p&gt;

&lt;h2 id=&#34;自定义配置的新选择&#34;&gt;自定义配置的新选择&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;kustomize&lt;/strong&gt; 中工具的声明与规范是由名为 &lt;code&gt;kustomization.yaml&lt;/code&gt; 的文件定义。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;kustomize&lt;/strong&gt; 将会读取声明文件和 Kubernetes API 资源文件，将其组合然后将完整的资源进行标准化的输出。输出的文本可以被其他工具进一步处理，或者直接通过 &lt;strong&gt;kubectl&lt;/strong&gt; 应用于集群。&lt;/p&gt;

&lt;p&gt;例如，如果 &lt;code&gt;kustomization.yaml&lt;/code&gt; 文件包括：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;commonLabels:
  app: hello
resources:
- deployment.yaml
- configMap.yaml
- service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;确保这三个文件与 &lt;code&gt;kustomization.yaml&lt;/code&gt; 位于同一目录下，然后运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kustomize build
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将创建包含三个资源的 YAML 流，其中 &lt;code&gt;app: hello&lt;/code&gt; 为每个资源共同的标签。&lt;/p&gt;

&lt;p&gt;同样的，你可以使用 &lt;strong&gt;&lt;em&gt;commonAnnotations&lt;/em&gt;&lt;/strong&gt; 字段给所有资源添加注释， &lt;strong&gt;&lt;em&gt;namePrefix&lt;/em&gt;&lt;/strong&gt; 字段为所有的资源添加共同的前缀名。这些琐碎而有常见的定制只是一个开始。&lt;/p&gt;

&lt;p&gt;一个更常见的例子是，你需要为一组相同资源设置不同的参数。例如：开发、演示和生产的参数。&lt;/p&gt;

&lt;p&gt;为此，&lt;strong&gt;Kustomize&lt;/strong&gt; 允许用户以一个应用描述文件 （YAML 文件）为基础（Base YAML），然后通过 Overlay 的方式生成最终部署应用所需的描述文件。两者都是由 kustomization 文件表示。基础（Base）声明了共享的内容（资源和常见的资源配置），Overlay 则声明了差异。&lt;/p&gt;

&lt;p&gt;这里是一个目录树，用于管理集群应用程序的 &lt;strong&gt;&lt;em&gt;演示&lt;/em&gt;&lt;/strong&gt; 和 &lt;strong&gt;&lt;em&gt;生产&lt;/em&gt;&lt;/strong&gt; 配置参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;someapp/
├── base/
│   ├── kustomization.yaml
│   ├── deployment.yaml
│   ├── configMap.yaml
│   └── service.yaml
└── overlays/
    ├── production/
    │   └── kustomization.yaml
    │   ├── replica_count.yaml
    └── staging/
        ├── kustomization.yaml
        └── cpu_count.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;someapp/base/kustomization.yaml&lt;/code&gt; 文件指定了公共资源和常见自定义配置（例如，它们一些相同的标签，名称前缀和注释）。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;someapp/overlays/production/kustomization.yaml&lt;/code&gt; 文件的内容可能是：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;commonLabels:
  env: production
bases:
- ../../base
patches:
- replica_count.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 kustomization 指定了一个 &lt;strong&gt;&lt;em&gt;patch&lt;/em&gt;&lt;/strong&gt; 文件 &lt;code&gt;replica_count.yaml&lt;/code&gt; ，其内容可能是：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: the-deployment
spec:
  replicas: 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;patch&lt;/em&gt;&lt;/strong&gt; 是部分的资源声明，在这个例子中是 Deployment 的补丁 &lt;code&gt;someapp/base/deployment.yaml&lt;/code&gt; ，仅修改了副本数用以处理生产流量。&lt;/p&gt;

&lt;p&gt;该补丁不仅仅是一个无上下文 {parameter name，value} 元组。其作为部分 deployment spec，可以通过验证，即使与其余配置隔离读取，也具有明确的上下文和用途。&lt;/p&gt;

&lt;p&gt;要为生产环境创建资源，请运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kustomize build someapp/overlays/production
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行结果将作为一组完整资源打印到标准输出，并准备应用于集群。可以用类似的命令定义演示环境的配置。&lt;/p&gt;

&lt;h2 id=&#34;综上所述&#34;&gt;综上所述&lt;/h2&gt;

&lt;p&gt;使用 &lt;strong&gt;kustomize&lt;/strong&gt; ，您可以仅使用 Kubernetes API 资源文件就可以管理任意数量的 Kubernetes 定制配置。kustomize 的每个产物都是纯 YAML 的，每个都可以进行验证和运行的。&lt;strong&gt;kustomize&lt;/strong&gt; 鼓励通过 fork/modify/rebase 这样的&lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/blob/master/docs/workflows.md&#34; target=&#34;_blank&#34;&gt;工作流&lt;/a&gt;来管理海量的应用描述文件。&lt;/p&gt;

&lt;p&gt;尝试&lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/tree/master/examples/helloWorld&#34; target=&#34;_blank&#34;&gt;hello world&lt;/a&gt;示例，开始使用 &lt;strong&gt;kustomize&lt;/strong&gt; 吧！有关的反馈与讨论，可以通过加入&lt;a href=&#34;https://groups.google.com/forum/#!forum/kustomize&#34; target=&#34;_blank&#34;&gt;邮件列表&lt;/a&gt;或提 &lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/issues/new&#34; target=&#34;_blank&#34;&gt;issue&lt;/a&gt;，欢迎提交PR。&lt;/p&gt;

&lt;h2 id=&#34;译者按&#34;&gt;译者按&lt;/h2&gt;

&lt;p&gt;随着 kubernetes 1.14 的发布，kustomize 被集成到 &lt;code&gt;kubectl&lt;/code&gt; 中，用户可以利用 &lt;code&gt;kubectl apply -k dir/&lt;/code&gt; 将指定目录的 &lt;code&gt;kustomization.yaml&lt;/code&gt; 提交到集群中。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;原文链接&lt;/strong&gt; &lt;a href=&#34;https://kubernetes.io/blog/2018/05/29/introducing-kustomize-template-free-configuration-customization-for-kubernetes/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.io/blog/2018/05/29/introducing-kustomize-template-free-configuration-customization-for-kubernetes/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Kustomize 帮你管理 kubernetes 应用（一）：什么是 Kustomize ？</title>
      <link>https://guoxudong.io/en/post/kustomize-1/</link>
      <pubDate>Mon, 15 Apr 2019 13:32:59 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/kustomize-1/</guid>
      <description>

&lt;h2 id=&#34;初识-kustomize&#34;&gt;初识 Kustomize&lt;/h2&gt;

&lt;p&gt;第一次听说 Kustomize 其实是在 kubernetes 1.14 发布时候，它被集成到 &lt;code&gt;kubectl&lt;/code&gt; 中，成为了一个子命令，但也只是扫了一眼，并没有深究。真正让我注意到它，并主动开始了解其功能和使用方法的，是张磊大神在云栖社区发表的一篇文章&lt;a href=&#34;https://yq.aliyun.com/articles/697883&#34; target=&#34;_blank&#34;&gt;《从Kubernetes 1.14 发布，看技术社区演进方向》&lt;/a&gt;，他在文中是这么说的：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Kustomize 允许用户以一个应用描述文件 （YAML 文件）为基础（Base YAML），然后通过 Overlay 的方式生成最终部署应用所需的描述文件，而不是像 Helm 那样只提供应用描述文件模板，然后通过字符替换（Templating）的方式来进行定制化。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这不正我在苦苦寻找的东西嘛！自从公司确定了应用容器化的方案，至今已有半年多了，这期间我们的服务一个接一个的实现了容器化，部署到了 kubernetes 集群中。kubernetes 集群也有原先了1个测试集群，几个节点，发展到了如今的多个集群，几十个节点。而在推进容器化的过程中，每个服务都对对应多个应用描述文件（ YAML 文件），而根据环境的不同，又配置了多套的应用描述文件。随着服务越部越多，应用描述文件更是呈爆炸式的增长。&lt;/p&gt;

&lt;p&gt;感谢 devops 文化，它是我不需要为每个应用去写 YAML 文件，各个应用的开发组承担了这一工作，我只需要为他们提供基础模板即可。但应用上线后出现的 OOM 、服务无法拉起等 YAML 文件配置有误导致的问题接踵而至，使得我必须要深入各个服务，为他们配置符合他们配置。虽然也使用了 &lt;code&gt;helm&lt;/code&gt; ，但是其只提供应用描述文件模板，在不同环境拉起一整套服务会节省很多时间，而像我们这种在指定环境快速迭代的服务，并不会减少很多时间。针对这种情况，我已经计划要自己开发一套更符合我们工作这种场景的应用管理服务，集成在我们自己的 devops 平台中。&lt;/p&gt;

&lt;p&gt;这时 Kustomize 出现了，我明锐的感觉到 Kustomize 可能就是解决我现阶段问题的一剂良药。&lt;/p&gt;

&lt;h2 id=&#34;什么是-kustomize&#34;&gt;什么是 Kustomize ？&lt;/h2&gt;

&lt;blockquote&gt;
&lt;h4 id=&#34;kubernetes-native-configuration-management&#34;&gt;Kubernetes native configuration management&lt;/h4&gt;

&lt;p&gt;Kustomize introduces a template-free way to customize application configuration that simplifies the use of off-the-shelf applications. Now, built into &lt;code&gt;kubectl&lt;/code&gt; as &lt;code&gt;apply -k&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kustomize&lt;/code&gt;  允许用户以一个应用描述文件 （YAML 文件）为基础（Base YAML），然后通过 Overlay 的方式生成最终部署应用所需的描述文件。而其他用户可以完全不受影响的使用任何一个 Base YAML 或者任何一层生成出来的 YAML 。这使得每一个用户都可以通过类似fork/modify/rebase 这样 Git 风格的流程来管理海量的应用描述文件。这种 PATCH 的思想跟 Docker 镜像是非常相似的，它可以规避“字符替换”对应用描述文件的入侵，也不需要用户学习额外的 DSL 语法（比如 Lua）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;而其成为 &lt;code&gt;kubectl&lt;/code&gt; 子命令则代表这 &lt;code&gt;kubectl&lt;/code&gt; 本身的插件机制的成熟，未来可能有更多的工具命令集成到 &lt;code&gt;kubectl&lt;/code&gt; 中。拿张磊大神的这张图不难看出，在 kubernetes 原生应用管理系统中，应用描述文件在整个应用管理体系中占据核心位置，通过应用描述文件可以组合和编排多种 kubernetes API 资源，kubernetes 通过控制器来保证集群中的资源与应用状态与描述文件完全一致。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g23cqlrodkj21bq0r8znk.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kustomize 不像 Helm 那样需要一整套独立的体系来完成管理应用，而是完全采用 kubernetes 的设计理念来完成管理应用的目的。同时使用起来也更加的得心应手。&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize - kustomize.io&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://yq.aliyun.com/articles/697883&#34; target=&#34;_blank&#34;&gt;从Kubernetes 1.14 发布，看技术社区演进方向 - yq.aliyun.com&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>从边车模式到 Service Mesh</title>
      <link>https://guoxudong.io/en/post/sidercar-to-servicemesh/</link>
      <pubDate>Wed, 10 Apr 2019 14:03:25 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/sidercar-to-servicemesh/</guid>
      <description>

&lt;p&gt;所谓边车模式（ Sidecar pattern ），也译作挎斗模式，是分布式架构中云设计模式的一种。因为其非常类似于生活中的边三轮摩托车而得名。该设计模式通过给应用程序加上一个“边车”的方式来拓展应用程序现有的功能。这种设计模式出现的很早，实现的方式也多种多样。现在这个模式更是随着微服务的火热与 Service Mesh 的逐渐成熟而进入人们的视野。&lt;/p&gt;

&lt;h2 id=&#34;什么是边车模式&#34;&gt;什么是边车模式&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://wx1.sinaimg.cn/large/ad5fbf65ly1g18zhnoh76j20dw0dw752.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在 &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/architecture/patterns/&#34; target=&#34;_blank&#34;&gt;Azure Architecture Center&lt;/a&gt; 的云设计模式中是这么介绍边车模式的：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Deploy components of an application into a separate process or container to provide isolation and encapsulation.&lt;/p&gt;

&lt;p&gt;&amp;mdash; &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/architecture/patterns/sidecar&#34; target=&#34;_blank&#34;&gt;Sidecar pattern&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;这里要注意的是： 这里的 Sidecar 是分布式架构中云设计模式的一种，与我们目前在使用的 Istio 或 Linkerd 中的 Sidecar 是设计与实现的区别，后文中提到的边车模式均是指这种设计模式，请勿与 Istio 或 其他 Service Mesh 软件 中的 Sidecar 混淆。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;边车模式&lt;/strong&gt;是一种分布式架构的设计模式。如上图所示，边车就是加装在摩托车旁来达到拓展功能的目的，比如行驶更加稳定，可以拉更多的人和货物，坐在边车上的人可以给驾驶员指路等。边车模式通过给应用服务加装一个“边车”来达到&lt;strong&gt;控制&lt;/strong&gt;和&lt;strong&gt;逻辑&lt;/strong&gt;的分离的目的。&lt;/p&gt;

&lt;p&gt;比如日志记录、监控、流量控制、服务注册、服务发现、服务限流、服务熔断等在业务服务中不需要实现的控制面功能，可以交给“边车”，业务服务只需要专注实现业务逻辑即可。如上图那样，应用服务你只管开好你的车，打仗的事情就交给边车上的代理就好。这与分布式和微服务架构完美契合，真正的实现了控制和逻辑的分离与解耦。&lt;/p&gt;

&lt;h2 id=&#34;边车模式设计&#34;&gt;边车模式设计&lt;/h2&gt;

&lt;p&gt;在设计上边车模式与网关模式有类似之处，但是其粒度更细。其为每个服务都配备一个“边车”，这个“边车“可以理解为一个 agent ，这个服务所有的通信都是通过这个 agent 来完成的，这个 agent 同服务一起创建，一起销毁。像服务注册、服务发现、监控、流量控制、日志记录、服务限流和服务服务熔断等功能完全可以做成标准化的组件和模块，不需要在单独实现其功能来消耗业务开发的精力和时间来开发和调试这些功能，这样可以开发出真正高内聚低耦合的软件。&lt;/p&gt;

&lt;p&gt;这里有两种方法来实现边车模式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;通过 SDK 、 Lib 等软件包的形式，在开发时引入该软件包依赖，使其与业务服务集成起来。&lt;/p&gt;

&lt;p&gt;这种方法可以与应用密切集成，提高资源利用率并且提高应用性能。但是这种方法是对代码有侵入的，受到编程语言和软件开发人员水平的限制，但当该依赖有 bug 或者需要升级时，业务代码需要重新编译和发布。同时，如果该依赖宣布停止维护或者闭源，那么会给该服务带来不小的影响。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;以 Sidecar 的形式，在运维的时候与应用服务集成在一起。&lt;/p&gt;

&lt;p&gt;这种方式对应用服务没有侵入性，不受编程语言和开发人员水平的限制，做到了控制与逻辑分开部署。但是会增加应用延迟，并且管理和部署的复杂度会增加。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;边车模式解决了什么问题&#34;&gt;边车模式解决了什么问题&lt;/h2&gt;

&lt;p&gt;边车模式在概念上是比较简单的，但是在实践中还是要了解边车模式到底解决了什么问题，我们为什么要使用边车模式？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;控制与逻辑分离的问题&lt;/p&gt;

&lt;p&gt;边车模式是基于将控制与逻辑分离和解耦的思想，通俗的讲就是让专业的人做专业的事，业务代码只需要关心其复杂的业务逻辑，其他的事情”边车“会帮其处理，从这个角度看，可能叫跟班或者秘书模式也不错 :)&lt;/p&gt;

&lt;p&gt;日志记录、监控、流量控制、服务注册、服务发现、服务限流、服务熔断、鉴权、访问控制和服务调用可视化等，这些功能从本质上和业务服务的关系并不大，而传统的软件工程在开发层面完成这些功能，这导致了各种各样维护上的问题。&lt;/p&gt;

&lt;p&gt;就好像一个厨师不是必须去关心食材的产地、饭店的选址、是给大厅的客人上菜还是给包房的客人上菜&amp;hellip;他只需要做好菜就好，虽然上面的这些事他都可以做。而传统的软件工程就像是一个小饭店的厨师，他即是老板又是厨师，既需要买菜又需要炒菜，所有的事情都要他一个人做，如果客人一多，就会变的手忙脚乱；而控制与逻辑分离的软件，其逻辑部分就像是高档酒店的厨师，他只需要将菜做好即可，其他的事情由像”边车“这样的成员帮其处理。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;解决服务之间调用越来越复杂的问题&lt;/p&gt;

&lt;p&gt;随着分布式架构越来越复杂和微服务越拆越细，我们越来越迫切的希望有一个统一的控制面来管理我们的微服务，来帮助我们维护和管理所有微服务，这时传统开发层面上的控制就远远不够了。而边车模式可以很好的解决这个问题。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;从边车模式到-service-mesh&#34;&gt;从边车模式到 Service Mesh&lt;/h2&gt;

&lt;p&gt;边车模式有效的分离了系统控制和业务逻辑，可以将所有的服务进行统一管理，让开发人员更专注于业务开发，显著的提升开发效率。而遵循这种模式进行实践从很早以前就开始了，开发人员一直试图将上文中我们提到的功能（如：流量控制、服务注册、服务发现、服务限流、服务熔断等）提取成一个标准化的 Sidecar ，通过 Sidecar 代理来与其他系统进行交互，这样可以大大简化业务开发和运维。而随着分布式架构和微服务被越来越多的公司和开发者接受并使用，这一需求日益凸显。&lt;/p&gt;

&lt;p&gt;这就是 Service Mesh 服务网格诞生的契机，它是 CNCF（Cloud Native Computing Foundation，云原生基金会）目前主推的新一代微服务架构。 William Morgan 在 &lt;a href=&#34;https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/&#34; target=&#34;_blank&#34;&gt;What&amp;rsquo;s a service mesh? And why do I need one?&lt;/a&gt; 【&lt;a href=&#34;https://blog.maoxianplay.com/posts/whats-a-service-mesh-and-why-do-i-need-one/&#34; target=&#34;_blank&#34;&gt;译文&lt;/a&gt;】中解释了什么是 Service Mesh 。&lt;/p&gt;

&lt;p&gt;Service Mesh 有如下几个特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;应用程序间通讯的中间层&lt;/li&gt;
&lt;li&gt;轻量级网络代理&lt;/li&gt;
&lt;li&gt;应用程序无感知&lt;/li&gt;
&lt;li&gt;解耦应用程序的重试/超时、监控、追踪和服务发现&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Service Mesh 将底层那些难以控制的网络通讯统一管理，诸如：流量管控，丢包重试，访问控制等。而上层的应用层协议只需关心业务逻辑即可。Service Mesh 是一个用于处理服务间通信的基础设施层，它负责为构建复杂的云原生应用传递可靠的网络请求。&lt;/p&gt;

&lt;h2 id=&#34;你真的需要-service-mesh-吗&#34;&gt;你真的需要 Service Mesh 吗？&lt;/h2&gt;

&lt;p&gt;正如 NGINX 在其博客上发表的一篇文章名叫 &lt;a href=&#34;https://www.nginx.com/blog/do-i-need-a-service-mesh/&#34; target=&#34;_blank&#34;&gt;Do I Need a Service Mesh? &lt;/a&gt; 【&lt;a href=&#34;http://www.servicemesher.com/blog/do-i-need-a-service-mesh/&#34; target=&#34;_blank&#34;&gt;译文&lt;/a&gt;】 的文章中提到：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;As the complexity of the application increases, service mesh becomes a realistic alternative to implementing capabilities service-by-service.&lt;/p&gt;

&lt;p&gt;随着应用程序复杂性的增加，服务网格将成为实现服务到服务的能力的现实选择。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g1yqgvxvzrj20sg0fxgnw.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;随着我们的微服务越来越细分，我们所要管理的服务正在成倍的增长着，Kubernetes 提供了丰富的功能，使得我们可以快速的部署和调度这些服务，同时也提供了我们熟悉的方式来实现那些复杂的功能，但是当临界点到来时，可能就是我们真正要去考虑使用 Service Mesh 的时候了。&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Sidecar pattern ： &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/architecture/patterns/sidecar&#34; target=&#34;_blank&#34;&gt;https://docs.microsoft.com/en-us/azure/architecture/patterns/sidecar&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;What&amp;rsquo;s a service mesh? And why do I need one?： &lt;a href=&#34;https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/&#34; target=&#34;_blank&#34;&gt;https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Do I Need a Service Mesh?：&lt;a href=&#34;https://www.nginx.com/blog/do-i-need-a-service-mesh/&#34; target=&#34;_blank&#34;&gt;https://www.nginx.com/blog/do-i-need-a-service-mesh/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Devops入门手册</title>
      <link>https://guoxudong.io/en/post/devops-tutorial/</link>
      <pubDate>Tue, 09 Apr 2019 13:21:56 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/devops-tutorial/</guid>
      <description>

&lt;h1 id=&#34;devops-是什么&#34;&gt;DevOps 是什么？&lt;/h1&gt;

&lt;p&gt;“DevOps” 这个词是 &lt;code&gt;development&lt;/code&gt; 和 &lt;code&gt;operations&lt;/code&gt; 这两个词的组合。它是一种促进开发和运维团队之间的协作，以自动化和可重复的方式更快地将代码部署到生产中的&lt;strong&gt;文化&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;DevOps 帮助团体提高软件和服务的交付速度。它使团队能够更好地为客户服务，并提高在市场中的竞争力。&lt;/p&gt;

&lt;p&gt;简而言之， DevOps 可以定义为通过更好的沟通和协作，使开发和运维保持一致。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g1wbobryucj20db07dq2w.jpg&#34; alt=&#34;what is devops&#34; /&gt;&lt;/p&gt;

&lt;p&gt;本手册中，您将学到：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#devops-是什么&#34;&gt;DevOps 是什么？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#为什么需要-devops&#34;&gt;为什么需要 DevOps ？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#devops-与传统运维有什么不同&#34;&gt;DevOps 与传统运维有什么不同？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#为什么使用-devops&#34;&gt;为什么使用 DevOps ？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#devops-的生命周期&#34;&gt;DevOps 的生命周期&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#devops-的工作流&#34;&gt;DevOps 的工作流&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;devops-与敏捷有什么不同-devops-vs-agile&#34; target=&#34;_blank&#34;&gt;DevOps 与敏捷有什么不同？ DevOps VS Agile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#devops-原则&#34;&gt;DevOps 原则&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#谁可以做-devops-工程师&#34;&gt;谁可以做 DevOps 工程师？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#devops-工程师的角色-职责和技能&#34;&gt;DevOps 工程师的角色、职责和技能&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#devops-工程师可以挣多少钱&#34;&gt;DevOps 工程师可以挣多少钱？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#devops-培训认证&#34;&gt;DevOps 培训认证&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#devops-自动化工具&#34;&gt;DevOps 自动化工具&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#devops-的未来是怎样的&#34;&gt;DevOps 的未来是怎样的？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#总结&#34;&gt;总结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;为什么需要-devops&#34;&gt;为什么需要 DevOps ？&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;在实行 DevOps 之前，开发和运维团队是完全孤立的。&lt;/li&gt;
&lt;li&gt;测试和部署是设计在构建之后完成的独立活动。因此，他们比实际构建周期消耗更多时间。&lt;/li&gt;
&lt;li&gt;在不使用 DevOps 的情况下，团队成员将大量时间花在测试，部署和设计上，而不是构建项目。&lt;/li&gt;
&lt;li&gt;手动部署代码会导致生产中出现人为错误。&lt;/li&gt;
&lt;li&gt;开发和运维团队都有各自的时间表，时间的不同步导致生产交付进一步延误。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;提高软件交付率是业务方最迫切的需求。根据 Forrester Consulting Study 统计，只有17％的团队可以足够快地交付软件。更是证明了这一痛点。&lt;/p&gt;

&lt;h1 id=&#34;devops-与传统运维有什么不同&#34;&gt;DevOps 与传统运维有什么不同？&lt;/h1&gt;

&lt;p&gt;让我们将传统软件瀑布开发模型与 DevOps 进行比较，以了解 DevOps 带来的变化。&lt;/p&gt;

&lt;p&gt;我们假设有一个应用程序计划在2周内上线，代码完成80％。该应用程序是一个新的发布，从购买服务器开始&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;瀑布式开发&lt;/th&gt;
&lt;th&gt;DevOps&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;订购新服务器后，开发团队需要进行测试。运维团队根据需求文档开始部署基础设施。&lt;/td&gt;
&lt;td&gt;订购新服务器后，开发和运维团队根据需求文档共同调试部署新服务器。这样开发人员可以更好地了解服务器的基础架构。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;关于故障转移，冗余策略，数据中心位置和存储要求的规划存在偏差，因为开发人员对应用程序有深入了解，但他们无法提供任何协助。&lt;/td&gt;
&lt;td&gt;由于开发人员的加入，有关故障转移，冗余策略，灾难恢复，数据中心位置和存储要求的规划非常准确。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;运维团队对开发团队的进展一无所知。只能根据运维团队理解制定监控计划。&lt;/td&gt;
&lt;td&gt;在 DevOps 中，运维团队完全了解开发人员的进展。通过互动，共同制定满足运维和业务需求的监控计划。他们还使用应用程序性能监视（APM）工具以优化应用。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;在上线之前，压力测试使应用程序崩溃。发布延迟了。&lt;/td&gt;
&lt;td&gt;在上线之前，压力测试使应用程序有点慢。开发团队迅速解决了瓶颈问题。该应用程序按时发布。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&#34;为什么使用-devops&#34;&gt;为什么使用 DevOps ？&lt;/h1&gt;

&lt;p&gt;DevOps 允许敏捷开发团队实施持续集成和持续交付。这有助于他们更快地将产品推向市场。&lt;/p&gt;

&lt;p&gt;其他重要原因是：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;可预测性：&lt;/strong&gt; DevOps 可以显着降低新版本的故障率。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自愈性：&lt;/strong&gt; 可以随时将应用回滚到较早的版本。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可维护性：&lt;/strong&gt; 在新版本崩溃或当前系统不可用的情况下，可以毫不费力地进行恢复。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;上线时间：&lt;/strong&gt; DevOps 通过简化软件交付流程将上线时间缩短至50％。对于互联网和移动应用时间更短。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更高的质量：&lt;/strong&gt; DevOps 帮助团队提高应用程序开发的质量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;降低风险：&lt;/strong&gt; DevOps 在软件交付的生命周期中包含安全检查。它有助于减少整个软件生命周期中的安全风险。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;弹性：&lt;/strong&gt; 软件系统的运行状态更稳定，更安全，更改是可审计的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;成本效益：&lt;/strong&gt; DevOps 在软件开发过程中提供了成本效益，这始终是互联网公司管理层所期望的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;将大的代码库分成小块：&lt;/strong&gt; DevOps 是基于敏捷编程方法的。因此，它允许将大的代码库分解为更小且易于管理的块。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;什么时候使用-devops&#34;&gt;什么时候使用 DevOps ？&lt;/h3&gt;

&lt;p&gt;DevOps 应该用于大型分布式应用程序，例如电子商务站点或托管在云平台上的应用程序。&lt;/p&gt;

&lt;h3 id=&#34;什么时候不使用-devops&#34;&gt;什么时候不使用 DevOps？&lt;/h3&gt;

&lt;p&gt;它不应该用于关键任务应用程序，如银行，电力设施和其他敏感数据站点。此类应用程序需要对生产环境进行严格的访问控制，详细的变更管理策略，完善的数据中心访问控制策略。&lt;/p&gt;

&lt;h1 id=&#34;devops-的生命周期&#34;&gt;DevOps 的生命周期&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://wx3.sinaimg.cn/large/ad5fbf65gy1g1wekkedpcj20k509mjrp.jpg&#34; alt=&#34;devops Lifecycle&#34; /&gt;&lt;/p&gt;

&lt;p&gt;DevOps 是开发和运维之间的深度集成。在不了解 DevOps 生命周期的情况下，是无法真正理解 DevOps 的。&lt;/p&gt;

&lt;p&gt;以下是有关 DevOps生命周期的简要信息：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;开发&lt;/p&gt;

&lt;p&gt;在此阶段，整个开发过程分为小的开发周期。这有利于 DevOps 团队加快软件开发和交付过程。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;测试&lt;/p&gt;

&lt;p&gt;QA 团队使用 &lt;code&gt;Selenium&lt;/code&gt; 等自动化测试工具来识别和修复新代码中的错误。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;集成&lt;/p&gt;

&lt;p&gt;在此阶段，新功能与主分支代码集成，并进行测试。只有持续集成和测试才能实现持续交付。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;部署&lt;/p&gt;

&lt;p&gt;在此阶段，部署过程持续进行。它的执行方式是任何时候在代码中进行的任何更改都不应影响高流量网站的运行。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;监测&lt;/p&gt;

&lt;p&gt;在此阶段，运维团队将负责处理不合适的系统行为或生产中发现的错误。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;devops-的工作流&#34;&gt;DevOps 的工作流&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g1wewdq1elj20g009fa9y.jpg&#34; alt=&#34; DevOps Work Flow &#34; /&gt;&lt;/p&gt;

&lt;p&gt;工作流允许排列和分离用户最需要的任务。它还能够在配置任务时反应其最理想过程。&lt;/p&gt;

&lt;h1 id=&#34;devops-与敏捷有什么不同-devops-vs-agile&#34;&gt;DevOps 与敏捷有什么不同？ DevOps VS Agile&lt;/h1&gt;

&lt;p&gt;这是一个典型的IT流程&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g1wfmrcbafj20nq05wdg0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;敏捷解决了客户和开发人员沟通中的问题&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g1wfn81bchj20no05q3ys.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;DevOps 解决了开发人员运维人员沟通中的问题&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g1wfnk7fi3j20nt05vt90.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;敏捷&lt;/th&gt;
&lt;th&gt;DevOps&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;强调打破开发人员和管理层之间的障碍。&lt;/td&gt;
&lt;td&gt;DevOps 是关于软件开发和运维团队的。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;解决客户需求与开发团队之间的距离。&lt;/td&gt;
&lt;td&gt;解决开发和运维团队之间的距离。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;重点关注功能和非功能准备。&lt;/td&gt;
&lt;td&gt;它侧重于运维和业务准备。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;敏捷开发主要涉及公司对开发方式的思考。&lt;/td&gt;
&lt;td&gt;DevOps 强调以最可靠和最安全的方式部署软件，而这些方式并不总是最快的。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;敏捷开发非常注重培训所有团队成员，使他们拥有各种相同的技能。因此，当出现问题时，任何团队成员都可以在没有团队领导的情况下从别的成员那里获得帮助。&lt;/td&gt;
&lt;td&gt;DevOps 在开发和运维团队之间传播技能，并保持一致的沟通。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;敏捷开发管理 “sprint” ，意味着时间更短（不到一个月），并且在此期间将产生和发布多个功能。&lt;/td&gt;
&lt;td&gt;DevOps 努力争取主要版本的稳定可靠，而不是更小和更频繁的发布版本。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&#34;devops-原则&#34;&gt;DevOps 原则&lt;/h1&gt;

&lt;p&gt;这里有六个在采用 DevOps 时必不可少的原则：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;以客户为中心：&lt;/strong&gt; DevOps 团队必须以客户为中心，因为是他们不断向我的产品和服务投资。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;端到端的责任：&lt;/strong&gt; DevOps 团队需要在产品的整个生命周期提供性能支持。这提高了产品的水平和质量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;持续改进：&lt;/strong&gt; DevOps 文化专注于持续改进，以尽量减少浪费。它不断加快产品或服务改进的速度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自动化一切：&lt;/strong&gt; 自动化是 DevOps 流程的重要原则。这不仅适用于软件开发，同时也适用于整个基础架构环境。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作为一个团队工作：&lt;/strong&gt; 在 DevOps 文化角色中，设计人员，开发人员和测试人员已经定义。他们所需要做的就是作为一个团队完成合作。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;监控和测试所有内容：&lt;/strong&gt; DevOps 团队拥有强大的监控和测试程序是非常重要的。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;谁可以做-devops-工程师&#34;&gt;谁可以做 DevOps 工程师？&lt;/h1&gt;

&lt;p&gt;DevOps 工程师是一名IT专业人员，他与软件开发人员，系统运维人员和其他IT人员一起管理代码发布。DevOps 应具备与开发，测试和运维团队进行沟通和协作的硬技能和软技能。&lt;/p&gt;

&lt;p&gt;DevOps 方法需要对代码版本进行频繁的增量更改，这意味着频繁的部署和测试方案。尽管 DevOps 工程师需要偶尔从头开始编码，但重要的是他们应该具备软件开发语言的基础知识。&lt;/p&gt;

&lt;p&gt;DevOps 工程师将与开发团队的工作人员一起解决连接代码的元素（如库或软件开发工具包）所需的编码和脚本。&lt;/p&gt;

&lt;h1 id=&#34;devops-工程师的角色-职责和技能&#34;&gt;DevOps 工程师的角色、职责和技能&lt;/h1&gt;

&lt;p&gt;DevOps 工程师负责软件应用程序平台的生产和持续维护。&lt;/p&gt;

&lt;p&gt;以下是 DevOps 工程师的一些角色，职责和技能：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;能够跨平台和应用程序域执行系统故障排除和问题解决。&lt;/li&gt;
&lt;li&gt;通过开放的，标准的平台有效管理项目。&lt;/li&gt;
&lt;li&gt;提高项目可见性和可追溯性。&lt;/li&gt;
&lt;li&gt;通过协作提高开发质量并降低开发成本。&lt;/li&gt;
&lt;li&gt;分析、设计和评估自动化脚本和系统。&lt;/li&gt;
&lt;li&gt;通过使用最佳的云安全解决方案确保系统的安全。&lt;/li&gt;
&lt;li&gt;DevOps 工程师应该具备问题解决者和快速学习者的软技能。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;devops-工程师可以挣多少钱&#34;&gt;DevOps 工程师可以挣多少钱？&lt;/h1&gt;

&lt;p&gt;DevOps 是最热门的IT专业之一。这就是为什么那里都有很多机会的原因。因此，即使是初级DevOps工程师的薪酬水平也相当高。在美国，初级DevOps工程师的平均年薪为78,696美元。&lt;/p&gt;

&lt;h1 id=&#34;devops-培训认证&#34;&gt;DevOps 培训认证&lt;/h1&gt;

&lt;p&gt;DevOps 培训认证可以帮助任何渴望成为 DevOps 工程师职业的人。认证可从 Amazon web services 、 Red Hat 、 Microsoft Academy 、 DevOps Institute 获得。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/cn/certification/certified-devops-engineer-professional/&#34; target=&#34;_blank&#34;&gt;AWS Certified DevOps Engineer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;此 DevOps 工程师证书将测试您如何使用最常见的 DevOps 模式在 AWS 上开发，部署和维护应用程序。它还会评估 DevOps 方法的核心原则。&lt;/p&gt;

&lt;p&gt;该认证有两个必要条件：认证费用为300美元，持续时间为170分钟。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.redhat.com/en/services/training-and-certification&#34; target=&#34;_blank&#34;&gt;Red Hat Certification&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;红帽为 DevOps 专业人士提供不同级别的认证，如下所示:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Red Hat Certificate of Expertise in Platform-as-a-Service&lt;/li&gt;
&lt;li&gt;Red Hat Certificate of Expertise in Containerized Application Development&lt;/li&gt;
&lt;li&gt;Red Hat Certificate of Expertise in Ansible Automation&lt;/li&gt;
&lt;li&gt;Red Hat Certificate of Expertise in Configuration Management&lt;/li&gt;
&lt;li&gt;Red Hat Certificate of Expertise in Container Administration&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://devopsinstitute.com/&#34; target=&#34;_blank&#34;&gt;Devops Institute&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Devops Institute是围绕新兴 DevOps 实践的全球学习社区。该组织正在为 DevOps 能力资格设置质量标准。Devops Institute目前提供三个课程和认证。&lt;/p&gt;

&lt;p&gt;公司提供的认证课程有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DevOps Foundation&lt;/li&gt;
&lt;li&gt;DevOps Foundation Certified&lt;/li&gt;
&lt;li&gt;Certified Agile Service Manager&lt;/li&gt;
&lt;li&gt;Certified Agile Process Owner&lt;/li&gt;
&lt;li&gt;DevOps Test Engineering&lt;/li&gt;
&lt;li&gt;Continuous Delivery Architecture&lt;/li&gt;
&lt;li&gt;DevOps Leader&lt;/li&gt;
&lt;li&gt;DevSecOps Engineering&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;devops-自动化工具&#34;&gt;DevOps 自动化工具&lt;/h1&gt;

&lt;p&gt;所有测试流程自动化并对其进行配置以实现至关重要的速度和灵活性。此过程称为 DevOps 自动化。&lt;/p&gt;

&lt;p&gt;维护庞大的IT基础架构的大型 DevOps 团队面临的困难可以简要分为六个不同的类别。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;基础设施自动化&lt;/li&gt;
&lt;li&gt;配置管理&lt;/li&gt;
&lt;li&gt;部署自动化&lt;/li&gt;
&lt;li&gt;性能管理&lt;/li&gt;
&lt;li&gt;日志管理&lt;/li&gt;
&lt;li&gt;监测&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;让我们看看每个类别中的工具以及它们如何解决痛点：&lt;/p&gt;

&lt;h3 id=&#34;基础设施自动化&#34;&gt;基础设施自动化&lt;/h3&gt;

&lt;p&gt;亚马逊网络服务（AWS）：作为云服务，您无需建立实际的数据中心。此外，它们易于按需扩展。没有前期硬件成本。它可以配置为自动根据流量配置更多服务器。&lt;/p&gt;

&lt;h3 id=&#34;配置管理&#34;&gt;配置管理&lt;/h3&gt;

&lt;p&gt;Chef：它是一个有用的 DevOps 工具，用于提升速度，规模和一致性。它可用于简化复杂任务并执行配置管理。使用此工具，DevOps 团队可以避免在一万台服务器上进行更改。相反，只需要在一个地方进行更改，这些更改会自动反映在其他服务器中。&lt;/p&gt;

&lt;h3 id=&#34;部署自动化&#34;&gt;部署自动化&lt;/h3&gt;

&lt;p&gt;Jenkins：该工具有助于持续集成和测试。通过在部署构建后快速查找问题，更​​轻松地集成项目更改。&lt;/p&gt;

&lt;h3 id=&#34;日志管理&#34;&gt;日志管理&lt;/h3&gt;

&lt;p&gt;Splunk：可以解决在一个地方聚合，存储和分析所有日志的问题的工具。&lt;/p&gt;

&lt;h3 id=&#34;性能管理&#34;&gt;性能管理&lt;/h3&gt;

&lt;p&gt;App Dynamic：它是一个 DevOps 工具，提供实时性能监控。此工具收集的数据可帮助开发人员在发生问题时进行调试。&lt;/p&gt;

&lt;h3 id=&#34;监控&#34;&gt;监控&lt;/h3&gt;

&lt;p&gt;Nagios：在基础架构和相关服务出现故障时通知相关人员也很重要。Nagios 就是这样一种工具，它可以帮助 DevOps 团队找到并纠正问题。&lt;/p&gt;

&lt;h1 id=&#34;devops-的未来是怎样的&#34;&gt;DevOps 的未来是怎样的？&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;团队将代码部署周期转换为数周和数月，而不是数年。&lt;/li&gt;
&lt;li&gt;很快就会看到，DevOps 工程师可以比企业中的任何其他人更多地接近和管理终端用户。&lt;/li&gt;
&lt;li&gt;DevOps 正在成为IT人员的重要技能。例如，Linux 招聘进行的一项调查发现，25％的受访者的求职者寻求 DevOps 工作。&lt;/li&gt;
&lt;li&gt;DevOps 和持续交付将继续存在。因为公司需要发展，他们别无选择，只能改变。然而，DevOps 概念的主流化则需要5到10年。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;DevOps 是一种促进开发和运维团队之间的协作，以自动化和可重复的方式更快地将代码部署到生产中的&lt;strong&gt;文化&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;在 DevOps 出现之前运维和开发团队完全独立。&lt;/li&gt;
&lt;li&gt;手动部署代码会导致生产中出现人为错误。&lt;/li&gt;
&lt;li&gt;在旧的软件开发流程中，运维团队不了解开发团队的进度。因此，运维团队只能根据他们自己的理解制定了基础设施的购买和监控计划。&lt;/li&gt;
&lt;li&gt;在 DevOps 流程中，运维团队充分了解开发人员的进度。采购和监控计划准确无误。&lt;/li&gt;
&lt;li&gt;DevOps 提供可维护性，可预测性，更高质量的代码和更准确的上线时间。&lt;/li&gt;
&lt;li&gt;敏捷流程侧重于功能和非功能准备，而 DevOps 则侧重于IT基础架构方面。&lt;/li&gt;
&lt;li&gt;DevOps 生命周期包括开发，测试，集成，部署和监控。&lt;/li&gt;
&lt;li&gt;DevOps 工程师将与开发团队工作人员合作，以解决编码和脚本编写需求。&lt;/li&gt;
&lt;li&gt;DevOps 工程师应该具备问题解决者的软技能，并且是一个快速学习者。&lt;/li&gt;
&lt;li&gt;DevOps 认证可从 Amazon web services，Red Hat，Microsoft Academy，DevOps Institute 获得&lt;/li&gt;
&lt;li&gt;DevOps 可帮助团队将代码部署周期转换为数周和数月，而不是数年。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;原文链接&lt;/strong&gt; &lt;a href=&#34;https://www.guru99.com/devops-tutorial.html&#34; target=&#34;_blank&#34;&gt;https://www.guru99.com/devops-tutorial.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>单节点版rancher升级指南</title>
      <link>https://guoxudong.io/en/post/rancher-update-2.2.1/</link>
      <pubDate>Sun, 31 Mar 2019 11:15:35 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/rancher-update-2.2.1/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;Rancher 不仅可以在任何云提供商的任何地方部署 Kubernetes 集群，而且还将它们集中在集中式的身份验证和访问控制之下。由于它与资源的运行位置无关，因此您可以轻松地在不同的环境部署你的 kubernetes 集群并操作他们。 Rancher 不是将部署几个独立的 Kubernetes 集群，而是将它们统一为一个单独的托管Kubernetes Cloud。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;目前我们使用的是 rancher 2.1.1版本，在去年 rancher 发布 &lt;code&gt;v2.1.*&lt;/code&gt; 版本的时候做过一次升级，当时遇到了很多问题，虽然都一一解决，但是并没有有效的记录下来，这里在升级 &lt;code&gt;v2.2.*&lt;/code&gt; 版本的时候做一个记录以便在今后升级的时候的提供参考作用。&lt;/p&gt;

&lt;h2 id=&#34;升级前的准备&#34;&gt;升级前的准备&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;首先查看当前 rancher 版本，记下这个版本号后面需要使用。查看方式就是登陆 rancher 在左下角就可以看到当前版本号，我们这里使用的&lt;code&gt;v2.1.1&lt;/code&gt;版本。&lt;/li&gt;
&lt;li&gt;打开官方文档，这里推荐对照官方文档进行升级，一般官方文档都会及时更新并提供最佳升级方法，而一般的博客会因为其写作时间、使用版本、部署环境的不同有所偏差。官方文档： &lt;a href=&#34;https://www.cnrancher.com/docs/rancher/v2.x/cn/upgrades/single-node-upgrade/&#34; target=&#34;_blank&#34;&gt;https://www.cnrancher.com/docs/rancher/v2.x/cn/upgrades/single-node-upgrade/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;升级&#34;&gt;升级&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;首先获取正在运行的 rancher 容器 ID,由以下命令可知 &lt;code&gt;RANCHER_CONTAINER_ID&lt;/code&gt; 为 &lt;code&gt;83167cb60134&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker ps
CONTAINER ID        IMAGE                    COMMAND             CREATED             STATUS              
PORTS                                       NAMES
83167cb60134        rancher/rancher:latest   &amp;quot;entrypoint.sh&amp;quot;     4 months ago        Up 4 months         0.0.0.0:80-&amp;gt;80/tcp, 0.0.0.0:443-&amp;gt;443/tcp   priceless_newton
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;停止该容器&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker stop {RANCHER_CONTAINER_ID}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建正在运行的 Rancher Server 容器的数据卷容器，将在升级中使用，这里命名为 &lt;code&gt;rancher-data&lt;/code&gt; 容器。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;替换{RANCHER_CONTAINER_ID}为上一步中的容器ID。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;替换{RANCHER_CONTAINER_TAG}为你当前正在运行的Rancher版本，如上面的先决条件中所述。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker create --volumes-from {RANCHER_CONTAINER_ID} --name rancher-data rancher/rancher:{RANCHER_CONTAINER_TAG}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;备份 &lt;code&gt;rancher-data&lt;/code&gt; 数据卷容器&lt;/p&gt;

&lt;p&gt;如果升级失败，可以通过此备份还原Rancher Server，容器命名:rancher-data-snapshot-&lt;CURRENT_VERSION&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;替换{RANCHER_CONTAINER_ID}为上一步中的容器ID。&lt;/li&gt;
&lt;li&gt;替换{CURRENT_VERSION}为当前安装的Rancher版本的标记。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;替换{RANCHER_CONTAINER_TAG}为当前正在运行的Rancher版本，如先决条件中所述 。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker create --volumes-from {RANCHER_CONTAINER_ID} --name rancher-data-snapshot-{CURRENT_VERSION} rancher/rancher:{RANCHER_CONTAINER_TAG}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;拉取Rancher的最新镜像,这里确保有外网，可能拉取到新的镜像，如果没有外网，这里就需要将镜像上传到私有镜像仓库，将拉取地址设置为私有镜像仓库即可&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker pull rancher/rancher:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;通过 &lt;code&gt;rancher-data&lt;/code&gt; 数据卷容器启动新的 Rancher Server 容器。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;这里要注意到，我们这是使用的是独立容器+外部七层负载均衡，是通过阿里云SLB进行SSL证书认证，需要在启动的时候增加&lt;code&gt;--no-cacerts&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d --volumes-from rancher-data --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher:latest --no-cacerts
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;升级过程会需要一定时间，不要在升级过程中终止升级，强制终止可能会导致数据库迁移错误。&lt;/p&gt;

&lt;p&gt;升级 Rancher Server后， server 容器中的数据会保存到 &lt;code&gt;rancher-data&lt;/code&gt; 容器中，以便将来升级。&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;删除旧版本 Rancher Server 容器&lt;/p&gt;

&lt;p&gt;如果你只是停止以前的Rancher Server容器(并且不删除它),则旧版本容器可能随着主机重启后自动运行，导致容器端口冲突。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;升级成功&lt;/p&gt;

&lt;p&gt;访问 rancher 可以看到右下角版本已经完成更新。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65gy1g1lzcmucn6j20ck03qt8p.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Service Mesh是什么，我们又为什么需要它</title>
      <link>https://guoxudong.io/en/post/whats-a-service-mesh-and-why-do-i-need-one/</link>
      <pubDate>Mon, 25 Mar 2019 18:17:20 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/whats-a-service-mesh-and-why-do-i-need-one/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;作者：William Morgan 发表于2017年4月25日，2018年11月26日有所修改。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Service Mesh 是一个专门使服务与服务之间的通信变得安全、快速和可靠的的基础设施。如果你正在在构建一个云原生（ Cloud Native ）应用，那么你一定需要 Service Mesh 。&lt;/p&gt;

&lt;p&gt;在过去的一年中， Service Mesh 成为了云原生技术栈的关键组件。像 Paypal ,  Ticketmaster 和 Credit Karma 这样的大厂，已经将 Service Mesh 加入到他们的全部应用中。并且在2017年1月，开源的 Service Mesh 软件 Linkerd 加入云原生基金会（ CNCF ），成为云原生基金会（ CNCF ）的官方项目。但是什么是真正的 Service Mesh ？它又为何突然变的如此重要？&lt;/p&gt;

&lt;p&gt;在这篇文章，我会讲解 Service Mesh 的定义，并通过应用服务架构过去十年的发展追溯其起源。并将 Service Mesh 与其他相似的概念（包括 API 网关，边缘代理以及 ESB （enterprise service bus））进行区分。最终，将会描述 Service Mesh 的发展方向，以及随着云原生概念的普及，Service Mesh 发生的变化。&lt;/p&gt;

&lt;h2 id=&#34;什么是-service-mesh&#34;&gt;什么是 Service Mesh&lt;/h2&gt;

&lt;p&gt;Service Mesh 这个服务网络专注于处理服务和服务间的通讯。其主要负责构造一个稳定可靠的服务通讯的基础设施，并让整个架构更为的先进和 Cloud Native。在工程中，Service Mesh 基本来说是一组轻量级的服务代理和应用逻辑的服务在一起，并且对于应用服务是透明的。&lt;/p&gt;

&lt;p&gt;Service Mesh 作为独立层的概念与云原生应用的兴起有关。在云原生模式，单个应用可能有数百个服务组成，每个服务又可能有上千个实例，而每个实例都有可能被像 kubernetes 这样的服务调度器不断调度从而不断变化状态。而这些复杂的通信又普遍是服务运行时行为的一部分，这时确保端到端的通信的性能和可靠性就变的至关重要。&lt;/p&gt;

&lt;h2 id=&#34;service-mesh-就是一个网络模型吗&#34;&gt;Service Mesh 就是一个网络模型吗？&lt;/h2&gt;

&lt;p&gt;Service Mesh 是一个位于 TCP/IP 上的抽象层的网络模型。它假定底层 L3/L4 网络存在并且能够从一点向另一点传输数据。（它还假定这个网络和环境的其他方面一样不可靠，所以 Service Mesh 也必须能够处理网络故障。）&lt;/p&gt;

&lt;p&gt;在某些方面，Service Mesh 就像是网络七层模型中的第四层 TCP 协议。其把底层的那些非常难控制的网络通讯方面的控制面的东西都管了（比如：丢包重传、拥塞控制、流量控制），而更为上面的应用层的协议，只需要关心自己业务应用层上的事了。&lt;/p&gt;

&lt;p&gt;与 TCP 不同的是， Service Mesh 想要达成的目的不仅仅是正常的网络通讯。它为应用提供了统一的，可视化的以及可控制的控制平面。Service Mesh 是要将服务间的通信从无法发现和控制的基础设施中分离出来，并对其进行监控、管理和控制。&lt;/p&gt;

&lt;h2 id=&#34;service-mesh-实际上做了什么&#34;&gt;Service Mesh 实际上做了什么？&lt;/h2&gt;

&lt;p&gt;在云原生应用中传递可靠的请求是十分复杂的。而 &lt;a href=&#34;https://linkerd.io/#_ga=2.114183109.310878331.1553762133-1927878916.1553476024&#34; target=&#34;_blank&#34;&gt;Linkerd&lt;/a&gt; 提供了服务熔断、重试、负载均衡、熔断降级等功能，通过其强大的功能来管理那些必须运行在复杂环境中的服务。&lt;/p&gt;

&lt;p&gt;这里列举一个通过 Linkerd 向服务发出请求的简单流程：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;通过 Linkerd 的动态路由规则来确定打算连接哪个服务。这个请求是要路由到生产环境还是演示环境？是请求本地数据中心的服务还是云上的服务？是请求正在测试的最新版的服务还是已经在生产中经过验证的老版本？所有的这些路由规则都是动态配置的，可以全局应用也可以部分应用。&lt;/li&gt;
&lt;li&gt;找到正确的目的服务后， Linkerd 从一个或几个相关的服务发现端点检索实例池。如果这些信息与 Linkerd 的服务发现信息不同， Linkerd 会决定信任哪些信息来源。&lt;/li&gt;
&lt;li&gt;Linkerd 会根据观察到的最近的响应延迟来选择速度最快的实例。&lt;/li&gt;
&lt;li&gt;Linkerd 发送请求给这个实例，记录延迟和响应类型。&lt;/li&gt;
&lt;li&gt;如果这个实例挂了、无响应或者无法处理请求， Linkerd 会再另一个实例上重试这个请求。（但只有在请求是幂等的时候）&lt;/li&gt;
&lt;li&gt;如果一个实例一直请求失败， Linkerd 会将其移出定时重试的负载均衡池。&lt;/li&gt;
&lt;li&gt;如果请求超时， Linkerd 会主动将请求失效，而不是进一步重试从而增加负载。&lt;/li&gt;
&lt;li&gt;Linkerd 会记录指标和分布式的追踪上述行为的各个方面，将他们保存在集中的指标系统中。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;以上只是简化版的介绍， Linkerd 还可以启动和重试 TLS ，执行协议升级，动态切换流量，甚至在故障之后数据中心的切换。
&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65ly1g1in1q1jnuj20sg0gbt99.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;值得注意的是，这些功能旨在为每个实例和应用程序提供弹性伸缩。而大规模的分布式系统（无论是如何构建的）都有一个共同特点：都会因为许多小的故障，而升级为全系统灾难性的故障。Service Mesh 则被设计为通过快速的失效和减少负载来保护整个系统免受这样灾难性的故障。&lt;/p&gt;

&lt;h2 id=&#34;为什么-service-mesh-是必要的&#34;&gt;为什么 Service Mesh 是必要的？&lt;/h2&gt;

&lt;p&gt;Service Mesh 本质上并不是什么新技术，而是功能所在位置的转变。Web 应用需要管理复杂的服务通信，Service Mesh 模式的起源和演变过程可以追溯到15年前。&lt;/p&gt;

&lt;p&gt;参考2000年左右中型 Web 应用的典型三层架构，在这个架构中，应用被分为三层：应用逻辑、web 服务逻辑、存储逻辑。层之间的通信虽然复杂，但是毕竟范围有限，最多只有2跳。这里并不是 “Mesh” 的，但在每层中处理跳转的代码是存在通信逻辑的。&lt;/p&gt;

&lt;p&gt;当这种架构向更大规模发展的时候，这种通信方式就无以为继了。像 Google , Netflix , 和 Twitte ，在面临巨大的请求流量的时候，他们的实现了云原生应用的前身：应用被分割成了许多服务（现在称作“微服务”），这些服务组成了一种网格结构。在这些系统中，通用通信层突然兴起，表现为“胖客户端”的形式 - Twitter 的 Finagle, Netflix 的 Hystrix 和 Google 的 Stubby 都是很典型的例子。&lt;/p&gt;

&lt;p&gt;现在看来，像 Finagle 、Stubby 和 Hystrix 这样的库就是最早的 Service Mesh。虽然它们是为特定环境、语言和框架定制了，但都是作为基础设施专门用于管理服务间的通信，并（在 Finagle 和 Hystrix 开源的情况下）在其他公司的应用中被使用。&lt;/p&gt;

&lt;p&gt;这三个组件都有应用自适应机制，以便在负载中进行拓展，并处理在云环境中的部分故障。但是对于数百个服务或数千个实例，以及不时需要重新调度的业务层实例，单个请求通过的调用链可能变的非常复杂，而且服务可能由不同的语言编写，这时基于库的解决方案可能就不再适用了。&lt;/p&gt;

&lt;p&gt;服务通信的复杂性和重要性导致我们急需一个专门的基础设施层来处理服务间的通信，该层需要与业务代码解耦，并且具有捕获底层环境的动态机制。这就是 Service Mesh 。&lt;/p&gt;

&lt;h2 id=&#34;service-mesh-的未来&#34;&gt;Service Mesh 的未来&lt;/h2&gt;

&lt;p&gt;Service Mesh 在云生态下迅速的成长，并且有着令人激动的未来等待探索。对无服务器计算（Serverless， 例如 Amazon 的 &lt;a href=&#34;https://aws.amazon.com/lambda/&#34; target=&#34;_blank&#34;&gt;Lambda&lt;/a&gt;）适用的 Service Mesh 网络模型，在云生态系统中角色的自然拓展。Service Mesh 可能成为服务身份和访问策略这些在云原生领域还是比较新的技术的基础。最后，Service Mesh ，如之前的TCP / IP，将推进加入到底层的基础架构中。就像 Linkerd 是由像 Finagle 这样的系统发展而来，Service Mesh 将作为单独的用户空间代理添加到云原生技术栈中继续发展。&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;Service Mesh 是云原生技术栈的关键技术。Linkerd 成立仅1年就成为了云原生基金会（CNCF）的一部分，拥有蓬勃发展的社区和贡献者。使用者从像 Monzo 这样颠覆英国银行业的创业公司，到像 Paypal、 Ticketmaster 和 Credit Karma 这样的互联网大厂，再到像 Houghton Mifflin Harcourt 这样经营了数百年的公司。&lt;/p&gt;

&lt;p&gt;使用者和贡献者每天都在 Linkerd 社区展示 Service Mesh 创造的价值。我们将致力于打造这一令人惊叹的产品，并继续发展壮大我们的社区，&lt;a href=&#34;https://linkerd.io/#_ga=2.40265824.310878331.1553762133-1927878916.1553476024&#34; target=&#34;_blank&#34;&gt;加入我们吧&lt;/a&gt;！&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;原文链接&lt;/strong&gt; &lt;a href=&#34;https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/&#34; target=&#34;_blank&#34;&gt;https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Istio初探之Bookinfo样例部署</title>
      <link>https://guoxudong.io/en/post/istio-bookinfo-demo/</link>
      <pubDate>Thu, 21 Mar 2019 09:42:18 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/istio-bookinfo-demo/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;之前介绍了 Istio 和 Service Mesh 能给我们带来什么，我们为什么要用 Istio ，但大家对 Istio 的认识可能还没有那么深刻。正如Linux 的创始人 &lt;a href=&#34;https://en.wikipedia.org/wiki/Linus_Torvalds&#34; target=&#34;_blank&#34;&gt;Linus Torvalds&lt;/a&gt; 的那句话：&lt;strong&gt;Talk is cheap. Show me the code.&lt;/strong&gt; 这里我们部署一个demo，由四个单独的微服务构成&lt;strong&gt;（注意这里的四个微服务是由不同的语言编写的）&lt;/strong&gt;，用来演示多种 Istio 特性。这个应用模仿在线书店的一个分类，显示一本书的信息。页面上会显示一本书的描述，书籍的细节（ISBN、页数等），以及关于这本书的一些评论。&lt;/p&gt;

&lt;h2 id=&#34;bookinfo-应用&#34;&gt;Bookinfo 应用&lt;/h2&gt;

&lt;p&gt;Bookinfo 应用分为四个单独的微服务：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;pre&gt;&lt;code class=&#34;language-productpage```&#34;&gt;- ```details``` ：这个微服务包含了书籍的信息。
- ```reviews``` ：这个微服务包含了书籍相关的评论。它还会调用 ratings 微服务。
- ```ratings``` ：ratings 微服务中包含了由书籍评价组成的评级信息。

这里主要使用```reviews```来演示 Istio 特性，```reviews``` 微服务有 3 个版本：

- v1 版本不会调用 ```ratings``` 服务。
- v2 版本会调用 ```ratings``` 服务，并使用 1 到 5 个黑色星形图标来显示评分信息。
- v3 版本会调用 ```ratings``` 服务，并使用 1 到 5 个红色星形图标来显示评分信息。

下图展示了这个应用的端到端架构。
![Istio 注入之前的 Bookinfo 应用](https://istio.io/docs/examples/bookinfo/noistio.svg)
&amp;lt;center&amp;gt;Istio 注入之前的 Bookinfo 应用&amp;lt;/center&amp;gt;

Bookinfo 是一个异构应用，几个微服务是由不同的语言编写的。这些服务对 Istio **并无依赖**，但是构成了一个有代表性的服务网格的例子：它由多个服务、多个语言构成，并且 reviews 服务具有多个版本。

## 部署应用
这里 Istio 的安装部署就不在赘述了。

值得注意的是：如果使用的是**阿里云**容器服务安装的 Istio ，需要在 ```容器服务```-```市场```-```应用目录``` 中选择 ```gateway``` 进行安装，这里提供了多种 ```gateway``` ，我们选择 ```istio-ingressgateway```，选择直接安装的话会默认创建 ```LoadBalancer``` 类型的Service，会自动创建一个经典网络SLB，这里是可以调整的，会在后续的文章中进行详细讲解，这里不做赘述。

在 Istio 中运行这一应用，无需对应用自身做出任何改变。我们只要简单的在 Istio 环境中对服务进行配置和运行，具体一点说就是把 Envoy sidecar 注入到每个服务之中。这个过程所需的具体命令和配置方法由运行时环境决定，而部署结果较为一致，如下图所示：

![Bookinfo 应用](https://istio.io/docs/examples/bookinfo/withistio.svg)
&amp;lt;center&amp;gt;Bookinfo 应用&amp;lt;/center&amp;gt;

所有的微服务都和 Envoy sidecar 集成在一起，被集成服务所有的出入流量都被 sidecar 所劫持，这样就为外部控制准备了所需的 Hook，然后就可以利用 Istio 控制平面为应用提供服务路由、遥测数据收集以及策略实施等功能。

### 下载安装
到 GitHub 中 istio 的 [release](https://github.com/istio/istio/releases) 中下载相应版本的 istio 包，下载后将 ```bin``` 目录配置到环境变量 ```PATH``` 中 ```export PATH=&amp;quot;/istio/bin:$PATH&amp;quot;``` ，这里我们使用的是 ```istio 1.0.5``` 版本

Bookinfo 这个应用就在 ```samples/```目录下

## 在 阿里云容器服务（kubernetes） 中运行

启动应用容器，这里提供两种注入方法：**手工注入**和**自动注入**

- 自动注入

需要修改 namespace ，为其添加 label 标签，这样所以在这个 namespace 中创建的应用都会被自动注入 sidecar 

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
$ kubectl label namespace {inject-namespace} istio-injection=enabled
$ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
- 手工注入

需要使用 istioctl 命令生成注入后应用的配置，然后在部署应用

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
$ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml | kubectl apply -f -&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
由于是测试，这里我们使用手工注入的方法。
上面的命令会启动全部的四个服务，其中也包括了 ```reviews``` 服务的三个版本（```v1```、```v2``` 以及 ```v3```）

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$ istioctl kube-inject -f bookinfo.yaml | kubectl apply -f -
service/details created
deployment.extensions/details-v1 configured
service/ratings created
deployment.extensions/ratings-v1 created
service/reviews created
deployment.extensions/reviews-v1 created
deployment.extensions/reviews-v2 created
deployment.extensions/reviews-v3 created
service/productpage created
deployment.extensions/productpage-v1 created
$ kubectl get po
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-8685d68cf9-8fwdb       &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;     Running   0          1h
productpage-v1-5fd9fddc97-tx88z   &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;     Running   0          1h
ratings-v1-7c4d756c55-cn76d       &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;     Running   0          1h
reviews-v1-5d868db586-w28q5       &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;     Running   0          1h
reviews-v2-787647c7d9-7sc52       &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;     Running   0          1h
reviews-v3-6964c86584-8728m       &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;     Running   0          1h
$ kubectl get svc
NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)            AGE
details              ClusterIP   10.11.224.17    &lt;none&gt;        9080/TCP           1h
productpage          ClusterIP   10.11.16.86     &lt;none&gt;        9080/TCP           1h
ratings              ClusterIP   10.11.244.59    &lt;none&gt;        9080/TCP           1h
reviews              ClusterIP   10.11.162.37    &lt;none&gt;        9080/TCP           1h&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
可以看到 Bookinfo 应用已经正常运行

### 指定 ingress 和 IP 的端口

1. 为为应用程序定义入口网关：

    ```bash
    $ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml
    ```

2. 确认网关创建完成

    ```bash
    $ kubectl get gateway
    NAME               AGE
    bookinfo-gateway   1h
    ```

3. 快速查询访问地址，这里的是之前在阿里云上创建的 ```LoadBalancer``` 类型的 Service

    ```bash
    $ kubectl get svc istio-ingressgateway -n istio-system
    NAME                   TYPE           CLUSTER-IP    EXTERNAL-IP       PORT(S)                  AGE
    istio-ingressgateway   LoadBalancer   10.11.18.83   xxx.xxx.xxx.xxx   80:xxx/TCP,443:xxx/TCP   2h
    ```

### 查看效果
访问 http://{EXTERNAL-IP}/productpage 注意：这里最后不能有/，否则将找不到页面
![image](http://wx4.sinaimg.cn/large/ad5fbf65ly1g1ad2jg6p3j21g90mxgo7.jpg)
多次刷新浏览器，将在 ```productpage``` 中看到评论的不同的版本，它们会按照 round robin（红星、黑星、没有星星）的方式展现，这三个展示分来来自```v1```、```v2```和```v3```版本，因为还没有使用 Istio 来控制版本的路由，所以这里显示的是以轮询的负载均衡算法进行展示。

### 请求路由
BookInfo示例部署了三个版本的reviews服务，因此需要设置一个缺省路由。否则当多次访问该应用程序时，会发现有时输出会包含带星级的评价内容，有时又没有。出现该现象的原因是当没有为应用显式指定缺省路由时，Istio会将请求随机路由到该服务的所有可用版本上。

在使用 Istio 控制 Bookinfo 版本路由之前，你需要在目标规则中定义好可用的版本 。

运行以下命令为 Bookinfo 服务创建的默认的目标规则：

- 如果不需要启用双向TLS，请执行以下命令：

    ```bash
    $ kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml
    ```

- 如果需要启用双向 TLS，请执行以下命令：

    ```bash
    $ kubectl apply -f samples/bookinfo/networking/destination-rule-all-mtls.yaml
    ```

    等待几秒钟，等待目标规则生效。你可以使用以下命令查看目标规则：

    ```bash
    kubectl get destinationrules
    NAME          AGE
    details       28s
    productpage   28s
    ratings       28s
    reviews       28s
    ```

### 将所有微服务的缺省版本设置为v1
通过运行如下命令，将所有微服务的缺省版本设置为v1：

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
$ kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
可以通过下面的命令来显示所有已创建的路由规则：

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
$ kubectl get virtualservices
NAME       AGE
bookinfo      33m
details       8s
productpage   8s
ratings       8s
reviews       8s&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
显示已创建的详细路由规划：

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
$ kubectl get virtualservices -o yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
由于路由规则是通过异步方式分发到代理的，过一段时间后规则才会同步到所有pod上。因此需要等几秒钟后再尝试访问应用。

在浏览器中打开 Bookinfo 应用程序的URL: http://{EXTERNAL-IP}/productpage。

![image](http://wx4.sinaimg.cn/large/ad5fbf65ly1g1adqyf9dej21g70oitbd.jpg)

可以看到 Bookinfo 应用程序的 ```productpage``` 页面，显示的内容中不包含带星的评价信息，这是因为 ```reviews:v1``` 服务不会访问ratings服务。

### 将来自特定用户的请求路由到reviews:v2
本例中，首先使用 Istio 将100%的请求流量都路由到了 Bookinfo 服务的```v1```版本；然后再设置了一条路由规则，路由规则基于请求的 header（例如一个用户cookie）选择性地将特定的流量路由到了 ```reviews``` 服务的```v2```版本。

通过运行如下命令，把来自测试用户&amp;quot;jason&amp;quot;的请求路由到 ```reviews:v2 ```，以启用ratings服务。

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
$ kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
通过如下命令确认规则是否创建：

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
$ kubectl get virtualservice reviews -o yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
    {&amp;ldquo;apiVersion&amp;rdquo;:&amp;ldquo;networking.istio.io/v1alpha3&amp;rdquo;,&amp;ldquo;kind&amp;rdquo;:&amp;ldquo;VirtualService&amp;rdquo;,&amp;ldquo;metadata&amp;rdquo;:{&amp;ldquo;annotations&amp;rdquo;:{},&amp;ldquo;name&amp;rdquo;:&amp;ldquo;reviews&amp;rdquo;,&amp;ldquo;namespace&amp;rdquo;:&amp;ldquo;default&amp;rdquo;},&amp;ldquo;spec&amp;rdquo;:{&amp;ldquo;hosts&amp;rdquo;:[&amp;ldquo;reviews&amp;rdquo;],&amp;ldquo;http&amp;rdquo;:[{&amp;ldquo;match&amp;rdquo;:[{&amp;ldquo;headers&amp;rdquo;:{&amp;ldquo;end-user&amp;rdquo;:{&amp;ldquo;exact&amp;rdquo;:&amp;ldquo;jason&amp;rdquo;}}}],&amp;ldquo;route&amp;rdquo;:[{&amp;ldquo;destination&amp;rdquo;:{&amp;ldquo;host&amp;rdquo;:&amp;ldquo;reviews&amp;rdquo;,&amp;ldquo;subset&amp;rdquo;:&amp;ldquo;v2&amp;rdquo;}}]},{&amp;ldquo;route&amp;rdquo;:[{&amp;ldquo;destination&amp;rdquo;:{&amp;ldquo;host&amp;rdquo;:&amp;ldquo;reviews&amp;rdquo;,&amp;ldquo;subset&amp;rdquo;:&amp;ldquo;v1&amp;rdquo;}}]}]}}
creationTimestamp: &amp;ldquo;2019-03-21T06:01:10Z&amp;rdquo;
generation: 1
name: reviews
namespace: default
resourceVersion: &amp;ldquo;62486214&amp;rdquo;
selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/virtualservices/reviews
uid: b9e41681-4b9e-11e9-a679-00163e045478
spec:
hosts:
- reviews
http:
- match:
    - headers:
        end-user:
        exact: jason
    route:
    - destination:
        host: reviews
        subset: v2
- route:
    - destination:
        host: reviews
        subset: v1&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
确认规则已创建之后，在浏览器中打开BookInfo应用程序的URL: http://{EXTERNAL-IP}/productpage。

以&amp;quot;jason&amp;quot;用户登录 ```productpage``` 页面，您可以在每条评价后面看到星级信息。

这里登录用户名为 ```jason``` ，密码随便输入即可

![image](http://wx4.sinaimg.cn/large/ad5fbf65ly1g1adtjugp3j21gb0iygoa.jpg)

### 流量转移
除了基于内容的路由，Istio还支持基于权重的路由规则。

首先，将所有微服务的缺省版本设置为v1：

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
$ kubectl replace -f samples/bookinfo/networking/virtual-service-all-v1.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
其次，使用下面的命令把50%的流量从reviews:v1转移到reviews:v3:

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
$ kubectl replace -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
在浏览器中多次刷新productpage页面，大约有50%的几率会看到页面中出现带红星的评价内容。

说明： 注意该方式和使用容器编排平台的部署特性来进行版本迁移是完全不同的。容器编排平台使用了实例scaling来对流量进行管理。而通过Istio，两个版本的reviews服务可以独立地进行扩容和缩容，并不会影响这两个版本服务之间的流量分发。

如果觉得 ```reviews：v3``` 微服务已经稳定，你可以通过以下命令， 将 ```virtual service``` 100％的流量路由到 ```reviews：v3```，从而实现一个灰度发布的功能。

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
$ kubectl replace -f samples/bookinfo/networking/virtual-service-reviews-v3.yaml
```&lt;/p&gt;

&lt;h2 id=&#34;在华为云-cce-上运行&#34;&gt;在华为云（CCE）上运行&lt;/h2&gt;

&lt;p&gt;华为云率先将 Istio 作为产品投入到公有云中进行商业应用，开通方式十分简单，只要在华为云CCE上创建集群，然后申请 Istio 公测即可。&lt;/p&gt;

&lt;p&gt;为了方便测试 Bookinfo 应用在华为云上提供了一键体验应用，点击即可省去刚刚那一系列的 &lt;code&gt;kubectl&lt;/code&gt; 操作&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65ly1g1afbs7oq4j21g90id0vv.jpg&#34; alt=&#34;image&#34; /&gt;
&lt;center&gt;一键创建体验应用&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65ly1g1afgth1cgj219b0a7tb1.jpg&#34; alt=&#34;image&#34; /&gt;
&lt;center&gt;点击灰度发布即可&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65ly1g1afjc5hvgj21fv0o1q6q.jpg&#34; alt=&#34;image&#34; /&gt;
&lt;center&gt;创建金丝雀发布&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65ly1g1afnqyqlhj20ze0o00vl.jpg&#34; alt=&#34;image&#34; /&gt;
&lt;center&gt;选择灰度发布的组件&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65ly1g1afp1c5ltj20zk0le765.jpg&#34; alt=&#34;image&#34; /&gt;
&lt;center&gt;填写版本号&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65ly1g1afq846bjj20z80nowgl.jpg&#34; alt=&#34;image&#34; /&gt;
&lt;center&gt;选择镜像版本&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65ly1g1afra8rmhj21050mfgpb.jpg&#34; alt=&#34;image&#34; /&gt;
&lt;center&gt;版本创建完成后配置灰度策略&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65ly1g1afwpan6qj21090mste1.jpg&#34; alt=&#34;image&#34; /&gt;
&lt;center&gt;选择相应策略，策略下发即可&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;总的来说，华为云的 Istio 确实已经是商业化应用，这里只是展示了部分灰度发布的功能。其他比如流量治理，流量监控等功能还没展示，这些功能做的十分细致，值得尝试。&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://help.aliyun.com/document_detail/90563.html?spm=a2c4g.11186623.6.759.5dbd1f5fSB2m9T&#34; target=&#34;_blank&#34;&gt;在Kubernetes上基于Istio实现Service Mesh智能路由&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://support.huaweicloud.com/bestpractice-cce/cce_bestpractice_0012.html&#34; target=&#34;_blank&#34;&gt;基于ISTIO服务网格的灰度发布&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>浅析ServiceMesh &amp; Istio</title>
      <link>https://guoxudong.io/en/post/istio-share/</link>
      <pubDate>Wed, 20 Mar 2019 09:12:28 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/istio-share/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;公司于&lt;strong&gt;18年10月&lt;/strong&gt;正式确认服务容器化，到&lt;strong&gt;18年12月4日&lt;/strong&gt;第一个服务正式部署到生产环境kubernetes集群，再到&lt;strong&gt;如今&lt;/strong&gt;已有&lt;strong&gt;23&lt;/strong&gt;个服务完成了生产环境容器化的切换，更多的服务在测试环境容器化部署随时可以切换到生产环境。目前新项目的开发，大部分都直接在测试环境容器化部署，不再需要新购ECS搭建测试环境。随着容器化的深入，服务间的通信和联系变的更加复杂，其中通信的可视化、流量的控制和服务质量的评估问题日益凸显，成为了微服务方案的短板。这个时候&lt;code&gt;Service mesh&lt;/code&gt;就进入了我们的视野。&lt;/p&gt;

&lt;h2 id=&#34;service-mesh是什么&#34;&gt;Service mesh是什么&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-Service&#34;&gt;
**这里注意：**```istio```只是```Service mesh```服务网格的一种。

### 服务网格的特点

服务网格有如下几个特点：

- 应用程序间通讯的中间层
- 轻量级网络代理
- 应用程序无感知
- 解耦应用程序的重试/超时、监控、追踪和服务发现

目前两款流行的服务网格开源软件 [Linkerd](https://linkerd.io) 和 [Istio](https://Istio.io) 都可以直接在 Kubernetes 中集成，其中 Linkerd 已经成为 CNCF 成员，Istio 在 2018年7月31日宣布 [1.0](https://istio.io/zh/blog/2018/announcing-1.0/)。

### 服务网格的发展历史

* Spring Cloud

    Spring Cloud 诞生于2015年，Spring Cloud 最早在功能层面为微服务治理定义了一系列标准特性，比如：智能路由、服务熔断、服务注册于发现等这些名词我最早看到都是在 Sprint Cloud 相关文章中。同时也有一些缺点，比如：需要在代码级别对诸多组件进行控制，并且都依赖于 Java 的实现，这与微服务的多语言协作背道而驰；没有对资源的调度、devops等提供相关支持，需要借助平台来完成；众所周知的Eureka闭源等。

* Linkerd

    Service mesh 这个命名就是来源于Linkerd。Linkerd 很好地结合了 kubernetes 所提供的功能，于2017年加入CNCF。

* Istio

    2017年5月， Google、 IBM 和 Lyft 宣布了Istio的诞生。一经发布，便立即获得Red Hat、F5等大厂响应，社区活跃度高涨，很快超越了 Linkerd，成为了 Service mesh 的代表产品。

* 国内服务网格

    这里不得不提的是国内服务网格的兴起，在 Service mesh 概念具体定义以前，国内的许多厂商就已经开始了微服务进程，同时在做自己的微服务治理产品。而在 Service mesh 概念普及之后，厂商意识到了自己产品也具有 Service mesh 的特点，将自己的服务治理平台进行了改造和完善，推出了自己的 Service mesh 产品。例如，微博、腾讯和华为都有自己的服务网格产品，华为更是已经将产品投入到公有云中进行商业应用。蚂蚁金服的 SOFAMesh 则是针对大流量的生产场景，在 Istio 的架构基础上进行修改并推广。

## Istio又是什么
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Istio``` 提供了一个完整的解决方案，通过为整个服务网格提供行为洞察和操作控制来满足微服务应用程序的多样化需求。Istio 允许您连接、保护、控制和观测服务。在较高的层次上，Istio 有助于降低这些部署的复杂性，并减轻开发团队的压力。它是一个完全开源的服务网格，可以透明地分层到现有的分布式应用程序上。它也是一个平台，包括允许它集成到任何日志记录平台、遥测或策略系统的 API。Istio 的多样化功能集使您能够成功高效地运行分布式微服务架构，并提供保护、连接和监控微服务的统一方法。&lt;/p&gt;

&lt;h3 id=&#34;istio的架构&#34;&gt;Istio的架构&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;Istio&lt;/code&gt;总的来说由两部分组成：&lt;strong&gt;控制平面&lt;/strong&gt;和&lt;strong&gt;数据平面&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;数据平面由一组以 sidecar 方式部署的智能代理（Envoy）组成。sidecar通过注入的方式和业务容器共存于一个 Pod 中，会劫业务容器的流量，接受控制面组件的控制，可以调节和控制微服务及 Mixer 之间所有的网络通信。&lt;/li&gt;
&lt;li&gt;控制平面是 Istio 的核心，负责管理和配置代理来路由流量。此外控制平面配置 Mixer 以实施策略和收集遥测数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下图显示了构成每个面板的不同组件：
&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65ly1g199o3s4g5j20lw0kijux.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;图片为原创，转载请标记出处&lt;a href=&#34;https://blog.maoxianplay.com/&#34; target=&#34;_blank&#34;&gt;https://blog.maoxianplay.com/&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;

&lt;h4 id=&#34;envoy&#34;&gt;Envoy&lt;/h4&gt;

&lt;p&gt;Istio 使用 Envoy 代理的扩展版本，Envoy 是以 C++ 开发的高性能代理，用于调解服务网格中所有服务的所有入站和出站流量。Envoy 的许多内置功能被 Istio 发扬光大，例如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;动态服务发现&lt;/li&gt;
&lt;li&gt;负载均衡&lt;/li&gt;
&lt;li&gt;TLS 终止&lt;/li&gt;
&lt;li&gt;HTTP/2 &amp;amp; gRPC 代理&lt;/li&gt;
&lt;li&gt;熔断器&lt;/li&gt;
&lt;li&gt;健康检查、基于百分比流量拆分的灰度发布&lt;/li&gt;
&lt;li&gt;故障注入&lt;/li&gt;
&lt;li&gt;丰富的度量指标&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Envoy 被部署为 sidecar，和对应服务在同一个 Kubernetes pod 中。这允许 Istio 将大量关于流量行为的信号作为属性提取出来，而这些属性又可以在 Mixer 中用于执行策略决策，并发送给监控系统，以提供整个网格行为的信息。&lt;/p&gt;

&lt;p&gt;Sidecar 代理模型还可以将 Istio 的功能添加到现有部署中，而无需重新构建或重写代码。可以阅读更多来了解为什么我们在设计目标中选择这种方式。&lt;/p&gt;

&lt;h4 id=&#34;mixer&#34;&gt;Mixer&lt;/h4&gt;

&lt;p&gt;Mixer 是一个独立于平台的组件，负责在服务网格上执行访问控制和使用策略，并从 Envoy 代理和其他服务收集遥测数据。代理提取请求级属性，发送到 Mixer 进行评估。&lt;/p&gt;

&lt;p&gt;Mixer 中包括一个灵活的插件模型，使其能够接入到各种主机环境和基础设施后端，从这些细节中抽象出 Envoy 代理和 Istio 管理的服务。&lt;/p&gt;

&lt;h4 id=&#34;pilot&#34;&gt;Pilot&lt;/h4&gt;

&lt;p&gt;Pilot 为 Envoy sidecar 提供服务发现功能，为智能路由（例如 A/B 测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能。它将控制流量行为的高级路由规则转换为特定于 Envoy 的配置，并在运行时将它们传播到 sidecar。&lt;/p&gt;

&lt;p&gt;Pilot 将平台特定的服务发现机制抽象化并将其合成为符合 Envoy 数据平面 API 的任何 sidecar 都可以使用的标准格式。这种松散耦合使得 Istio 能够在多种环境下运行（例如，Kubernetes、Consul、Nomad），同时保持用于流量管理的相同操作界面。&lt;/p&gt;

&lt;h4 id=&#34;citadel&#34;&gt;Citadel&lt;/h4&gt;

&lt;p&gt;Citadel 通过内置身份和凭证管理赋能强大的服务间和最终用户身份验证。可用于升级服务网格中未加密的流量，并为运维人员提供基于服务标识而不是网络控制的强制执行策略的能力。从 0.5 版本开始，Istio 支持基于角色的访问控制，以控制谁可以访问您的服务，而不是基于不稳定的三层或四层网络标识。&lt;/p&gt;

&lt;h4 id=&#34;galley-1-1版本新增&#34;&gt;Galley(1.1版本新增)&lt;/h4&gt;

&lt;p&gt;Galley 代表其他的 Istio 控制平面组件，用来验证用户编写的 Istio API 配置。随着时间的推移，Galley 将接管 Istio 获取配置、 处理和分配组件的顶级责任。它将负责将其他的 Istio 组件与从底层平台（例如 Kubernetes）获取用户配置的细节中隔离开来。&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;在云原生架构下，容器的使用给予了异构应用程序的更多可行性，Kubernetes 增强了应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，Istio可以使开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情而专注于程序开发，赋予开发者更多的创造性。&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://istio.io/zh/&#34; target=&#34;_blank&#34;&gt;Istio 官方文档 - istio.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/fleeto/istio-for-beginner&#34; target=&#34;_blank&#34;&gt;《深入浅出istio》&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Service Mesh是什么</title>
      <link>https://guoxudong.io/en/post/istio-servicemesh/</link>
      <pubDate>Tue, 19 Mar 2019 16:12:56 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/istio-servicemesh/</guid>
      <description>

&lt;p&gt;本文转自&lt;a href=&#34;http://www.servicemesher.com/istio-handbook/concepts-and-principle/what-is-service-mesh.html&#34; target=&#34;_blank&#34;&gt;《Istio 服务网格进阶实战》&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;什么是服务网格&#34;&gt;什么是服务网格？&lt;/h1&gt;

&lt;p&gt;Service mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 &lt;a href=&#34;https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/&#34; target=&#34;_blank&#34;&gt;WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE?&lt;/a&gt; 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。&lt;/p&gt;

&lt;p&gt;服务网格是用于处理服务间通信的专用基础设施层。它负责通过包含现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，服务网格通常通过一组轻量级网络代理来实现，这些代理与应用程序代码一起部署，而不需要感知应用程序本身。——  &lt;a href=&#34;https://twitter.com/wm&#34; target=&#34;_blank&#34;&gt;Willian Morgan&lt;/a&gt; Buoyant CEO&lt;/p&gt;

&lt;p&gt;服务网格（Service Mesh）这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。&lt;/p&gt;

&lt;h2 id=&#34;服务网格的特点&#34;&gt;服务网格的特点&lt;/h2&gt;

&lt;p&gt;服务网格有如下几个特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;应用程序间通讯的中间层&lt;/li&gt;
&lt;li&gt;轻量级网络代理&lt;/li&gt;
&lt;li&gt;应用程序无感知&lt;/li&gt;
&lt;li&gt;解耦应用程序的重试/超时、监控、追踪和服务发现&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前两款流行的服务网格开源软件 &lt;a href=&#34;https://linkerd.io&#34; target=&#34;_blank&#34;&gt;Linkerd&lt;/a&gt; 和 &lt;a href=&#34;https://Istio.io&#34; target=&#34;_blank&#34;&gt;Istio&lt;/a&gt; 都可以直接在 Kubernetes 中集成，其中 Linkerd 已经成为 CNCF 成员，Istio 在 2018年7月31日宣布 &lt;a href=&#34;https://istio.io/zh/blog/2018/announcing-1.0/&#34; target=&#34;_blank&#34;&gt;1.0&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;理解服务网格&#34;&gt;理解服务网格&lt;/h2&gt;

&lt;p&gt;如果用一句话来解释什么是服务网格，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用服务网格也就无须关心服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给服务网格就可以了。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://philcalcado.com/&#34; target=&#34;_blank&#34;&gt;Phil Calçado&lt;/a&gt; 在他的这篇博客 &lt;a href=&#34;http://philcalcado.com/2017/08/03/pattern_service_mesh.html&#34; target=&#34;_blank&#34;&gt;Pattern: Service Mesh&lt;/a&gt; 中详细解释了服务网格的来龙去脉：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;从最原始的主机之间直接使用网线相连&lt;/li&gt;
&lt;li&gt;网络层的出现&lt;/li&gt;
&lt;li&gt;集成到应用程序内部的控制流&lt;/li&gt;
&lt;li&gt;分解到应用程序外部的控制流&lt;/li&gt;
&lt;li&gt;应用程序的中集成服务发现和断路器&lt;/li&gt;
&lt;li&gt;出现了专门用于服务发现和断路器的软件包/库，如 &lt;a href=&#34;https://finagle.github.io/&#34; target=&#34;_blank&#34;&gt;Twitter 的 Finagle&lt;/a&gt; 和 &lt;a href=&#34;https://code.fb.com/networking-traffic/introducing-proxygen-facebook-s-c-http-framework/&#34; target=&#34;_blank&#34;&gt;Facebook  的 Proxygen&lt;/a&gt;，这时候还是集成在应用程序内部&lt;/li&gt;
&lt;li&gt;出现了专门用于服务发现和断路器的开源软件，如 &lt;a href=&#34;http://netflix.github.io/&#34; target=&#34;_blank&#34;&gt;Netflix OSS&lt;/a&gt;、Airbnb 的 &lt;a href=&#34;https://github.com/airbnb/synapse&#34; target=&#34;_blank&#34;&gt;synapse&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/airbnb/nerve&#34; target=&#34;_blank&#34;&gt;nerve&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;最后作为微服务的中间层服务网格出现&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;服务网格的架构如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/00704eQkly1fswh7dbs1pj30id0bpmxl.jpg&#34; alt=&#34;Service Mesh 架构图&#34; /&gt;&lt;/p&gt;

&lt;p&gt;图片来自：&lt;a href=&#34;http://philcalcado.com/2017/08/03/pattern_service_mesh.html&#34; target=&#34;_blank&#34;&gt;Pattern: Service Mesh&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;服务网格作为 sidecar 运行，对应用程序来说是透明，所有应用程序间的流量都会通过它，所以对应用程序流量的控制都可以在 serivce mesh 中实现。&lt;/p&gt;

&lt;h2 id=&#34;服务网格如何工作&#34;&gt;服务网格如何工作？&lt;/h2&gt;

&lt;p&gt;下面以 Istio 为例讲解服务网格如何在 Kubernetes 中工作。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Istio 将服务请求路由到目的地址，根据其中的参数判断是到生产环境、测试环境还是 staging 环境中的服务（服务可能同时部署在这三个环境中），是路由到本地环境还是公有云环境？所有的这些路由信息可以动态配置，可以是全局配置也可以为某些服务单独配置。&lt;/li&gt;
&lt;li&gt;当 Istio 确认了目的地址后，将流量发送到相应服务发现端点，在 Kubernetes 中是 service，然后 service 会将服务转发给后端的实例。&lt;/li&gt;
&lt;li&gt;Istio 根据它观测到最近请求的延迟时间，选择出所有应用程序的实例中响应最快的实例。&lt;/li&gt;
&lt;li&gt;Istio 将请求发送给该实例，同时记录响应类型和延迟数据。&lt;/li&gt;
&lt;li&gt;如果该实例挂了、不响应了或者进程不工作了，Istio 将把请求发送到其他实例上重试。&lt;/li&gt;
&lt;li&gt;如果该实例持续返回 error，Istio 会将该实例从负载均衡池中移除，稍后再周期性得重试。&lt;/li&gt;
&lt;li&gt;如果请求的截止时间已过，Istio 主动失败该请求，而不是再次尝试添加负载。&lt;/li&gt;
&lt;li&gt;Istio 以 metric 和分布式追踪的形式捕获上述行为的各个方面，这些追踪信息将发送到集中 metric 系统。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;为何使用服务网格&#34;&gt;为何使用服务网格？&lt;/h2&gt;

&lt;p&gt;服务网格并没有给我们带来新功能，它是用于解决其他工具已经解决过的问题，只不过这次是在云原生的 Kubernetes 环境下的实现。&lt;/p&gt;

&lt;p&gt;在传统的 MVC 三层 Web 应用程序架构下，服务之间的通讯并不复杂，在应用程序内部自己管理即可，但是在现今的复杂的大型网站情况下，单体应用被分解为众多的微服务，服务之间的依赖和通讯十分复杂，出现了 twitter 开发的 &lt;a href=&#34;https://twitter.github.io/finagle/&#34; target=&#34;_blank&#34;&gt;Finagle&lt;/a&gt;、Netflix 开发的 &lt;a href=&#34;https://github.com/Netflix/Hystrix&#34; target=&#34;_blank&#34;&gt;Hystrix&lt;/a&gt; 和 Google 的 Stubby 这样的 “胖客户端” 库，这些就是早期的服务网格，但是它们都仅适用于特定的环境和特定的开发语言，并不能作为平台级的服务网格支持。&lt;/p&gt;

&lt;p&gt;在云原生架构下，容器的使用给予了异构应用程序的更多可行性，Kubernetes 增强的应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，同时开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情而专注于程序开发，赋予开发者更多的创造性。&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/&#34; target=&#34;_blank&#34;&gt;WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? - buoyant.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/attest-engineering/Istio-a-service-mesh-for-aws-ecs-937f201f847a&#34; target=&#34;_blank&#34;&gt;Istio: A service mesh for AWS ECS - medium.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://istio.io/blog/istio-service-mesh-for-microservices.html&#34; target=&#34;_blank&#34;&gt;初次了解 Istio - istio.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.christianposta.com/microservices/application-network-functions-with-esbs-api-management-and-now-service-mesh/&#34; target=&#34;_blank&#34;&gt;Application Network Functions With ESBs, API Management, and Now.. Service Mesh? - blog.christianposta.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://philcalcado.com/2017/08/03/pattern_service_mesh.html&#34; target=&#34;_blank&#34;&gt;Pattern: Service Mesh - philcalcado.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.servicemesher.com/envoy/&#34; target=&#34;_blank&#34;&gt;Envoy 官方文档中文版 - servicemesher.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://istio.io/zh&#34; target=&#34;_blank&#34;&gt;Istio 官方文档 - istio.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/servicemesher/awesome-servicemesh/&#34; target=&#34;_blank&#34;&gt;servicemesher/awesome-servicemesh - github.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>自动合并Kubeconfig，实现多k8s集群切换</title>
      <link>https://guoxudong.io/en/post/merge-kubeconfig/</link>
      <pubDate>Sun, 17 Mar 2019 10:45:02 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/merge-kubeconfig/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;随着微服务和容器化的深入人心，以及kubernetes已经成为容器编排领域的事实标准，越来越多的公司将自己的服务迁移到kubernetes集群中。而随着kubernetes集群的增加，集群管理的问题就凸显出来，不同的环境存在不同的集群，不同的业务线不同的集群，甚至有些开发人员都有自己的集群。诚然，如果集群是使用公有云如阿里云或华为云的容器服务，可以登录其控制台进行集群管理；或者使用rancher这用的多集群管理工具进行统一的管理。但是在想操作&lt;code&gt;istio&lt;/code&gt;特有的容器资源，或者想使用&lt;code&gt;istioctl&lt;/code&gt;的时候，或者像我一样就是想使用&lt;code&gt;kubectl&lt;/code&gt;命令的同学，这个时候多集群的切换就显的十分重要了。&lt;/p&gt;

&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-kubectl```命令行工具通过```kubeconfig```文件的配置来选择集群以及集群的API&#34;&gt;
## 原理
使用```kubeconfig```文件，您可以组织您的群集，用户和名称空间。 还可以定义上下文以快速轻松地在群集和名称空间之间切换。

### 上下文(Context) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kubeconfig```文件中的上下文元素用于以方便的名称对访问参数进行分组。 每个上下文有三个参数：集群，命名空间和用户。 默认情况下，kubectl命令行工具使用当前上下文中的参数与集群进行通信。可以使用下面的命令设置上下文：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;配置内容&#34;&gt;配置内容&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;kubectl config view
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;如果设置了&lt;code&gt;--kubeconfig&lt;/code&gt;标志，则只使用指定的文件。该标志只允许有一个实例。&lt;/li&gt;
&lt;li&gt;如果环境变量&lt;code&gt;KUBECONFIG&lt;/code&gt;存在，那么就使用该环境变量&lt;code&gt;KUBECONFIG&lt;/code&gt;里面的值，如果不存在该环境变量&lt;code&gt;KUBECONFIG&lt;/code&gt;，那么默认就是使用&lt;code&gt;$HOME/.kube/config&lt;/code&gt;文件。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;kubeconfig-内容&#34;&gt;&lt;code&gt;kubeconfig&lt;/code&gt;内容&lt;/h3&gt;

&lt;p&gt;从下面kubeconfig文件的配置来看集群、用户、上下文、当前上下文的关系就比较明显了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Config
preferences: {}

clusters:
- cluster:
name: {cluster-name}

users:
- name: {user-name}

contexts:
- context:
    cluster: {cluster-name}
    user: {user-name}
name: {context-name}

current-context: {context-name}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;为何要自动合并&#34;&gt;为何要自动合并&lt;/h2&gt;

&lt;p&gt;在日常的工作中，如果我们需要操作多个集群，会得到多个kubeconfig配置文件。一般的kubeconfig文件都是yaml格式的，但是也有少部分的集群kubeconfig时已json文件的形式给出的（比如华为云的=。=），比如我们公司再阿里云、华为云和自建环境上均存在kubernetes集群，平时操作要在多集群之间切换，这也就催生了我写这个工具（其实就是一个脚本）的动机。&lt;/p&gt;

&lt;h2 id=&#34;自动合并生成kubeconfig&#34;&gt;自动合并生成kubeconfig&lt;/h2&gt;

&lt;p&gt;众所周知，yaml是一种直观的能够被电脑识别的数据序列化格式，是一个可读性高并且容易被人类阅读的语言和json相比（没有格式化之前）可读性更强。而我这个工具并不是很关心kubeconfig的格式，只要将想要合并的kubeconfig放入指定文件即可。&lt;/p&gt;

&lt;p&gt;GitHub：&lt;a href=&#34;https://github.com/sunny0826/mergeKubeConfig&#34; target=&#34;_blank&#34;&gt;https://github.com/sunny0826/mergeKubeConfig&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;适用环境&#34;&gt;适用环境&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;需要在终端使用命令行管理多集群&lt;/li&gt;
&lt;li&gt;kubernetes集群中安装了istio，需要使用&lt;code&gt;istioctl&lt;/code&gt;命令，但是集群节点并没有安装&lt;code&gt;istioctl&lt;/code&gt;，需要在本地终端操作&lt;/li&gt;
&lt;li&gt;不愿频繁编辑.kube目录中的config文件的同学&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;准备工作&#34;&gt;准备工作&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Python环境：2.7或者3均可&lt;/li&gt;
&lt;li&gt;需要依赖包：&lt;code&gt;PyYAML&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;开始使用&#34;&gt;开始使用&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;安装依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install PyYAML
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;运行脚本&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;默认运行方式，kubeconfig文件放入&lt;code&gt;configfile&lt;/code&gt;文件,注意删掉作为示例的两个文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python merge.py
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;自定义kubeconfig文件目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python merge.py -d {custom-dir}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;运行后操作&#34;&gt;运行后操作&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;将生成的config文件放入.kube目录中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp config ~/.kube
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看所有的可使用的kubernetes集群角色&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config get-contexts
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;更多关于kubernetes配置文件操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config --help
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;切换kubernetes配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context {your-contexts}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;在使用kubernetes初期，在多集群之间我一直是频繁的切换&lt;code&gt;.kube/config&lt;/code&gt;文件来达到切换操作集群的目的。这也导致了我的&lt;code&gt;.kube&lt;/code&gt;目录中存在这多个类似于&lt;code&gt;al_test_config.bak&lt;/code&gt;、&lt;code&gt;al_prod_config.bak&lt;/code&gt;、&lt;code&gt;hw_test_config.bak&lt;/code&gt;的文件，本地环境已经自建环境，在集群切换的时候十分头疼。而后来使用&lt;code&gt;--kubeconfig&lt;/code&gt;来进行切换集群，虽然比之前的方法要方便很多，但是并不十分优雅。这个简单的小工具一举解决了我的文件，对于我这个&lt;code&gt;kubectl&lt;/code&gt;重度依赖者来说十分重要。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alpine Linux详解</title>
      <link>https://guoxudong.io/en/post/alpine-linux/</link>
      <pubDate>Fri, 15 Mar 2019 09:53:02 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/alpine-linux/</guid>
      <description>

&lt;h3 id=&#34;简介&#34;&gt;简介&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Small. Simple. Secure.Alpine Linux is a security-oriented, lightweight Linux distribution based on musl libc and busybox.&lt;/p&gt;

&lt;p&gt;Alpine Linux 是一个社区开发的面向安全应用的轻量级Linux发行版。 Alpine 的意思是“高山的”，它采用了musl libc和busybox以减小系统的体积和运行时资源消耗，同时还提供了自己的包管理工具apk。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;适用环境&#34;&gt;适用环境&lt;/h3&gt;

&lt;p&gt;由于其小巧、安全、简单以及功能完备的特点，被广泛应用于众多Docker容器中。我司目前使用的基础镜像均是基于该系统，&lt;a href=&#34;https://hub.docker.com/_/alpine&#34; target=&#34;_blank&#34;&gt;dockerhub&lt;/a&gt;上有提供各种语言的基础镜像.如：&lt;code&gt;node:8-alpine&lt;/code&gt;、&lt;code&gt;python:3.6-alpine&lt;/code&gt;，同时也可以基于alpine镜像制作符合自己需求的基础镜像。&lt;/p&gt;

&lt;h3 id=&#34;简单的镜像构建示例&#34;&gt;简单的镜像构建示例&lt;/h3&gt;

&lt;p&gt;这里提供一个python3的基础镜像的&lt;code&gt;Dockerfile&lt;/code&gt;，&lt;a href=&#34;https://pip.pypa.io/en/latest/installing/&#34; target=&#34;_blank&#34;&gt;get-pip.py&lt;/a&gt;可在 &lt;a href=&#34;https://pip.pypa.io/en/latest/installing/&#34; target=&#34;_blank&#34;&gt;https://pip.pypa.io/en/latest/installing/&lt;/a&gt; 下载。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-docker&#34;&gt;FROM alpine

MAINTAINER guoxudong@keking.cn

# 拷贝安装pip的脚本
COPY get-pip.py /get-pip.py

# 设置alpine的镜像地址为阿里云的地址
RUN echo &amp;quot;https://mirrors.aliyun.com/alpine/v3.6/main/&amp;quot; &amp;gt; /etc/apk/repositories \
    # 安装依赖包
    &amp;amp;&amp;amp; apk update \
    &amp;amp;&amp;amp; apk add --no-cache bash \
    # libevent-dev libxml2-dev  libffi libxml2 libxslt libxslt-dev  \
    python3 gcc g++ python3-dev python-dev linux-headers libffi-dev openssl-dev \
    # 由于通过apk安装的pip总是基于python2.7的版本，不符合项目要求，此处使用get-pip.py的方式
    #安装基于python3.6的pip
    &amp;amp;&amp;amp; python3 /get-pip.py \
    # 删除不必要的脚本
    &amp;amp;&amp;amp; cd .. \
    &amp;amp;&amp;amp; rm -f /get-pip.py \
    # 此环境专用做运行django项目，因此移除不必要的工具，减少空间
    #    &amp;amp;&amp;amp; pip uninstall -y pip setuptools wheel \
    # 最后清空apk安装时产生的无用文件
    &amp;amp;&amp;amp; rm -rf /var/cache/apk/*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;对比&lt;/strong&gt;：同样版本的python，对比镜像大小，可见使用alpine的优势&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;~ docker images | grep python
python                                  3.4                 ccbffa0d70d9        2 months ago        922MB
alpine-python3                          latest              69e41b673a50        2 months ago        297MB
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;apk包管理&#34;&gt;apk包管理&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;镜像源配置&lt;/p&gt;

&lt;p&gt;官方镜像源列表：&lt;a href=&#34;http://dl-cdn.alpinelinux.org/alpine/MIRRORS.txt&#34; target=&#34;_blank&#34;&gt;http://dl-cdn.alpinelinux.org/alpine/MIRRORS.txt&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;MIRRORS.txt中是当前Alpine官方提供的镜像源（Alpine安装的时候系统自动选择最佳镜像源）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;国内镜像源&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;清华TUNA镜像源：&lt;a href=&#34;https://mirror.tuna.tsinghua.edu.cn/alpine/&#34; target=&#34;_blank&#34;&gt;https://mirror.tuna.tsinghua.edu.cn/alpine/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;中科大镜像源：&lt;a href=&#34;http://mirrors.ustc.edu.cn/alpine/&#34; target=&#34;_blank&#34;&gt;http://mirrors.ustc.edu.cn/alpine/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;阿里云镜像源：&lt;a href=&#34;http://mirrors.aliyun.com/alpine/&#34; target=&#34;_blank&#34;&gt;http://mirrors.aliyun.com/alpine/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;镜像源配置&lt;/p&gt;

&lt;p&gt;这里推荐使用阿里云镜像源，由于公司应用都是部署在阿里云上，使用阿里云镜像源会快很多&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vi /etc/apk/repositories
# 将这两行插入到repositories文件开头
http://mirrors.aliyun.com/alpine/v3.9/main
http://mirrors.aliyun.com/alpine/v3.9/community
# 后面是原有的默认配置
http://dl-cdn.alpinelinux.org/alpine/v3.8/main
http://dl-cdn.alpinelinux.org/alpine/v3.8/community
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;apk包管理命令&lt;/p&gt;

&lt;p&gt;这里介绍一些常用的操作apk包管理命令&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;pre&gt;&lt;code class=&#34;language-apk&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;bash-4.3# apk &amp;ndash;help
apk-tools 2.10.0, compiled for x86_64.&lt;/p&gt;

&lt;p&gt;Installing and removing packages:
add       Add PACKAGEs to &amp;lsquo;world&amp;rsquo; and install (or upgrade) them, while ensuring that all dependencies are met
del       Remove PACKAGEs from &amp;lsquo;world&amp;rsquo; and uninstall them&lt;/p&gt;

&lt;p&gt;System maintenance:
fix       Repair package or upgrade it without modifying main dependencies
update    Update repository indexes from all remote repositories
upgrade   Upgrade currently installed packages to match repositories
cache     Download missing PACKAGEs to cache and/or delete unneeded files from cache&lt;/p&gt;

&lt;p&gt;Querying information about packages:
info      Give detailed information about PACKAGEs or repositories
list      List packages by PATTERN and other criteria
dot       Generate graphviz graphs
policy    Show repository policy for packages&lt;/p&gt;

&lt;p&gt;Repository maintenance:
index     Create repository index file from FILEs
fetch     Download PACKAGEs from global repositories to a local directory
verify    Verify package integrity and signature
manifest  Show checksums of package contents&lt;/p&gt;

&lt;p&gt;Use apk &lt;command&gt; &amp;ndash;help for command-specific help.
Use apk &amp;ndash;help &amp;ndash;verbose for a full command listing.&lt;/p&gt;

&lt;p&gt;This apk has coffee making abilities.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
- ```apk info``` 列出所有已安装的软件包
- ```apk apk update``` 更新最新本地镜像源
- ```apk upgrade``` 升级软件
- ```apk search``` 搜索可用软件包，**搜索之前最好先更新镜像源**

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
$ apk search #查找所以可用软件包
$ apk search -v #查找所以可用软件包及其描述内容
$ apk search -v &amp;lsquo;acf*&amp;rsquo; #通过软件包名称查找软件包
$ apk search -v -d &amp;lsquo;docker&amp;rsquo; #通过描述文件查找特定的软件包&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
- ```apk add``` 从仓库中安装最新软件包，并自动安装必须的依赖包,也可以从第三方仓库添加软件包
    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
$ apk add curl busybox-extras       #软件以空格分开这里，这里列举我们用的最多的curl和telnet
bash-4.3# apk add &amp;ndash;no-cache curl
bash-4.3# apk add mongodb &amp;ndash;update-cache &amp;ndash;repository &lt;a href=&#34;http://mirrors.ustc.edu.cn/alpine/v3.6/main/&#34; target=&#34;_blank&#34;&gt;http://mirrors.ustc.edu.cn/alpine/v3.6/main/&lt;/a&gt; &amp;ndash;allow-untrusted    #从指定镜像源拉取&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
安装指定版本软件包

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
bash-4.3# apk add mongodb=4.0.5-r0
bash-4.3# apk add &amp;lsquo;mongodb&lt;4.0.5&#39;
bash-4.3# apk add &#39;mongodb&gt;4.0.5&amp;rsquo;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
升级指定软件包

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bash
bash-4.3# apk add &amp;ndash;upgrade busybox #升级指定软件包
```&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：安装之前最好修改本地镜像源，更新镜像源，搜索软件包是否存在，选择合适岸本在进行安装。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;apk del&lt;/code&gt; 卸载并删除指定软件包&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;结语&#34;&gt;结语&lt;/h3&gt;

&lt;p&gt;Alpine以其小巧、简单在docker容器中得到了广泛的应用。但是Alpine Linux使用了musl，可能和其他Linux发行版使用的glibc实现会有些不同。这里主要介绍了它的基础用法，但是足以满足日常运维需要。毕竟在kubernetes集群中操作容器内环境较直接在虚拟机或者物理机上操作更为复杂，由于缩减的容器的大小，导致和CentOS或Ubuntu相比缺少许多功能。而缺少的这些功能又不想在基础镜像中安装导致容器变大，这个时候就可以在容器运行后，根据实际需要安装即可。&lt;/p&gt;

&lt;h3 id=&#34;参考文档&#34;&gt;参考文档&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management&#34; target=&#34;_blank&#34;&gt;https://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>阿里云环境Istio初探</title>
      <link>https://guoxudong.io/en/post/istio-demo/</link>
      <pubDate>Wed, 13 Mar 2019 15:45:43 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/istio-demo/</guid>
      <description>

&lt;h1 id=&#34;istio应用部署样例&#34;&gt;istio应用部署样例&lt;/h1&gt;

&lt;p&gt;该实例为一套istio服务上线流程：&lt;code&gt;注入&lt;/code&gt;-&amp;gt;&lt;code&gt;部署&lt;/code&gt;-&amp;gt;&lt;code&gt;创建目标规则&lt;/code&gt;-&amp;gt;&lt;code&gt;创建默认路由&lt;/code&gt;。就大多数istio服务网格应用均可基于这一流程上线。&lt;/p&gt;

&lt;h3 id=&#34;部署istio&#34;&gt;部署istio&lt;/h3&gt;

&lt;p&gt;istio有多种部署方式，阿里云、华为云等云服务商均提供一键安装，同时也可以通过GitHub下载release包，使用&lt;code&gt;install/kubernetes/istio-demo.yaml&lt;/code&gt;部署，或者使用helm部署。&lt;strong&gt;这里采用阿里云容器服务一键部署istio&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65ly1g117xxixlvj20a00ajdgb.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;部署两个版本的服务&#34;&gt;部署两个版本的服务&lt;/h3&gt;

&lt;p&gt;这里选择一个简单的Python项目作为服务端，这里使用&lt;a href=&#34;https://github.com/fleeto&#34; target=&#34;_blank&#34;&gt;崔秀龙&lt;/a&gt;老哥的&lt;a href=&#34;https://github.com/fleeto/flaskapp/blob/master/app/main.py&#34; target=&#34;_blank&#34;&gt;flaskapp&lt;/a&gt;服务，该服务的作用就是提供2个url路径：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一个是/env，用户获取容器中的环境变量，例如 &lt;a href=&#34;http://flaskapp/env/version&#34; target=&#34;_blank&#34;&gt;http://flaskapp/env/version&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;另一个是/fetch ，用于获取在参数url中指定的网址的内容，例如 &lt;a href=&#34;http://flaskapp/fetch?url=http://weibo.com&#34; target=&#34;_blank&#34;&gt;http://flaskapp/fetch?url=http://weibo.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;创建2个Deployment，分别命名为 flaskapp-v1 和 flaskapp-v2 ，同时创建一个 Service ,将其命名为flaskapp。代码文件为 &lt;code&gt;flaskapp.istio.yaml&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: v1
kind: Service
metadata:
name: flaskapp
labels:
    app: flaskapp
spec:
selector:
    app: flaskapp
ports:
- name: http
    port: 80
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
name: flaskapp-v1
spec:
replicas: 1
template:
    metadata:
    labels:
        app: flaskapp
        version: v1
    spec:
    containers:
    - name: flaskapp
        image: dustise/flaskapp
        imagePullPolicy: IfNotPresent
        env:
        - name: version
        value: v1
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
name: flaskapp-v2
spec:
replicas: 1
template:
    metadata:
    labels:
        app: flaskapp
        version: v2
    spec:
    containers:
    - name: flaskapp
        image: dustise/flaskapp
        imagePullPolicy: IfNotPresent
        env:
        - name: version
        value: v2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;两个版本Deployment的镜像一致，但是使用了不同的version标签区分，分别为 v1 和 v2 。实际环境中的镜像是不同的&lt;/li&gt;
&lt;li&gt;在两个Deployment中都有一个名为version的环境变量，分别为 v1 和 v2 。这里设置是为了方便后续区分服务。&lt;/li&gt;
&lt;li&gt;两个Deployment中都使用了 app 和 version 标签，在 istio 网格应用中通常会使用这两个标签作为应用和版本的标识。&lt;/li&gt;
&lt;li&gt;Service 中的 Selector 仅使用了一个 app 标签，这意味着该 Service 对两个 Deployment 都是有效的。&lt;/li&gt;
&lt;li&gt;将在 Service 中定义的端口根据 &lt;strong&gt;istio 规范&lt;/strong&gt;命名为http。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;istio注入并部署服务端&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ istioctl kube-inject -f flask.istio.yaml | kubectl apply -f -
service/flaskapp created
deployment.extensions/flaskapp-v1 created
deployment.extensions/flaskapp-v2 created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在rancher查看注入情况&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65ly1g1045ku3dcj20cj05kglp.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这里也可以使用&lt;code&gt;kubectl describe po flaskapp-v1-7d4f9b8459-2ncnf&lt;/code&gt;命令查看Pod容器，这里可以看到Pod中多了一个容器，名为&lt;code&gt;istio-proxy&lt;/code&gt;，这就表示注入成功了。而前面&lt;code&gt;istio-init&lt;/code&gt;的初始化容器，这个容器是用于初始化劫持的。&lt;/p&gt;

&lt;h3 id=&#34;部署客户端&#34;&gt;部署客户端&lt;/h3&gt;

&lt;p&gt;这里的客户端是一个安装了测试工具的镜像，测试的内容可以在容器内通过shell完成。代码文件为 &lt;code&gt;sleep.istio.yaml&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: v1
kind: Service
metadata:
name: sleep
labels:
    app: sleep
    version: v1
spec:
selector:
    app: sleep
    version: v1
ports:
- name: ssh
    port: 80
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
name: sleep
spec:
replicas: 1
template:
    metadata:
    labels:
        app: sleep
        version: v1
    spec:
    containers:
    - name: sleep
        image: dustise/sleep
        imagePullPolicy: IfNotPresent
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;istio注入并部署客户端&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ istioctl kube-inject -f sleep.istio.yaml | kubectl apply -f -
service/sleep created
deployment.extensions/sleep created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;sleep&lt;/code&gt;应用的Pod进入Running状态就可以进行验证了&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;验证服务&#34;&gt;验证服务&lt;/h3&gt;

&lt;p&gt;直接在sleep容器中执行命令行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ for i in `seq 10`;do http --body http://flaskapp/env/version;done
v1
v2
...
v1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该命令使用一个for循环，重复访问 &lt;a href=&#34;http://flaskapp/env/version&#34; target=&#34;_blank&#34;&gt;http://flaskapp/env/version&lt;/a&gt; ，查看内容，结果为 v1 和 v2 随机出现，各占一半。出现 v1 和 v2 版本轮流调用的效果，达到了基本的负载均衡的功能。&lt;/p&gt;

&lt;h3 id=&#34;创建目标规则&#34;&gt;创建目标规则&lt;/h3&gt;

&lt;p&gt;目标规则代码 &lt;code&gt;flaskapp-destinationrule.yaml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
name: flaskapp
spec:
host: flaskapp
subsets:
- name: v1
    labels:
    version: v1
- name: v2
    labels:
    version: v2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;部署目标规则（这里使用kubectl和istioctl均可）&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl apply -f flaskapp-destinationrule.yaml
Created config destination-rule/default/flaskapp at revision 59183403
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;创建默认路由&#34;&gt;创建默认路由&lt;/h3&gt;

&lt;p&gt;默认路由代码 &lt;code&gt;flaskapp-default-vs-v2.yaml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
name: flaskapp-default-v2
spec:
hosts: 
- flaskapp
http:
- route:
    - destination:
    host: flaskapp
    subset: v2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;部署默认路由&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl apply -f flaskapp-default-vs-v2.yaml
Created config virtual-service/default/flaskapp-default-v2 at revision 59185583
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;验证路由规则是否生效&#34;&gt;验证路由规则是否生效&lt;/h3&gt;

&lt;p&gt;再次在sleep容器中执行命令，查看新定义的流量管理规则是否生效&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ for i in `seq 10`;do http --body http://flaskapp/env/version;done
v2
v2
v2
v2
v2
v2
v2
v2
v2
v2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里就可以看到，设置的默认路由已经生效了，多次重复访问，返回的内容都是来自环境变量 version 设置为 v2 的版本，也就是v2版本。&lt;/p&gt;

&lt;h4 id=&#34;kiali查看调用情况&#34;&gt;kiali查看调用情况&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65ly1g104tydblxj21az0li40i.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到流量都进入了v2版本中&lt;/p&gt;

&lt;h3 id=&#34;小结&#34;&gt;小结&lt;/h3&gt;

&lt;p&gt;这里实现了一个极简的istio应用，可以帮助新手快速入门，官网提供的Bookinfo应用较为复杂。这里提供的小例子更为简洁易懂，非常利于入门。&lt;/p&gt;

&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/fleeto/istio-for-beginner&#34; target=&#34;_blank&#34;&gt;《深入浅出Istio》&lt;/a&gt;    &amp;mdash;   崔秀龙&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Jenkins Pipeline集成Sonar进行代码质量检测</title>
      <link>https://guoxudong.io/en/post/sonar-pipline/</link>
      <pubDate>Thu, 07 Mar 2019 09:14:39 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/sonar-pipline/</guid>
      <description>

&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;

&lt;p&gt;image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;h3 id=&#34;jenkins-pipeline&#34;&gt;jenkins pipeline&lt;/h3&gt;

&lt;p&gt;Jenkins Pipeline (或简称为 &amp;ldquo;Pipeline&amp;rdquo; )是一套插件，将持续交付的实现和实施集成到 Jenkins 中。&lt;/p&gt;

&lt;p&gt;持续交付Pipeline自动化的表达了这样一种流程：将基于版本控制管理的软件持续的交付到您的用户和消费者手中。&lt;/p&gt;

&lt;p&gt;Jenkins Pipeline 提供了一套可扩展的工具，用于将“简单到复杂”的交付流程实现为“持续交付即代码”。 Jenkins Pipeline 的定义通常被写入到一个文本文件（称为 &lt;code&gt;Jenkinsfile&lt;/code&gt; ）中，该文件可以被检入到项目的源代码控制库中。&lt;/p&gt;

&lt;p&gt;摘自&lt;a href=&#34;https://jenkins.io/zh/&#34; target=&#34;_blank&#34;&gt;Jenkins官方文档&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;sonarqube&#34;&gt;SonarQube&lt;/h3&gt;

&lt;p&gt;SonarQube is an open source platform to perform automatic reviews with static analysis of code to detect bugs, code smells and security vulnerabilities on 25+ programming languages including Java, C#, JavaScript, TypeScript, C/C++, COBOL and more.&lt;/p&gt;

&lt;p&gt;SonarQube是一个开源的平台，以执行与代码的静态分析，自动审查，可以检测在25+的编程语言如Java，C＃，JavaScript，TypeScript，C/C++，COBOL等的代码缺陷和安全漏洞。&lt;/p&gt;

&lt;h3 id=&#34;owasp&#34;&gt;OWASP&lt;/h3&gt;

&lt;p&gt;OWASP，全称是：Open Web Application Security Project，翻译为中文就是：开放式Web应用程序安全项目，是一个非营利组织，不附属于任何企业或财团，这也是该组织可以不受商业控制地进行安全开发及安全普及的重要原因，&lt;a href=&#34;https://en.wikipedia.org/wiki/OWASP/&#34; target=&#34;_blank&#34;&gt;详细介绍&lt;/a&gt;。OWASP Dependency-Check，它识别项目依赖关系，并检查是否存在任何已知的、公开的、漏洞，基于OWASP Top 10 2013。&lt;/p&gt;

&lt;h2 id=&#34;场景&#34;&gt;场景&lt;/h2&gt;

&lt;p&gt;在devops理念中，CI/CD毫无疑问是最重要的一环，而代码质量检查则是CI中必不可少的一步。在敏捷开发的思想下，代码的迭代周期变短，交付速度提升，这个时候代码的质量就很难保证，测试只能保证功能完整与可用，而代码的质量纯靠review的话效率又很低，这个时候sonar就可以很好的帮助开发自动化检测代码质量，降低bug数量，也可以根据扫描结果养成良好的编程习惯，同时也可以减少测试的工作量，真正提升整个团队效率，实现devops理念。&lt;/p&gt;

&lt;h2 id=&#34;前提&#34;&gt;前提&lt;/h2&gt;

&lt;p&gt;jenkins、sonarqube服务已经搭建完成，jenkins安装sonar插件&lt;code&gt;SonarQube Scanner for Jenkins&lt;/code&gt;，jenkins、sonarqube安装Dependency-Check插件&lt;code&gt;OWASP Dependency-Check Plugin&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;版本：jenkins2.166，sonarqube6.7.6&lt;/p&gt;

&lt;h2 id=&#34;配置&#34;&gt;配置&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;下载安装jenkins插件&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[系统管理]&lt;/strong&gt;-&lt;strong&gt;[插件管理]&lt;/strong&gt;-&lt;strong&gt;[可选插件]&lt;/strong&gt;-&lt;strong&gt;[SonarQube Scanner for Jenkins]&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65ly1g0u4q3ae1bj20t90233yt.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;SonarQube生成token，&lt;strong&gt;这个token不会显示第二次，所以一定要记住&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/mw690/ad5fbf65ly1g0u5902q6nj213f0hgwgn.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;SonarQube配置Dependency-Check&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[配置]&lt;/strong&gt;-&lt;strong&gt;[Dependency-Check]&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[注意：]&lt;/strong&gt;这里去掉 &lt;code&gt;${WORKSPACE}/&lt;/code&gt;，否则将报&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Java&#34;&gt;[INFO] Dependency-Check XML report does not exists. Please check property sonar.dependencyCheck.reportPath:/data/jenkinsHome/workspace/xxx/${WORKSPACE}/dependency-check-report.xml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65ly1g0yvjjcvdaj211b0jhgod.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在pom.xml文件中添加&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;plugin&amp;gt;
    &amp;lt;groupId&amp;gt;org.sonarsource.scanner.maven&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;sonar-maven-plugin&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;3.6.0.1398&amp;lt;/version&amp;gt;
&amp;lt;/plugin&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置jenkins&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[系统管理]&lt;/strong&gt;-&lt;strong&gt;[系统设置]&lt;/strong&gt;-&lt;strong&gt;[SonarQube servers]&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx3.sinaimg.cn/large/ad5fbf65ly1g0u50l8q4lj215o0b3myw.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;sonar添加webhook&lt;/p&gt;

&lt;p&gt;在代码扫描成功后，扫描结果需要回调jenkins，添加的Jenkins的webhook结构为：http://[jenkins_url]/sonarqube-webhook/&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[配置]&lt;/strong&gt;-&lt;strong&gt;[web回调接口]&lt;/strong&gt;-&lt;strong&gt;[URL]&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx4.sinaimg.cn/large/ad5fbf65ly1g0v4m590vhj212k0pw0vo.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;编辑jenkins pipeline&lt;/p&gt;

&lt;p&gt;在jenkinsfile文件中添加配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Groovy&#34;&gt;stage(&#39;依赖安全检查&#39;) {
    steps{
        dependencyCheckAnalyzer datadir: &#39;&#39;, hintsFile: &#39;&#39;, includeCsvReports: false, includeHtmlReports: true, includeJsonReports: false, includeVulnReports: true, isAutoupdateDisabled: false, outdir: &#39;&#39;, scanpath: &#39;&#39;, skipOnScmChange: false, skipOnUpstreamChange: false, suppressionFile: &#39;&#39;, zipExtensions: &#39;&#39;
    }
}

stage(&#39;静态代码检查&#39;) {
    steps {
        echo &amp;quot;starting codeAnalyze with SonarQube......&amp;quot;
        withSonarQubeEnv(&#39;sonar&#39;) {
            //注意这里withSonarQubeEnv()中的参数要与之前SonarQube servers中Name的配置相同
            withMaven(maven: &#39;M3&#39;) {
                sh &amp;quot;mvn clean package -Dmaven.test.skip=true sonar:sonar -Dsonar.projectKey={项目key} -Dsonar.projectName={项目名称} -Dsonar.projectVersion={项目版本} -Dsonar.sourceEncoding=UTF-8 -Dsonar.exclusions=src/test/** -Dsonar.sources=src/ -Dsonar.java.binaries=target/classes -Dsonar.host.url={SonarQube地址} -Dsonar.login={SonarQube的token}&amp;quot;
            }
        }
        script {
            timeout(1) {
                //这里设置超时时间1分钟，不会出现一直卡在检查状态
                //利用sonar webhook功能通知pipeline代码检测结果，未通过质量阈，pipeline将会fail
                def qg = waitForQualityGate(&#39;sonar&#39;)
                //注意：这里waitForQualityGate()中的参数也要与之前SonarQube servers中Name的配置相同
                if (qg.status != &#39;OK&#39;) {
                    error &amp;quot;未通过Sonarqube的代码质量阈检查，请及时修改！failure: ${qg.status}&amp;quot;
                }
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;参数解释：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sonar.projectKey：项目key (必填项)&lt;/li&gt;
&lt;li&gt;sonar.projectName：项目名称（必填项）&lt;/li&gt;
&lt;li&gt;sonar.projectVersion：项目版本（必填项）&lt;/li&gt;
&lt;li&gt;sonar.sources：源码位置(相对路径）&lt;/li&gt;
&lt;li&gt;sonar.java.binaries：编译后的class位置（必填项，相对路径同上）&lt;/li&gt;
&lt;li&gt;sonar.exclusions：排除的扫描的文件路径&lt;/li&gt;
&lt;li&gt;sonar.host.url：SonarQube地址&lt;/li&gt;
&lt;li&gt;sonar.login：SonarQube生成的token&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;运行&#34;&gt;运行&lt;/h2&gt;

&lt;p&gt;执行jenkins构建，构建成功后会显示如下，则证明sonar代码扫描成功且通过代码质量阈检查
&lt;img src=&#34;https://wx1.sinaimg.cn/mw690/ad5fbf65ly1g0u6qrh8qrj21fu0q2dmw.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;查看sonar报告，这里有两种方式&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;可直接登录SonarQube查看报告
&lt;img src=&#34;https://ws1.sinaimg.cn/mw690/ad5fbf65ly1g0u6vbspv5j21260myadw.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;也可直接在jenkins页面点击SonarQube图标进入，点击以下标记均可进去
&lt;img src=&#34;https://ws2.sinaimg.cn/mw690/ad5fbf65ly1g0u6xzcryhj21fn0q7wkm.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;其他&#34;&gt;其他&lt;/h2&gt;

&lt;h3 id=&#34;问题一-无法扫描代码-错误提示&#34;&gt;问题一：无法扫描代码，错误提示&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-Java&#34;&gt;hudson.remoting.ProxyException: hudson.AbortException: SonarQube installation defined in this job (sonar) does not match any configured installation. Number of installations that can be configured: 1.
If you want to reassign jobs to a different SonarQube installation, check the documentation under https://redirect.sonarsource.com/plugins/jenkins.html
    at hudson.plugins.sonar.SonarInstallation.checkValid(SonarInstallation.java:94)
    at hudson.plugins.sonar.SonarBuildWrapper.setUp(SonarBuildWrapper.java:67)
    at org.jenkinsci.plugins.workflow.steps.CoreWrapperStep$Execution.start(CoreWrapperStep.java:80)
    at org.jenkinsci.plugins.workflow.cps.DSL.invokeStep(DSL.java:268)
Caused: hudson.remoting.ProxyException: org.codehaus.groovy.runtime.InvokerInvocationException: hudson.AbortException: SonarQube installation defined in this job (sonar) does not match any configured installation. Number of installations that can be configured: 1.
If you want to reassign jobs to a different SonarQube installation, check the documentation under https://redirect.sonarsource.com/plugins/jenkins.html
    at org.jenkinsci.plugins.workflow.cps.CpsStepContext.replay(CpsStepContext.java:499)
    at org.jenkinsci.plugins.workflow.cps.DSL.invokeStep(DSL.java:295)
    at org.jenkinsci.plugins.workflow.cps.DSL.invokeStep(DSL.java:207)
    at org.jenkinsci.plugins.workflow.cps.DSL.invokeDescribable(DSL.java:395)
    at org.jenkinsci.plugins.workflow.cps.DSL.invokeMethod(DSL.java:179)
    at org.jenkinsci.plugins.workflow.cps.CpsScript.invokeMethod(CpsScript.java:122)
    at sun.reflect.GeneratedMethodAccessor1200.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93)
    at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)
    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1213)
    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1022)
    at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:42)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113)
    at org.kohsuke.groovy.sandbox.impl.Checker$1.call(Checker.java:157)
    at org.kohsuke.groovy.sandbox.GroovyInterceptor.onMethodCall(GroovyInterceptor.java:23)
    at org.jenkinsci.plugins.scriptsecurity.sandbox.groovy.SandboxInterceptor.onMethodCall(SandboxInterceptor.java:155)
    at org.kohsuke.groovy.sandbox.impl.Checker$1.call(Checker.java:155)
    at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:159)
    at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:129)
    at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:129)
    at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:129)
    at com.cloudbees.groovy.cps.sandbox.SandboxInvoker.methodCall(SandboxInvoker.java:17)
Caused: hudson.remoting.ProxyException: java.lang.IllegalArgumentException: Failed to prepare withSonarQubeEnv step
    at org.jenkinsci.plugins.workflow.cps.DSL.invokeDescribable(DSL.java:397)
    at org.jenkinsci.plugins.workflow.cps.DSL.invokeMethod(DSL.java:179)
    at org.jenkinsci.plugins.workflow.cps.CpsScript.invokeMethod(CpsScript.java:122)
    at sun.reflect.GeneratedMethodAccessor1200.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93)
    at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)
    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1213)
    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1022)
    at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:42)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113)
    at org.kohsuke.groovy.sandbox.impl.Checker$1.call(Checker.java:157)
    at org.kohsuke.groovy.sandbox.GroovyInterceptor.onMethodCall(GroovyInterceptor.java:23)
    at org.jenkinsci.plugins.scriptsecurity.sandbox.groovy.SandboxInterceptor.onMethodCall(SandboxInterceptor.java:155)
    at org.kohsuke.groovy.sandbox.impl.Checker$1.call(Checker.java:155)
    at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:159)
    at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:129)
    at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:129)
    at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:129)
    at com.cloudbees.groovy.cps.sandbox.SandboxInvoker.methodCall(SandboxInvoker.java:17)
    at WorkflowScript.run(WorkflowScript:27)
    at ___cps.transform___(Native Method)
    at com.cloudbees.groovy.cps.impl.ContinuationGroup.methodCall(ContinuationGroup.java:57)
    at com.cloudbees.groovy.cps.impl.FunctionCallBlock$ContinuationImpl.dispatchOrArg(FunctionCallBlock.java:109)
    at com.cloudbees.groovy.cps.impl.FunctionCallBlock$ContinuationImpl.fixArg(FunctionCallBlock.java:82)
    at sun.reflect.GeneratedMethodAccessor249.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.cloudbees.groovy.cps.impl.ContinuationPtr$ContinuationImpl.receive(ContinuationPtr.java:72)
    at com.cloudbees.groovy.cps.impl.ClosureBlock.eval(ClosureBlock.java:46)
    at com.cloudbees.groovy.cps.Next.step(Next.java:83)
    at com.cloudbees.groovy.cps.Continuable$1.call(Continuable.java:174)
    at com.cloudbees.groovy.cps.Continuable$1.call(Continuable.java:163)
    at org.codehaus.groovy.runtime.GroovyCategorySupport$ThreadCategoryInfo.use(GroovyCategorySupport.java:122)
    at org.codehaus.groovy.runtime.GroovyCategorySupport.use(GroovyCategorySupport.java:261)
    at com.cloudbees.groovy.cps.Continuable.run0(Continuable.java:163)
    at org.jenkinsci.plugins.workflow.cps.SandboxContinuable.access$101(SandboxContinuable.java:34)
    at org.jenkinsci.plugins.workflow.cps.SandboxContinuable.lambda$run0$0(SandboxContinuable.java:59)
    at org.jenkinsci.plugins.scriptsecurity.sandbox.groovy.GroovySandbox.runInSandbox(GroovySandbox.java:121)
    at org.jenkinsci.plugins.workflow.cps.SandboxContinuable.run0(SandboxContinuable.java:58)
    at org.jenkinsci.plugins.workflow.cps.CpsThread.runNextChunk(CpsThread.java:182)
    at org.jenkinsci.plugins.workflow.cps.CpsThreadGroup.run(CpsThreadGroup.java:332)
    at org.jenkinsci.plugins.workflow.cps.CpsThreadGroup.access$200(CpsThreadGroup.java:83)
    at org.jenkinsci.plugins.workflow.cps.CpsThreadGroup$2.call(CpsThreadGroup.java:244)
    at org.jenkinsci.plugins.workflow.cps.CpsThreadGroup$2.call(CpsThreadGroup.java:232)
    at org.jenkinsci.plugins.workflow.cps.CpsVmExecutorService$2.call(CpsVmExecutorService.java:64)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at hudson.remoting.SingleLaneExecutorService$1.run(SingleLaneExecutorService.java:131)
    at jenkins.util.ContextResettingExecutorService$1.run(ContextResettingExecutorService.java:28)
    at jenkins.security.ImpersonatingExecutorService$1.run(ImpersonatingExecutorService.java:59)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Finished: FAILURE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因：withSonarQubeEnv()中的参数与之前SonarQube servers中Name的配置不同，导致没有找到找到SonarQube&lt;/p&gt;

&lt;h3 id=&#34;问题二-sonarqube的token配置不对-导致无法连接sonar&#34;&gt;问题二：SonarQube的token配置不对，导致无法连接sonar&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-Java&#34;&gt;[ERROR] Failed to execute goal org.sonarsource.scanner.maven:sonar-maven-plugin:3.6.0.1398:sonar (default-cli) on project callcenter: Not authorized. Please check the properties sonar.login and sonar.password. -&amp;gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[Pipeline] }
[withMaven] artifactsPublisher - Archive artifact pom.xml under cn/keking/callcenter/callcenter/0.0.1-SNAPSHOT/callcenter-0.0.1-SNAPSHOT.pom
[withMaven] artifactsPublisher - Archive artifact target/callcenter-0.0.1-SNAPSHOT.jar under cn/keking/callcenter/callcenter/0.0.1-SNAPSHOT/callcenter-0.0.1-SNAPSHOT.jar
[withMaven] artifactsPublisher - Archive artifact target/callcenter-0.0.1-SNAPSHOT-api.jar under cn/keking/callcenter/callcenter/0.0.1-SNAPSHOT/callcenter-0.0.1-SNAPSHOT-api.jar
[withMaven] junitPublisher - Archive test results for Maven artifact cn.keking.callcenter:callcenter:jar:0.0.1-SNAPSHOT generated by maven-surefire-plugin:test (default-test): target/surefire-reports/*.xml
[withMaven] junitPublisher - Jenkins JUnit Attachments Plugin not found, can&#39;t publish test attachments.Recording test results
None of the test reports contained any result
[withMaven] Jenkins Task Scanner Plugin not found, don&#39;t display results of source code scanning for &#39;TODO&#39; and &#39;FIXME&#39; in pipeline screen.
[withMaven] Publishers: Pipeline Graph Publisher: 1 ms, Generated Artifacts Publisher: 891 ms, Junit Publisher: 4 ms, Dependencies Fingerprint Publisher: 5 ms
[Pipeline] // withMaven
[Pipeline] }
WARN: Unable to locate &#39;report-task.txt&#39; in the workspace. Did the SonarScanner succedeed?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因：sonar.login的token配置不正确或者没有配置&lt;/p&gt;

&lt;h3 id=&#34;问题三-jenkins-pipeline在sonarqube回调时显示超时&#34;&gt;问题三：jenkins pipeline在SonarQube回调时显示超时&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-Java&#34;&gt;[Pipeline] waitForQualityGate
Checking status of SonarQube task &#39;AWlX97LSgWqXn-z33SO5&#39; on server &#39;sonar&#39;
SonarQube task &#39;AWlX97LSgWqXn-z33SO5&#39; status is &#39;IN_PROGRESS&#39;
Cancelling nested steps due to timeout
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因：SonarQube没有配置webhook回调，导致请求超时，按照步骤4配置webhook即可解决&lt;/p&gt;

&lt;h3 id=&#34;问题四-sonar找不到dependency-check-xml&#34;&gt;问题四：sonar找不到Dependency-Check XML&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-Java&#34;&gt;[INFO] Sensor Dependency-Check [dependencycheck]
[INFO] Process Dependency-Check report
[INFO] Dependency-Check XML report does not exists. Please check property sonar.dependencyCheck.reportPath:/data/jenkinsHome/workspace/xxx/${WORKSPACE}/dependency-check-report.xml
[INFO] Analysis skipped/aborted due to missing report file
[INFO] Dependency-Check HTML report does not exists. Please check property sonar.dependencyCheck.htmlReportPath:/data/jenkinsHome/workspace/xxx/${WORKSPACE}/dependency-check-report.html
[INFO] HTML-Dependency-Check report does not exist.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因：SonarQube配置Dependency-Check插件有误，按照上文配置即可&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;sonar与jenkins集成的方式还有很多，不止pipeline+maven这一种，还有配置在jenkins构建任务中、直接使用sonar脚本等方法。采用这样方法，一方面是配置相对简单，不需要每个构建任务都进行配置，只需要将jenkinsfile中拷入相应代码并修改几个参数即可。同时可以在静态代码扫描期间完整maven打包，减少持续集成的时间。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>解决kubernetes中ingress-nginx配置问题</title>
      <link>https://guoxudong.io/en/post/k8s-ingress-config/</link>
      <pubDate>Wed, 06 Mar 2019 14:42:05 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/k8s-ingress-config/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;随着公司容器化的深入，越来越多的服务陆续迁移到kubernetes集群中，有些问题在测试环境并未凸显，但是在生产环境中这些问题就显得格外的扎眼。这里就对实践中kubernetes集群中的7层负载均衡器ingress遇到的问题进行总结。&lt;/p&gt;

&lt;h2 id=&#34;http-s-负载均衡器-ingress&#34;&gt;HTTP(S)负载均衡器-ingress&lt;/h2&gt;

&lt;p&gt;Ingress是kubernetes API的标准资源类型之一，其本质就是一组基于DNS名称(host)或URL路径把请求转发至指定的Service资源的规则，&lt;strong&gt;用于将集群外的请求流量转发至集群内部完成服务发布&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;Ingress控制器(Ingress Controller)可以由任何具有反向代理(HTTP/HTTPS)功能的服务程序实现，如Nginx、Envoy、HAProxy、Vulcand和Traefik等。Ingress控制器本身也作为Pod对象与被代理的运行为Pod资源的应用运行于同一网络中。我们在这里选择了NGINX Ingress Controller，由于对NGINX的配置较为熟悉，同时我们使用的kubernetes是阿里云的容器服务，构建集群的时候，容器服务会自带NGINX Ingress Controller。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx2.sinaimg.cn/large/ad5fbf65ly1g0t3yj7wecj20w50doab9.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;根据实际情况ingress调优&#34;&gt;根据实际情况Ingress调优&lt;/h2&gt;

&lt;h3 id=&#34;1-解决400-request-header-or-cookie-too-large问题&#34;&gt;1. 解决400 Request Header Or Cookie Too Large问题&lt;/h3&gt;

&lt;p&gt;image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;h4 id=&#34;现象&#34;&gt;现象&lt;/h4&gt;

&lt;p&gt;微信小程序需要调用后端接口，需要在header中传一段很长的token参数，直接使用浏览器访问该端口可以访问通，但是在加上token访问之后，会报“400 Request Header Or Cookie Too Large”&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;html&amp;gt;
    &amp;lt;head&amp;gt;
        &amp;lt;title&amp;gt;400 Request Header Or Cookie Too Large&amp;lt;/title&amp;gt;
    &amp;lt;/head&amp;gt;
    &amp;lt;body&amp;gt;
        &amp;lt;center&amp;gt;
            &amp;lt;h1&amp;gt;400 Bad Request&amp;lt;/h1&amp;gt;
        &amp;lt;/center&amp;gt;
        &amp;lt;center&amp;gt;Request Header Or Cookie Too Large&amp;lt;/center&amp;gt;
        &amp;lt;hr&amp;gt;
        &amp;lt;center&amp;gt;nginx/1.15.6&amp;lt;/center&amp;gt;
    &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;问题定位&#34;&gt;问题定位&lt;/h4&gt;

&lt;p&gt;直接修改Service使用nodeport的形式访问，则没有报错，初步定位需要在ingress中nginx配置客户端的请求头，进入Ingress Controller的Pod查询配置，果然是请求头空间不足。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat nginx.conf | grep client_header_buffer_size
    client_header_buffer_size       1k;
$ cat nginx.conf | grep large_client_header_buffers
    large_client_header_buffers     4 8k;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;解决方法&#34;&gt;解决方法&lt;/h4&gt;

&lt;p&gt;在ingress中添加注释&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;nginx.ingress.kubernetes.io/server-snippet: client_header_buffer_size 2046k;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false-1&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Server snippet&lt;/strong&gt;&lt;br&gt;Using the annotation &lt;code&gt;nginx.ingress.kubernetes.io/server-snippet&lt;/code&gt; it is possible to add custom configuration in the server configuration block.
&lt;br&gt;该注释是将自定义配置加入nginx的server配置中&lt;/p&gt;

&lt;p&gt;image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false-2&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;h3 id=&#34;2-解决请求超时问题&#34;&gt;2. 解决请求超时问题&lt;/h3&gt;

&lt;h4 id=&#34;现象-1&#34;&gt;现象&lt;/h4&gt;

&lt;p&gt;有一个数据导出功能，需要将大量数据进行处理，然后以Excel格式返回，在导出一个大约3W条数据的时候，出现访问超时情况。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws2.sinaimg.cn/mw690/ad5fbf65ly1g0ubdwwzo5j21b30bjaat.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;解决方法-1&#34;&gt;解决方法&lt;/h4&gt;

&lt;p&gt;调整proxy_read_timeout，连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理
在ingress中添加注释&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;nginx.ingress.kubernetes.io/proxy-read-timeout: 600
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;这里需要注意的事该注释的value需要时number类型，不能加s，否则将不生效&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;3-增加白名单&#34;&gt;3. 增加白名单&lt;/h3&gt;

&lt;h4 id=&#34;现象-2&#34;&gt;现象&lt;/h4&gt;

&lt;p&gt;在实际的使用中，会有一部分应用需要设置只可以在办公场地的网络使用，之前使用阿里云 SLB 的时候可以针对端口进行访问控制，但是现在走 ingress ，都是从80 or 443端口进，所以需要在 ingress 设置&lt;/p&gt;

&lt;h4 id=&#34;解决方法-2&#34;&gt;解决方法&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Whitelist source range&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You can specify allowed client IP source ranges through the nginx.ingress.kubernetes.io/whitelist-source-range annotation. The value is a comma separated list of CIDRs, e.g. 10.0.0.0/24,172.10.0.1.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在 ingress 里配置 &lt;code&gt;nginx.ingress.kubernetes.io/whitelist-source-range&lt;/code&gt; ，如有多个ip段，用逗号分隔即可&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;nginx.ingress.kubernetes.io/whitelist-source-range: 10.0.0.0/24
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想全局适用，可以在阿里云 SLB 里操作，也可以将该配置加入到 &lt;code&gt;NGINX ConfigMap&lt;/code&gt; 中。&lt;/p&gt;

&lt;p&gt;image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false-3&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;p&gt;根据工作中遇到的实际问题，持续更新中&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;使用NGINX ingress controller的好处就是对于nginx配置相对比较熟悉，性能也不差。相关nginx配置的对应的ingress可以在 &lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/&lt;/a&gt; 上查到。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pod质量服务类别(QoS)</title>
      <link>https://guoxudong.io/en/post/k8s-qos/</link>
      <pubDate>Mon, 04 Mar 2019 19:18:13 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/k8s-qos/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;根据Pod对象的requests和limits属性，kubernetes将Pod对象归类到BestEffort、Burstable和Guaranteed三个服务质量（Quality of Service，QoS）类别。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Guaranteed

&lt;ul&gt;
&lt;li&gt;cpu:requests=limits&lt;/li&gt;
&lt;li&gt;memory:requests=limits&lt;/li&gt;
&lt;li&gt;这类Pod具有最高优先级&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Burstable

&lt;ul&gt;
&lt;li&gt;至少一个容器设置了cpu或内存资源的requests&lt;/li&gt;
&lt;li&gt;这类Pod具有中等优先级&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;BestEffort

&lt;ul&gt;
&lt;li&gt;未有任何一个容器设置requests或limits属性&lt;/li&gt;
&lt;li&gt;这类Pod具有最低优先级&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/ad5fbf65ly1g0rv2ipzqkj20hx0edmx8.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;同级别优先级的Pod资源在OOM时，与自身的requests属性相比，其内存占用比例最大的Pod对象将被首先杀死。如上图同属Burstable类别的Pod A将先于Pod B被杀死，虽然其内存用量小，但与自身的requests值相比，它的占用比例95%要大于Pod B的80%。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>阿里云日志服务采集k8s日志并实现livetail功能</title>
      <link>https://guoxudong.io/en/post/dashboard-k8s/</link>
      <pubDate>Thu, 14 Feb 2019 14:07:06 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/dashboard-k8s/</guid>
      <description>

&lt;h1 id=&#34;前言&#34;&gt;前言&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;目前的项目日志都是通过Logtail直接采集，投递到OSS持久化，同时可以通过阿里云日志服务、devops自建平台进行查看（虽然大部分人是直接登录ECS查看=。=），
在开始进行容器化之后，同样遇到日志的问题，目前的解决方案是阿里云日志服务持久化和展现格式化后的日志、使用rancher查看实时日志，
但是之前由于rancher平台出现一些问题，导致不能及时查看日志的情况，在这个背景下对阿里云日志服务采集k8s日志和livetail进行搭建并调研此方案是否可行。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;简介-转自阿里云官方文档&#34;&gt;简介（转自阿里云官方文档）&lt;/h1&gt;

&lt;p&gt;日志服务（Log Service，简称 LOG）是针对日志类数据的一站式服务，在阿里巴巴集团经历大量大数据场景锤炼而成。您无需开发就能快捷完成日志数据采集、消费、投递以及查询分析等功能，提升运维、运营效率，建立 DT 时代海量日志处理能力。&lt;/p&gt;

&lt;h1 id=&#34;kubernetes日志采集组件安装&#34;&gt;kubernetes日志采集组件安装&lt;/h1&gt;

&lt;h2 id=&#34;安装logtail&#34;&gt;安装Logtail&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;进入阿里云容器服务找到集群id
&lt;img src=&#34;https://guoxudong.io/images/source/log_ser.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;通过ssh登录master节点，或者任意安装了kubectl并配置了该集群kubeconfig的服务器&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;执行命令，将${your_k8s_cluster_id}替换为集群id&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget http://logtail-release-cn-hangzhou.oss-cn-hangzhou.aliyuncs.com/kubernetes/alicloud-log-k8s-install.sh -O alicloud-log-k8s-install.sh; chmod 744 ./alicloud-log-k8s-install.sh; sh ./alicloud-log-k8s-install.sh ${your_k8s_cluster_id}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Project k8s-log-${your_k8s_cluster_id}下会自动创建名为config-operation-log的Logstore，用于存储alibaba-log-controller的运行日志。请勿删除此Logstore，否则无法为alibaba-log-controller排查问题。&lt;/li&gt;
&lt;li&gt;若您需要将日志采集到已有的Project，请执行安装命令sh ./alicloud-log-k8s-install.sh${your_k8s_cluster_id} ${your_project_name} ，并确保日志服务Project和您的Kubernetes集群在同一地域。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;该条命令其实就是执行了一个shell脚本，使用helm安装了采集kubernetes集群日志的组件&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-vim&#34;&gt;#!/bin/bash

if [ $# -eq 0 ] ; then
    echo &amp;quot;[Invalid Param], use sudo ./install-k8s-log.sh {your-k8s-cluster-id}&amp;quot;
    exit 1
fi
    
clusterName=$(echo $1 | tr &#39;[A-Z]&#39; &#39;[a-z]&#39;)
curl --connect-timeout 5  http://100.100.100.200/latest/meta-data/region-id
    
if [ $? != 0 ]; then
    echo &amp;quot;[FAIL] ECS meta server connect fail, only support alibaba cloud k8s service&amp;quot;
    exit 1
fi
    
regionId=`curl http://100.100.100.200/latest/meta-data/region-id`
aliuid=`curl http://100.100.100.200/latest/meta-data/owner-account-id`
    
helmPackageUrl=&amp;quot;http://logtail-release-$regionId.oss-$regionId.aliyuncs.com/kubernetes/alibaba-cloud-log.tgz&amp;quot;
wget $helmPackageUrl -O alibaba-cloud-log.tgz
if [ $? != 0 ]; then
    echo &amp;quot;[FAIL] download alibaba-cloud-log.tgz from $helmPackageUrl failed&amp;quot;
    exit 1
fi
    
project=&amp;quot;k8s-log-&amp;quot;$clusterName
if [ $# -ge 2 ]; then
    project=$2
fi
    
echo [INFO] your k8s is using project : $project
    
helm install alibaba-cloud-log.tgz --name alibaba-log-controller \
    --set ProjectName=$project \
    --set RegionId=$regionId \
    --set InstallParam=$regionId \
    --set MachineGroupId=&amp;quot;k8s-group-&amp;quot;$clusterName \
    --set Endpoint=$regionId&amp;quot;-intranet.log.aliyuncs.com&amp;quot; \
    --set AlibabaCloudUserId=&amp;quot;:&amp;quot;$aliuid \
    --set LogtailImage.Repository=&amp;quot;registry.$regionId.aliyuncs.com/log-service/logtail&amp;quot; \
    --set ControllerImage.Repository=&amp;quot;registry.$regionId.aliyuncs.com/log-service/alibabacloud-log-controller&amp;quot;
    
installRst=$?
    
if [ $installRst -eq 0 ]; then
    echo &amp;quot;[SUCCESS] install helm package : alibaba-log-controller success.&amp;quot;
    exit 0
else
    echo &amp;quot;[FAIL] install helm package failed, errno &amp;quot; $installRst
    exit 0
fi
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;命令执行后，会在kubernetes集群中的每个节点运行一个日志采集的pod：logatail-ds，该pod位于kube-system&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://guoxudong.io/images/source/log_detail.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装完成后，可使用以下命令来查看pod状态，若状态全部成功后，则表示安装完成&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm status alibaba-log-controller
LAST DEPLOYED: Thu Nov 22 15:09:35 2018
NAMESPACE: default
STATUS: DEPLOYED
    
RESOURCES:
==&amp;gt; v1/ServiceAccount
NAME                    SECRETS  AGE
alibaba-log-controller  1        6d
    
==&amp;gt; v1beta1/CustomResourceDefinition
NAME                                   AGE
aliyunlogconfigs.log.alibabacloud.com  6d
    
==&amp;gt; v1beta1/ClusterRole
alibaba-log-controller  6d
    
==&amp;gt; v1beta1/ClusterRoleBinding
NAME                    AGE
alibaba-log-controller  6d
    
==&amp;gt; v1beta1/DaemonSet
NAME        DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE SELECTOR  AGE
logtail-ds  16       16       16     16          16         &amp;lt;none&amp;gt;         6d
    
==&amp;gt; v1beta1/Deployment
NAME                    DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
alibaba-log-controller  1        1        1           1          6d
    
==&amp;gt; v1/Pod(related)
NAME                                     READY  STATUS   RESTARTS  AGE
logtail-ds-2fqs4                         1/1    Running  0         6d
logtail-ds-4bz7w                         1/1    Running  1         6d
logtail-ds-6vg88                         1/1    Running  0         6d
logtail-ds-7tp6v                         1/1    Running  0         6d
logtail-ds-9575c                         1/1    Running  0         6d
logtail-ds-bgq84                         1/1    Running  0         6d
logtail-ds-kdlhr                         1/1    Running  0         6d
logtail-ds-lknxw                         1/1    Running  0         6d
logtail-ds-pdxfk                         1/1    Running  0         6d
logtail-ds-pf4dz                         1/1    Running  0         6d
logtail-ds-rzsnw                         1/1    Running  0         6d
logtail-ds-sqhbv                         1/1    Running  0         6d
logtail-ds-vvtwn                         1/1    Running  0         6d
logtail-ds-wwmhg                         1/1    Running  0         6d
logtail-ds-xbp4j                         1/1    Running  0         6d
logtail-ds-zpld9                         1/1    Running  0         6d
alibaba-log-controller-85f8fbb498-nzhc8  1/1    Running  0         6d
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;配置日志组件展示&#34;&gt;配置日志组件展示&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在集群内安装好日志组件后，登录阿里云日志服务控制台，就会发现有一个新的project，名称为k8s-log-{集群id}
&lt;img src=&#34;https://guoxudong.io/images/source/log_src_de.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建Logstore
&lt;img src=&#34;https://guoxudong.io/images/source/log-1.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据导入
&lt;img src=&#34;https://guoxudong.io/images/source/log-2.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;选择数据类型中选择docker标准输出
&lt;img src=&#34;https://guoxudong.io/images/source/log-3.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据源配置，这里可以使用默认的
&lt;img src=&#34;https://guoxudong.io/images/source/log-4.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;选择数据源
&lt;img src=&#34;https://guoxudong.io/images/source/log-5.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置好之后等待1-2分钟，日志就会进来了
&lt;img src=&#34;https://guoxudong.io/images/source/log-6.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;为了快速查询和过滤，需要配置索引
&lt;img src=&#34;https://guoxudong.io/images/source/log-7.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;添加容器名称、命名空间、pod名称作为索引（后续使用livetail需要）
&lt;img src=&#34;https://guoxudong.io/images/source/log-8.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;这样就完成了一个k8s集群日志采集和展示的基本流程了&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;livetail功能使用&#34;&gt;livetail功能使用&lt;/h1&gt;

&lt;h2 id=&#34;背景介绍&#34;&gt;背景介绍&lt;/h2&gt;

&lt;p&gt;在线上运维的场景中，往往需要对日志队列中进入的数据进行实时监控，从最新的日志数据中提取出关键的信息进而快速地分析出异常原因。在传统的运维方式中，如果需要对日志文件进行实时监控，需要到服务器上对日志文件执行命令tail -f，如果实时监控的日志信息不够直观，可以加上grep或者grep -v进行关键词过滤。日志服务在控制台提供了日志数据实时监控的交互功能LiveTail，针对线上日志进行实时监控分析，减轻运维压力。&lt;/p&gt;

&lt;h2 id=&#34;使用方法&#34;&gt;使用方法&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;这里选择来源类型为kubernetes，命名空间、pod名称、容器名称为上一步新建的3个索引的内容，过滤关键字的功劳与tail命令后加的grep命令是一样的，用于关键词过滤
&lt;img src=&#34;https://guoxudong.io/images/source/log-9.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;点击开启livetail，这时就有实时日志展示出来了
&lt;img src=&#34;https://guoxudong.io/images/source/log-10.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;以上就是阿里云livetail日志服务功能&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>kubernetes中pod同步时区问题</title>
      <link>https://guoxudong.io/en/post/pod-timezone/</link>
      <pubDate>Wed, 30 Jan 2019 20:18:13 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/pod-timezone/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;新版监控大屏于18年最后一天正式上线，之后陆续进行了几次优化和修改，最近发现一个比较大的bug，就是监控显示的时间轴不对，显示的就是和目前的时间相差8小时，这就引出了docker中的时区问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;问题的原因&#34;&gt;问题的原因&lt;/h1&gt;

&lt;p&gt;默认的情况，在K8S里启动一个容器，该容器的设置的时区是UTC0，但是对用户而言，主机环境并不在UTC0。我们在UTC8。如果不把容器的时区和主机主机设置为一致，则在查找日志等时候将非常不方便，也容易造成误解。但是K8S以及Docker容器没有一个简便的设置/开关在系统层面做配置。都需要我们从单个容器入手做设置，具体有两个方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;直接修改镜像的时间设置，好处是应用部署时无需做特殊设置，但是需要手动构建Docker镜像。&lt;/li&gt;
&lt;li&gt;部署应用时，单独读取主机的“/etc/localtime”文件，即创建pod时同步时区，无需修改镜像，但是每个应用都要单独设置。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;问题的解决&#34;&gt;问题的解决&lt;/h1&gt;

&lt;p&gt;这里我们选择第二种方法，即修改部署应用的yaml文件，创建pod时同步时区&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
    name: myweb
spec:
    replicas: 2
    template:
        metadata:
        labels:
            app: myweb
        spec:
        containers:
        - name: myweb
            image: nginx:apline
            ports:
            - containerPort: 80
        #挂载到pod中
            volumeMounts:
            - name: host-time
            mountPath: /etc/localtime    
        #需要被挂载的宿主机的时区文件
        volumes:
        - name: host-time
            hostPath:
            path: /etc/localtime
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;效果对比&#34;&gt;效果对比&lt;/h1&gt;

&lt;h2 id=&#34;修改时区前&#34;&gt;修改时区前&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://guoxudong.io/images/source/time-1.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;修改时区后&#34;&gt;修改时区后&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://guoxudong.io/images/source/time-2.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>为ingress配置SSL证书，实现HTTPS访问</title>
      <link>https://guoxudong.io/en/post/https-ingress/</link>
      <pubDate>Sat, 29 Dec 2018 21:28:13 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/https-ingress/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;devops平台率先在公司内使用kubernetes集群提供后端服务，但是由于之前一直处于探索阶段，所以使用的事http的方式提供后端服务，但是在开发统一入口后，出现了访问HTTPS页面的跨域问题，由此引出了后端服务配置SSL证书的问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;使用rancher配置ssl证书&#34;&gt;使用rancher配置SSL证书&lt;/h1&gt;

&lt;h2 id=&#34;下载ssl证书文件&#34;&gt;下载SSL证书文件&lt;/h2&gt;

&lt;p&gt;首先需要获得SSL证书文件，可以直接在阿里云SSL证书管理控制台下载&lt;/p&gt;

&lt;p&gt;选中需要下载证书，选择下载nginx证书
&lt;img src=&#34;https://guoxudong.io/images/source/zhengshu.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;将证书上传项目&#34;&gt;将证书上传项目&lt;/h2&gt;

&lt;p&gt;打开rancher，选择要使用证书的项目，点击资源中的证书&lt;/p&gt;

&lt;h2 id=&#34;将证书上传项目-1&#34;&gt;将证书上传项目&lt;/h2&gt;

&lt;p&gt;打开rancher，选择要使用证书的项目，点击资源中的证书
&lt;img src=&#34;https://guoxudong.io/images/source/https-1.png&#34; alt=&#34;image&#34; /&gt;
添加证书，点击从文件上传
&lt;img src=&#34;https://guoxudong.io/images/source/https-2.png&#34; alt=&#34;image&#34; /&gt;
上传证书文件中的秘钥和证书，点击保存即可&lt;/p&gt;

&lt;h1 id=&#34;使用yaml上传证书&#34;&gt;使用yaml上传证书&lt;/h1&gt;

&lt;p&gt;这个证书的原理其实是在相应的命名空间创建了一个包含证书信息的secrets&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
data:
    tls.crt: {私钥}
    tls.key: {证书}
kind: Secret
metadata:
    name: keking-cn
    namespace: devops-plat
type: kubernetes.io/tls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在kubernetes上运行该yaml即可&lt;/p&gt;

&lt;h1 id=&#34;rancher中证书绑定&#34;&gt;rancher中证书绑定&lt;/h1&gt;

&lt;p&gt;选中需要绑定证书的ingress，点击编辑，选中证书，保存即可（由于ingress-controller中没有绑定默认证书，所以这里不能选中默认）
&lt;img src=&#34;https://guoxudong.io/images/source/https-3.png&#34; alt=&#34;image&#34; /&gt;
保存完毕，证书即可生效&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>阿里云部署rancher2.1采坑记</title>
      <link>https://guoxudong.io/en/post/install-rancher/</link>
      <pubDate>Thu, 29 Nov 2018 18:28:13 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/install-rancher/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;近期由于公司需要将部署在ucloud上的rancher迁移到阿里云上，所以将部署到阿里云的图中遇到的问题和踩到的坑在这里进行记录。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;无法删除namespace&#34;&gt;无法删除namespace&lt;/h1&gt;

&lt;p&gt;在安装新环境的rancher之前，需要将kubernetes集群中cattle-system ns下面的cluster-agent和node-agent干掉，这里我选择直接删除cattle-system这个命名空间&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl delete ns cattle-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然而问题来了，在删除命名空间之后，这个命名空间并没有立刻被删除，而是一直处于Terminating状态，这里我专门写了一篇文章解决这个问题，这里就不再赘述&lt;/p&gt;

&lt;h1 id=&#34;阿里云证书配置&#34;&gt;阿里云证书配置&lt;/h1&gt;

&lt;p&gt;由于之前使用的ucloud的机器进行测试，使用默认自签名证书并没有使用SSL证书，所以在配置证书这里遇到的许多的问题&lt;/p&gt;

&lt;p&gt;首先根据官方文档使用权威CA机构颁发的证书，这里使用的是本公司自己的证书&lt;/p&gt;

&lt;p&gt;获取证书方法：
&lt;img src=&#34;https://guoxudong.io/images/source/jinrussl.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;点击下载证书，选择nginx证书下载
&lt;img src=&#34;https://guoxudong.io/images/source/zhengshu.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;之后将下载的证书上传到rancher所在服务器，并配置好数据卷挂载&lt;/p&gt;

&lt;p&gt;将下面代码的挂载地址指向证书文件，运行代码&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d --restart=unless-stopped \
-p 80:80 -p 443:443 \
-v /root/var/log/auditlog:/var/log/auditlog \
-e AUDIT_LEVEL=3 \
-v /etc/your_certificate_directory/fullchain.pem:/etc/rancher/ssl/cert.pem \
-v /etc/your_certificate_directory/privkey.pem:/etc/rancher/ssl/key.pem \
rancher/rancher:latest --no-cacerts
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后会自动冲dockerhub上拉取最新的rancher进行进行安装，之后使用命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker ps
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看容器是否在运行，如果运行正常，则后端的配置就完成了&lt;/p&gt;

&lt;p&gt;划重点：这是是在后端配置了证书，所以在阿里云的配置上要使用四层TCP监听&lt;/p&gt;

&lt;p&gt;这个地方可是坑了我许久，我一直在前端配置https七层监听，导致一直无法正常访问，一度已经到了怀疑人生的地步=。=&lt;/p&gt;

&lt;p&gt;之后就是简单的阿里云SLB配置四层TCP监听，这里也就不再赘述了&lt;/p&gt;

&lt;h1 id=&#34;k8s集群导入rancher&#34;&gt;k8s集群导入rancher&lt;/h1&gt;

&lt;p&gt;前后端都准备就绪，现在就可以访问rancher了，访问rancher根据页面提示进行基本配置，登录后选择添加集群&lt;/p&gt;

&lt;p&gt;选择导入现有集群
&lt;img src=&#34;https://guoxudong.io/images/source/add.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;为集群创建一个rancher中的名称，然后根据提示将命令拷贝到k8s集群所在宿主机执行即可，注意：这里由于配置了证书，所以选择有证书，不绕过证书的那个命令执行，之后就可看到集群数据导入中
&lt;img src=&#34;https://guoxudong.io/images/source/wating.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;等待几秒即可开心的使用rancher了！&lt;/p&gt;

&lt;h1 id=&#34;关于rancher部署后访问集群api超时问题&#34;&gt;关于rancher部署后访问集群api超时问题&lt;/h1&gt;

&lt;p&gt;经过排查，原因是阿里云在容器服务对外连接处设置了TLS双向认证，导致rancher的外网ip经常性的被拦截，导致超时&lt;/p&gt;

&lt;p&gt;解决办法：&lt;/p&gt;

&lt;p&gt;对k8s集群中rancher的cattle-cluster-agent传递内网参数，将其配置为内网连接，就可以正常访问了&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl -n cattle-system patch deployments cattle-cluster-agent --patch &#39;{
    &amp;quot;spec&amp;quot;: {
        &amp;quot;template&amp;quot;: {
                &amp;quot;spec&amp;quot;: {
                    &amp;quot;hostAliases&amp;quot;: [{
                                    &amp;quot;hostnames&amp;quot;:[&amp;quot;rancher.keking.cn&amp;quot;],  #rancher的域名
                                    &amp;quot;ip&amp;quot;: &amp;quot;10.0.0.219&amp;quot;  #rancher部署地址
                                    }]
                        }
                    }
            }
}&#39;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes删除一直处于Terminating状态的namespace</title>
      <link>https://guoxudong.io/en/post/k8s-d-n/</link>
      <pubDate>Fri, 16 Nov 2018 18:18:13 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/k8s-d-n/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;近期由于公司需要将部署在ucloud上的rancher迁移到阿里云上，所以需要将原有Rancher依赖的namespace（cattle-system）删除，但在删除中出现了删除的namespace一直处于Terminating状态的情况&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://guoxudong.io/images/source/d-n-1.png&#34; alt=&#34;imgage&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;解决方案&#34;&gt;解决方案&lt;/h1&gt;

&lt;p&gt;运行命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl edit namespaces cattle-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到namespaces的yaml配置：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://guoxudong.io/images/source/d-n-2.png&#34; alt=&#34;imgage&#34; /&gt;&lt;/p&gt;

&lt;p&gt;将finalizer的value删除，这里将其设置为[]&lt;/p&gt;

&lt;p&gt;保存即可看到该namespace已被删除&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://guoxudong.io/images/source/d-n-3.png&#34; alt=&#34;imgage&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>kubernetes集群概述</title>
      <link>https://guoxudong.io/en/post/k8s-topo/</link>
      <pubDate>Wed, 03 Oct 2018 12:18:13 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/k8s-topo/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;随着2017年AWS，Azure和阿里云相继在其原有容器服务上新增了对kubernetes的支持，而Docker官网也在同年10月宣布同时支持Swarm好kubernetes容器编排系统。kubernetes俨然已成为容器编排领域事实上的标准，而2018年更是各大公司相继将服务迁移到kubernetes上，而kubernetes则以惊人更新速度，保持着每个季度发布一个大版本的速度高速发展着。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;kubernetes特征&#34;&gt;kubernetes特征&lt;/h1&gt;

&lt;p&gt;kubernetes是一种在一组主机上运行和协同容器化应用程序的系统，旨在提供可预测性、可拓展性与高可用性的方法来完全管理容器化应用和服务的生命周期平台。用户可以定义应用程序的运行方式，以及与其他应用程序或外部世界交互的途径，并能实现服务的扩容和缩容，执行平滑滚动更新，以及在不同版本的应用程序之间调度流量以测试功能或回滚有问题的部署。kubernetes提供了接口和可组合帆软平台原语，使得用户能够以高度的灵活性和可靠性定义及管理应用程序。&lt;/p&gt;

&lt;h1 id=&#34;kubernetes组件及网络通信&#34;&gt;kubernetes组件及网络通信&lt;/h1&gt;

&lt;p&gt;kubernetes集群的客户端可以分为两类：API Server客户端和应用程序（运行为Pod中的容器）客户端。
&lt;img src=&#34;https://guoxudong.io/images/source/kubernetes-topo.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一类客户端通常包含用户和Pod对象两种，它们通过API Server访问kubernetes集群完成管理任务，例如，管理集群上的各种资源对象。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;第二类客户端一般也包含人类用户和Pod对象两种，它们的访问目标是Pod上运行于容器中的应用程序提供的各种具体的服务，如redis或nginx等，不过，这些访问请求通常要经由Service或Ingress资源对象进行。另外，第二类客户端的访问目标对象的操作要经由第一类客户端创建和配置完成后才进行。&lt;/p&gt;

&lt;p&gt;访问API Server时，人类用户一般借助于命令行工具kubectl或图形UI（例如kubernetes dashboard）进行，也通过编程接口进行访问，包括REST API。访问Pod中的应用时，其访问方式要取决于Pod中的应用程序，例如，对于运行Nginx容器的Pod来说，其最常用工具就是浏览器。&lt;/p&gt;

&lt;p&gt;管理员（开发人员或运维人员）使用kubernetes集群的常见操作包括通过控制器创建Pod，在Pod的基础上创建Service供第二类客户端访问，更新Pod中的应用版本（更新和回滚）以及对应用规模进行扩容或缩容等，另外还有集群附件管理、存储卷管理、网络及网络策略管理、资源管理和安全管理等。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>精简docker镜像</title>
      <link>https://guoxudong.io/en/post/image-size/</link>
      <pubDate>Thu, 27 Sep 2018 20:28:13 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/image-size/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;精简Docker镜像的好处很多，不仅可以节省存储空间和带宽，还能减少安全隐患。优化镜像大小的手段多种多样，因服务所使用的基础开发语言不同而有差异。本文将介绍精简Docker镜像的几种通用方法。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;精简docker镜像大小的必要性&#34;&gt;精简Docker镜像大小的必要性&lt;/h1&gt;

&lt;p&gt;Docker镜像由很多镜像层（Layers）组成（最多127层），镜像层依赖于一系列的底层技术，比如文件系统(filesystems)、写时复制(copy-on-write)、联合挂载(union mounts)等技术，你可以查看Docker社区文档以了解更多有关Docker存储驱动的内容，这里就不再赘述技术细节。总的来说，Dockerfile中的每条指令都会创建一个镜像层，继而会增加整体镜像的尺寸。&lt;/p&gt;

&lt;p&gt;下面是精简Docker镜像尺寸的好处：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;减少构建时间&lt;/li&gt;
&lt;li&gt;减少磁盘使用量&lt;/li&gt;
&lt;li&gt;减少下载时间&lt;/li&gt;
&lt;li&gt;因为包含文件少，攻击面减小，提高了安全性&lt;/li&gt;
&lt;li&gt;提高部署速度&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;五点建议减小Docker镜像尺寸&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;一-优化基础镜像&#34;&gt;一、优化基础镜像&lt;/h1&gt;

&lt;p&gt;优化基础镜像的方法就是选用合适的更小的基础镜像，常用的 Linux 系统镜像一般有 Ubuntu、CentOs、Alpine，其中Alpine更推荐使用。大小对比如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;guoxudong@ubuntu ~/s&amp;gt; docker images
REPOSITORY         TAG             IMAGE ID            CREATED             SIZE
ubuntu             latest        74f8760a2a8b        8 days ago          82.4MB
alpine             latest        11cd0b38bc3c        2 weeks ago         4.41MB
centos               7           49f7960eb7e4        7 weeks ago         200MB
debian             latest        3bbb526d2608        8 days ago          101MB
guoxudong@ubuntu ~/s&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alpine是一个高度精简又包含了基本工具的轻量级Linux发行版，基础镜像只有4.41M，各开发语言和框架都有基于Alpine制作的基础镜像，强烈推荐使用它。Alpine镜像各个语言和框架支持情况，可以参考《优化Docker镜像、加速应用部署》。
查看上面的镜像尺寸对比结果，你会发现最小的镜像也有4.41M，那么有办法构建更小的镜像吗？答案是肯定的，例如 gcr.io/google_containers/pause-amd64:3.1 镜像仅有742KB。为什么这个镜像能这么小？在为大家解密之前，再推荐两个基础镜像：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;scratch镜像&lt;/p&gt;

&lt;p&gt;scratch是一个空镜像，只能用于构建其他镜像，比如你要运行一个包含所有依赖的二进制文件，如Golang程序，可以直接使用scratch作为基础镜像。现在给大家展示一下上文提到的Google pause镜像Dockerfile：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-docker&#34;&gt;FROM scratch
ARG ARCH
ADD bin/pause-${ARCH} /pause
ENTRYPOINT [&amp;quot;/pause&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Google pause镜像使用了scratch作为基础镜像，这个镜像本身是不占空间的，使用它构建的镜像大小几乎和二进制文件本身一样大，所以镜像非常小。当然在我们的Golang程序中也会使用。对于一些Golang/C程序，可能会依赖一些动态库，你可以使用自动提取动态库工具，比如ldd、linuxdeployqt等提取所有动态库，然后将二进制文件和依赖动态库一起打包到镜像中。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;busybox镜像&lt;/p&gt;

&lt;p&gt;scratch是个空镜像，如果希望镜像里可以包含一些常用的Linux工具，busybox镜像是个不错选择，镜像本身只有1.16M，非常便于构建小镜像。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;二-串联-dockerfile-指令&#34;&gt;二、串联 Dockerfile 指令&lt;/h1&gt;

&lt;p&gt;大家在定义Dockerfile时，如果太多的使用RUN指令，经常会导致镜像有特别多的层，镜像很臃肿，而且甚至会碰到超出最大层数（127层）限制的问题，遵循 Dockerfile 最佳实践，我们应该把多个命令串联合并为一个 RUN（通过运算符&amp;amp;&amp;amp;和/ 来实现），每一个 RUN 要精心设计，确保安装构建最后进行清理，这样才可以降低镜像体积，以及最大化的利用构建缓存。&lt;/p&gt;

&lt;p&gt;下面是一个优化前Dockerfile：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-docker&#34;&gt;FROM ubuntu
ENV VER     3.0.0 
ENV TARBALL http://download.redis.io/releases/redis-$VER.tar.gz 
# ==&amp;gt; Install curl and helper tools...
RUN apt-get update 
RUN apt-get install -y  curl make gcc 
# ==&amp;gt; Download, compile, and install...
RUN curl -L $TARBALL | tar zxv 
WORKDIR  redis-$VER 
RUN make 
RUN make install 
#...
# ==&amp;gt; Clean up...
WORKDIR / 
RUN apt-get remove -y --auto-remove curl make gcc 
RUN apt-get clean 
RUN rm -rf /var/lib/apt/lists/*  /redis-$VER 
#...
CMD [&amp;quot;redis-server&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;构建镜像，名称叫 &lt;code&gt;test/test:0.1&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;我们对Dockerfile做优化，优化后Dockerfile：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-docker&#34;&gt;FROM ubuntu
ENV VER     3.0.0 
ENV TARBALL http://download.redis.io/releases/redis-$VER.tar.gz
RUN echo &amp;quot;==&amp;gt; Install curl and helper tools...&amp;quot;  &amp;amp;&amp;amp; \ 
apt-get update                      &amp;amp;&amp;amp; \
apt-get install -y  curl make gcc   &amp;amp;&amp;amp; \
echo &amp;quot;==&amp;gt; Download, compile, and install...&amp;quot;  &amp;amp;&amp;amp; \
curl -L $TARBALL | tar zxv  &amp;amp;&amp;amp; \
cd redis-$VER               &amp;amp;&amp;amp; \
make                        &amp;amp;&amp;amp; \
make install                &amp;amp;&amp;amp; \
echo &amp;quot;==&amp;gt; Clean up...&amp;quot;  &amp;amp;&amp;amp; \
apt-get remove -y --auto-remove curl make gcc  &amp;amp;&amp;amp; \
apt-get clean                                  &amp;amp;&amp;amp; \
rm -rf /var/lib/apt/lists/*  /redis-$VER
#...
CMD [&amp;quot;redis-server&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;构建镜像，名称叫 &lt;code&gt;test/test:0.2&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;对比两个镜像大小：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master:/tmp/iops# docker images
REPOSITORY       TAG           IMAGE ID            CREATED             SIZE
test/test        0.2         58468c0222ed        2 minutes ago       98.1MB
test/test        0.1         e496cf7243f2        6 minutes ago       307MB
root@k8s-master:/tmp/iops#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，将多条RUN命令串联起来构建的镜像大小是每条命令分别RUN的三分之一。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;提示：&lt;/strong&gt;为了应对镜像中存在太多镜像层，Docker 1.13版本以后，提供了一个压扁镜像功能，即将 Dockerfile 中所有的操作压缩为一层。这个特性还处于实验阶段，Docker默认没有开启，如果要开启，需要在启动Docker时添加-experimental 选项，并在Docker build 构建镜像时候添加 &amp;ndash;squash 。我们不推荐使用这个办法，请在撰写 Dockerfile 时遵循最佳实践编写，不要试图用这种办法去压缩镜像。&lt;/p&gt;

&lt;h1 id=&#34;三-使用多阶段构建&#34;&gt;三、使用多阶段构建&lt;/h1&gt;

&lt;p&gt;Dockerfile中每条指令都会为镜像增加一个镜像层，并且你需要在移动到下一个镜像层之前清理不需要的组件。实际上，有一个Dockerfile用于开发（其中包含构建应用程序所需的所有内容）以及一个用于生产的瘦客户端，它只包含你的应用程序以及运行它所需的内容。这被称为“建造者模式”。Docker 17.05.0-ce版本以后支持多阶段构建。使用多阶段构建，你可以在Dockerfile中使用多个FROM语句，每条FROM指令可以使用不同的基础镜像，这样您可以选择性地将服务组件从一个阶段COPY到另一个阶段，在最终镜像中只保留需要的内容。&lt;/p&gt;

&lt;p&gt;下面是一个使用 &lt;code&gt;COPY --from&lt;/code&gt; 和 &lt;code&gt;FROM ... AS ...&lt;/code&gt; 的Dockerfile：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-docker&#34;&gt;# Compile
FROM golang:1.9.0 AS builder
WORKDIR /go/src/v9.git...com/.../k8s-monitor
COPY . .
WORKDIR /go/src/v9.git...com/.../k8s-monitor
RUN make build
RUN mv k8s-monitor /root
# Package
# Use scratch image
FROM scratch
WORKDIR /root/
COPY --from=builder /root .
EXPOSE 8080
CMD [&amp;quot;/root/k8s-monitor&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;构建镜像，你会发现生成的镜像只有上面COPY 指令指定的内容，镜像大小只有2M。这样在以前使用两个Dockerfile（一个Dockerfile用于开发和一个用于生产的瘦客户端），现在使用多阶段构建就可以搞定。&lt;/p&gt;

&lt;h1 id=&#34;四-构建业务服务镜像技巧&#34;&gt;四、构建业务服务镜像技巧&lt;/h1&gt;

&lt;p&gt;Docker在build镜像的时候，如果某个命令相关的内容没有变化，会使用上一次缓存（cache）的文件层，在构建业务镜像的时候可以注意下面两点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;不变或者变化很少的体积较大的依赖库和经常修改的自有代码分开；&lt;/li&gt;
&lt;li&gt;因为cache缓存在运行Docker build命令的本地机器上，建议固定使用某台机器来进行Docker build，以便利用cache。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面是构建Spring Boot应用镜像的例子，用来说明如何分层。其他类型的应用，比如Java WAR包，Nodejs的npm 模块等，可以采取类似的方式。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;在Dockerfile所在目录，解压缩maven生成的jar包&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ unzip &amp;lt;path-to-app-jar&amp;gt;.jar -d app
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Dockerfile 我们把应用的内容分成4个部分COPY到镜像里面：其中前面3个基本不变，第4个是经常变化的自有代码。最后一行是解压缩后，启动spring boot应用的方式。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-docker&#34;&gt;FROM openjdk:8-jre-alpine
LABEL maintainer &amp;quot;opl-xws@xiaomi.com&amp;quot;
COPY app/BOOT-INF/lib/ /app/BOOT-INF/lib/
COPY app/org /app/org
COPY app/META-INF /app/META-INF
COPY app/BOOT-INF/classes /app/BOOT-INF/classes
EXPOSE 8080
CMD [&amp;quot;/usr/bin/java&amp;quot;, &amp;quot;-cp&amp;quot;, &amp;quot;/app&amp;quot;, &amp;quot;org.springframework.boot.loader.JarLauncher&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这样在构建镜像时候可大大提高构建速度。&lt;/p&gt;

&lt;h1 id=&#34;五-其他优化办法&#34;&gt;五、其他优化办法&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;RUN命令中执行apt、apk或者yum类工具技巧，如果在RUN命令中执行apt、apk或者yum类工具，可以借助这些工具提供的一些小技巧来减少镜像层数量及镜像大小。举几个例子：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在执行apt-get install -y 时增加选项— no-install-recommends ，可以不用安装建议性（非必须）的依赖，也可以在执行apk add 时添加选项&amp;ndash;no-cache 达到同样效果；&lt;/li&gt;
&lt;li&gt;执行yum install -y 时候， 可以同时安装多个工具，比如yum install -y gcc gcc-c++ make &amp;hellip;。将所有yum install 任务放在一条RUN命令上执行，从而减少镜像层的数量；&lt;/li&gt;
&lt;li&gt;组件的安装和清理要串联在一条指令里面，如 apk &amp;ndash;update add php7 &amp;amp;&amp;amp; rm -rf /var/cache/apk/* ，因为Dockerfile的每条指令都会产生一个文件层，如果将apk add &amp;hellip;和 rm -rf &amp;hellip; 命令分开，清理无法减小apk命令产生的文件层的大小。 Ubuntu或Debian可以使用 rm -rf /&lt;strong&gt;var&lt;/strong&gt;/lib/apt/lists/* 清理镜像中缓存文件；CentOS等系统使用yum clean all 命令清理。&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;压缩镜像&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Docker 自带的一些命令还能协助压缩镜像，比如 export 和 import&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d test/test:0.2
$ docker export 747dc0e72d13 | docker import - test/test:0.3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用这种方式需要先将容器运行起来，而且这个过程中会丢失镜像原有的一些信息，比如：导出端口，环境变量，默认指令。&lt;/p&gt;

&lt;p&gt;查看这两个镜像history信息，如下，可以看到test/test:0.3 丢失了所有的镜像层信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@k8s-master:/tmp/iops# docker history test/test:0.3
IMAGE               CREATED             CREATED BY          SIZE                COMMENT
6fb3f00b7a72        15 seconds ago                          84.7MB              Imported from -
root@k8s-master:/tmp/iops# docker history test/test:0.2
IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT
58468c0222ed        2 hours ago         /bin/sh -c #(nop)  CMD [&amp;quot;redis-server&amp;quot;]         0B     
1af7ffe3d163        2 hours ago         /bin/sh -c echo &amp;quot;==&amp;gt; Install curl and helper...   15.7MB 
8bac6e733d54        2 hours ago         /bin/sh -c #(nop)  ENV TARBALL=http://downlo...   0B     
793282f3ef7a        2 hours ago         /bin/sh -c #(nop)  ENV VER=3.0.0                0B     
74f8760a2a8b        8 days ago          /bin/sh -c #(nop)  CMD [&amp;quot;/bin/bash&amp;quot;]            0B     
&amp;lt;missing&amp;gt;           8 days ago          /bin/sh -c mkdir -p /run/systemd &amp;amp;&amp;amp; echo &#39;do...   7B
&amp;lt;missing&amp;gt;           8 days ago          /bin/sh -c sed -i &#39;s/^#\s*\(deb.*universe\)$...   2.76kB
&amp;lt;missing&amp;gt;           8 days ago          /bin/sh -c rm -rf /var/lib/apt/lists/*          0B
&amp;lt;missing&amp;gt;           8 days ago          /bin/sh -c set -xe   &amp;amp;&amp;amp; echo &#39;#!/bin/sh&#39; &amp;gt; /...   745B   
&amp;lt;missing&amp;gt;           8 days ago          /bin/sh -c #(nop) ADD file:5fabb77ea8d61e02d...   82.4MB 
root@k8s-master:/tmp/iops#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;社区里还有很多压缩工具，比如Docker-squash ，用起来更简单方便，并且不会丢失原有镜像的自带信息，大家有兴趣可以试试。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Docker容器启动退出解决方案</title>
      <link>https://guoxudong.io/en/post/docker-quit/</link>
      <pubDate>Thu, 27 Sep 2018 19:27:03 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/docker-quit/</guid>
      <description>

&lt;h1 id=&#34;现象&#34;&gt;现象&lt;/h1&gt;

&lt;p&gt;启动docker容器&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run –name [CONTAINER_NAME] [CONTAINER_ID] 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看容器运行状态&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker ps -a 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;发现刚刚启动的mydocker容器已经退出&lt;/p&gt;

&lt;h1 id=&#34;原因&#34;&gt;原因&lt;/h1&gt;

&lt;p&gt;docker容器的主线程（dockfile中CMD执行的命令）结束，容器会退出&lt;/p&gt;

&lt;h1 id=&#34;解决办法&#34;&gt;解决办法&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;可以使用交互式启动&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -i [CONTAINER_NAME or CONTAINER_ID]
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;上面的不太友好，建议使用后台模式和tty选项&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -dit [CONTAINER_NAME or CONTAINER_ID]
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Docker 容器在后台以守护态（Daemonized）形式运行，可以通过添加 -d 参数来实现&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo docker run -d ubuntu:14.04 /bin/sh -c &amp;quot;while true; do echo hello world; sleep 1; done&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在脚本最后一行添加&lt;strong&gt;tail -f /dev/null&lt;/strong&gt;，这个命令永远完成不了，所以该脚本一直不会执行完，所以该容器永远不会退出。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TIPs&lt;/strong&gt;:退出时，使用&lt;code&gt;[ctrl + D]&lt;/code&gt;，这样会结束docker当前线程，容器结束，可以使用 &lt;code&gt;[ctrl + P]&lt;/code&gt; &lt;code&gt;[ctrl + Q]&lt;/code&gt; 退出而不终止容器运行&lt;/p&gt;

&lt;p&gt;如下命令，会在指定容器中执行指定命令， &lt;code&gt;[ctrl+D]&lt;/code&gt; 退出后不会终止容器运行&lt;/p&gt;

&lt;p&gt;docker默认会把容器内部pid=1的作为默认的程序&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>容器技术概述</title>
      <link>https://guoxudong.io/en/post/con-in/</link>
      <pubDate>Thu, 30 Aug 2018 18:45:22 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/con-in/</guid>
      <description>

&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;自从微服务（Microservice）的出现，出于业务的需要，IT应用模型不断的变革。开发模式从瀑布式到敏捷开发；开发、运维和测试互相配合的devops思想；应用程序架构从单体模型到分层模型再到微服务；部署方式也从面向物理机到虚拟键再到容器；应用程序的基础架构从自建机房到托管再到云计算，等等。这些变革使得IT技术应用的效率大大提升，同时却以&lt;strong&gt;更低的成本交付更高质量的产品&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;尤其是以Docker为代表的容器技术的出现，终结了devops中交付和部署环节因环节、配置及程序本身的不同而造成的动辄几种甚至十几种部署配置的困境，将它们统一在容器镜像（image）之上。这就是我在工作中遇到最先遇到的困境，同时也是我开始研究容器技术的契机。&lt;/p&gt;

&lt;p&gt;如今，越来越多的企业或组织开始开始选择以镜像文件为交付载体。容器镜像之内直接包含了应用程序及其依赖的系统环境、库、基础程序等，从而能够在容器引擎上直接运行。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;容器技术概述&#34;&gt;容器技术概述&lt;/h1&gt;

&lt;p&gt;容器是一种轻量级、可移植、自包含的软件打包技术，它使得应用程序可以在几乎任何地方以相同的方式运行。&lt;/p&gt;

&lt;p&gt;容器有应用程序本身和它的环境依赖（库和其他应用程序）两部分组成，并在宿主机（Host）操作系统的用户空间中运行，但与操作系统的其他进程互相隔离，他们的实现机制有别于VMWare、KVM、Xen等实现方案的虚拟化技术。容器与虚拟机的对比关系如下图
&lt;img src=&#34;https://guoxudong.io/images/source/vs.png&#34; alt=&#34;image&#34; /&gt;
由于同一个宿主机上的所有容器都共享其底层操作系统（内核空间），这就使得容器在体积上要比传统的虚拟机小很多。另外，启动容器无须启动整个操作系统，所以容器部署和启动的速度更快，开销更小，也更容易迁移。事实上，容器赋予了应用程序超强的可移植能力。&lt;/p&gt;

&lt;h1 id=&#34;容器技术的优势&#34;&gt;容器技术的优势&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;开发方面：“一次构建、到处运行”（Build Once, Run Anywhere）。容器意味着环境隔离和可重复性，开发人员只需为应用创建一个运行环境，并将其打包成容器便可在各种部署环境上运行，并与它所在的宿主机环境隔离。&lt;/li&gt;
&lt;li&gt;运维方面：“一次配置，运行所以”（Configure Once, Run Anything）。一旦配置好标准的容器运行时环境，服务器就可以运行任何容器，这使得运维人员的工作变得更高效、一致和可重复。容器消除了开发、测试、生产环境的不一致性。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>centos7.2 安装k8s v1.11.0</title>
      <link>https://guoxudong.io/en/post/install-k8s/</link>
      <pubDate>Tue, 14 Aug 2018 20:07:03 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/install-k8s/</guid>
      <description>

&lt;h1 id=&#34;前言&#34;&gt;前言&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;最近由于公司业务发展到了瓶颈，原有的技术架构已经逐渐无法满足业务开发和测试的需求，出现了应用测试环境搭建复杂，有许多套（真的很多很多）应用环境，应用在持续集成/持续交付也遇到了很大的困难，经过讨论研究决定对应用和微服务进行容器化，这就是我首次直面docker和k8s的契机.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;kubernetes-介绍&#34;&gt;Kubernetes 介绍&lt;/h1&gt;

&lt;p&gt;Kubernetes 是 Google 团队发起的开源项目，它的目标是管理跨多个主机的容器，提供基本的部署，维护以及运用伸缩，主要实现语言为
Go 语言。
Kubernetes的特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;易学：轻量级，简单，容易理解&lt;/li&gt;
&lt;li&gt;便携：支持公有云，私有云，混合云，以及多种云平台&lt;/li&gt;
&lt;li&gt;可拓展：模块化，可插拔，支持钩子，可任意组合&lt;/li&gt;
&lt;li&gt;自修复：自动重调度，自动重启，自动复制&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;准备工作&#34;&gt;准备工作&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;注：以下操作都是在root权限下执行的&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;安装docker-ce，这里使用docker-ce-17.09.0.c版本，安装方法见&lt;a href=&#34;https://guoxudong.io/2018/install-docker&#34;&gt;之前的教程&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装Kubeadm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#安装 Kubeadm 首先我们要配置好阿里云的国内源，执行如下命令：
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
EOF

#之后，执行以下命令来重建yum缓存：
yum -y install epel-releaseyum
clean all
yum makecache
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来需要安装指定版本的Kubeadm（这里要安装指定版本，因为后续依赖的镜像由于有墙无法拉取，这里我们只有指定版本的镜像），注意：&lt;strong&gt;这里是安装指定版本的Kubeadm，k8s的版本更新之快完全超出你的想象！&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;yum -y install kubelet-1.11.0-0
yum -y install kubeadm-1.11.0-0
yum -y install kubectl-1.11.0-0
yum -y install kubernetes-cni
    
#执行命令启动Kubeadm服务：
systemctl enable kubelet &amp;amp;&amp;amp; systemctl start kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置 Kubeadm 所用到的镜像
这里是重中之重，因为在国内的原因，无法访问到 Google 的镜像库，所以我们需要执行以下脚本来从 Docker Hub 仓库中获取相同的镜像，并且更改 TAG 让其变成与 Google 拉去镜像一致。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;新建一个 Shell 脚本，填入以下代码之后保存&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-vim&#34;&gt;#docker.sh
#!/bin/bash
images=(kube-proxy-amd64:v1.11.0 kube-scheduler-amd64:v1.11.0 kube-controller-manager-amd64:v1.11.0 kube-apiserver-amd64:v1.11.0 etcd-amd64:3.2.18 coredns:1.1.3 pause-amd64:3.1 kubernetes-dashboard-amd64:v1.8.3 k8s-dns-sidecar-amd64:1.14.9 k8s-dns-kube-dns-amd64:1.14.9 k8s-dns-dnsmasq-nanny-amd64:1.14.9 )
for imageName in ${images[@]} ; do
docker pull keveon/$imageName
docker tag keveon/$imageName k8s.gcr.io/$imageName
docker rmi keveon/$imageName
done
# 个人新加的一句，V 1.11.0 必加
docker tag da86e6ba6ca1 k8s.gcr.io/pause:3.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;保存后使用chmod命令赋予脚本执行权限&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;chmod -R 777 ./docker.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;执行脚本拉取镜像&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sh docker.sh
#这里就开始了漫长的拉取镜像之路
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;关闭掉swap&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo swapoff -a
#要永久禁掉swap分区，打开如下文件注释掉swap那一行
# sudo vi /etc/stab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;关闭SELinux的&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 临时禁用selinux
# 永久关闭 修改/etc/sysconfig/selinux文件设置
sed -i &#39;s/SELINUX=permissive/SELINUX=disabled/&#39; /etc/sysconfig/selinux
# 这里按回车，下面是第二条命令
setenforce 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;关闭防火墙&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;systemctl disable firewalld.service &amp;amp;&amp;amp; systemctl stop firewalld.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置转发参数&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 配置转发相关参数，否则可能会出错
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
vm.swappiness=0
EOF
# 这里按回车，下面是第二条命令
sysctl --system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;这里就完成了k8s集群搭建的准备工作，集群搭建的话以上操作结束后将操作完的系统制作成系统镜像，方便集群搭建&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;正式安装&#34;&gt;正式安装&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;以下的操作都只在主节点上进行：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;初始化镜像&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubeadm init --kubernetes-version=v1.11.0 --pod-network-cidr=10.10.0.0/16  #这里填写集群所在网段
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;之后的输出会是这样：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;I0712 10:46:30.938979   13461 feature_gate.go:230] feature gates: &amp;amp;{map[]}
[init] using Kubernetes version: v1.11.0
[preflight] running pre-flight checks
I0712 10:46:30.961005   13461 kernel_validator.go:81] Validating kernel version
I0712 10:46:30.961061   13461 kernel_validator.go:96] Validating kernel config
    [WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 18.03.1-ce. Max validated version: 17.03
    [WARNING Hostname]: hostname &amp;quot;g2-apigateway&amp;quot; could not be reached
    [WARNING Hostname]: hostname &amp;quot;g2-apigateway&amp;quot; lookup g2-apigateway on 100.100.2.138:53: no such host
[preflight/images] Pulling images required for setting up a Kubernetes cluster
[preflight/images] This might take a minute or two, depending on the speed of your internet connection
[preflight/images] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;
[kubelet] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;
[kubelet] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;
[preflight] Activating the kubelet service
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [g2-apigateway kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.16.8.62]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Generated etcd/ca certificate and key.
[certificates] Generated etcd/server certificate and key.
[certificates] etcd/server serving cert is signed for DNS names [g2-apigateway localhost] and IPs [127.0.0.1 ::1]
[certificates] Generated etcd/peer certificate and key.
[certificates] etcd/peer serving cert is signed for DNS names [g2-apigateway localhost] and IPs [172.16.8.62 127.0.0.1 ::1]
[certificates] Generated etcd/healthcheck-client certificate and key.
[certificates] Generated apiserver-etcd-client certificate and key.
[certificates] valid certificates and keys now exist in &amp;quot;/etc/kubernetes/pki&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/admin.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/kubelet.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/controller-manager.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/scheduler.conf&amp;quot;
[controlplane] wrote Static Pod manifest for component kube-apiserver to &amp;quot;/etc/kubernetes/manifests/kube-apiserver.yaml&amp;quot;
[controlplane] wrote Static Pod manifest for component kube-controller-manager to &amp;quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&amp;quot;
[controlplane] wrote Static Pod manifest for component kube-scheduler to &amp;quot;/etc/kubernetes/manifests/kube-scheduler.yaml&amp;quot;
[etcd] Wrote Static Pod manifest for a local etcd instance to &amp;quot;/etc/kubernetes/manifests/etcd.yaml&amp;quot;
[init] waiting for the kubelet to boot up the control plane as Static Pods from directory &amp;quot;/etc/kubernetes/manifests&amp;quot;
[init] this might take a minute or longer if the control plane images have to be pulled
[apiclient] All control plane components are healthy after 41.001672 seconds
[uploadconfig] storing the configuration used in ConfigMap &amp;quot;kubeadm-config&amp;quot; in the &amp;quot;kube-system&amp;quot; Namespace
[kubelet] Creating a ConfigMap &amp;quot;kubelet-config-1.11&amp;quot; in namespace kube-system with the configuration for the kubelets in the cluster
[markmaster] Marking the node g2-apigateway as master by adding the label &amp;quot;node-role.kubernetes.io/master=&#39;&#39;&amp;quot;
[markmaster] Marking the node g2-apigateway as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[patchnode] Uploading the CRI Socket information &amp;quot;/var/run/dockershim.sock&amp;quot; to the Node API object &amp;quot;g2-apigateway&amp;quot; as an annotation
[bootstraptoken] using token: o337m9.ceq32wg9g2gro7gx
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the &amp;quot;cluster-info&amp;quot; ConfigMap in the &amp;quot;kube-public&amp;quot; namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

kubeadm join 10.10.207.253:6443 --token t69z6h.lr2etdbg9mfx5r15 --discovery-token-ca-cert-hash sha256:90e3a748c0eb4cb7058f3d0ee8870ee5d746214ab0589b5e841fd5d68fec8f00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;这里注意最后一行：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;kubeadm join 10.10.207.253:6443 --token t69z6h.lr2etdbg9mfx5r15 --discovery-token-ca-cert-hash sha256:90e3a748c0eb4cb7058f3d0ee8870ee5d746214ab0589b5e841fd5d68fec8f00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;证明集群主节点安装成功，这里要记得保存这条命令，以便之后各个节点加入集群&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;配置kubetl认证信息&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export KUBECONFIG=/etc/kubernetes/admin.conf
# 如果你想持久化的话，直接执行以下命令【推荐】
echo &amp;quot;export KUBECONFIG=/etc/kubernetes/admin.conf&amp;quot; &amp;gt;&amp;gt; ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;安装flanel网络&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p /etc/cni/net.d/

cat &amp;lt;&amp;lt;EOF&amp;gt; /etc/cni/net.d/10-flannel.conf
{
&amp;quot;name&amp;quot;: &amp;quot;cbr0&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;flannel&amp;quot;,
&amp;quot;delegate&amp;quot;: {
&amp;quot;isDefaultGateway&amp;quot;: true
}
}
EOF

mkdir /usr/share/oci-umount/oci-umount.d -p

mkdir /run/flannel/

cat &amp;lt;&amp;lt;EOF&amp;gt; /run/flannel/subnet.env
FLANNEL_NETWORK=10.244.0.0/16
FLANNEL_SUBNET=10.244.1.0/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=true
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;最后需要新建一个flannel.yml文件：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
name: flannel
rules:
- apiGroups:
    - &amp;quot;&amp;quot;
    resources:
    - pods
    verbs:
    - get
- apiGroups:
    - &amp;quot;&amp;quot;
    resources:
    - nodes
    verbs:
    - list
    - watch
- apiGroups:
    - &amp;quot;&amp;quot;
    resources:
    - nodes/status
    verbs:
    - patch
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
name: flannel
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: flannel
subjects:
- kind: ServiceAccount
name: flannel
namespace: kube-system
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: v1
kind: ServiceAccount
metadata:
name: flannel
namespace: kube-system
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
kind: ConfigMap
apiVersion: v1
metadata:
name: kube-flannel-cfg
namespace: kube-system
labels:
    tier: node
    app: flannel
data:
cni-conf.json: |
    {
    &amp;quot;name&amp;quot;: &amp;quot;cbr0&amp;quot;,
    &amp;quot;type&amp;quot;: &amp;quot;flannel&amp;quot;,
    &amp;quot;delegate&amp;quot;: {
        &amp;quot;isDefaultGateway&amp;quot;: true
    }
    }
net-conf.json: |
    {
    &amp;quot;Network&amp;quot;: &amp;quot;10.10.0.0/16&amp;quot;,    #这里换成集群所在的网段
    &amp;quot;Backend&amp;quot;: {
        &amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot;
    }
    }
image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
name: kube-flannel-ds
namespace: kube-system
labels:
    tier: node
    app: flannel
spec:
template:
    metadata:
    labels:
        tier: node
        app: flannel
    spec:
    hostNetwork: true
    nodeSelector:
        beta.kubernetes.io/arch: amd64
    tolerations:
    - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
    serviceAccountName: flannel
    initContainers:
    - name: install-cni
        image: quay.io/coreos/flannel:v0.9.1-amd64
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conf
        volumeMounts:
        - name: cni
        mountPath: /etc/cni/net.d
        - name: flannel-cfg
        mountPath: /etc/kube-flannel/
    containers:
    - name: kube-flannel
        image: quay.io/coreos/flannel:v0.9.1-amd64
        command: [ &amp;quot;/opt/bin/flanneld&amp;quot;, &amp;quot;--ip-masq&amp;quot;, &amp;quot;--kube-subnet-mgr&amp;quot; ]
        securityContext:
        privileged: true
        env:
        - name: POD_NAME
        valueFrom:
            fieldRef:
            fieldPath: metadata.name
        - name: POD_NAMESPACE
        valueFrom:
            fieldRef:
            fieldPath: metadata.namespace
        volumeMounts:
        - name: run
        mountPath: /run
        - name: flannel-cfg
        mountPath: /etc/kube-flannel/
    volumes:
        - name: run
        hostPath:
            path: /run
        - name: cni
        hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
        configMap:
            name: kube-flannel-cfg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;执行：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create -f ./flannel.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认情况下，master节点不参与工作负载，但如果希望安装出一个all-in-one的k8s环境，则可以执行以下命令：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;让master节点成为一个node节点：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl taint nodes --all node-role.kubernetes.io/master-
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;查看节点信息：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;会看到如下的输出：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NAME            STATUS     ROLES     AGE       VERSION
k8s-master      Ready      master    18h       v1.11.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;以下是节点配置&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在配置好主节点之后，就可以配置集群的其他节点了，这里建议直接安装之前做好准备工作的系统镜像
进入节点机器之后，直接执行之前保存好的命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubeadm join 10.10.207.253:6443 --token t69z6h.lr2etdbg9mfx5r15 --discovery-token-ca-cert-hash sha256:90e3a748c0eb4cb7058f3d0ee8870ee5d746214ab0589b5e841fd5d68fec8f00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行完后会看到：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;[preflight] running pre-flight checks
        [WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs_wrr ip_vs_sh ip_vs ip_vs_rr] or no builtin kernel ipvs support: map[ip_vs_rr:{} ip_vs_wrr:{} ip_vs_sh:{} nf_conntrack_ipv4:{} ip_vs:{}]
you can solve this problem with following methods:
1. Run &#39;modprobe -- &#39; to load missing kernel modules;
2. Provide the missing builtin kernel ipvs support

I0725 09:59:27.929247   10196 kernel_validator.go:81] Validating kernel version
I0725 09:59:27.929356   10196 kernel_validator.go:96] Validating kernel config
[discovery] Trying to connect to API Server &amp;quot;10.10.207.253:6443&amp;quot;
[discovery] Created cluster-info discovery client, requesting info from &amp;quot;https://10.10.207.253:6443&amp;quot;
[discovery] Requesting info from &amp;quot;https://10.10.207.253:6443&amp;quot; again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &amp;quot;10.10.207.253:6443&amp;quot;
[discovery] Successfully established connection with API Server &amp;quot;10.10.207.253:6443&amp;quot;
[kubelet] Downloading configuration for the kubelet from the &amp;quot;kubelet-config-1.11&amp;quot; ConfigMap in the kube-system namespace
[kubelet] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;
[kubelet] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;
[preflight] Activating the kubelet service
[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...
[patchnode] Uploading the CRI Socket information &amp;quot;/var/run/dockershim.sock&amp;quot; to the Node API object &amp;quot;k8s-node1&amp;quot; as an annotation

This node has joined the cluster:
* Certificate signing request was sent to master and a response
was received.
* The Kubelet was informed of the new secure connection details.

Run &#39;kubectl get nodes&#39; on the master to see this node join the cluster.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;这里就表示执行完毕了，可以去主节点执行命令：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;可以看到节点已加入集群：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NAME        STATUS    ROLES     AGE       VERSION
k8s-master  Ready     master    20h       v1.11.0
k8s-node1   Ready     &amp;lt;none&amp;gt;    20h       v1.11.0
k8s-node2   Ready     &amp;lt;none&amp;gt;    20h       v1.11.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这期间可能需要等待一段时间，状态才会全部变为ready&lt;/p&gt;

&lt;h1 id=&#34;kubernetes-dashboard安装&#34;&gt;kubernetes-dashboard安装&lt;/h1&gt;

&lt;p&gt;详见：&lt;a href=&#34;https://guoxudong.io/2018/dashboard-k8s&#34;&gt;kubernetes安装dashboard&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;采坑指南&#34;&gt;采坑指南&lt;/h1&gt;

&lt;p&gt;有时会出现master节点一直处于notready的状态，这里可能是没有启动flannel，只需要按照上面的教程配置好flannel，然后执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create -f ./flannel.yml
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>centos7安装指定版本的docker</title>
      <link>https://guoxudong.io/en/post/install-docker/</link>
      <pubDate>Tue, 14 Aug 2018 20:05:21 +0800</pubDate>
      
      <guid>https://guoxudong.io/en/post/install-docker/</guid>
      <description>

&lt;h1 id=&#34;前言&#34;&gt;前言&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;在使用&lt;strong&gt;centos7&lt;/strong&gt;，并使用荫安装搬运工的时候，往往不希望安装最新版本的搬运工，而是希望安装与自己熟悉或者当前业务环境需要的版本，例如目前Kubernetes支持的最新搬运工版本为v17.03，所以就产生了安装指定版本码头工人的需求。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;安装步骤&#34;&gt;安装步骤&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 安装依赖包
yum install -y yum-utils device-mapper-persistent-data lvm2

# 添加Docker软件包源
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

#关闭测试版本list（只显示稳定版）
sudo yum-config-manager --enable docker-ce-edge
sudo yum-config-manager --enable docker-ce-test

# 更新yum包索引
yum makecache fast

#NO.1 直接安装Docker CE （will always install the highest  possible version，可能不符合你的需求）
yum install docker-ce

#NO.2 指定版本安装
yum list docker-ce --showduplicates|sort -r 
#找到需要安装的
yum install docker-ce-17.09.0.ce -y
#启动docker
systemctl start docker &amp;amp; systemctl enable docker
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;采坑指南&#34;&gt;采坑指南&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;当然本着万事皆有坑的原则，这里也是有坑的，在安装中也是会遇到如下的问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在执行一下命令的时候：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;yum install docker-ce-17.03.0.ce -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;会出现如下的报错：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;--&amp;gt; Finished Dependency Resolution
Error: Package: docker-ce-17.03.0.ce-1.el7.centos.x86_64 (docker-ce-stable)
        Requires: docker-ce-selinux &amp;gt;= 17.03.0.ce-1.el7.centos
        Available: docker-ce-selinux-17.03.0.ce-1.el7.centos.noarch (docker-ce-stable)
            docker-ce-selinux = 17.03.0.ce-1.el7.centos
        Available: docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch (docker-ce-stable)
            docker-ce-selinux = 17.03.1.ce-1.el7.centos
        Available: docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch (docker-ce-stable)
            docker-ce-selinux = 17.03.2.ce-1.el7.centos
You could try using --skip-broken to work around the problem
You could try running: rpm -Va --nofiles --nodigest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在出现这个问题之后，需要执行以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#要先安装docker-ce-selinux-17.03.2.ce，否则安装docker-ce会报错
yum install https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm

#然后再安装 docker-ce-17.03.2.ce，就能正常安装
yum install docker-ce-17.03.2.ce-1.el7.centos
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
