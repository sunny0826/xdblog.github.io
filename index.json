[{"authors":["admin","郭旭东"],"categories":null,"content":" 郭旭东，2018年4月加入凯京科技。曾任高级研发和运维开发工程师、阿里云MVP、CCF 会员，现任凯京科技研发中心技术支持部运维负责人，负责公司运维团队建设。热爱开源，乐于分享，致力于推行devops理念及相关技术，提升开发效率，提高交付质量与速度，目前专注于云原生应用的实践，并探索更高效的运维系统架构。\n关于我们 凯京科技致力于推动物流行业的数字化和智慧化，为小微物流企业和车主提供线上化的开放平台，探索解决物流行业运输成本高、融资门槛高等痛点，为物流行业降本增效。 凯京科技拥有蚂蚁金服、红杉资本、中航信托、德邦证券等知名股东。凯京科技联合优质股东资源，共同打造最懂小微物流企业和车主的数据、科技和普惠金融服务，通过凯京物流云线上化平台，将广大物流人日常经营所需的货物运输管理、车辆管理、资金结算管理、金融服务管理等全面线上化、数字化，让物流人生意更好做。\n","date":1583134156,"expirydate":-62135596800,"kind":"taxonomy","lang":"zh","lastmod":1583134156,"objectID":"cd54ffc1a2f3316b02c71b69207ea6f1","permalink":"https://guoxudong.io/authors/guoxudong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/guoxudong/","section":"authors","summary":"郭旭东，2018年4月加入凯京科技。曾任高级研发和运维开发工程师、阿里云MVP、CCF 会员，现任凯京科技研发中心技术支持部运维负责人，负责公","tags":null,"title":"郭旭东","type":"authors"},{"authors":["admin"],"categories":null,"content":" 郭旭东，2018年4月加入凯京科技。曾任高级研发和运维开发工程师、阿里云MVP、CCF 会员，现任凯京科技研发中心架构\u0026amp;运维部运维负责人，负责公司运维团队建设。热爱开源，致力于推行devops理念及相关技术，提升开发效率，提高交付质量与速度，专注于云平台的容器化实践，探索更高效的运维系统架构。\n关于我们 凯京科技致力于推动物流行业的数字化和智慧化，为小微物流企业和车主提供线上化的开放平台，探索解决物流行业运输成本高、融资门槛高等痛点，为物流行业降本增效。 凯京科技拥有蚂蚁金服、红杉资本、中航信托、德邦证券等知名股东。凯京科技联合优质股东资源，共同打造最懂小微物流企业和车主的数据、科技和普惠金融服务，通过凯京物流云线上化平台，将广大物流人日常经营所需的货物运输管理、车辆管理、资金结算管理、金融服务管理等全面线上化、数字化，让物流人生意更好做。\n","date":1561939200,"expirydate":-62135596800,"kind":"taxonomy","lang":"zh","lastmod":1561939200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://guoxudong.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"郭旭东，2018年4月加入凯京科技。曾任高级研发和运维开发工程师、阿里云MVP、CCF 会员，现任凯京科技研发中心架构\u0026amp;运维部运维负责","tags":null,"title":"郭旭东","type":"authors"},{"authors":["郭旭东"],"categories":["Kubernetes"],"content":" 前言 最近接到一个需求，需要展示 ingress 上面的访问日志，由于我们的业务系统都部署在 Kubernetes 上面，通过 ingress 进行访问，所以这里的访问日志，其实就是我们全部业务系统的访问日志。\n日志采集方面，阿里云天生就提供了 nginx-ingress 日志和采集和展示，本身提供很多不错的基于 ingress 日志数据的图表与分析。如果你使用的是阿里云 ACK 容器服务，那么极端推荐使用，配置方法见官方文档：https://help.aliyun.com/document_detail/86532.html。\n让人头秃的是，我们这次不但要采集 ingress 日志上比较常规的 url client_ip method status 等字段，还要采集我们系统在 Request Headers 里面自定义的参数，这些参数是默认的 ingress 并不展示的，所以需要我们进行调整。\n开始 首先明确需要调整的组件：\n nginx-ingress 的 ConfigMap：用于打印自定义日志字段 AliyunLogConfig：这个是阿里云日志服务的 CRD 扩展，需要在这个里面加入新增的字段名和修改后的正则表达式 在日志服务控制台，添加新增字段的指定字段查询 新增展示仪表盘  调整 ingress 日志输出 我们 ingress 组件使用的是 nginx-ingress-container，这里要调整日志输出格式，老规矩，直接官方文档：https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/。\n从文档可见，只需要调整 ingress-nginx 的 ConfigMap nginx-configuration data 中的 log-format-upstream 字段即可。\n 知识点：\n官方文档里面给的说明不是很详细，没有提到 Request Headers 里自定义的字段应该怎么表示（也有可能是我眼瘸没看见），但经过我多次试验发现，Request Headers 里的字段在 log-format-upstream 中应该使用 $http_{your field} 表示，比如 $http_cookie；而带 - 的字段则需要将 - 改为 _，并且使用小写，比如 app-Id 就应使用 $http_app_id 表示。\n  修改 ConfigMap，ingress-controller 将进行热更新，看到如下日志，就证明配置已完成更新，接下来就可以看到你自定义字段的值已经打印出来了。\nI0302 08:20:58.393365 9 controller.go:200] Backend successfully reloaded.  调整阿里云日志组件配置  执行下面的步骤请确保已经按照官方文档正确部署阿里云日志服务在您的 K8S 集群之后，并且已达到要求的版本。   日志已经成功打印了，接下来就是调整日志采集的字段了，这里只需要调整日志服务 CRD 的扩展配置即可。\n$ kubectl edit AliyunLogConfig k8s-nginx-ingress  在修改配置之前，推荐先去 https://regex101.com/ 验证正则表达式是否正确，将调整过的正则表达式和 ingress-controller 打印的日志贴入下图指定位置，就可以看出正则表达式是否正确。\n然后将添加的字段名称（这个名称将作为 key 在日志服务中展示，可以与 header 中的字段不同）和正则表达式贴入如下 CRD 中。\napiVersion: log.alibabacloud.com/v1alpha1 kind: AliyunLogConfig metadata: # your config name, must be unique in you k8s cluster name: k8s-nginx-ingress spec: # logstore name to upload log logstore: nginx-ingress # product code, only for k8s nginx ingress productCode: k8s-nginx-ingress # logtail config detail logtailConfig: inputType: plugin # logtail config name, should be same with [metadata.name] configName: k8s-nginx-ingress inputDetail: plugin: inputs: - type: service_docker_stdout detail: IncludeLabel: io.kubernetes.container.name: nginx-ingress-controller Stderr: false Stdout: true processors: - type: processor_regex detail: KeepSource: false Keys: - client_ip - x_forward_for - remote_user - time - method - url - version - status - body_bytes_sent - http_referer - http_user_agent - request_length - request_time - proxy_upstream_name - upstream_addr - upstream_response_length - upstream_response_time - upstream_status - req_id - host - #需要添加的字段名称 - ... NoKeyError: true NoMatchError: true Regex: #修改后的正则表达式 SourceKey: content  日志控制台新增字段 如果上面的操作无误的话，日志服务中就会展示您添加的字段了，如果配置有误，所有的自定义字段都会不显示，只会显示保留字段名称。\n添加指定字段查询，就可以快速查看添加的字段了。\n新增展示仪表盘 日志既然已经取到了，那么展示就很容易了，直接在查询栏中输入分析语句，日志服务支持 SQL 聚合日志，并直接生成统计图表，点击添加到仪表盘可以就可以添加到现有仪表盘或者新建一个仪表盘。\n成果 之后进行一些微调，添加过滤栏，由于这里统计的是登录用户，你甚至都可以添加一个词云来看看哪些用于使用系统比较频繁。当然，想添加什么都看您的喜好，日志在你手里，想怎么分析都可以。\n结语 本次实现的功能并不是什么高深的功能，只不过是一个简单的访问日志记录和展示，相信每个系统其实都有一套这种功能。但是这种实现方式在我看来优点更多：\n 无代码：全程没有写一行代码，如果有的话，也就是业务需要统一 Request Headers 里面的字段。 配置简单：只需要修改 nginx ConfigMap 中的一个字段，并在 CRD 中添加字段名称和正在表达式，唯一的难度可能就是正则表达式。 配置快：整体的配置时间很短，加上查文档和调整图表也不过半天的时间，肯定比 提需求-评估-开发-测试-验收 全流程走一遍，前端后端撕一遍要快的多的多的多。 高度定制：可以根据自己的喜好，随意定制图表。   最近发现阿里云日志服务是一个宝藏产品，从安全到 k8s 业务，从成本控制到疫情动态，日志服务真的就是把所有没有前端开发资源的服务都帮了一把。 \u0026mdash; 摘自本人朋友圈\n ","date":1583134156,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1583134156,"objectID":"b2434427d381fe9ba2d7a0fd81071c03","permalink":"https://guoxudong.io/post/nginx-ingress-log-aliyun/","publishdate":"2020-03-02T15:29:16+08:00","relpermalink":"/post/nginx-ingress-log-aliyun/","section":"post","summary":"修改 nginx-ingress 日志，并结合阿里云日志服务制作系统访问日志统计图表。","tags":["ingress","阿里云","日志服务"],"title":"修改 Nginx Ingress 日志打印格式","type":"post"},{"authors":["Almas Raza","John Allen"],"categories":["翻译"],"content":" 根据 Portworx 在2018年进行的一项调查，五分之四的企业现在正在使用容器，其中83％的企业正在生产环境中使用。而这个数字在2017年只有67％，很明显，容器不仅仅是一种时尚。\n但是，随着容器的流行，一些公司开始在 Kubernetes 内建立有效的流量控制和安全策略。\n作为容器调度和集群管理平台，Kubernetes 致力于提供出色的基础架构，因此被无数公司采用。它刚刚开源五周年，最近在福布斯发表的一篇名为《Kubernetes “the most popular open source project of our times”》的文章表示，Kubernetes 已被 Capital One，ING Group，Philips，VMware 和 Huawei 等公司使用。\n对于使用微服务架构（MSA）开发来应用程序的公司来说，Kubernetes 具有许多优势，特别是在应用程序部署方面。\n出于上面这些原因，研发团队有必要了解 Kubernetes 独有的流量和安全情况。在本文中，我们将介绍：\n Kubernetes 是什么。 Kubernetes 面临的挑战。 Kubernetes 中的七个最重要的流量和安全要求。 关于开发和操作简便性的注意事项。  让我们开始吧。\nKubernetes 是什么 Kubernetes 是一个开源的容器编排系统。根据 Kubernetes’ own definition，它是一个可移植且可扩展的程序，用于管理容器化的工作负载和服务，并提供以容器为中心的管理环境。\n下图描述了 Kubernetes 的基本工作方式。图中可以看到一个主节点和两个工作节点。主节点用来告诉工作程序节点需要做什么工作，而工作程序节点则执行主节点提供给它们的指令。同时可以添加其他 Kubernetes 工作节点以扩展基础架构。\n如果仔细观察，您会发现在每个部分中都出现了 “Docker” 一词。Docker 是一个容器平台，非常适合在单个物理机或虚拟机（VM）上运行容器。\n但是，如果您要在多个不同的应用程序中使用数百个容器，且您不希望将它们全部放在一台计算机上。这是催生 Kubernetes 的挑战之一。\n使用 overlay 网络（如上图中的红色条所示），主节点中的容器不必知道它需要与之通信的容器位于哪个节点，就可以直接与之通信。\nKubernetes 的另一个主要功能是将信息打包到 “pod” 中，如果应用程序由多个容器组成，则可以将这些容器组成一个 pod ，并共享整个生命周期。\nKubernetes 面临的挑战 像所有其他容器编排系统一样，Kubernetes 也面临的诸多挑战，其中包括：\n 内部和外部网络是隔离的。 容器和容器的 IP 地址会发生变化。 微服务之间没有访问控制。 没有应用程序层的可见性。  让我们更深入地探讨这些挑战。Kubernetes 的网络不是常规的网络，因为尽管使用了 overlay 网络，但内部和外部网络却是彼此不通的。\n另外，Kubernetes 会隔离发生故障的节点或 Pod，以防止它们关闭整个应用程序。这可能导致节点之间的IP地址频繁更改。想要发现容器或容器的IP地址的服务就必须弄清楚新的IP地址是什么。\n当涉及微服务之间的访问控制时，对于企业而言，重要的是要认识到 Kubernetes 节点之间的流量也能够流入外部物理设备或 VM。这可能会消耗资源并削弱安全性。\n最后，无法在应用程序层检查信息是一个大问题。没有这种可见性，企业可能会错过收集详细分析信息的关键机会。\nKubernetes 和云安全要求 到目前为止，我们已经讨论了 Kubernetes 的基本功能以及它所带来的挑战。现在，基于 A10 Networks 15年的经验，我们将继续讨论 Kubernetes 和云安全性的要求。\n我们将讨论如下七点要求：\n 高级应用程序交付控制器（ADC） 使负载均衡器（LB）配置与基础架构保持同步 南北向流量的安全 为大规模部署准备的中央控制器 微服务之间的访问控制 东西向流量加密 应用流量分析  1. 高级应用程序交付控制器（ADC） 虽然企业可能已经在其基础架构的其他区域使用了高级应用程序交付控制器，但也有必要为 Kubernetes 部署一个。默认情况下，这将允许管理员操作在 Kubernetes 前的高级负载均衡器。\nKubernetes 已经配备了名为 kube-proxy 的网络代理。它提供了简单的用法：通过在三层中调整 iptables 规则来工作。但这是非常基本的，并与大多数企业操作习惯的有所不同。\n许多人会将 ADC 或负载均衡器放在他们的 Kubernetes 前。这样就可以创建一个静态的虚拟 IP，所有人都可以使用它，并动态配置所有内容。\n随着 Pod 和容器的启动，可以动态配置 ADC，以提供对新应用程序的访问，同时实现网络安全策略，并在某些情况下实施业务数据规则。通常，这是通过使用 “Ingress controller” 来实现的，其可以监控到新的容器和容器的启动，并且可以配置 ADC 以提供对新应用程序的访问权限，或者将更改通知给另一个 “Kubernetes controller” 节点。\n2. 使负载均衡器（LB）配置与基础架构保持同步 由于在 Kubernetes 中一切都是可以不断变化的，因此位于集群前的负载均衡器是无法追踪所有事情的。除非您有类似上图紫色框所示的东西。\n该紫色框为 Ingress Controller，当容器启动或停止时，会在 Kubernetes 中创建一个事件。然后，Ingress Controller 会识别该事件并做出相应的响应。\n如上图所示，Ingress Controlle 识别到容器已启动，并将其放入负载均衡池。这样，应用程序控制器（无论是在云之上还是内部）都可以保持最新状态。\n这减轻了管理员的负担，并且比手动管理效率更高。\n3. 南北向流量的安全 南北和东西方都是用来描述流量流向的通用术语。南北流量是指流量流入和流出 Kubernetes。\n如前所述，企业需要在 Kubernetes 前放置一些设备来监视流量。例如，防火墙，DDoS 防护或任何其他可捕获恶意流量的设备。\n这些设备在流量管理方面也很有用。因此，如果流量需要流向特定的区域，这是理想的选择。Ingress Controller 在这方面也可以提供很多帮助。\n如果企业可以通过统一的解决方案使这种功能自动化，那么他们可以得到：\n 更简化操作 更好的应用程序性能 可在不中断前端的情况下进行后端更改 自动化的安全策略  4. 为大规模部署准备的中央控制器 企业还需要考虑到横向扩展，特别是在安全性方面。\n如上图所示，Ingress Controller（由紫色框表示）仍然存在，但是这次它正在处理来自多个 Kubernetes 节点的请求，并且正在观测整个 Kubernetes 集群。\nIngress Controller 前方的蓝色圆圈是 A10 Networks Harmony Controller。这种控制器可以实现高效的负载分配，并且可以将信息快速发送到适当的位置。\n使用这样的中央控制器，必须选择一种在现有解决方案上进行少量额外配置，就可进行扩容和缩容的解决方案。\n5. 微服务之间的访问控制 与流入和流出 Kubernetes 的南北流量相反，东西向流量在 Kubernetes 节点之间流动。在上图中，您可以看到东西向流量是如何运作的。\n当流量在 Kubernetes 节点之间流动时，可以通过物理网络，虚拟网络或 overlay 网络来发送该流量。如果不通过某种方式来监控那些东西向的流量，那么对流量如何从一个 pod 或容器流向另一个 pod 或容器的了解就变得非常困难。\n另外，它还可能带来严重的安全风险：获得对一个容器的访问权限的攻击者可以访问整个内部网络。\n幸运的是，企业可以通过“服务网格”（例如 A10 Secure Service Mesh）来解决这个问题。通过充当容器之间的代理以实现安全规则，这可以确保东西向的流量安全，并且还可以帮助扩展，负载均衡，服务监视等。\n此外，服务网格可以在 Kubernetes 内部运行，而无需将流量发送到物理设备或 VM。使用服务网格，东西向的流量状况如下所示：\n通过这种解决方案，像金融机构这样的企业可以轻松地将信息保留在应有的位置，而不用担心影响安全性。\n6. 东西向流量加密 如果没有适当的加密，未加密的信息可能会从一个物理 Kubernetes 节点流到另一个。这是一个严重的问题，特别是对于需要处理特别敏感信息的金融机构和其他企业。\n这就是为什么对于企业而言，在评估云安全产品时，重要的是选择一种可以在离开节点时对流量进行加密，并在进入节点时对其进行解密的方法。\n供应商可以通过两种方式提供这种类型的保护：\n第一个选择是 Sidecar 代理部署，这种方法也是最受欢迎的。\n通过这样的部署，管理员可以告诉 Kubernetes，每当启动特定 pod 时，应在该 pod 中启动一个或多个其他容器。\n通常，其他容器是某种类型的代理，可以管理从 Pod 流入和流出的流量。\n从上图可以看出，Sidecar 代理部署的不利之处在于，每个 pod 都需要启动一个 Sidecar，因此将占用一定数量的资源。\n另一方面，企业也可以选择中心辐射代理部署。在这种类型的部署中，一个代理会处理从每个 Kubernetes 节点流出的流量。这样只需要较少的资源。\n7. 应用流量分析 最后一点是，企业了解应用程序层流量的详细信息至关重要。\n有了可同时监控南北和东西向流量的控制器，就已经有了两个理想的点来收集流量信息。\n这样做既可以帮助优化应用程序，又可以提高安全性，还可以拓展多种不同的功能。从最简单到最高级的顺序排列，这些功能可以实现：\n 通过描述性分析进行性能监控。大多数供应商都提供此功能。 通过诊断分析更快地进行故障排除。少数供应商提供此功能。 通过机器学习系统生成的预测分析获得建议。更少的供应商提供此功能。 通过真实直观的AI生成的规范分析进行自适应控制。只有最好，最先进的供应商才能提供此功能。  因此，当企业与供应商交流时，至关重要的是确定他们的产品可以提供哪些功能。\n使用 A10 Networks 的类似产品，可以查看大图分析以及相关的单个数据包，日志条目或问题。具有这种粒度的产品是企业应寻求的产品。\n关于开发和操作简便性的注意事项 最后，让我们看一下企业在 Kubernetes 中的流量和安全性方面应该追寻的东西。考虑这些因素还可以为开发和运维团队大大简化工作：\n 具有统一解决方案的简单体系结构。 集中管理和控制，便于进行分析和故障排除。 使用常见的配置格式，例如 YAML 和 JSON。 无需更改应用程序代码或配置即可实现安全性和收集分析信息。 自动化应用安全策略。  如果公司优先考虑以上这些，则企业可以在使用 Kubernetes 时享受简单、自动化和安全的流量。您的基础设施、架构和运维团队都会对此感到满意。\n 原文信息\n作者：Almas Raza、John Allen\n发表时间：31 Jul 2019 9:51am\n地址：https://thenewstack.io/7-requirements-for-optimized-traffic-flow-and-security-in-kubernetes/\n  ","date":1582006542,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1582006542,"objectID":"1366dddffa7101bb0fe34539f9fb7edc","permalink":"https://guoxudong.io/post/7-requirements-for-optimized-traffic-flow-and-security-in-kubernetes/","publishdate":"2020-02-18T14:15:42+08:00","relpermalink":"/post/7-requirements-for-optimized-traffic-flow-and-security-in-kubernetes/","section":"post","summary":"翻译【7 Requirements for Optimized Traffic Flow and Security in Kubernetes】","tags":["翻译","Kubernetes","安全","流量"],"title":"Kubernetes 中优化流量和安全性需要注意的7点要求","type":"post"},{"authors":["郭旭东"],"categories":["边缘计算"],"content":" 前言 k3s 是由 Rancher Labs 于2019年年初推出的一款轻量级 Kubernetes 发行版，满足在边缘计算环境中运行在 x86、ARM64 和 ARMv7 处理器上的小型、易于管理的 Kubernetes 集群日益增长的需求。\nk3s 除了在边缘计算领域的应用外，在研发侧的表现也十分出色。我们可以快速在本地拉起一个轻量级的 k8s 集群，而 k3d 则是 k3s 社区创建的一个小工具，可以在一个 docker 进程中运行整个 k3s 集群，相比直接使用 k3s 运行在本地，更好管理和部署。\n在日常工作中，时长要在本地集群和多个远程集群之间切换来完成运维工作，这时使用 kubecm 快速将 k3s 集群的 kubeconfig 与现有集群的 kubeconfig 合并，并可快速切换集群，开发运维两不误。\n安装 k3d k3d 提供了多种安装方式，十分方便。\n使用脚本安装 直接使用 wget 和 curl 安装\nwget -q -O - https://raw.githubusercontent.com/rancher/k3d/master/install.sh | bash # 或 curl -s https://raw.githubusercontent.com/rancher/k3d/master/install.sh | bash  安装指定版本\nwget -q -O - https://raw.githubusercontent.com/rancher/k3d/master/install.sh | TAG=v1.3.4 bash # 或 curl -s https://raw.githubusercontent.com/rancher/k3d/master/install.sh | TAG=v1.3.4 bash  使用 Homebrew 安装 MacOS 或安装了 Homebrew 的 Linux 可以使用 brew 安装：\nbrew install k3d  其他 还可以直接前往 release 页面 下载二进制可执行文件，或者直接使用 go install github.com/rancher/k3d 安装。\n创建 k3s 集群 创建 k3s 集群也十分简单，一行命令就可拉起，速度非常快。\n$ k3d create -n k3s-local INFO[0000] Created cluster network with ID facae4a046b169721805f93ec21ba1acb65b9efb8cf35866529178cb0fba75a9 INFO[0000] Created docker volume k3d-k3s-local-images INFO[0000] Creating cluster [k3s-local] INFO[0000] Creating server using docker.io/rancher/k3s:v1.0.1... INFO[0000] SUCCESS: created cluster [k3s-local] INFO[0000] You can now use the cluster with: export KUBECONFIG=\u0026quot;$(k3d get-kubeconfig --name='k3s-local')\u0026quot; kubectl cluster-info  但是一般情况下，如果没有梯子的话，k3s 集群虽然拉起来很快，但因为拉不到镜像，集群组件都无法正常拉起。\n$ export KUBECONFIG=\u0026quot;$(k3d get-kubeconfig --name='k3s-local')\u0026quot; $ kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE helm-install-traefik-8wxmr 0/1 ContainerCreating 0 3m30s metrics-server-6d684c7b5-j4sc7 0/1 ContainerCreating 0 3m30s coredns-d798c9dd-j6lpw 0/1 ContainerCreating 0 3m30s local-path-provisioner-58fb86bdfd-wv7sw 0/1 ContainerCreating 0 3m30s $ kubectl describe pod coredns-d798c9dd-j6lpw -n kube-system ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled \u0026lt;unknown\u0026gt; default-scheduler Successfully assigned kube-system/coredns-d798c9dd-j6lpw to k3d-k3s-local-server Warning FailedCreatePodSandBox 7s (x7 over 4m30s) kubelet, k3d-k3s-local-server Failed create pod sandbox: rpc error: code = Unknown desc = failed to get sandbox image \u0026quot;k8s.gcr.io/pause:3.1\u0026quot;: failed to pull image \u0026quot;k8s.gcr.io/pause:3.1\u0026quot;: failed to pull and unpack image \u0026quot;k8s.gcr.io/pause:3.1\u0026quot;: failed to resolve reference \u0026quot;k8s.gcr.io/pause:3.1\u0026quot;: failed to do request: Head https://k8s.gcr.io/v2/pause/manifests/3.1: dial tcp 64.233.189.82:443: i/o timeout  离线安装 如果没有梯子的话，就只能选择使用离线安装。\n下载离线镜像 前往 release 页面 下载指定版本的镜像，这里我们下载最新的 v1.17.2+k3s1 镜像。\n下载到 ~/airgap 目录中，并进行解压，将解压后的目录重命名为 1.17.2。\n运行离线镜像 这里再次运行 k3d，部署 k3s 集群。这里要注意的是，挂载离线镜像的话，必须使用 -i flag 来指定镜像版本，这里我们使用的是 v1.17.2+k3s1 版本，而镜像的 tag 则是 v1.17.2-k3s1，如果不确定 tag，可以去 DockerHub 上查看。\n$ k3d create -n k3s-local -i rancher/k3s:v1.17.2-k3s1 -v $(pwd)/airgap/v1.17.2/:/var/lib/rancher/k3s/agent/images/ INFO[0000] Created cluster network with ID 10b3fca995fcb491ae1fe1c901672bf6f0a0fd6f51785ba8403947d2773ebd43 INFO[0000] Created docker volume k3d-k3s-local-images INFO[0000] Creating cluster [k3s-local] INFO[0000] Creating server using docker.io/rancher/k3s:v1.17.2-k3s1... INFO[0000] SUCCESS: created cluster [k3s-local] INFO[0000] You can now use the cluster with: export KUBECONFIG=\u0026quot;$(k3d get-kubeconfig --name='k3s-local')\u0026quot; kubectl cluster-info  查看 k3s 集群组件启动状态：\n$ export KUBECONFIG=\u0026quot;$(k3d get-kubeconfig --name='k3s-local')\u0026quot; $ kubectl get pod -A -w NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-58fb86bdfd-7jzbw 0/1 ContainerCreating 0 6m35s kube-system coredns-d798c9dd-jhmds 1/1 Running 0 6m35s kube-system metrics-server-6d684c7b5-4x2cd 1/1 Running 0 6m35s kube-system traefik-6787cddb4b-9v7r4 0/1 ContainerCreating 0 16s kube-system svclb-traefik-fzrqj 0/2 ContainerCreating 0 15s kube-system helm-install-traefik-h8k2j 0/1 Completed 0 6m35s kube-system svclb-traefik-fzrqj 2/2 Running 0 21s  使用 kubecm 在 k3s 集群启动成功后，使用 kubecm，将 k3s 的 kubeconfig 与现有 kubeconfig 合并。\nkubecm add -f $(k3d get-kubeconfig --name='k3s-local') -n k3s -c  切换集群，选择 k3s。\n$ kubecm s Use the arrow keys to navigate: ↓ ↑ → ← and / toggles search Select Kube Context 😼 k3s(*) prod-tg test ↓ banma --------- Info ---------- Name: k3s Cluster: cluster-485d6mhcfm User: user-485d6mhcfm  现在就可以在本地使用 k3s 集群进行开发工作，而有运维工作的时候，使用 kubecm switch 快速切换集群。\n结语 k3s 同时支持 x86_64、ARM64 和 ARMv7 架构，它可以十分灵活地跨任何边缘基础架构工作。不提 k3s 在边缘计算领域的应用，与之前使用的 minikube 相比，k3s 裁剪掉了许多用不到的功能，并且安装更简单，启动更快，空间占用也更小。相信 k3s 在开发侧的作用也会越来越大，使云原生应用的开发更加的便利。\n","date":1581911499,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1581911499,"objectID":"3936f4eb6081098c2ab5bc8663ee5e7a","permalink":"https://guoxudong.io/post/k3d-k3s-kubecm/","publishdate":"2020-02-17T11:51:39+08:00","relpermalink":"/post/k3d-k3s-kubecm/","section":"post","summary":"使用 k3d 在本地快速搭建轻量级 k8s 集群 - k3s，并使用 kubecm 管理所有集群。","tags":["k3s","k3d","kubecm","Kubernetes"],"title":"K3d+Kubecm 本地开发运维两不误","type":"post"},{"authors":["郭旭东"],"categories":["肺炎疫情"],"content":" 前言 新型冠状病毒疫情汹涌而来，全国各地严防死守，而疫情的实时数据也通过不同的渠道，如微信城市服务的疫情动态订阅、支付宝的疫情实时追踪、新浪新闻的疫情实时动态等等，各种平台纷纷将疫情的实时动态进行展示，确保人们可以第一时间了解疫情的发展情况。\n而无论是哪一家的数据推送和展示，都是面向大众的，并不能个性化的展示我们最关心的那些数据，所以这时就需要自制一个疫情动态展示的 Dashboard 了。\n说到 Dashboard，第一个联想到的当然就是 Grafana 了，Grafana 是自2014年以来推出的多平台开源分析和交互式可视化软件。连接支持的数据源，它会提供 Web 图表的展示以及报警。终端用户可以通过插件进行拓展，从而使用交互式的查询及展示复杂的监控仪表盘。\n项目准备 明确目标，我们这里需要定制一个 Dashboard 用于展示疫情动态，由于我目前在上海，需要展示全国和上海的确诊、疑似、治愈和死亡病例数；同时还需要一个病例发展曲线，用来观察疫情发展趋势；各省区情况已经上海各区情况也是需要的。\nGrafana 只是一个展示数据的工具，首先需要的是数据源，目前市面上并没有可以直接用于 Grafana 的疫情数据源，这里我们需要：\n 需要一个 Grafana，无论是在你的笔记本电脑上，还是在你的 K8S 集群中（这里推荐使用 docker 进行运行 Grafana，如果部署在 K8S 集群中，那更好）。\n 安装 SimpleJson 插件，它可以将 json 格式的数据，用作 Grafana 的数据源。\n  开发数据源 数据源这里使用 Python Bottle 进行开发，当然你也可以选择 flask，都是一样的，我使用 Bottle 的原因是之前开发的 Grafana 数据源是使用 Bottle 开发的，这里直接拿来就可以用，调试配置甚至用于构建 docker 镜像的 Dockerfile 和用于部署 K8S 的 deploy.yaml 都有现成可以用的。使用 Python 开发 Grafana 数据源很简单，只有符合 SimpleJson 的格式要求即可。可以根据 Oz Nahum Tiram 的博文 Visualize almost anything with Grafana and Python 来学习如果使用 Python 作为 Grafana 的数据源。\n在对数据源的定制中，使用两种类型的的数据：\n timeserie 类型：\n用于展示全国（含港澳台）和上海地区的疫情实时动态，展示确诊、疑似、治愈和死亡数，并且展示较昨日增加的数量，绘制了【确诊/疑似】数和【治愈/死亡】数的对比曲线。\n这里只要将全国确诊数 gntotal 与 当前时间戳组合返回即可，其他指标也是这种方式。\n@app.post('/query') def query(): print(request.json) body = [] all_data = getDataSync() time_stamp = int(round(time.time() * 1000)) for target in request.json['targets']: name = target['target'] if name == 'gntotal': body.append({'target': 'gntotal', 'datapoints': [[all_data['gntotal'], time_stamp]]}) body = dumps(body) return HTTPResponse(body=body, headers={'Content-Type': 'application/json'})  table 类型：\n用于绘制中国各省确诊、疑似、治愈和死亡病例数表格，以及上海各区确诊、疑似、治愈和死亡病例数表格。\n取出数据中的名称以及确诊、疑似、治愈和死亡数，append 到 rows 中即可。\n@app.post('/query') def query(): print(request.json) body = [] all_data = getDataSync() sh_data = getShDataSync() if request.json['targets'][0]['type'] == 'table': rows = [] for data in all_data['list']: row = [data['name'], data['value'], data['susNum'], data['cureNum'], data['deathNum']] rows.append(row) sh_rows = [] for data in sh_data['city']: row = [data['name'], data['conNum'], data['susNum'], data['cureNum'], data['deathNum']] sh_rows.append(row) bodies = {'all': [{ \u0026quot;columns\u0026quot;: [ {\u0026quot;text\u0026quot;: \u0026quot;省份\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;name\u0026quot;}, {\u0026quot;text\u0026quot;: \u0026quot;确诊\u0026quot;, \u0026quot; type\u0026quot;: \u0026quot;conNum\u0026quot;}, {\u0026quot;text\u0026quot;: \u0026quot;疑似\u0026quot;, \u0026quot; type\u0026quot;: \u0026quot;susNum\u0026quot;}, {\u0026quot;text\u0026quot;: \u0026quot;治愈\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;cureNum\u0026quot;}, {\u0026quot;text\u0026quot;: \u0026quot;死亡\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;deathNum\u0026quot;} ], \u0026quot;rows\u0026quot;: rows, \u0026quot;type\u0026quot;: \u0026quot;table\u0026quot; }], 'sh': [{ \u0026quot;columns\u0026quot;: [ {\u0026quot;text\u0026quot;: \u0026quot;省份\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;name\u0026quot;}, {\u0026quot;text\u0026quot;: \u0026quot;确诊\u0026quot;, \u0026quot; type\u0026quot;: \u0026quot;value\u0026quot;}, {\u0026quot;text\u0026quot;: \u0026quot;疑似\u0026quot;, \u0026quot; type\u0026quot;: \u0026quot;susNum\u0026quot;}, {\u0026quot;text\u0026quot;: \u0026quot;治愈\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;cureNum\u0026quot;}, {\u0026quot;text\u0026quot;: \u0026quot;死亡\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;deathNum\u0026quot;} ], \u0026quot;rows\u0026quot;: sh_rows, \u0026quot;type\u0026quot;: \u0026quot;table\u0026quot; }]} series = request.json['targets'][0]['target'] body = dumps(bodies[series]) return HTTPResponse(body=body, headers={'Content-Type': 'application/json'})   选择展示 Panel 类型 总的来说，使用了4种 Panel 进行展示：\n 展示病例数的展示块，使用 Singlestat 展示数据对比曲线，使用 Graph 展示表格，使用 Table 文字标题，使用 Text  配置数据源 病例数展示块： 这里只有一个值，所以要选择 First。\n病例数发展趋势图： 这里将【确诊/疑似】和【治愈/死亡】数进行对比。\n数据表格： 效果 整体效果还可以，先已用作公司大屏展示疫情情况（这里我司用于展示屏幕较小，只不过是一个小米电视，故字体和展示块都做的大了一些）。\n构建 将代码打包成为 docker 镜像，就可以运行在任意环境以及 K8S 集群了，镜像已上传 dockerhub 直接拉取镜像，开箱即食。\n# Dockerfile FROM python:3.7.3-alpine3.9 LABEL maintainer=\u0026quot;sunnydog0826@gmail.com\u0026quot; COPY . /app RUN echo \u0026quot;https://mirrors.aliyun.com/alpine/v3.9/main/\u0026quot; \u0026gt; /etc/apk/repositories \\ \u0026amp;\u0026amp; apk update \\ \u0026amp;\u0026amp; apk add --no-cache gcc g++ python3-dev python-dev linux-headers libffi-dev openssl-dev make \\ \u0026amp;\u0026amp; pip3 install -r /app/requestments.txt -i http://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com WORKDIR /app ENTRYPOINT [\u0026quot;uwsgi\u0026quot;,\u0026quot;--ini\u0026quot;,\u0026quot;uwsgi.ini\u0026quot;]  运行  拉取镜像\ndocker pull guoxudongdocker/feiyan-datasource  运行镜像\ndocker run -d --name datasource -p 8088:3000 guoxudongdocker/feiyan-datasource  添加数据源\n选择 SimpleJson 类型的数据源，点击添加，填入数据源地址：\n 导入 Dashboard\n点击 Upload.json file，选择 wuhan2020-grafana/dashboard.json\n 使用 K8S 部署（可选）\nkubectl apply -f deploy.yaml   结语 截止目前（2020年2月14日），病例数还在不断的增加，但是疑似病例数趋势开始下降，可以看出，目前新型肺炎的确诊速度增加了；治愈数也在不断的增加；上海地区和其他地区比起来，虽然有大批返工人员进入，但是并没有增加特别多的病例数，各个社区严防死守的效果初显；同时上海一直保持着死亡1人的情况，而且中国首例新型肺炎治愈的也在上海。总的来说只要大家注意预防，待在家中，多消毒，多通风，一定可以战胜疫情，度过难关。\n导入 Dashboard 的 json 文件和部署 K8S 的 yaml 文件都可以在 GitHub 上找到。\n项目地址：https://github.com/sunny0826/wuhan2020-grafana\n","date":1581646372,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1581646372,"objectID":"f704c08f5bf2bfb7ea5c660f129eaf8c","permalink":"https://guoxudong.io/post/feiyan-grafana/","publishdate":"2020-02-14T10:12:52+08:00","relpermalink":"/post/feiyan-grafana/","section":"post","summary":"开发 Grafana Dashboard 展示新型肺炎疫情动态。","tags":["肺炎疫情"],"title":"使用 Grafana 展示肺炎疫情动态","type":"post"},{"authors":["郭旭东"],"categories":["效率"],"content":" 前言  “非常时期呼唤非常担当”   这是我司 CEO 张世伟在复工当天致全体员工公开信的题目。2020年，在第一个不走亲戚的春节过后，我们鼠年的工作，以远程办公的形式开始了。\n在进行了一周的远程办公之后，我们于2月10日正式复工，但是由于我抵沪自我隔离未满7日，只得继续在家远程办公，但是不能因为在家办公就放低对自己的要求，故结合在假期浏览的一系列介绍远程办公的文章，以及疫情防控知识，总结了这篇远程及在公司办公指南，希望对大家有所帮助。\n取消晨会 之所以将这个放在最前面，是由于这个在平时办公中的“正常操作”，在远程办公的环境下，成为了一场灾难。2月3日开始，企业开始复工，但由于疫情还在扩散，所以大家不约而同的选择了远程办公\u0026hellip;而往日里那些“稳如老狗”的远程办公软件，阿里巴巴的钉钉，腾讯的企业微信，华为的 WeLink 相继出现了比较大的延迟甚至崩溃现象。同时还有其他的原因：\n 家里的网络条件不佳，可能出现延迟或者掉线的现象 同一时间，只有能一个人说话，沟通效率很低 一些不必要的发言，比如需要单独沟通的问题，占用大量大家的时间 出现多人等一人的情况 😆 没洗脸，没洗头，素颜等其他问题  同时由于中小学线上课堂的开始，各路人民教师纷纷转型十八线主播，网络环境更是变的拥挤不堪。\n故取消线上晨会，这里就显的比较合理，同时可以采用异步的协同方式，通过 jira 和 confluence 进行工作的协同。利用 jira 实时更新自己任务的状态，项目组长与组员也可以实时通过 jira 了解项目的实时进度，这样就减少了面对面的沟通，安全又高效。\n例如：\n目标分级 远程办公比统一办公需要更加细粒度的工作分级，什么事情应该马上做，什么事情可以放一放，什么时候对接的同事有时间，这些都需要规划的更加仔细。\n将工作内容进行细粒度拆分，使用个人看板，时间管理工具甚至记事本拆分自己每天的工作时间安排，每周对自己工作的任务列表进行回顾，将已完成的标记，将本周新增加的任务加入进来，这样很容易让我们在周一的时候对工作的优先级有个快速梳理，并且不容易遗漏一些细节工作。\n推荐使用四象限法来对自己目前的工作进行分级，根据紧急情况和重要情况，对自己的工作进行分级，优先处理那些紧急且重要的事情，同样个人看板也可以帮助您记录完成的任务和待完成的任务，这样写日报/周报就可以直接拿来用，不用再绞尽脑汁回忆今天/本周都做了什么，个人看板这里推荐 Trello，个人免费使用。\n提升个人效率 清晰区分工作时间和休息时间 在家办公，工作效率很容易变差。毕竟沙发和床就在身边，很容易工作着突然就想去休息一会儿。这里就需要在家里划分出一个 【工作区】 ，并在工作区中准备上班所需的一切；同时要按时起床，穿戴整齐，吃好早点，然后进入工作区开始一天的工作，通过这种简单的 仪式感 可以让我们快速进入工作状态，更好的提高效率，不至于出现「在家工作居然比在公司下班晚」的情况。\n按日程工作 切记，在家办公只是地点变了，工作时间和内容并没有变。按照上文中提到的目标分级，合理的将目标分为日程，并在自己的看板中进行记录，保持钉钉/微信实时在线，防止突然出现紧急且重要的突发任务，同时也将非紧急问题都推后处理，让聚焦工作的整块时间尽量不受干扰。\n珍惜自己的时间也珍惜他人的时间 这是每个职场人应有的自律，少开无效会议，多用异步协同代替实时协同；非紧急事件给对方更多响应缓冲时间；能在 Confluence 里搜到的内容尽量不在群里提问。（运维组提供钉钉应答机器人，基础资源相关的问题，可以先 @ 群内机器人进行提问）。\n榜样作用 主要负责人自主性地高效率完成工作，尽量少传播负面的信息。负责人主动去执行公司规章和在疫情期间发挥表率作用，为其他同事提供良好的表率，小到请假流程的规范，大到对疫区同事的生活的关心和支持。榜样的作用，可以让同事们心安，更快的投入到工作当中，齐心协力，共同度过难关。\n安全保障  安全第一！安全第一！安全第一！重要的事情说三遍。   道路千万条，安全第一条。在这个特殊的时期，我们做的所有努力其实都是建立在自身安全的前提下，如果没有自身安全，那么一切工作就都变的没有意义了。\n下面介绍一些在不同场合中的安全建议：\n上班路上 尽量避免乘坐公共交通，建议步行、骑行或乘坐私家车上班。如必须乘坐公共交通工具时，务必全程佩戴口罩，途中尽量避免用手触摸车上物品。\n戴口罩的同时也要戴上手套（冬天的手套或者户外手套皆可），在地铁、汽车和公共场所不要摘下手套。同样要小心楼梯扶手、电梯按钮。尽量远离他人1米：特别是在公共场所时，应该适当与他人保持距离。如在路上时，尽可能远离其他人（至少1米）。\n到达园区之后，尽量走楼梯，不要乘电梯。\n照片提供人：@王沛\n办公室内  办公室内不开中央空调，请增添衣物   保持办公区环境清洁，建议每日通风3次，每次20-30分钟，通风时注意保暖。人与人之间保持1米以上距离，办公室中不要摘除口罩，并且不要打开中央空调，请增添衣物。\n到达公司的第一件事，先洗手，之后再清理工位，有条件的同学使用酒精或者消毒湿巾将手可以接触到的地方进行消毒，键盘、鼠标、笔记本、桌面等都是首要清洁目标。同时不要用手触摸眼睛，有条件的同学可以带上护目镜。\n会议室内 工作办公中减少会议（可以用远程会议方式代替），如果非要开会需要做到以下三点：\n 为参会人员配备一次性医用口罩等物品； 会场座位间隔建议1米以上； 会议时间较长的建议每小时通风1次。  午饭时间 饭前要洗手，建议自己带饭，带瓶装矿泉水，不要和别人一起进餐，尤其不要面对面进餐，避免肉类生食。建议营养配餐，清淡适口。\n保持乐观的心态 由于我的母亲是医生的缘故，在这次疫情期间多次听说有人感觉自己出现了肺炎症状，但是检测后却十分正常，这就是 过度焦虑 引发的身体不适。\n对于没有去过疫区，并且没有和疫区出来的人进行过接触的人，主要还是应该把精力放在预防和消毒上，不要疑神疑鬼，保持乐观的心态和充足的睡眠，相信我们一定能度过这次的难关！\n参考资料  疫情下的阿里员工，约定这样上班\n 调查完150个远程办公团队，我们发现了这几个真相\n 开工在即，在办公室中如何练成“百毒不侵”？\n  ","date":1581303146,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1581303146,"objectID":"684ee07e191732a3c83e7027cefa4b08","permalink":"https://guoxudong.io/post/work-4-home/","publishdate":"2020-02-10T10:52:26+08:00","relpermalink":"/post/work-4-home/","section":"post","summary":"在疫情期间，如何安全有效的 在家/公司 办公","tags":["远程办公","高效办公","安全办公"],"title":"疫情期间【在家/公司】远程办公指南","type":"post"},{"authors":["郭旭东"],"categories":["kubernetes"],"content":" 前言 该项目脱胎于 mergeKubeConfig 项目，最早写该项目的目的是在一堆杂乱无章的 kubeconfig 中自由的切换。随着需要操作的 Kubernetes 集群越来越多，在不同的集群之间切换也越来越麻烦，而操作 Kubernetes 集群的本质不过是通过 kubeconfig 访问 Kubernetes 集群的 API Server，以操作 Kubernetes 的各种资源，而 kubeconfig 不过是一个 yaml 文件，用来保存访问集群的密钥，最早的 mergeKubeConfig 不过是一个操作 yaml 文件的 Python 脚本。而随着 golang 学习的深入，也就动了重写这个项目的念头，就这样 kubecm 诞生了。\nkubecm kubecm 由 golang 编写，支持 Mac Linux 和 windows 平台，delete rename switch 提供比较实用的交互式的操作，目前的功能包括：\n add ：添加新的 kubeconfig 到 $HOME/.kube/config 中 completion ：命令行自动补全功能 delete：删除已有的 context ，提供交互式和指定删除两种方式 merge：将指定目录中的 kubeconfig 合并为一个 kubeconfig 文件 rename：重名指定的 context，提供交互式和指定重命名两种方式 switch：交互式切换 context  安装 kubecm 支持 Mac Linux 和 windows 平台，安装方式也比较简单：\nMacOS  使用 brew 或者直接下载二进制可执行文件   brew install sunny0826/tap/kubecm  Linux  下载二进制可执行文件   # linux x86_64 curl -Lo kubecm.tar.gz https://github.com/sunny0826/kubecm/releases/download/v${VERSION}/kubecm_${VERSION}_Linux_x86_64.tar.gz tar -zxvf kubecm.tar.gz kubecm cd kubecm sudo mv kubecm /usr/local/bin/  Windows  下载二进制可执行文件，并将文件移动到 $PATH 中即可   命令行自动补全  kubecm 提供了和 kubectl 一样的 completion 命令行自动补全功能（支持 bash/zsh）   以 zsh 为例，在 $HOME/.zshrc 中添加\nsource \u0026lt;(kubecm completion zsh)  然后使用 source 命令，使其生效\nsource $HOME/.zshrc  之后，在输入 kubecm 后按 tab 键，就可以看到命令行自动补全的内容\n操作 kubeconfig  kubecm 可以实现 kubeconfig 的查看、添加、删除、合并、重命名和切换   查看 # 查看 $HOME/.kube/config 中所有的 context kubecm  添加 # 添加 example.yaml 到 $HOME/.kube/config.yaml，该方式不会覆盖源 kubeconfig，只会在当前目录中生成一个 config.yaml 文件 kubecm add -f example.yaml # 功能同上，但是会将 example.yaml 中的 context 命名为 test kubecm add -f example.yaml -n test # 添加 -c 会覆盖源 kubeconfig kubecm add -f example.yaml -c  删除 # 交互式删除 kubecm delete # 删除指定 context kubecm delete my-context  合并 # 合并 test 目录中的 kubeconfig,该方式不会覆盖源 kubeconfig，只会在当前目录中生成一个 config.yaml 文件 kubecm merge -f test # 添加 -c 会覆盖源 kubeconfig kubecm merge -f test -c  重命名 # 交互式重命名 kubecm rename # 将 dev 重命名为 test kubecm rename -o dev -n test # 重命名 current-context 为 dev kubecm rename -n dev -c  切换默认 namespace # 交互式切换 namespace kubecm namespace # 或者 kubecm ns # 切换默认 namespace 为 kube-system kubecm ns kube-system  效果展示 视频介绍   结语 kubecm 项目的初衷为学习 golang 并熟悉 client-go 的使用，随着使用的深入，断断续续增加了不少功能，开发出了一个看上去还算正规的项目。总的来说都是根据自己的喜好来开发的业余项目，欢迎各位通过 ISSUE 来进行交流和讨论。\n","date":1575857266,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1575857266,"objectID":"c71ec97d1a710872078b68a80210ac7c","permalink":"https://guoxudong.io/post/kubecm/","publishdate":"2019-12-09T10:07:46+08:00","relpermalink":"/post/kubecm/","section":"post","summary":"介绍一款小工具：kubecm，帮助你管理杂乱无章的 kubeconfig。 ","tags":["kubernetes"],"title":"Kubecm：管理你的 kubeconfig","type":"post"},{"authors":["郭旭东"],"categories":["kubernetes"],"content":" 前言 这次要介绍一个 Kubernetes 资源观测工具，实时监控 Kubernetes 集群中各种资源的新建、更新和删除，并实时通知到各种协作软件/聊天软件，目前支持的通知渠道有：\n slack hipchat mattermost flock webhook  我这边开发了钉钉的通知渠道，但是在上游 ISSUE#198 中提出的贡献请求并没有得到回应，所以这边只能 fork 了代码，然后自己进行了开发，以支持钉钉通知。\n安装 这里推荐使用 helm 进行安装，快速部署\nhelm install kubewatch stable/kubewatch \\ --set rbac.create=true \\ --set slack.channel='#YOUR_CHANNEL' \\ --set slack.token='xoxb-YOUR_TOKEN' \\ --set resourcesToWatch.pod=true \\ --set resourcesToWatch.daemonset=true  如果想使用钉钉通知，则可以在 GitHub 上拉取我的代码，代码中包含 helm chart 包，可直接进行安装\ngit clone https://github.com/sunny0826/kubewatch-chat.git cd kubewatch-chat helm install kubewatch kubewatch \\ --set dingtalk.sign=\u0026quot;XXX\u0026quot; \\ --set dingtalk.token=\u0026quot;XXXX-XXXX-XXXX\u0026quot;  钉钉配置 在钉钉中创建 智能群助手 ，之后\n获取 token 复制的 webhook 中 https://oapi.dingtalk.com/robot/send?access_token={YOUR_TOKEN}, {YOUR_TOKEN} 就是要填入的 token。\n安全设置 钉钉智能群助手在更新后新增了安全设置，提供三种验证方式 自定义关键词 加签 IP地址（段），这里推荐使用 IP地址（段）的方式，直接将 Kubernetes 集群的出口 IP 填入设置即可。同时也提供了 加签 的方式，拷贝秘钥，将其填入 dingtalk.sign 中。\n项目配置 编辑 kubewatch/value.yaml ，修改配置\n## Global Docker image parameters ## Please, note that this will override the image parameters, including dependencies, configured to use the global value ## Current available global Docker image parameters: imageRegistry and imagePullSecrets ## # global: # imageRegistry: myRegistryName # imagePullSecrets: # - myRegistryKeySecretName slack: enabled: false channel: \u0026quot;\u0026quot; token: \u0026quot;xoxb\u0026quot; hipchat: enabled: false # room: \u0026quot;\u0026quot; # token: \u0026quot;\u0026quot; # url: \u0026quot;\u0026quot; mattermost: enabled: false # channel: \u0026quot;\u0026quot; # url: \u0026quot;\u0026quot; # username: \u0026quot;\u0026quot; flock: enabled: false # url: \u0026quot;\u0026quot; webhook: enabled: false # url: \u0026quot;\u0026quot; dingtalk: enabled: true token: \u0026quot;\u0026quot; sign: \u0026quot;\u0026quot; # namespace to watch, leave it empty for watching all. namespaceToWatch: \u0026quot;\u0026quot; # Resources to watch resourcesToWatch: deployment: true replicationcontroller: false replicaset: false daemonset: false services: false pod: true job: false persistentvolume: false image: registry: docker.io # repository: bitnami/kubewatch repository: guoxudongdocker/kubewatch-chart # tag: 0.0.4-debian-9-r405 tag: latest pullPolicy: Always ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistryKeySecretName ## String to partially override kubewatch.fullname template (will maintain the release name) ## # nameOverride: ## String to fully override kubewatch.fullname template ## # fullnameOverride: rbac: # If true, create \u0026amp; use RBAC resources # create: true serviceAccount: # Specifies whether a ServiceAccount should be created create: true # The name of the ServiceAccount to use. # If not set and create is true, a name is generated using the fullname template name: resources: {} # limits: # cpu: 100m # memory: 300Mi # requests: # cpu: 100m # memory: 300Mi # Affinity for pod assignment # Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity # affinity: {} # Tolerations for pod assignment # Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ tolerations: [] # Node labels for pod assignment # Ref: https://kubernetes.io/docs/user-guide/node-selection/ nodeSelector: {} podAnnotations: {} podLabels: {} replicaCount: 1  使用 value.yaml 安装\ngit clone https://github.com/sunny0826/kubewatch-chat.git cd kubewatch-chat helm install my-release -f kubewatch/values.yaml  Slack 配置 Slack 为 kubewatch 默认的通知软件，这里就不简介 Slack 的安装和注册，直接从创建 APP 开始\n创建一个 APP 进去创建 APP 页面\n选择 App Name 和 Development Slack Workspace\n添加 Bot 用户 添加 App 到 Workspace 获取 Bot-token 通知效果 在 Slack 中，创建 更新 删除 分别以绿、黄和红色代表\n在钉钉中，我进行了汉化\n结语 对于 kubewatch 我们这里主要用作监控各种 CronJob 的定时触发状态，已经 ConfigMap 和 Secrets 的状态变化，同时也观察 HPA 触发的弹性伸缩的状态，可以实时观测到业务高峰的到来，是一个不错的小工具。\n","date":1575450591,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1575450591,"objectID":"f2b91b63275f7fbab7fa86f4fcc1705d","permalink":"https://guoxudong.io/post/kubewatch/","publishdate":"2019-12-04T17:09:51+08:00","relpermalink":"/post/kubewatch/","section":"post","summary":"用于观测 Kubernetes 资源情况，并实时通知到各种协作软件/聊天软件","tags":["kubernetes","容器"],"title":"小工具介绍：KubeWatch","type":"post"},{"authors":["郭旭东"],"categories":["kubernetes"],"content":" 前言 在近日的一个风和日丽的下午，正在快乐的写 bug 时，突然间钉钉就被 call 爆了，原来是 k8s 测试集群的一个 namespace 突然不见了。这个 namespace 里面有 60 多个服务，瞬间全部没有了……虽然得益于我们的 CI/CD 系统，这些服务很快都重新部署并正常运行了，但是如果在生产环境，那后果就是不可想象的了。在排查这个问题发生的原因的同时，集群资源的灾备和恢复功能就提上日程了，这时 Velero 就出现了。\nVelero Velero 是 VMWare 开源的 k8s 集群备份、迁移工具。可以帮助我们完成 k8s 的例行备份工作，以便在出现上面问题的时候可以快速进行恢复。同时也提供了集群迁移功能，可以将 k8s 资源迁移到其他 k8s 集群的功能。Velero 将集群资源保存在对象存储中，默认情况下可以使用 AWS、Azure、GCP 的对象存储，同时也给出了插件功能用来拓展其他平台的存储，这里我们用到的就是阿里云的对象存储 OSS，阿里云也提供了 Velero 的插件，用于将备份存储到 OSS 中。下面我就介绍一下如何在阿里云容器服务 ACK 使用 Velero 完成备份和迁移。\n Velero 地址：https://github.com/vmware-tanzu/velero\nACK 插件地址：https://github.com/AliyunContainerService/velero-plugin\n 下载 Velero 客户端 Velero 由客户端和服务端组成，服务器部署在目标 k8s 集群上，而客户端则是运行在本地的命令行工具。\n 前往 Velero 的 Release 页面 下载客户端，直接在 GitHub 上下载即可 解压 release 包 将 release 包中的二进制文件 velero 移动到 $PATH 中的某个目录下 执行 velero -h 测试  创建 OSS bucket 创建一个 OSS bucket 用于存储备份文件，这里也可以用已有的 bucket，之后会在 bucket 中创建 backups、metadata、restores三个目录，这里建议在已有的 bucket 中创建一个子目录用于存储备份文件。\n创建 OSS 的时候一定要选对区域，要和 ACK 集群在同一个区域，存储类型和读写权限选择标准存储和私有：\n创建阿里云 RAM 用户 这里需要创建一个阿里云 RAM 的用户，用于操作 OSS 以及 ACK 资源。\n 新建权限策略\n策略内容：\n{ \u0026quot;Version\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: [ \u0026quot;ecs:DescribeSnapshots\u0026quot;, \u0026quot;ecs:CreateSnapshot\u0026quot;, \u0026quot;ecs:DeleteSnapshot\u0026quot;, \u0026quot;ecs:DescribeDisks\u0026quot;, \u0026quot;ecs:CreateDisk\u0026quot;, \u0026quot;ecs:Addtags\u0026quot;, \u0026quot;oss:PutObject\u0026quot;, \u0026quot;oss:GetObject\u0026quot;, \u0026quot;oss:DeleteObject\u0026quot;, \u0026quot;oss:GetBucket\u0026quot;, \u0026quot;oss:ListObjects\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;*\u0026quot; ], \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot; } ] }  新建用户\n在新建用户的时候要选择 编程访问，来获取 AccessKeyID 和 AccessKeySecret，这里请创建一个新用于用于备份，不要使用老用户的 AK 和 AS。\n  部署服务端  拉取 Velero 插件 到本地\ngit clone https://github.com/AliyunContainerService/velero-plugin  配置修改\n 修改 install/credentials-velero 文件，将新建用户中获得的 AccessKeyID 和 AccessKeySecret 填入，这里的 OSS EndPoint 为之前 OSS 的访问域名（注：这里需要选择外网访问的 EndPoint。）：\nALIBABA_CLOUD_ACCESS_KEY_ID=\u0026lt;ALIBABA_CLOUD_ACCESS_KEY_ID\u0026gt; ALIBABA_CLOUD_ACCESS_KEY_SECRET=\u0026lt;ALIBABA_CLOUD_ACCESS_KEY_SECRET\u0026gt; ALIBABA_CLOUD_OSS_ENDPOINT=\u0026lt;ALIBABA_CLOUD_OSS_ENDPOINT\u0026gt;  修改 install/01-velero.yaml，将 OSS 配置填入：\n--- apiVersion: velero.io/v1 kind: BackupStorageLocation metadata: labels: component: velero name: default namespace: velero spec: config: {} objectStorage: bucket: \u0026lt;ALIBABA_CLOUD_OSS_BUCKET\u0026gt; # OSS bucket 名称 prefix: \u0026lt;OSS_PREFIX\u0026gt; # bucket 子目录 provider: alibabacloud --- apiVersion: velero.io/v1 kind: VolumeSnapshotLocation metadata: labels: component: velero name: default namespace: velero spec: config: region: \u0026lt;REGION\u0026gt; # 地域，如果是华东2（上海），则为 cn-shanghai provider: alibabacloud  k8s 部署 Velero 服务\n# 新建 namespace kubectl create namespace velero # 部署 credentials-velero 的 secret kubectl create secret generic cloud-credentials --namespace velero --from-file cloud=install/credentials-velero # 部署 CRD kubectl apply -f install/00-crds.yaml # 部署 Velero kubectl apply -f install/01-velero.yaml  测试 Velero 状态\n$ velero version Client: Version: v1.1.0 Git commit: a357f21aec6b39a8244dd23e469cc4519f1fe608 Server: Version: v1.1.0  可以看到 Velero 的客户端和服务端已经部署成功。\n 服务端清理\n在完成测试或者需要重新安装时，执行如下命令进行清理：\nkubectl delete namespace/velero clusterrolebinding/velero kubectl delete crds -l component=velero    备份测试 velero-plugin 项目中已经给出 example 用于测试备份。\n 部署测试服务\nkubectl apply -f examples/base.yaml  对 nginx-example 所在的 namespace 进行备份\nvelero backup create nginx-backup --include-namespaces nginx-example --wait  模拟 namespace 被误删\nkubectl delete namespaces nginx-example  使用 Velero 进行恢复\nvelero restore create --from-backup nginx-backup --wait   集群迁移 迁移方法同备份，在备份后切换集群，在新集群恢复备份即可。\n高级用法  定时备份\n对集群资源进行定时备份，则可在发生意外的情况下，进行恢复（默认情况下，备份保留 30 天）。\n# 每日1点进行备份 velero create schedule \u0026lt;SCHEDULE NAME\u0026gt; --schedule=\u0026quot;0 1 * * *\u0026quot; # 每日1点进行备份，备份保留48小时 velero create schedule \u0026lt;SCHEDULE NAME\u0026gt; --schedule=\u0026quot;0 1 * * *\u0026quot; --ttl 48h # 每6小时进行一次备份 velero create schedule \u0026lt;SCHEDULE NAME\u0026gt; --schedule=\u0026quot;@every 6h\u0026quot; # 每日对 web namespace 进行一次备份 velero create schedule \u0026lt;SCHEDULE NAME\u0026gt; --schedule=\u0026quot;@every 24h\u0026quot; --include-namespaces web  定时备份的名称为：\u0026lt;SCHEDULE NAME\u0026gt;-\u0026lt;TIMESTAMP\u0026gt;，恢复命令为：velero restore create --from-backup \u0026lt;SCHEDULE NAME\u0026gt;-\u0026lt;TIMESTAMP\u0026gt;。\n 备份删除\n直接执行命令进行删除\nvelero delete backups \u0026lt;BACKUP_NAME\u0026gt;  备份资源查看\n备份查看\nvelero backup get  查看定时备份\nvelero schedule get  查看可恢复备份\nvelero restore get  备份排除项目\n可为资源添加指定标签，添加标签的资源在备份的时候被排除。\n# 添加标签 kubectl label -n \u0026lt;ITEM_NAMESPACE\u0026gt; \u0026lt;RESOURCE\u0026gt;/\u0026lt;NAME\u0026gt; velero.io/exclude-from-backup=true # 为 default namespace 添加标签 kubectl label -n default namespace/default velero.io/exclude-from-backup=true   问题汇总 时区问题 进行定时备份时，发现备份使用的事 UTC 时间，并不是本地时间，经过排查后发现是 velero 镜像的时区问题，在调整后就会正常定时备份了，这里我重新调整了时区，直接调整镜像就好，修改 install/01-velero.yaml 文件，将镜像替换为 registry-vpc.cn-shanghai.aliyuncs.com/keking/velero:latest。\nimage: caption: \u0026quot;Image from: [**Pexels**](https://www.pexels.com)\u0026quot; focal_point: \u0026quot;\u0026quot; preview_only: false --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: velero namespace: velero spec: replicas: 1 selector: matchLabels: deploy: velero template: metadata: annotations: prometheus.io/path: /metrics prometheus.io/port: \u0026quot;8085\u0026quot; prometheus.io/scrape: \u0026quot;true\u0026quot; labels: component: velero deploy: velero spec: serviceAccountName: velero containers: - name: velero # sync from gcr.io/heptio-images/velero:latest image: registry-vpc.cn-shanghai.aliyuncs.com/keking/velero:latest # 修复时区后的镜像 imagePullPolicy: IfNotPresent command: - /velero args: - server - --default-volume-snapshot-locations=alibabacloud:default env: - name: VELERO_SCRATCH_DIR value: /scratch - name: ALIBABA_CLOUD_CREDENTIALS_FILE value: /credentials/cloud volumeMounts: - mountPath: /plugins name: plugins - mountPath: /scratch name: scratch - mountPath: /credentials name: cloud-credentials initContainers: - image: registry.cn-hangzhou.aliyuncs.com/acs/velero-plugin-alibabacloud:v1.2 imagePullPolicy: IfNotPresent name: velero-plugin-alibabacloud volumeMounts: - mountPath: /target name: plugins volumes: - emptyDir: {} name: plugins - emptyDir: {} name: scratch - name: cloud-credentials secret: secretName: cloud-credentials  版本问题 截止发稿时，Velero 已经发布了 v1.2.0 版本，目前 ACK 的 Velero 的插件还未升级。\n结语 近日正好有 k8s 集群服务迁移服务的需求，使用 Velero 完成了服务的迁移，同时也每日进行集群资源备份，其能力可以满足容器服务的灾备和迁移场景，实测可用，现已运行在所有的 k8s 集群。\n","date":1573607602,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1573607602,"objectID":"c12bbc3b7b62a701db3f5eb2dd453273","permalink":"https://guoxudong.io/post/aliyun-velero/","publishdate":"2019-11-13T09:13:22+08:00","relpermalink":"/post/aliyun-velero/","section":"post","summary":"本文介绍了使用 Velero 来进行 k8s 集群资源进行备份和迁移。","tags":["阿里云","kubernetes","velero"],"title":"使用 Velero 进行集群备份与迁移","type":"post"},{"authors":["郭旭东"],"categories":["kubernetes"],"content":" 前言 对于阿里云用户来说，阿里云监控是一个很不错的产品，首先它在配额内使用是免费的！免费的！免费的！重要的事情说三遍。他的功能类似于 zabbix，但是比 zabbix 提供了更多的监控项，基本上在云上使用的资源都可以通过云监控来实时监控。而它提供的开箱即用方式，天然集成云资源，并提供多种告警方式，免去了监控与告警系统搭建与维护的繁琐，并且减少了资源的消耗，比购买 ECS 自己搭建 zabbix 要少消耗很多资源。同时阿里云监控和阿里云其他服务一样，也提供了比较完整的 OpenApi 以及各种语言的 sdk，可以基于阿里云的 OpenApi 将其与自己的系统集成。我们之前也是这么做的，但是随着监控项的增加，以及经常需要在办公场地监控投屏的专项监控页，光凭我们的运维开发工程师使用 vue 写速度明显跟不上，而且页面的美观程度也差很多。\n手写前端 VS Grafana 手写前端虽然可定制化程度更高，但是需要消耗大量精力进行调试，对于运维人员，哪怕是运维开发也是吃不消的（前端小哥哥和小姐姐是不会来帮你的，下图就是我去年拿 vue 写的伪 Grafana 展示页面，花费了大约一周时间在调整这些前端元素）。 Grafana 则标准化程度很高，展示也更加符合大众审美，某些定制化需求可以通过自定义 DataSource 或者 AJAX 插件的 iframe 模式完成。开发后端 DataSource 肯定就没有前端调整 css 那么痛苦和耗时了，整体配置开发一个这样的页面可能只消耗一人天就能完成。而在新产品上线时，构建一个专项监控展示页面速度就更快了，几分钟内就能完成。 关于阿里云监控 云监控（CloudMonitor）是一项针对阿里云资源和互联网应用进行监控的服务。\n云监控为云上用户提供开箱即用的企业级开放型一站式监控解决方案。涵盖 IT 设施基础监控，外网网络质量拨测监控，基于事件、自定义指标、日志的业务监控。为您全方位提供更高效、更全面、更省钱的监控服务。通过提供跨产品、跨地域的应用分组管理模型和报警模板，帮助您快速构建支持几十种云产品、管理数万实例的高效监控报警管理体系。通过提供 Dashboard，帮助您快速构建自定义业务监控大盘。使用云监控，不但可以帮助您提升您的系统服务可用时长，还可以降低企业 IT 运维监控成本。\n云监控服务可用于收集获取阿里云资源的监控指标或用户自定义的监控指标，探测服务可用性，以及针对指标设置警报。使您全面了解阿里云上的资源使用情况、业务的运行状况和健康度，并及时收到异常报警做出反应，保证应用程序顺畅运行。\n关于 Grafana Grafana 是一个跨平台的开源的度量分析和可视化工具，可以通过将采集的数据查询然后可视化的展示，并及时通知。由于云监控的 Grafana 还没有支持告警，所以我们这里只用了 Grafana 的可视化功能，而告警本身就是云监控自带的，所以也不需要依赖 Grafana 来实现。而我们的 Prometheus 也使用了 Grafana 进行数据可视化，所以有现成的 Grafana-Server 使用。\n阿里云监控对接 Grafana 首先 Grafana 服务的部署方式这里就不做介绍了，请使用较新版本的 Grafana，最好是 5.5.0+。后文中也有我开源的基于阿里云云监控的 Grafana 的 helm chart，可以使用 helm 安装，并会直接导入云监控的指标，这个会在后文中介绍。\n安装阿里云监控插件 进入插件目录进行安装\ncd /var/lib/grafana/plugins/ git clone https://github.com/aliyun/aliyun-cms-grafana.git service grafana-server restart  如果是使用 docker 或者部署在 k8s 集群，这里也可以使用环境变量在 Grafana 部署的时候进行安装\n... spec: containers: - env: - name: GF_INSTALL_PLUGINS # 多个插件请使用,隔开 value: grafana-simple-json-datasource,https://github.com/aliyun/aliyun-cms-grafana/archive/master.zip;aliyun-cms-grafana ...  您也可以下载 aliyun-cms-grafana.zip 插件解压后，上传服务器的 Grafana 的 plugins 目录下，重启 grafana-server 即可。\n配置云监控 DataSource  Grafana 启动后，进入 Configuration 页面，选择 DataSource Tab 页，单击右上方的Add data source，添加数据源。 选中CMS Grafana Service，单击select。  填写配置项，URL 根据云监控所在地域填写，并且填写阿里云账号的 accessKeyId 和 accessSecret，完成后单击Save\u0026amp;Test。   创建 Dashboard  单击 Create -\u0026gt; Dashboard -\u0026gt; Add Query 配置图标，数据源选择之前添加的 CMS Grafana Service，然后文档中的配置项填入指标即可（这里要注意的是，云监控 API 给返回的只有实例 ID，并没有自定义的实例名称，这里需要手动将其填入 Y - column describe 中；而且只支持输入单个 Dimension，若输入多个，默认选第一个，由于这些问题才有了后续我开发的 cms-grafana-builder 的动机）。  配置参考 云产品监控项，   使用 helm chart 的方式部署 Grafana 项目地址：https://github.com/sunny0826/cms-grafana-builder\ncms-grafana-builder 由于上文中的问题，我们需要手动选择每个实例 ID 到 Dimension 中，并且还要讲该实例的名称键入 Y - column describe 中，十分的繁琐，根本不可能大批量的输入。\n这就是我开发这个 Grafana 指标参数生成器的原因，起初只是一个 python 脚本，用来将我们要监控的指标组装成一个 Grafana 可以使用 json 文件，之后结合 Grafana 的容器化部署方法，将其做成了一个 helm chart。可以在启动的时候自动将需要的参数生成，并且每日会对所有指标进行更新，这样就不用每次新购或者释放掉资源后还需要再跑一遍脚本。\n部署 只需要将项目拉取下来运行 helm install 命令\nhelm install my-release kk-grafana-cms \\ --namespace {your_namespace} \\ --set access_key_id={your_access_key_id} \\ --set access_secret={your_access_secret} \\ --set region_id={your_aliyun_region_id} \\ --set password={admin_password}  更多详情见 github README，欢迎提 issue 交流。\n指标选择 在部署成功后，可修改 ConfigMap：grafana-cms-metric，然后修改对应的监控指标项。\n效果 ECS: RDS: EIP: Redis: 后记 为了满足公司需求，后续还开发 DataSource 定制部分，用于公司监控大屏的展示，这部分是另一个项目，不在这个项目里，就不细说了，之后有机会总结后再进行分享。\n","date":1573096116,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1573096116,"objectID":"17fd9893f68861c9afc27193aae1220d","permalink":"https://guoxudong.io/post/aliyun-cms-grafana/","publishdate":"2019-11-07T11:08:36+08:00","relpermalink":"/post/aliyun-cms-grafana/","section":"post","summary":"本文介绍使用 Grafana 展示阿里云监控指标的方法，并提供了使用 helm chart 一键部署包含阿里云监控 dashboard 的 Grafana-Server。","tags":["阿里云","kubernetes","grafana"],"title":"使用 Grafana 展示阿里云监控指标","type":"post"},{"authors":["郭旭东"],"categories":["kubernetes"],"content":" 前言 9月25日晚受邀来到阿里云飞天园区参加阿里云MVP产品夜谈，在会上遇到了容器服务团队的负责人易立，并就容器服务进行了交流。此次参加夜谈的除了来自全球各地的阿里云MVP，还有来自安全团队、容器团队、AIoT 团队、大数据团队、数据库团队、人工智能团队、中间件团队、搜索引擎\u0026amp;智能推荐团队的负责人\u0026amp;产品经理。各个参会的MVP可以根据自己的研究方向或者感兴趣的方向选择，直接与团队负责人面对面交流，获取阿里云产品的最新信息，并提出使用意见，促进产品的发展。由于主要从事云原生\u0026amp;容器方面的工作，我选择了容器团队，与阿里云容器服务团队负责人易立就容器服务进行交流，本文记录了部分交流内容。\n容器服务交流 关于集群版本\u0026amp;集群升级 众所周知，Kubernetes 以非常稳定的3个月发布一个版本速度在高速迭代这，Kubernetes v1.16.0 也即将 release ，但是目前 ACK 的 Kubernetes 版本依旧为 v1.12.6-aliyun.1 已落后官方4个大版本。得到的回复是新版本 1.14 已经上线，下周就可以升级了，1.14 版本之前已经上线，只不过一直在灰度测试，下周（2019年9月29日）就全面放开升级了。截止写这篇文章的时候，我们的多个 Kubernetes 集群已成功升级到 v1.14.6-aliyun.1 ，虽然在升级的时候出现了一点小问题，但是最后还是顺利解决了。\n然后就是集群升级的问题，集群升级的时候会建议对所有节点打快照，确保节点安全，但是如果在节点升级当中失败，就会出现一半为新版本节点，一半为旧版本节点的问题。我们的一个节点升级失败，就出现了上述问题，最后还是将该节点容器驱散，并将该节点移出集群才解决了升级问题。希望集群升级提供整体状态保存\u0026amp;回退功能，确保如果升级失败（或者出现新旧版本不兼容问题）的时候可以安全回退到之前版本。\n关于容器服务前端展示 ACK 的 WEB 界面相对简陋，一直以来都是对 Kubernetes Dashboard 进行了简单的包装，和其他公有云相比确实不如。不过这也不是容器服务独有的问题，阿里云你产品众多，大部分都有这样的问题。与易立交流得知，容器服务团队目前主要的任务还是确保 Kubernetes 集群的安全稳定运行，他们在安全和可用性上花费的大量精力，貌似并没有拿到什么前端开发资源。我注意到像费用中心、日志服务等产品都有了新版页面，这里希望能容器服务页面也能尽快改版，提高页面操作的便捷和美观。\n关于授权管理 一直以来容器服务都有授权管理功能，后来都基于RAM重新做了授权管理功能。但是RAM权限管理策略十分复杂，配置起来也很麻烦，不同的策略结构和语法学习起来非常困难。在配置和管理起来非常困难，我们只能把所有权限收回，每项权限都要根据需求提工单来进行配置，还时长会出现配置不生效的问题。而且这个问题一提出，就引起了大家的共鸣，后了解得知，为了安全合规的要求，操作便捷和安全合规没法兼顾。这里希望授权管理上能在确保合规的同时，能提升RAM操作的便捷性。\n后记 关于容器服务的交流主要是以上几点，其他的还包括监控、存储和 CI/CD 方面进行了交流，同时也获得了不少建议。当面给阿里云提需求的机会并不多，我也是抓住机会，把日常使用 ACK 的问题汇总之后一股脑的丢了出去。有类似需求的同学可以在阿里云的聆听平台上给阿里云提交建议，以我的经验，合理的需求会很快审核通过并排期开发，换句话说就是“人人都可以是阿里云的产品经理”。\n","date":1569807155,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1569807155,"objectID":"d76de41331167f4ac1c804c8958592b7","permalink":"https://guoxudong.io/post/aliyun-product-meetup/","publishdate":"2019-09-30T09:32:35+08:00","relpermalink":"/post/aliyun-product-meetup/","section":"post","summary":"9月25日晚受邀来到阿里云飞天园区参加阿里云MVP产品夜谈，在会上遇到了容器服务团队的负责人易立，并就容器服务进行了交流。","tags":["阿里云","容器"],"title":"阿里云产品夜谈-容器服务交流","type":"post"},{"authors":["郭旭东"],"categories":["公益"],"content":" 前言 本次参加云栖大会，除了受到阿里云开发者社区的邀请，同时也受到了阿里码上公益团队的邀请，于9月25日下午参加了阿里巴巴技术公益专场。说来惭愧，作为一个码农关注并加入码上公益已半年有余，但是除了在平台上以自己的经验来给出各种建议外，并没有贡献什么实质的代码，这也可能是因为我专职运维开发，在devops和效能提升上有很多自己的见解，但对于前端UI和各种官网的构建并不是很在行。\n阿里巴巴与公益 一直以来我对于公益的理解还是停留在很浅的阶段，而且很长一段时间以来，公益还是停留在有限的圈子内，像我这样的技术人与公益完全就是两个世界。而阿里巴巴推出的码上公益则改变了这一点，这个平台让我了解到，原来公益除了捐助之外还能以自己的技术能力做出更大的贡献。此次技术公益专场，阿里发布了技术公益基金，我也是第一次见到这么多的阿里合伙人，可见公益在阿里巴巴集团内部的重要。阿里巴巴不只在集团内部推进公益，而且还为我们这样的普通人提供了像码上公益、蚂蚁森林、一书等平台和产品，让越来越多的人参与到公益事业中，让更多的人平等的享有技术红利。\n让代码更有温度 参加码上公益的初衷只不过是想为公益事业做一些力所能及的事情，尤其是使用代码这种方式，在公益事业上展示我们技术人的才华。也符合与我的价值观：技术让世界更美好。我们技术人可以通过一种比捐助更有温度的方式：代码，来让我们的世界越来越美好。\n在公益专场中有幸结识了 Michael HERMANN 老师，作为一个德国人，他却操这一口流利的中文，十几年如一日的在中国偏远的乡村中为那里的孩子带去教育，同时也为孩子们带去了希望。在专场后的公益沟通会有幸与 Michael 老师一桌，这是一位可爱的老人，十分关注中国偏远地区孩子们的教育问题，在他的身上我看到了一个作为公益人的坚持，在他眼里国籍、文化、地域都没有差别，有的只是希望这些生活在偏远地区的孩子都享有受教育的权利，十分值得敬佩。同时还结识了许多志同道合的码农朋友，大家都希望用自己温暖的代码为公益事业做出自己的贡献。\n我们只有一个地球 之后《用现代科技助力中国虎豹保护》的主题分享，让我了解到了，环境的保护也是公益中很重要的一部分。而科技的发展，让从事动物保护的人员可以做的更仔细更完善，对于AI、大数据的应用，使得对于野生动物的保护更精准。技术赋能野生动物保护，可以更好的保护野生动物，同时也让动物保护人员的工作更轻松，让这些常年在大山密林中保护野生动物的科学家不再那么辛苦，同时更好的保护野生动物。\n后记 之后南都公益基金会理事长徐永光分享了《互联网带来的时代改变》，了解到了互联网公益并不只是刚刚兴起，而是一直在努力；而联合国世界粮食计划署驻华代表屈四喜则带来了《为了“零饥饿”目标》。感谢阿里巴巴提供了这个机会，让我这样的技术人可以参与到公益事业当中，为公益事业贡献出自己的力量。\n","date":1569721931,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1569721931,"objectID":"160982f9deb28dbfa40c1ac5a98e2f76","permalink":"https://guoxudong.io/post/alibaba-public-welfare/","publishdate":"2019-09-29T09:52:11+08:00","relpermalink":"/post/alibaba-public-welfare/","section":"post","summary":"9月25日下午参加阿里巴巴技术公益专场记录与感想。","tags":["公益"],"title":"当码农遇见公益","type":"post"},{"authors":["郭旭东"],"categories":["devops"],"content":" 前言 使用 drone CI 已有小半年，在将原有项目的 CI 系统从 jenkins 向 drone 迁移的时候，也陆陆续续遇到了一些问题。在这段时间，也完成了使用官方插件到插件定制的转变，使得 drone CI 流程更贴合我们 devops 开发流程。通过这篇文章总结一下目前我们对 drone 进行的一些定制化开发以及使用技巧，由于 drone 官方的文档不是很详细，所以也希望通过这种方法来和其他使用 drone 的用户分享和交流使用经验。\n并行构建 在默认情况下，drone 会按照步骤执行，但是有时会遇到前后端在同一个 repo 的情况，这时使用并行构建就可以省去很多的构建时间。\n 构建流程：\n 在下面的示例里会展示一个如下流程：repo 中包含一个由 Java 写的服务以及一个 vue 前端项目，maven 构建和 npm 构建同时进行，maven 构建成功后会镜像 docker 镜像构建并上传镜像仓库，docker 构建成功后会镜像 k8s 部署，部署成功后会进行 vue 项目前端发布，在 k8s 部署成功并且前端发布成功后，进行钉钉构建成功同时，否则进行钉钉构建失败通知。\n前端构建 ———————————— 前端发布 \\ / \\ \\ / 钉钉通知 \\ / / 后端构建 —— 镜像构建 —— k8s部署 ——————   .drone.yml 配置\n kind: \u0026quot;pipeline\u0026quot; name: \u0026quot;default\u0026quot; steps: - name: \u0026quot;Maven编译\u0026quot; image: \u0026quot;guoxudongdocker/drone-maven\u0026quot; commands: - \u0026quot;mvn clean install\u0026quot; depends_on: [ \u0026quot;clone\u0026quot; ] - name: \u0026quot;构建镜像\u0026quot; image: \u0026quot;guoxudongdocker/drone-docker\u0026quot; settings: username: from_secret: \u0026quot;docker_user\u0026quot; password: from_secret: \u0026quot;docker_pass\u0026quot; dockerfile: \u0026quot;Dockerfile\u0026quot; repo: \u0026quot;registry-vpc.cn-shanghai.aliyuncs.com/guoxudong/test\u0026quot; registry: \u0026quot;registry-vpc.cn-shanghai.aliyuncs.com\u0026quot; tags: \u0026quot;${DRONE_BUILD_NUMBER}\u0026quot; depends_on: [ \u0026quot;Maven编译\u0026quot; ] - name: \u0026quot;Kubernetes 部署\u0026quot; image: \u0026quot;guoxudongdocker/kubectl\u0026quot; settings: config: \u0026quot;deploy/overlays/uat\u0026quot; timeout: 300 check: false depends_on: [ \u0026quot;构建镜像\u0026quot; ] - name: \u0026quot;前端构建\u0026quot; image: \u0026quot;guoxudongdocker/node-drone\u0026quot; commands: - \u0026quot;npm install\u0026quot; - \u0026quot;npm run build\u0026quot; depends_on: [ \u0026quot;clone\u0026quot; ] - name: \u0026quot;前端上传\u0026quot; image: \u0026quot;guoxudongdocker/node-drone\u0026quot; commands: - \u0026quot;do something\u0026quot; depends_on: [ \u0026quot;前端构建\u0026quot;,\u0026quot;Kubernetes 部署\u0026quot; ] - name: \u0026quot;钉钉通知\u0026quot; image: \u0026quot;guoxudongdocker/drone-dingtalk\u0026quot; settings: token: from_secret: \u0026quot;dingding\u0026quot; type: \u0026quot;markdown\u0026quot; message_color: true message_pic: true sha_link: true depends_on: [ \u0026quot;前端上传\u0026quot;,\u0026quot;Kubernetes 部署\u0026quot; ] when: status: - \u0026quot;failure\u0026quot; - \u0026quot;success\u0026quot;  多子项目构建 在使用 drone 中遇到的最大问题就是，我们有很多项目都是在一个 repo 中有很多子项目，而每个子项目都是 k8s 中的一个服务，这时一个 .drone.yml 文件很难把所有的服务都囊括。而又不想每个子项目拉一个分支管理，当前的模式就很不合适。\n插件开发 针对这个问题，我们对 drone 进行了定制化开发，会在每次提交代码后，对新提交的代码和老代码进行比较，筛选出做了修改的子项目，然后对有修改的子项目尽心 CI ，其余的子项目则不进行发布。\n而以上的方式仅适用于测试环境的快速迭代，生产环境则采用 tag 的模式，针对不同的子项目，打不同前缀的 tag ，比如子项目为 test1 ，则打 test1-v0.0.1 的 tag，就会对该子项目进行生产发布。\n构建效果  有修改的子项目\n 无修改的子项目\n  Kubernetes 发布状态检查 之前的 Kubernetes 发布只是将服务发布到 Kubernetes 集群，并不管服务是否正常启动。针对这个问题以及我们的 Kubernetes 应用管理模式，我们开发了 drone 的 Kubernetes 发布插件，该插件包括 kubectl 、kustomize、kubedog ，来完善我们的 Kubernetes 发布 step 。\n .drone.yml\n steps: - name: Kubernetes 部署 image: guoxudongdocker/kubectl volumes: - name: kube path: /root/.kube settings: check: false # 该参数为是否开启子模块检查 config: deploy/overlays/uat # 这里使用 kustomize ,详细使用方法请见 https://github.com/kubernetes-sigs/kustomize timeout: 300 # kubedog 的检测超时 name: {your-deployment-name} # 如果开启子模块检查则需要填入子模块名称 ... volumes: - name: kube host: path: /tmp/cache/.kube # kubeconfig 挂载位置 trigger: branch: - master # 触发 CI 的分支  使用该插件会如果为测试构建，则会自动设置 docker 镜像 tag 为 DRONE_BUILD_NUMBER ；如果为生产构建（git tag），则叫自动设置 docker 镜像 tag 为 DRONE_TAG ，然后通过 kubectl apply -k . 进行部署，同时使用 kubedog 进行部署状态检查，如果服务正常启动则该 step 通过，如果超时或者部署报错则该 step 失败。\n结语 根据我们目前的开发模式，对 drone 插件进行了全方位的开发。由于 dockerhub 的镜像拉取经常超时，则将镜像推送到了我们自己的镜像仓库；对钉钉通知也进行了优化；同时也根据我们目前的开发语言进行了插件的开发，提供了基于 Java 、Python 以及 Node.js 的 drone 插件，基本可以满足我们现在的 CI 需求，但随着 drone 的深入使用，越来越多的问题将会暴露出来。后续将会不断解决遇到的问题，持续优化。\n","date":1568181189,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1568181189,"objectID":"3492d1fe277fe03d425b7875868ae380","permalink":"https://guoxudong.io/post/drone-optimize/","publishdate":"2019-09-11T13:53:09+08:00","relpermalink":"/post/drone-optimize/","section":"post","summary":"通过这篇文章总结一下目前我们对 drone 进行了一些定制化开发以及使用技巧，由于 drone 官方的文档不是很详细，所以也希望通过这种方法来和其他使用 drone 的用户分享和交流使用经验。","tags":["devops","drone","工具"],"title":"玩转 Drone CI","type":"post"},{"authors":["郭旭东"],"categories":["部署安装"],"content":" 本文档介绍了在 Github / Gitee 的静态页托管Pages服务部署SSL证书，配置HTTPS安全访问的操作说明。\nPages服务 Github/Gitee的Pages是一个免费的静态网页托管服务，您可以使用Github或码云Pages托管博客、项目官网等静态网页。常见的静态站点生成器有：Hugo、Jekyll、Hexo等，可以用来生成静态站点。默认情况下，托管的站点使用 github.io / gitee.io 域名来访问站点，同时也支持自定义域名，并配置强制使用HTTPS。\n 注意：如果要在 Gitee Pages 上配置自定义域名+HTTPS，则需要开启 Gitee Pages Pro 。\n Github Pages 服务部署SSL证书 前提条件  GitHub 仓库 开启 GitHub Pages  证书签发  购买证书后点击申请\n 证书申请\n如果该域名是由阿里云购买，则选择自动DNS验证，如果不是在阿里云购买的，可以选择手动验证。\n 证书签发\n证书通过申请后，会收到证书签发的邮件。\n  设置自定义域名  解析域名\n在证书签发成功后，添加DNS解析，将绑定了SSL证书的域名解析到 YourRepo.github.io 。\n 配置域名\n解析之后将域名添加到 Custom domain 并且点击 Save ，Github会自动验证，出现Your site is published at https://YourDomainName.com/则证明解析成功。\n  Gitee Pages Pro 服务部署SSL证书 前提条件  Gitee 仓库 开启 Gitee Pages Pro   Gitee 需要开启 Gitee Pages Pro 服务才支持自定义域名+HTTPS。\n 证书签发 证书签发同 Github Pages。这里介绍非阿里云购买的域名，进行证书申请。\n 购买证书流程如上\n 申请证书\n证书验证方式选择手工DNS验证。\n 拷贝验证信息\n拷贝验证信息内的记录值。\n 验证解析\n进入购买域名所在网站进行DNS解析，这里以name.com为例：\n解析成功之后，返回阿里云SSL证书管理页面点击验证.\n 证书签发\n签发成功后会收到签发成功的邮件。\n  设置自定义域名  解析域名\n进入域名所在网站，添加DNS解析记录，将绑定了SSL证书的域名解析到gitee.gitee.io\n 配置域名\n 域名添加到自定义域名\n 配置证书\n 证书下载，选择 nginx 类型。\n gitee pages 配置证书，将证书文件与私钥文件贴入并提交。\n 勾选强制使用HTTPS，并保存。\n    验证 在Github/Gitee配置成功之后，您可在浏览器中输入 https://www.YourDomainName.com 验证证书安装结果。可以正常访问静态托管站点，并且浏览器地址栏显示绿色的小锁标识说明证书安装成功。\n","date":1566524215,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1566524215,"objectID":"deaf3224dbe7ec2fb608a59877ff08b7","permalink":"https://guoxudong.io/post/aliyun-ssl/","publishdate":"2019-08-23T09:36:55+08:00","relpermalink":"/post/aliyun-ssl/","section":"post","summary":"本文档介绍了在 Github/Gitee 的静态页托管Pages服务部署SSL证书，配置HTTPS安全访问的操作说明。","tags":["阿里云"],"title":"GitHub/Gitee 静态页托管页部署SSL证书","type":"post"},{"authors":["郭旭东"],"categories":["kubernetes"],"content":" 前言 最近对公司 Kubernetes 集群的 nginx-ingress-controller 进行了升级，但是升级后却出现了大问题，之前所有采用 nginx.ingress.kubernetes.io/rewrite-target: / 注释进行重定向的 Ingress 路由全部失效了，但是那些直接解析了域名，没有进行重定向的却没有发生这个问题。\n问题分析  首先检查对应服务健康状态，发现所有出问题的服务的状态均正常，同时受影响的之后 http 调用，而 RPC 调用却不受影响，这时问题就定位到了 ingress。 然后检查 nginx-ingress-controller ，发现 nginx-ingress-controller 的状态也是正常的，路由也是正常的。 最后发现受影响的只有添加了重定向策略的 ingress 。  问题解决 问题已经定位，接下来就是着手解决问题，这时候值得注意的就是之前进行了什么变更：升级了 nginx-ingress-controller 版本！看来问题就出现在新版本上，那么就打开官方文档：https://kubernetes.github.io/ingress-nginx/examples/rewrite/ 看一下吧。\nAttention  Starting in Version 0.22.0, ingress definitions using the annotation nginx.ingress.kubernetes.io/rewrite-target are not backwards compatible with previous versions. In Version 0.22.0 and beyond, any substrings within the request URI that need to be passed to the rewritten path must explicitly be defined in a capture group.\n 文档上给出了非常明显的警告⚠️：从 V0.22.0 版本开始将不再兼容之前的入口定义，再查看一下我的 nginx-ingress-controller 版本，果然问题出现来这里。\nNote  Captured groups are saved in numbered placeholders, chronologically, in the form $1, $2 \u0026hellip; $n. These placeholders can be used as parameters in the rewrite-target annotation.\n 示例 到这里问题已经解决了，在更新了 ingress 的配置之后，之前所有无法重定向的服务现在都已经可以正常访问了。修改见如下示例：\n$ echo ' apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/rewrite-target: /$2 name: rewrite namespace: default spec: rules: - host: rewrite.bar.com http: paths: - backend: serviceName: http-svc servicePort: 80 path: /something(/|$)(.*) ' | kubectl create -f -  结语 解决这个问题的实际时间虽然不长，但是着实让人出了一身冷汗，同时也给了我警示：变更有风险，升级需谨慎。在升级之前需要先浏览新版本的升级信息，同时需要制定完善的回滚策略，确保万无一失。\n","date":1565925337,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1565925337,"objectID":"76420f90a66ee6197f23f863e31ea15b","permalink":"https://guoxudong.io/post/nginx-ingress-error/","publishdate":"2019-08-16T11:15:37+08:00","relpermalink":"/post/nginx-ingress-error/","section":"post","summary":"记录 Nginx-Ingress 重定向失败问题的解决过程。","tags":["kubernetes","容器"],"title":"解决 Nginx-Ingress 重定向失败问题","type":"post"},{"authors":null,"categories":["翻译"],"content":"  随着3年前重构 Dailymotion 核心API的决定，我们希望提供一种更有效的方式来托管应用程序，促进我们的开发和生产工作流程。 最终决定使用容器编排平台来实现这一目标，那么自然就选择了 Kubernetes。\n 那么为什么要建立自己的Kubernetes平台？\n借由 Google Cloud 快速推动的 API 投入生产  2016年夏\n 三年前，在 Vivendi 收购 Dailymotion 之后，所有开发团队都专注于一个目标：提供全新的 Dailymotion 产品。\n根据对容器、编排解决方案和以前的经验的分析，使我们确信 Kubernetes 是正确的选择。许多开发人员已经掌握了这一概念并知道如何使用 Kubernetes ，这对我们的基础设施转型来说是一个巨大的优势。在基础架构方面，我们需要一个强大而灵活的平台来托管这些新型的云原生应用程序。而公有云为我们提供了极大的便利，于是我们决定在 Google Kubernetes Engine 上部署我们的应用程序，即使之后我们也会在自己的数据中心中进行混合部署。\n为何选择 GKE ？ 我们做出这个选择主要是出于技术原因，但也因为我们需要快速提供基础设施来满足 Dailymotion 的业务需求。并且对托管的应用程序（如地理分布，可伸缩性和弹性）有一些要求。\nDailymotion 的 GKE 集群\nDailymotion 作为一个全球性的视频平台，需要通过减少延迟来改善用户体验。之前我们仅在巴黎提供 API ，但这样并非最佳，我们希望能够在欧洲、亚洲以及美国托管我们的应用程序。\n这种延迟限制意味着我们在平台的网络设计方面面临着巨大的挑战。大多数云供应商要求我们在每个地区创建一个网络，并将所有这些网络通过 VPN 与托管服务互连，但 Google Cloud 允许我们在所有 Google 地区创建一个完全路由的单一网络，该网络在运营方面提供了便利并提高了效率。\n此外，Google Cloud 的网络和负载均衡服务非常棒。它可以将我们的用户路由到最近的集群，并且在发生故障的情况下，流量会自动路由到另一个区域而无需任何人为干预。\nGoogle 负载均衡监控\n我们的平台同样需要使用 GPU，而 Google Cloud 允许我们以非常有效的方式直接在我们的 Kubernetes 集群中使用它们。\n所有这一切使我们在启动后6个月开始接入 Google Cloud 基础架构上的生产流量。\n但是，尽管具有整体优势，但使用共有云服务还是要花费不少成本。这就是为什么我们要评估采取的每项托管服务，以便将来将其内部化。事实上，我们在2016年底开始构建我们的本地集群，并启动了我们的混合策略。\n在 Dailymotion 的内部构建容器编排平台  2016年秋\n 看到整个技术栈已经准备好在生产环境中应用，但API仍在开发中，这使得我们有时间专注搭建我们的本地集群。\nDailymotion 多年来在全球拥有自己的内容分发网络，每月有超过30亿的视频播放量。显然，我们希望利用现有的优势并在我们现有的数据中心部署自己的 Kubernetes 集群。\n我的目前拥有6个数据中心的2500多台服务器。所有这些都使用 Saltstack 进行配置，我们开始准备所有需要的公式来创建主节点、工作节点以及 Etcd 集群。\n网络部分 我们的网络是一个完全路由的网络。每个服务器使用Exabgp通过网络广播自己的IP。我们比较了几个网络插件， Calico 使用的是三层网络，因此这是唯一满足我们需求的网络插件。\n由于我们想要重用基础架构中的所有现有工具，首先要解决的问题是插入一个自制网络工具（我们所有服务器都使用它），通过我们的 Kubernetes 节点通过网络广播 IP 范围。我们让 Calico 为 pod 分配 IP，但不使用它与我们的网络设备进行BGP会话。路由实际上是由Exabgp处理的，它宣布了Calico使用的子网。这使我们可以从内部网络访问任何pod，尤其是来自我们的负载均衡器。\n我们如何管理入口流量 为了将传入的请求路由到正确的服务，我们希望使用 Ingress Controllers 与 Kubernetes 的入口资源集成。\n3年前，nginx-ingress-controller 是最成熟的控制器 ，并且 Nginx 已经使用多年，并以其稳定性和性能而闻名。\n在我们的设计中，我们决定在专用的 10Gbps 刀片服务器上托管我们的控制器。每个控制器都插入其所属集群的 kube-apiserver 端点。在这些服务器上，我们还使用Exabgp来广播公共或私有IP。我们的网络拓扑允许我们使用来自这些控制器的BGP将所有流量直接路由到我们的pod，而无需使用NodePort服务类型。这样可以避免节点之间的水平流量，从而提高效率。\n从 Internet 到 pods 的流量\n现在我们已经看到了我们如何构建混合平台，我们可以深入了解流量迁移本身。\n将流量从 Google Cloud 迁移到 Dailymotions 基础架构  2018年秋\n 经过近2年的构建、测试和微调，我们发现自己拥有完整的 Kubernetes 技术栈，可以接收部分流量。\n目前，我们的路由策略非常简单，但足以解决我们的问题。除了我们的公共IP（Google Cloud和Dailymotion）之外，我们还使用AWS Route 53 来定义策略并将终端用户流量引入我们选择的集群。\n使用Route 53的路由策略示例\n在 Google Cloud 上很简单，因为我们为所有群集使用唯一的IP，并且用户被路由到他最近的 GKE 群集。对于我们来说，我们不使用相同的技术，因此我们每个群集都有不同的IP。\n在此次迁移过程中，我们将目标国家逐步纳入我们的集群并分析其收益。\n由于我们的GKE集群配置了自动调节自定义指标，因此它们会根据传入流量进行扩展/缩小。\n在正常模式下，区域的所有流量都路由到我们的内部部署集群，而GKE集群则使用Route 53提供的运行状况检查作为故障转移。\n结语 我们接下来的步骤是完全自动化我们的路由策略，以实现自动混合策略，不断增强我们的用户体验。在效益方面，我们大大降低了云的成本，甚至改善了API响应时间。我们相信我们的云平台足以在需要时处理更多流量。\n","date":1565071290,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1565071290,"objectID":"6e3a65efbe515e1971fed4e2b26a9107","permalink":"https://guoxudong.io/post/how-we-built-our-hybrid-kubernetes-platfor/","publishdate":"2019-08-06T14:01:30+08:00","relpermalink":"/post/how-we-built-our-hybrid-kubernetes-platfor/","section":"post","summary":"随着3年前重构 Dailymotion 核心API的决定，我们希望提供一种更有效的方式来托管应用程序，促进我们的开发和生产工作流程。 最终决定使用容器编排平台来实现这一目标，那么自然就选择了 Kubernetes。","tags":["翻译","kubernetes"],"title":"如何构建混合Kubernetes平台","type":"post"},{"authors":["郭旭东"],"categories":["go"],"content":" 前言 经过上篇文章，我们已经可以在 GitHub 上构建一个看上去正经的 Golang 项目了，但这并不是结束。一个真正的用 Golang 构建的可执行文件是可以在全平台运行的，前文中我们使用 goreleaser 发布了一个非常漂亮的 release 页面，并在 release 页面上提供多平台的可执行文件下载。但是如果只是拿着可执行文件到处拷贝执行，总归不够优雅，所以这里就介绍如何在 Homebrew 上发布自己的 Golang 应用，如何像各种牛逼的项目那样使用 brew 一键安装自己的项目。\nHomebrew 对于使用 macOS 的用户来说，Homebrew 一点也不陌生，它类似于 CentOS 的 yum 和 Ubuntu 的 apt-get 。\nHomebrew 是最初由 Max Howell 用 Ruby 写的 OS X 软件管理系统，其代码开源在 GitHub 上。\nHomebrew 给自己贴了个一句话简介：The missing package manager for OS X。翻译过来成中文就是：macOS 缺失的软件包的管理器。名副其实地是，Homebrew 真的很好用。安装、升级、卸载等操作，在 Homebrew 的生态下，都只需要一条命令就可以了。并且 Homebrew 会自动为你解决软件包的依赖问题。\n发布步骤  创建 Formula 修改 rb 脚本 本地测试 创建 Tap 实际安装  创建 Formula 首先进入 release 页面，拷贝 macOS 的 Darwin 包地址\n然后通过命令在本地创建 Formula\nbrew create https://github.com/sunny0826/kubecm/releases/download/v0.0.1/kubecm_0.0.1_Darwin_x86_64.tar.gz  通过这个命令，brew 会创建一个名为 kubecm.rb 的文件在 /usr/local/Homebrew/Library/Taps/homebrew/homebrew-core/Formula/ 目录。\n修改 rb 脚本 然后在该目录可以看到 kubecm.rb 这个脚本：\n# Documentation: https://github.com/Homebrew/brew/blob/master/share/doc/homebrew/Formula-Cookbook.md # http://www.rubydoc.info/github/Homebrew/brew/master/Formula # PLEASE REMOVE ALL GENERATED COMMENTS BEFORE SUBMITTING YOUR PULL REQUEST! class OtfccMac64 \u0026lt; Formula desc \u0026quot;\u0026quot; homepage \u0026quot;\u0026quot; url \u0026quot;https://github.com/sunny0826/kubecm/releases/download/v0.0.1/kubecm_0.0.1_Darwin_x86_64.tar.gz\u0026quot; version \u0026quot;0.0.1\u0026quot; sha256 \u0026quot;f393b14f9c42c890b8a60949b13a8f9b5c3f814daa8b18901656ccc3b866f646\u0026quot; # depends_on \u0026quot;cmake\u0026quot; =\u0026gt; :build depends_on :x11 # if your formula requires any X11/XQuartz components def install # ENV.deparallelize # if your formula fails when building in parallel # Remove unrecognized options if warned by configure system \u0026quot;./configure\u0026quot;, \u0026quot;--disable-debug\u0026quot;, \u0026quot;--disable-dependency-tracking\u0026quot;, \u0026quot;--disable-silent-rules\u0026quot;, \u0026quot;--prefix=#{prefix}\u0026quot; # system \u0026quot;cmake\u0026quot;, \u0026quot;.\u0026quot;, *std_cmake_args system \u0026quot;make\u0026quot;, \u0026quot;install\u0026quot; # if this fails, try separate make/make install steps end test do # `test do` will create, run in and delete a temporary directory. # # This test will fail and we won't accept that! It's enough to just replace # \u0026quot;false\u0026quot; with the main program this formula installs, but it'd be nice if you # were more thorough. Run the test with `brew test otfcc-win32`. Options passed # to `brew install` such as `--HEAD` also need to be provided to `brew test`. # # The installed folder is not in the path, so use the entire path to any # executables being tested: `system \u0026quot;#{bin}/program\u0026quot;, \u0026quot;do\u0026quot;, \u0026quot;something\u0026quot;`. system \u0026quot;false\u0026quot; end end  默认提供的脚本并不适合我们，修改这个脚本：\nclass Kubecm \u0026lt; Formula desc \u0026quot;Merge multiple kubeconfig\u0026quot; homepage \u0026quot;https://github.com/sunny0826/kubecm\u0026quot; url \u0026quot;https://github.com/sunny0826/kubecm/releases/download/v0.0.1/kubecm_0.0.1_Darwin_x86_64.tar.gz\u0026quot; version \u0026quot;0.0.1\u0026quot; sha256 \u0026quot;8c2766e7720049ba0ce9e3d20b7511796a6ba224ce1386cd1d4ef8cc6e1315cd\u0026quot; # depends_on \u0026quot;cmake\u0026quot; =\u0026gt; :build def install bin.install \u0026quot;kubecm\u0026quot; end end  分布填上 desc 、 homepage 、url 等信息，由于这里下载的是 darwin 包，所以直接在 install 中填上 bin.install \u0026quot;kubecm\u0026quot; 即可。\n本地测试 保存脚本，然后使用 brew install kubecm 进行测试，查看结果：\nUpdating Homebrew... Fast-forwarded master to origin/master. Fast-forwarded master to origin/master. ==\u0026gt; Auto-updated Homebrew! Updated 2 taps (sunny0826/tap, homebrew/cask). ==\u0026gt; Updated Formulae sunny0826/tap/kubecm ==\u0026gt; Downloading https://github.com/sunny0826/kubecm/releases/download/v0.0.1/kubecm_0.0.1_Darwin_x86_64.tar.gz Already downloaded: /Users/guoxudong/Library/Caches/Homebrew/kubecm-86.64.tar.gz 🍺 /usr/local/Cellar/kubecm/86.64: 5 files, 5.4MB, built in 1 second  可以看到已经安装成功了！\n创建 Tap 在本地测试成功之后，就可以把他发布了。这里需要在 GitHub 上创建一个名为 homebrew-tap 的 repo 注意该 repo 需要以 homebrew- 为前缀，像是这样。\n然后将刚才的 kubecm.rb 脚本上传到这个 repo ，然后就可以通过 brew 的方式安装了。\n实际测试 发布好之后，就可以测试发布成功没有了。\n首先卸载之前使用本地脚本安装的应用：\n$ brew uninstall kubecm Uninstalling /usr/local/Cellar/kubecm/86.64... (5 files, 5.4MB)  然后使用\nbrew tap sunny0826/tap \u0026amp;\u0026amp; brew install kubecm  或者\nbrew install sunny0826/tap/kubecm  来进行安装。\n结语 到这我们就成功的在 Homebrew 上发布了自己的 Golang 应用，本篇中的方法仅适合 Golang 开发的二进制可执行文件的发布，其他语言的发布需要在 .rb 脚本上有所修改，更多内容请参考官方文档。这里要再介绍一下我用 Golang 开发的另一个小工具 kubecm ，该项目之前我是使用 python 开发的，用于合并多个 kubeconfig 文件，本次重写新增了查看所有 kubeconfig 和 删除 kubeconfig 中 context 等功能，同时也在 Homebrew 上发布，欢迎拍砖。\n","date":1564043277,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1564043277,"objectID":"ce459a58346e0302631338d1a5b9458b","permalink":"https://guoxudong.io/post/golang-to-homebrew/","publishdate":"2019-07-25T16:27:57+08:00","relpermalink":"/post/golang-to-homebrew/","section":"post","summary":"本文介绍如何在 Homebrew 上发布 Golang 项目。","tags":["go"],"title":"Golang 装逼指南 Ⅱ：在 Homwebrew 上发布 Golang 项目","type":"post"},{"authors":["郭旭东"],"categories":["go"],"content":" 前言 接触 golang 时间很长，但是真正动手开始写 golang 也就是在最近。虽然写的不多，但是见过的 golang 项目可是不计其数，从 Kubernetes 和 istio 到亲身参与的 kustomize 再到 Kubernetes 生态圈的众多小工具，比如： kubeval 、 kubedog 等。从项目使用者和贡献者的角度接触了各种形形色色的 golang 项目。作为一个开发人员，在享受各种开源项目带来便利的同时，也希望自己动手开发一个 golang 项目。以我阅项目无数的经验，那么肯定要构建一个看上去正规的 GitHub 项目。\nGoLand 设置 Go 开发环境的安装网上教程很多，这里就不做介绍了。这里主要介绍一下在 GoLand 上开发环境的设置，这里的设置主要在 MacOS 上进行，其他系统可能有所不同。\n使用Goland IDE vgo vgo 是基于 Go Module 规范的包管理工具，同官方的 go mod 命令工具类似。\n 开启 vgo，GoLand-\u0026gt;Preferences-\u0026gt;GO-\u0026gt;Go Modules(vgo)  手动修改 go.mod\n其中 latest 为最新版本，GoLand 会去下载最新依赖代码，下载成功后会修改 go.mod 并且生成 go.sum 依赖分析文件。\nmodule github.com/sunny0826/hamal go 1.12 require ( github.com/mitchellh/go-homedir latest github.com/spf13/cobra latest github.com/spf13/viper latest )  更新成功\n在更新成功后，会生成 go.sum 文件并修改 go.mod 文件。\nmodule github.com/sunny0826/hamal go 1.12 require ( github.com/mitchellh/go-homedir v1.1.0 github.com/spf13/cobra v0.0.5 github.com/spf13/viper v1.4.0 )  使用快捷键 ⌥(option)+↩(return) 或者点击鼠标右键, 选择 Sync packages of github.com/sunny0826/hamal 在 import 处导入依赖。\n  配置代理 如果要选出 golang 最劝退一个原因，那么依赖下载难肯定得票最高！这个时候一个合适的梯子就很重要了，如果没有这个梯子，上面的这步就完全无法完成。这里主要介绍 GoLand 上的配置，Shadowsocks 的安装和配置就不做介绍了。\nGoLand-\u0026gt;Preferences-\u0026gt;Appearance \u0026amp; Behavior-\u0026gt;System Settings-\u0026gt;HTTP Proxy 这里设置好之后，别忘了点击 Check connection 测试一下梯子搭成没有。 配置 go fmt、 goimports 和 golangci-lint 这三个工具都是 GoLand 自带的，设置起来十分简单:GoLand-\u0026gt;Preferences-\u0026gt;Tools-\u0026gt;File Watchers，点击添加即可。之后在写完代码之后就会自动触发这3个工具的自动检测，工具作用：\n go fmt : 统一的代码格式化工具。 golangci-lint : 静态代码质量检测工具，用于包的质量分析。 goimports : 自动 import 依赖包工具。  安装配置 golint GoLand 没有自带 golint 工具，需要手动安装：\nmkdir -p $GOPATH/src/golang.org/x/ cd $GOPATH/src/golang.org/x/ git clone https://github.com/golang/lint.git git clone https://github.com/golang/tools.git cd $GOPATH/src/golang.org/x/lint/golint go install  安装成功之后将会在 $GOPATH/bin 目录下看到自动生成了 golint 二进制工具文件。\nGoLand 配置 golint，修改 Name, Program, Arguments 三项配置，其中 Arguments 需要加上 -set_exit_status 参数，如图所示：\nTravis CI 持续集成 在 Github 上装逼怎么能少的了 Travis CI ，直接登录 Travis CI，使用 GitHub 登录，然后选择需要使用 Travis CI 的项目，在项目根目录添加 .travis.yml ，内容如下：\nlanguage: go go: - 1.12.5 sudo: required install: - echo \u0026quot;install\u0026quot; script: - echo \u0026quot;script\u0026quot;  这里只是一个示例，在每次 push 代码之后，都会触发 CI，具体语法可以参看官方文档。\n装逼重点： 你以为使用 Travis CI 就是为了持续集成吗？那就太天真了！使用 Travis CI 当然为了他的 Badges ，将 RESULT 拷贝到你的 README.md 里面就好了。\nGO Report Card 又一装逼重点：我们在 GoLand 上安装了 golint 等工具进行代码质量检测，在撸码的时候就能进行代码检查，那么这个就是为了纯装逼了。GO Report Card 是一个 golang 代码检测网站，你只需把 Github 地址填上去即可。获取 Badges 的方法和 Travis CI 类似，将 MarkDown 中的内容拷贝到 RERADME.md 中就好。\nGoReleaser 持续集成有了，代码检查也有了，再下面就是怎么发布一个漂亮的 release 了。如果还在手动发布 release ，那么就又掉 low 了。使用 GoReleaser 一行命令来发布一个漂亮的 release 吧。\n由于使用的的 MacOS ，这里使用 brew 来安装：\nbrew install goreleaser  在项目根目录生成 .goreleaser.yml 配置：\ngoreleaser init  配置好了以后要记得往 .gitignore 加上 dist，因为 goreleaser 会默认把编译编译好的文件输出到 dist 目录中。\ngoreleaser 配置好后，可以先编译测试一下：\ngoreleaser --skip-validate --skip-publish --snapshot  注意： 首次使用 goreleaser 要配置 GITHUB_TOKEN ，可以在这里申请，申请好之后运行下面的命令配置GITHUB_TOKEN\nexport GITHUB_TOKEN=\u0026lt;YOUR_TOKEN\u0026gt;  确保没有问题，那么就可以操作 git 和 goreleaser 来发布 release 了。\ngit add . git commit -m \u0026quot;add goreleaser\u0026quot; git tag -a v0.0.3 -m \u0026quot;First release\u0026quot; git push origin master git push origin v0.0.3  全部搞定后，一行命令起飞：\ngoreleaser  goreleaser 配合 CI 食用，效果更佳，这里就不做介绍了。 Badges 展示神器 这里介绍一个展示 Badges 的神器：https://shields.io/ 。这个网站提供各种各样的 Badges ，如果你愿意，完全可以把你的 GitHub README.md 填满，有兴趣的同学可以自取。 后记 到这里可以在 GitHub 上装逼的 golang 配置已经介绍的差不多了，其实还有 Codecov、CircleCI 等工具，这里就不做介绍了。这里要介绍的是我们的第一个 golang 项目 Hamal，该项目是一个命令行工具，用来在不同的镜像仓库之间同步镜像。由于我司推行混合云，使用了阿里云与华为云，而在阿里云或华为云环境互相推镜像的时候时间都比较长，所以开发这个小工具用于在办公网络镜像同步，同时也可以用来将我在 dockerhub 上托管的镜像同步到我们的私有仓库，欢迎拍砖。\n","date":1563503906,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1563503906,"objectID":"6836e615be8429ee2092d1749a00a74b","permalink":"https://guoxudong.io/post/golang-project/","publishdate":"2019-07-19T10:38:26+08:00","relpermalink":"/post/golang-project/","section":"post","summary":"接触 golang 时间很长，但是真正动手开始写 golang 也就是在最近。跟着我在 GitHub 上构建一个看上去正规的 Golang 项目。","tags":["go"],"title":"Golang 装逼指南：在 GitHub 上构建一个看上去正规的 Golang 项目","type":"post"},{"authors":null,"categories":["翻译"],"content":"  Dailymotion 在生产环境使用 Kubernetes 已经3年了，但是也面临着多集群部署应用的挑战，这也是在过去的几年中我一直努力优化工具和改进工作流的原因。\n 前言 本文将重点介绍我们如何在全球多个 Kubernetes 集群上部署我们的应用程序。\n为了将应用一次部署到多个 Kubernetes 集群，我们使用了 Helm，并将所有 chart 存储在一个 git 仓库中。我们使用 umbrella 来部署由多个服务组成的完整应用程序，这基本上是一个声明依赖关系的 chart ，其允许我们在单个命令行中引导我们的 API 及其服务。\n此外，我们在使用 Helm 之前会运行一个 python 脚本，用来进行检查，构建 chart ，添加 secrets 并部署我们的应用程序。所有这些任务都是使用 docker 镜像在 CI 平台上完成的。\n下面就进行详细介绍\n注意！： 当你阅读这篇博文的时候，Helm 3 的第一个 release 已经发布。这个版本带来了一系列增强功能，肯定会解决我们过去遇到的一些问题。\nimage: caption: \u0026ldquo;Image from: Pexels\u0026ldquo; focal_point: \u0026ldquo;\u0026rdquo;\npreview_only: false Charts 开发流程 在开发应用程序时，我们使用分支工作流，开发 chart 时也使用相同流程。\n 首先，dev 分支用于构建要在开发集群上进行测试的 chart 。 然后，当发起 PR 请求到 master 分支时，将发布到演示环境中进行验证。 最终，我们将 PR 请求提交的修改合并到 prod 分支，将这个修改应用于生产环境。  我们使用 Chartmuseum 作为私有仓库来存储 chart ，每个环境都有一个 。这样我们就可以在环境之间实现明确的隔离，并且确保该 chart 在生产环境中使用之前已经过测试。\n每个环境的 Chart 仓库\n值得注意的是，当开发人员 push 代码到他们的 dev 分支时，他们的 chart 版本也会自动 push 到 dev 环境的 Chartmuseum 。因此，所有开发人员都使用相同的 dev 存储库，他们必须小心的指定自己的 chart 版本，以避免使用其他人的对 chart 的更改。\n此外，我们的 python 脚本通过使用 Kubeval 在它们推送到 Chartmusem 之前验证 Kubernetes 对象与 Kubernetes OpenAPI 规范。\n chart 开发工作流\n  根据 gazr.io 规范设置我们的 pipeline 任务（lint，unit-test）。 push docker 镜像，该镜像包含部署应用程序的 Python 工具。 根据分支名称设置相应环境。 使用 Kubeval 检查 Kubernetes yamls 。 自动增加 chart 版本及其父项（取决于更改的 chart ）。 将 chart push 到与其环境对应的 Chartmuseum 。  管理集群差异  Cluster federation\n 我们使用 Kubernetes cluster federation，它允许我们从单个 API 端声明 Kubernetes 对象。但是我们遇到的问题是，无法在 federation 端中创建某些 Kubernetes 对象，因此很难维护 federation 对象和其他的群集对象。\n为了解决这个问题，我们决定独立管理我们的集群，反而使这个过程变得更加容易（我们使用的是 federation v1，v2 可能有所改善）。\n 平台地理分布\n 目前，我们的平台分布在6个地区，3个在自己的数据中心，3个在公有云。\n分布式部署\n Helm global values\n 4个全局的 Helm value 定义集群间的差异。这些是我们所有 chart 的最小默认值。\nglobal: cloud: True env: staging region: us-central1 clusterName: staging-us-central1  global values\n这些信息有助于我们为应用程序定义上下文，它们可用于监控，跟踪，记录，进行外部调用，扩展等许多内容\u0026hellip;\u0026hellip;\n cloud：我们有一个混合 Kubernetes 集群。例如，我们的 API 部署在 GCP 和我们自己的数据中心。 env：对于非生产环境，某些值可能会发生变化。本质上是资源定义和自动扩展配置。 region：此信息用于标识群集的位置，并可用于定义外部服务的最近端点。 clusterName：如果我们想要为每个群集定义一个值。  下面是一个具体的示例：\n{{/* Returns Horizontal Pod Autoscaler replicas for GraphQL*/}} {{- define \u0026quot;graphql.hpaReplicas\u0026quot; -}} {{- if eq .Values.global.env \u0026quot;prod\u0026quot; }} {{- if eq .Values.global.region \u0026quot;europe-west1\u0026quot; }} minReplicas: 40 {{- else }} minReplicas: 150 {{- end }} maxReplicas: 1400 {{- else }} minReplicas: 4 maxReplicas: 20 {{- end }} {{- end -}}  helm template example\n请注意，此逻辑在帮助模板中定义，以保持 Kubernetes YAML 的可读性。\n 声明应用\n 我们的部署工具基于几个 YAML 文件，下面是我们声明服务及其每个集群的扩展拓扑（副本数量）的示例。\nreleases: - foo.world foo.world: # Release name services: # List of dailymotion's apps/projects foobar: chart_name: foo-foobar repo: git@github.com:dailymotion/foobar contexts: prod-europe-west1: deployments: - name: foo-bar-baz replicas: 18 - name: another-deployment replicas: 3  service definition\n这是部署工作流的所有步骤，最后一步将在多个生产集群上同时部署应用程序。\nJenkins deployment steps\nSecrets 怎么办 在安全领域，我们专注于跟踪可能在不同位置传播的所有的 Secrets ，并将其存储在巴黎的 Vault 。\n我们的部署工具负责从 Vault 检索加密的值，并在部署时将其注入 Helm 。\n为此，我们定义了存储在 Vault 中的 Secrets 与我们的应用程序所需的 Secrets 之间的映射，如下所示：\nsecrets: - secret_id: \u0026quot;stack1-app1-password\u0026quot; contexts: - name: \u0026quot;default\u0026quot; vaultPath: \u0026quot;/kv/dev/stack1/app1/test\u0026quot; vaultKey: \u0026quot;password\u0026quot; - name: \u0026quot;cluster1\u0026quot; vaultPath: \u0026quot;/kv/dev/stack1/app1/test\u0026quot; vaultKey: \u0026quot;password\u0026quot;   定义将 Secrets 写入 Vault 时要遵循的通用规则。 如果 Secrets 有特定的上下文/集群，则必须添加特定条目。 否则，将使用默认值。 对于此列表中的每个项目，将在 Kubernetes Secrets 中插入一个 key/value 。这样我们 chart 中的 Secrets 模板非常简单。\napiVersion: v1 data: {{- range $key,$value := .Values.secrets }} {{ $key }}: {{ $value | b64enc | quote }} {{ end }} kind: Secret metadata: name: \u0026quot;{{ .Chart.Name }}\u0026quot; labels: chartVersion: \u0026quot;{{ .Chart.Version }}\u0026quot; tillerVersion: \u0026quot;{{ .Capabilities.TillerVersion.SemVer }}\u0026quot; type: Opaque   警告与限制 image: caption: \u0026ldquo;Image from: Pexels\u0026ldquo; focal_point: \u0026ldquo;\u0026rdquo;\npreview_only: false 操作多个存储库 目前，chart 和应用程序开发是分离的。这意味着开发人员必须处理两个 git 存储库，一个用于应用程序，另一个用于定义如何在 Kubernetes 上部署。而2个 git 存储库意味着两个工作流程，这对于新手来说可能相当复杂。\n管理 umbrella charts 可能很棘手 如前所述，umbrella charts 非常适合定义依赖关系并快速部署多个应用程序。同时我们使用 --reuse-values 选项，以避免每次部署作为 umbrella charts 一部分的应用程序时都要传递所有值。\n在我们的 CD 工作流中，只有2个值会定期更改：副本数量和镜像标签（版本）。对于其他更稳定的值，需要手动更新，而且这些值并不是很容易弄清楚。此外，我们曾遇到过部署 umbrella charts 的一个错误导致严重的中断。\n更新多个配置文件 添加新应用程序时，开发人员必须更改多个文件：应用程序声明， Secrets 列表，如果应用程序是 umbrella charts 的一部分，则将其添加到依赖。\n在 Vault 上， Jenkins 权限过大 目前，我们有一个 AppRole 可以读取 Vault 的所有 Secrets 。\n回滚过程不是自动化的 回滚需要在多个集群上运行该命令，这可能容易出错。我们制作本操作手册是因为我们要确保应用正确的版本 ID 。\nGitOps 实践  目标\n 我们的想法是将 chart 放回到它部署的应用程序的存储库下。工作流程与应用同时开发，例如，无论何时在主服务器上合并分支，都会自动触发部署。这种方法与当前工作流程的主要区别在于，所有内容都将通过 git 进行管理（应用程序本身以及我们在 Kubernetes 中部署它的方式）。\n这样做优点：\n 从开发人员的角度来看，更容易理解。学习如何在本地 chart 中应用更改将更容易。 将服务 deployment 定义在与此服务的代码相同的位置。 移除 umbrella charts 管理。服务将拥有自己的 Helm 版本。这使得应用程序生命周期管理（回滚，升级）形成闭环，不会影响其他服务。 git 功能对 chart 管理的好处：回滚，审计日志\u0026hellip;\u0026hellip;如果要还原 chart 更改，可以使用 git 进行更改。同时部署将自动触发。 我们考虑使用 Skaffold 等工具改进开发工作流程，这些工具允许开发人员在类似于生产的环境中测试他们的更改。   2步迁移\n 我们的开发人员已使用上述工作流程2年，因此我们需要尽可能顺利地进行迁移。这就是为什么我们决定在达到目标之前添加一个中间步骤。\n第一步很简单：\n 我们将保留一个类似的结构来配置我们的应用程序部署，但是在名为 “DailymotionRelease” 的单个对象中\napiVersion: \u0026quot;v1\u0026quot; kind: \u0026quot;DailymotionRelease\u0026quot; metadata: name: \u0026quot;app1.ns1\u0026quot; environment: \u0026quot;dev\u0026quot; branch: \u0026quot;mybranch\u0026quot; spec: slack_channel: \u0026quot;#admin\u0026quot; chart_name: \u0026quot;app1\u0026quot; scaling: - context: \u0026quot;dev-us-central1-0\u0026quot; replicas: - name: \u0026quot;hermes\u0026quot; count: 2 - context: \u0026quot;dev-europe-west1-0\u0026quot; replicas: - name: \u0026quot;app1-deploy\u0026quot; count: 2 secrets: - secret_id: \u0026quot;app1\u0026quot; contexts: - name: \u0026quot;default\u0026quot; vaultPath: \u0026quot;/kv/dev/ns1/app1/test\u0026quot; vaultKey: \u0026quot;password\u0026quot; - name: \u0026quot;dev-europe-west1-0\u0026quot; vaultPath: \u0026quot;/kv/dev/ns1/app1/test\u0026quot; vaultKey: \u0026quot;password\u0026quot;  每个应用程序一个版本（不再使用 umbrella charts ）\n 将 chart 加入应用程序 git 存储库中\n  我们已经开始向所有开发人员科普这个词，并且迁移过程已经开始。第一步仍然使用 CI 平台进行控制。我将在短期内撰写另一篇博文，介绍第二步：我们如何通过 Flux 实现向 GitOps 工作流程的迁移。将描述我们的设置和面临的挑战（多个存储库，Secrets 等）。 敬请期待！\n","date":1563085016,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1563085016,"objectID":"e4ed09e6443567e78c433e4a1ef54a44","permalink":"https://guoxudong.io/post/deploying-apps-on-multiple-kubernetes-clusters-with-hel/","publishdate":"2019-07-14T14:16:56+08:00","relpermalink":"/post/deploying-apps-on-multiple-kubernetes-clusters-with-hel/","section":"post","summary":"本文将重点介绍我们如何在全球多个 Kubernetes 集群上部署我们的应用程序。为了将应用一次部署到多个 Kubernetes 集群，我们使用了 Helm ，并将所有 chart 存储在一个 git 仓库中。","tags":["翻译","kubernetes"],"title":"使用 Helm 在多集群部署应用","type":"post"},{"authors":["郭旭东"],"categories":["devops"],"content":" 前言 说到 GitOps 和 ChatOps ，那就不得不谈到 DevOps 。 DevOps 作为一种文化，旨在促进开发、测试和运维人员之间的沟通与协作。而促进合作的方式，往往是使用一系列工具，完成这三个角色的相互协作。这带来的好处也是显而易见的：更快的交付速度和更低的人力成本。获益于 DevOps 和公有云，一个近百人的研发团队，可以只配备一到两个专职运维人员，降低的成本不言而喻。既然 DevOps 是一种文化，那么在不同的团队则会有不同的实践，而无论实践如何，其最终目的都是一样的：最大化的实现自动化，释放更多的人力资源，创建更大价值。\n而 GitOps 和 ChatOps ，则是 DevOps 的两种实践。这两种实践分别通过使用 版本控制软件 Git 和实时聊天软件来达到提升交付速度和研发效率的目的。\nGitOps GitOps 是一种实现持续交付的模型，它的核心思想是将应用系统的声明性基础架构和应用程序存放在 Git 的版本控制库中。\n将 Git 作为交付流水线的核心，每个开发人员都可以提交拉取请求（Pull Request）并使用 Gi​​t 来加速和简化 Kubernetes 的应用程序部署和运维任务。通过使用像 Git 这样的简单熟悉工具，开发人员可以更高效地将注意力集中在创建新功能而不是运维相关任务上。\n通过应用 GitOps ，应用系统的基础架构和应用程序代码可以快速查找来源——基础架构和应用程序代码都存放在 gitlab 、或者 github 等版本控制系统上。这使开发团队可以提高开发和部署速度并提高应用系统可靠性。\n将 GitOps 应用在持续交付流水线上，有诸多优势和特点：\n 安全的云原生 CI/CD 管道模型 更快的平均部署时间和平均恢复时间 稳定且可重现的回滚（例如，根据Git恢复/回滚/ fork） 与监控和可视化工具相结合，对已经部署的应用进行全方位的监控  在我看来 GitOps 的最大优势就是通过完善的 git 分支管理来达到管理所有 CI/CD 管道流水线的目的，不同的环境可以对应不同分支，在该环境出现问题时候，可以直接查找对应分支代码，达到快速排查问题的目的。而对于 Git 的熟悉，更是省去学习使用一般 DevOps 工具所需的学习成本和配置时间，开发人员可以无任何培训直接上手使用，进一步降低了时间与人力成本。\nChatOps ChatOps 以聊天室（聊天群），即实时聊天软件为中心，通过一系列的机器人去对接后台的各种服务，开发\u0026amp;测试\u0026amp;运维人员只需要在聊天窗口中与机器人对话，即可与后台服务进行交互，整个工作的展开就像是使唤一个智能助手那样简单自然。\nChatOps 带来了很多好处：\n 公开透明。所有的工作消息都在同一个聊天平台中沉淀并公开给所有相关成员，消除沟通壁垒，工作历史有迹可循，团队合作更加顺畅。 上下文共享。减少因工作台切换等对消息的截断，保证消息的完整性，让工作承接有序，各角色，各工具都成为完成工作流中的一环，打造真正流畅的工作体验。 移动友好。只需要在前台与预设好的机器人对话即可完成与后台工具、系统的交互，在移动环境下无需再与众多复杂的工具直接对接，大大提升移动办公的可行性。 DevOps 文化打造。用与机器人对话这种简单的方式降低 DevOps 的接受门槛，让这种自动化办公的理念更容易的扩展到团队的每一个角落。  对于 ChatOps 的理解最早要源于在 GitHub 上参与开源项目的一些经历，在向 Kubernetes 相关项目提交 PR 时，会有一个名叫 k8s-ci-robot 的小机器人来自动为该 RP 打上标签，并且根据你提交 PR 时的 comment 信息来为你分配 Reviewers，如果没有填的话，则会自动为你分配 Reviewers 等功能。同时可以在 comment 中输入命令，还可以进行其他的操作，详见：命令列表。而其实这个机器人的后端就是名为 Prow 的由 Google 发起的适应云原生 CI/CD 开源项目，有兴趣的话推荐阅读：Prow 快速入门向导。\n而一篇名为：《湾区日报是如何运作的？》 文章更是让我坚定信心开始开发自己 ChatOps 系统。该文章介绍作者是怎么运营一个名叫湾区日报的个人博客，这个博客通过11个渠道（网站，iOS app，Android app、微博，微信，Twitter，Chrome 浏览器推送、Facebook、邮件订阅、RSS、Telegram）推荐给读者，而这个11个渠道的发布都是通过 slack 和作者开发的小机器人完成。在我还在为使用脚本可以在多渠道发布个人技术博客而沾沾自喜的时候，人家早在多年前就开始使用 ChatOps 模式向多渠道使用多格式自动推送文章了。这也坚定了我开发我们自己的 ChatOps 系统的决心。\nGitOps \u0026amp; ChatOps 的实践 使用 Drone 实现 GitOps DevOps 文化早已在我司落地，这也是为什么我们有将近百人的研发团队，却只有两个专职运维的原因。CI/CD 方面我们之前使用的是 jenkins ， jenkins 是一个十分强大的工具，但是随着公司的发展，项目也越来越多，粗略统计了一下我们在 jenkins 中有几百个 Job ，虽然所有项目都使用 Jenkinsfile 的方式将 pipeline 持久化到了 gitlab 中，但是所有的 Job 配置，包括参数化构建配置，SCM 配置等都是保存在 jenkins 上，一旦有失，几百个 Job \u0026hellip;哭都没有地方哭去（别问我是怎么知道的）。\n经过调研我们选择了 drone CI 进行 GitOps ，通过自己开发不同功能的插件，完善了我们的整个 CI/CD 流水线。而插件的开发也并不是从头开始，而是直接 fork 现有的插件进行定制化的二次开发，有兴趣的可以到我的 GitHub 和 DockerHub 上查看。\n将项目配置进行了分离，配置使用单独的 git 仓库维护，同时整合了镜像安全扫描，钉钉通知等功能。 由于 drone CI 的配置文件 .drone.yml 需要统一规范，所以我们在自己的 DevOps 平台开发了 Drone 配置页面，帮助开发自主配置。我们提供了 Java 、 Node 、 Python 三种配置模板，并且由于 DevOps 平台已与 GitLab 集成，可以直接将生成的 .drone.yml 文件插入到相应 git 项目中。\n同时也提供了钉钉构建通知，在构建成功后会发送到相应的开发群组中，如果需希望自动发布的话，也可点击通知中的连接自行发布。\n自研平台配合钉钉 Outgoing 功能实现 ChatOps 前面的构建通知机器人使用的是钉钉的自定义机器人，将构建信息推送到各个项目群中。而钉钉机器人的 Outgoing 功能，则可用来实现 ChatOps 的功能（注意：钉钉的 Outgoing 功能目前还处于灰度测试阶段，想要使用的需要联系官方管理员开启该功能）。\n由于我司专职运维人员只有两位，管理着整个团队全部的基础设施。但是随着开发团队的扩张，运维人员每天要处理大量的咨询类工作，而这类工作有着重复性强和技术性弱的特点，对于运维人员的技术水平毫无提升，那么这类工作交给机器人岂不是更好。得益于我们 DevOps 平台完善的 API ，小助手机器人的开发并不困难。\n小助手机器人的诞生，极大的提高了咨询类工作的效率，同时也释放了运维人员的工作时间，运维人员可以将更多精力投注到更有技术含量的事情上。\n小助手机器人还有运维版本，功能包括：批量操作虚拟机、重启服务、DNS 解析、Kubernetes 信息检测\u0026amp;操作等功能，由于还是测试版本，这里就不做详细介绍了。\n结语 上文中简要的介绍了 GitOps 和 ChatOps 在我司的落地实践，从决定落地 GitOps 和 ChatOps 至今不过短短的2个月。得益于我司浓厚的 DevOps 文化氛围，让我可以在极短的时间内将 GitOps 和 ChatOps 落地实践。但毕竟实践的时间还短，很多需求还在收集和调研中，后续的开发还在持续进行。欢迎对 GitOps 和 ChatOps 感兴趣的同学一起交流，共同提升。\n参考资料  GitOps DevOps 理念升级，ChatOps 概述及实践经验  ","date":1562808257,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1562808257,"objectID":"625c5d2f92183aac08c31f2caff1f9a0","permalink":"https://guoxudong.io/post/gitops-and-chatops/","publishdate":"2019-07-11T09:24:17+08:00","relpermalink":"/post/gitops-and-chatops/","section":"post","summary":"本文介绍 GitOps 和 ChatOps 这两种 DevOps 实践，通过版本控制软件 Git 和实时聊天软件来达到提升交付速度和研发效率的目的。","tags":["devops"],"title":"GitOps 与 ChatOps 的落地实践","type":"post"},{"authors":["郭旭东"],"categories":["kubernetes"],"content":" 前言 今天接到一个将 NAS 数据卷挂载到 Kubernetes 集群的需求，需要将一个 NAS 数据卷挂载到集群中。这一很简单的操作由于好久没有操作了，去翻看了一下官方文档，发现官方文档还在停留在去年7月份\u0026hellip;为了防止之后还有相似情况的发生，这里将所有操作做一个简单记录。\n购买存储包（创建文件系统） 在挂载 NAS 之前，首先要先购买 NAS 文件存储，这里推荐购买存储包，100G 的 SSD 急速型一年只需1400多，而容量型只要279，对于我这种只有少量 NAS 存储需求的人来说是是靠谱的，因为我只需要5G的左右的存储空间，SSD 急速型 NAS 一年只要18块，完美。\n选择想要创建 NAS 所在 VPC 和 区域\n添加挂载点  点击添加挂载点  选择 VPC 网络、交换机和权限组   Linux 挂载 NAS 数据卷 在挂载点创建成功后，就可以将 NAS 数据卷挂载到 Linux 系统，这里以 CentOS 为例：\n安装 NFS 客户端 如果 Linux 系统要挂载 NAS ，首先需要安装 NFS 客户端\nsudo yum install nfs-utils  挂载 NFS 文件系统 这里阿里云早就进行了优化，点击创建的文件系统，页面上就可以 copy 挂载命令。页面提供了挂载地址的 copy 和挂载命令的 copy 功能。\n挂载命令：\nsudo mount -t nfs -o vers=4,minorversion=0,noresvport xxxxx.cn-shanghai.nas.aliyuncs.com:/ /mnt  查看挂载结果 直接在挂载数据卷所在服务上执行命令：\ndf -h  就可以看到结果：\nKubernetes 集群挂载 NAS 数据卷 K8S 的持久数据卷挂载大同小异，流程都是：创建PV -\u0026gt; 创建PVC -\u0026gt; 使用PVC\n下面就简单介绍在阿里云上的操作：\n创建存储卷（PV） 首先要创建存储卷，选择 容器服务 -\u0026gt; 存储卷 -\u0026gt; 创建\n这里要注意的是：挂载点域名使用上面面的挂载地址\n创建存储声明（PVC） 选择 NAS -\u0026gt; 已有存储卷\n选择刚才创建的存储卷\n使用PVC 使用的方法这里就不做详细介绍了，相关文章也比较多，这里就只记录 Deployment 中使用的 yaml 片段：\n... volumeMounts: - mountPath: /data # 挂载路径 name: volume-nas-test ... volumes: - name: volume-nas-test persistentVolumeClaim: claimName: nas-test # PVC 名称 ...  结语 这里只是做一个简单的记录，仅适用于阿里云 ACK 容器服务，同时也是 ACK 的一个简单应用。由于不经常对数据卷进行操作，这里做简单的记录，防止以后使用还要再看一遍文档。\n","date":1562569796,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1562569796,"objectID":"cd23ab85b476a5463bb681d85c60ee61","permalink":"https://guoxudong.io/post/nas-k8s/","publishdate":"2019-07-08T15:09:56+08:00","relpermalink":"/post/nas-k8s/","section":"post","summary":"记录在阿里云购买、配置、挂载 NAS 数据卷到 Kubernetes 集群，由于官方文档没有及时更新，可以看做是对官方文档的补充。","tags":["阿里云","kubernetes","容器"],"title":"阿里云 ACK 挂载 NAS 数据卷","type":"post"},{"authors":["郭旭东"],"categories":["应用程序开发"],"content":"  原文地址：https://semver.org/lang/zh-CN/\n语义化版本 2.0.0\n提供多种语言，语义化版本控制的规范是由 Gravatars 创办者兼 GitHub 共同创办者 Tom Preston-Werner 所建立。\n 摘要 版本格式：主版本号.次版本号.修订号，版本号递增规则如下：\n 主版本号：当你做了不兼容的 API 修改， 次版本号：当你做了向下兼容的功能性新增， 修订号：当你做了向下兼容的问题修正。  先行版本号及版本编译元数据可以加到“主版本号.次版本号.修订号”的后面，作为延伸。\n简介 在软件管理的领域里存在着被称作“依赖地狱”的死亡之谷，系统规模越大，加入的包越多，你就越有可能在未来的某一天发现自己已深陷绝望之中。\n在依赖高的系统中发布新版本包可能很快会成为噩梦。如果依赖关系过高，可能面临版本控制被锁死的风险（必须对每一个依赖包改版才能完成某次升级）。而如果依赖关系过于松散，又将无法避免版本的混乱（假设兼容于未来的多个版本已超出了合理数量）。当你专案的进展因为版本依赖被锁死或版本混乱变得不够简便和可靠，就意味着你正处于依赖地狱之中。\n作为这个问题的解决方案之一，我提议用一组简单的规则及条件来约束版本号的配置和增长。这些规则是根据（但不局限于）已经被各种封闭、开放源码软件所广泛使用的惯例所设计。为了让这套理论运作，你必须先有定义好的公共 API 。这可以透过文件定义或代码强制要求来实现。无论如何，这套 API 的清楚明了是十分重要的。一旦你定义了公共 API，你就可以透过修改相应的版本号来向大家说明你的修改。考虑使用这样的版本号格式：X.Y.Z （主版本号.次版本号.修订号）修复问题但不影响API 时，递增修订号；API 保持向下兼容的新增及修改时，递增次版本号；进行不向下兼容的修改时，递增主版本号。\n我称这套系统为“语义化的版本控制”，在这套约定下，版本号及其更新方式包含了相邻版本间的底层代码和修改内容的信息。\n语义化版本控制规范（SemVer） 以下关键词 MUST、MUST NOT、REQUIRED、SHALL、SHALL NOT、SHOULD、SHOULD NOT、 RECOMMENDED、MAY、OPTIONAL 依照 RFC 2119 的叙述解读。（译注：为了保持语句顺畅， 以下文件遇到的关键词将依照整句语义进行翻译，在此先不进行个别翻译。）\n 使用语义化版本控制的软件必须（MUST）定义公共 API。该 API 可以在代码中被定义或出现于严谨的文件内。无论何种形式都应该力求精确且完整。 标准的版本号必须（MUST）采用 X.Y.Z 的格式，其中 X、Y 和 Z 为非负的整数，且禁止（MUST NOT）在数字前方补零。X 是主版本号、Y 是次版本号、而 Z 为修订号。每个元素必须（MUST）以数值来递增。例如：1.9.1 -\u0026gt; 1.10.0 -\u0026gt; 1.11.0。 标记版本号的软件发行后，禁止（MUST NOT）改变该版本软件的内容。任何修改都必须（MUST）以新版本发行。 主版本号为零（0.y.z）的软件处于开发初始阶段，一切都可能随时被改变。这样的公共 API 不应该被视为稳定版。 1.0.0 的版本号用于界定公共 API 的形成。这一版本之后所有的版本号更新都基于公共 API 及其修改内容。 修订号 Z（x.y.Z | x \u0026gt; 0）必须（MUST）在只做了向下兼容的修正时才递增。这里的修正指的是针对不正确结果而进行的内部修改。 次版本号 Y（x.Y.z | x \u0026gt; 0）必须（MUST）在有向下兼容的新功能出现时递增。在任何公共 API 的功能被标记为弃用时也必须（MUST）递增。也可以（MAY）在内部程序有大量新功能或改进被加入时递增，其中可以（MAY）包括修订级别的改变。每当次版本号递增时，修订号必须（MUST）归零。 主版本号 X（X.y.z | X \u0026gt; 0）必须（MUST）在有任何不兼容的修改被加入公共 API 时递增。其中可以（MAY）包括次版本号及修订级别的改变。每当主版本号递增时，次版本号和修订号必须（MUST）归零。 先行版本号可以（MAY）被标注在修订版之后，先加上一个连接号再加上一连串以句点分隔的标识符来修饰。标识符必须（MUST）由 ASCII 字母数字和连接号 [0-9A-Za-z-] 组成，且禁止（MUST NOT）留白。数字型的标识符禁止（MUST NOT）在前方补零。先行版的优先级低于相关联的标准版本。被标上先行版本号则表示这个版本并非稳定而且可能无法满足预期的兼容性需求。范例：1.0.0-alpha、1.0.0-alpha.1、1.0.0-0.3.7、1.0.0-x.7.z.92。 版本编译元数据可以（MAY）被标注在修订版或先行版本号之后，先加上一个加号再加上一连串以句点分隔的标识符来修饰。标识符必须（MUST）由 ASCII 字母数字和连接号 [0-9A-Za-z-] 组成，且禁止（MUST NOT）留白。当判断版本的优先层级时，版本编译元数据可（SHOULD）被忽略。因此当两个版本只有在版本编译元数据有差别时，属于相同的优先层级。范例：1.0.0-alpha+001、1.0.0+20130313144700、1.0.0-beta+exp.sha.5114f85。 版本的优先层级指的是不同版本在排序时如何比较。判断优先层级时，必须（MUST）把版本依序拆分为主版本号、次版本号、修订号及先行版本号后进行比较（版本编译元数据不在这份比较的列表中）。由左到右依序比较每个标识符，第一个差异值用来决定优先层级：主版本号、次版本号及修订号以数值比较，例如：1.0.0 \u0026lt; 2.0.0 \u0026lt; 2.1.0 \u0026lt; 2.1.1。当主版本号、次版本号及修订号都相同时，改以优先层级比较低的先行版本号决定。例如：1.0.0-alpha \u0026lt; 1.0.0。有相同主版本号、次版本号及修订号的两个先行版本号，其优先层级必须（MUST）透过由左到右的每个被句点分隔的标识符来比较，直到找到一个差异值后决定：只有数字的标识符以数值高低比较，有字母或连接号时则逐字以 ASCII 的排序来比较。数字的标识符比非数字的标识符优先层级低。若开头的标识符都相同时，栏位比较多的先行版本号优先层级比较高。范例：1.0.0-alpha \u0026lt; 1.0.0-alpha.1 \u0026lt; 1.0.0-alpha.beta \u0026lt; 1.0.0-beta \u0026lt; 1.0.0-beta.2 \u0026lt; 1.0.0-beta.11 \u0026lt; 1.0.0-rc.1 \u0026lt; 1.0.0。  为什么要使用语义化的版本控制？ 这并不是一个新的或者革命性的想法。实际上，你可能已经在做一些近似的事情了。问题在于只是“近似”还不够。如果没有某个正式的规范可循，版本号对于依赖的管理并无实质意义。将上述的想法命名并给予清楚的定义，让你对软件使用者传达意向变得容易。一旦这些意向变得清楚，弹性（但又不会太弹性）的依赖规范就能达成。\n举个简单的例子就可以展示语义化的版本控制如何让依赖地狱成为过去。假设有个名为“救火车”的函式库，它需要另一个名为“梯子”并已经有使用语义化版本控制的包。当救火车创建时，梯子的版本号为 3.1.0。因为救火车使用了一些版本 3.1.0 所新增的功能， 你可以放心地指定依赖于梯子的版本号大等于 3.1.0 但小于 4.0.0。这样，当梯子版本 3.1.1 和 3.2.0 发布时，你可以将直接它们纳入你的包管理系统，因为它们能与原有依赖的软件兼容。\n作为一位负责任的开发者，你理当确保每次包升级的运作与版本号的表述一致。现实世界是复杂的，我们除了提高警觉外能做的不多。你所能做的就是让语义化的版本控制为你提供一个健全的方式来发行以及升级包，而无需推出新的依赖包，节省你的时间及烦恼。\n如果你对此认同，希望立即开始使用语义化版本控制，你只需声明你的函式库正在使用它并遵循这些规则就可以了。请在你的 README 文件中保留此页连结，让别人也知道这些规则并从中受益。\nFAQ 在 0.y.z 初始开发阶段，我该如何进行版本控制？\n最简单的做法是以 0.1.0 作为你的初始化开发版本，并在后续的每次发行时递增次版本号。\n如何判断发布 1.0.0 版本的时机？\n当你的软件被用于正式环境，它应该已经达到了 1.0.0 版。如果你已经有个稳定的 API 被使用者依赖，也会是 1.0.0 版。如果你很担心向下兼容的问题，也应该算是 1.0.0 版了。\n这不会阻碍快速开发和迭代吗？\n主版本号为零的时候就是为了做快速开发。如果你每天都在改变 API，那么你应该仍在主版本号为零的阶段（0.y.z），或是正在下个主版本的独立开发分支中。\n对于公共 API，若即使是最小但不向下兼容的改变都需要产生新的主版本号，岂不是很快就达到 42.0.0 版？\n这是开发的责任感和前瞻性的问题。不兼容的改变不应该轻易被加入到有许多依赖代码的软件中。升级所付出的代价可能是巨大的。要递增主版本号来发行不兼容的改版，意味着你必须为这些改变所带来的影响深思熟虑，并且评估所涉及的成本及效益比。\n为整个公共 API 写文件太费事了！\n为供他人使用的软件编写适当的文件，是你作为一名专业开发者应尽的职责。保持专案高效一个非常重要的部份是掌控软件的复杂度，如果没有人知道如何使用你的软件或不知道哪些函数的调用是可靠的，要掌控复杂度会是困难的。长远来看，使用语义化版本控制以及对于公共 API 有良好规范的坚持，可以让每个人及每件事都运行顺畅。\n万一不小心把一个不兼容的改版当成了次版本号发行了该怎么办？\n一旦发现自己破坏了语义化版本控制的规范，就要修正这个问题，并发行一个新的次版本号来更正这个问题并且恢复向下兼容。即使是这种情况，也不能去修改已发行的版本。可以的话，将有问题的版本号记录到文件中，告诉使用者问题所在，让他们能够意识到这是有问题的版本。\n如果我更新了自己的依赖但没有改变公共 API 该怎么办？\n由于没有影响到公共 API，这可以被认定是兼容的。若某个软件和你的包有共同依赖，则它会有自己的依赖规范，作者也会告知可能的冲突。要判断改版是属于修订等级或是次版等级，是依据你更新的依赖关系是为了修复问题或是加入新功能。对于后者，我经常会预期伴随着更多的代码，这显然会是一个次版本号级别的递增。\n如果我变更了公共 API 但无意中未遵循版本号的改动怎么办呢？（意即在修订等级的发布中，误将重大且不兼容的改变加到代码之中）\n自行做最佳的判断。如果你有庞大的使用者群在依照公共 API 的意图而变更行为后会大受影响，那么最好做一次主版本的发布，即使严格来说这个修复仅是修订等级的发布。记住， 语义化的版本控制就是透过版本号的改变来传达意义。若这些改变对你的使用者是重要的，那就透过版本号来向他们说明。\n我该如何处理即将弃用的功能？\n弃用现存的功能是软件开发中的家常便饭，也通常是向前发展所必须的。当你弃用部份公共 API 时，你应该做两件事：（1）更新你的文件让使用者知道这个改变，（2）在适当的时机将弃用的功能透过新的次版本号发布。在新的主版本完全移除弃用功能前，至少要有一个次版本包含这个弃用信息，这样使用者才能平顺地转移到新版 API。\n语义化版本对于版本的字串长度是否有限制呢？\n没有，请自行做适当的判断。举例来说，长到 255 个字元的版本已过度夸张。再者，特定的系统对于字串长度可能会有他们自己的限制。\n关于 语义化版本控制的规范是由 Gravatars 创办者兼 GitHub 共同创办者 Tom Preston-Werner 所建立。\n如果您有任何建议，请到 GitHub 上提出您的问题。\n许可证 知识共享 署名 3.0 (CC BY 3.0)\n","date":1562377242,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1562377242,"objectID":"d6fc08dd6087662b0feb22b953284abb","permalink":"https://guoxudong.io/post/semver/","publishdate":"2019-07-06T09:40:42+08:00","relpermalink":"/post/semver/","section":"post","summary":"语义化的版本控制，在这套约定下，版本号及其更新方式包含了相邻版本间的底层代码和修改内容的信息。","tags":["软件设计","云原生","微服务"],"title":"语义化版本控制规范（SemVer）","type":"post"},{"authors":null,"categories":["翻译"],"content":"  转自掘金社区，原文地址：https://juejin.im/post/5d1b2a656fb9a07edc0b7058\n Kubernetes 儿童插图指南 献给所有试图向孩子们解释软件工程的家长。\n很久很久以前，有一个叫 Phippy 的应用程序。她是一个简单的应用程序，由 PHP 编写且只有一个页面。她住在一个需要和其他可怕的应用程序分享环境的主机中，她不认识这些应用程序并且不愿意和他们来往。她希望她能拥有一个属于自己的环境：只有她自己和她可以称之为家的 Web 服务器。\n每个应用程序都有个运行所依赖的环境。对于 PHP 应用程序来说，这个环境可能包括 Web 服务器，一个可读文件系统和 PHP 引擎本身。\n有一天，一只善良的鲸鱼出现了。他建议小 Phippy 住在容器里，这样可能会更快乐。所以应用程序 Phippy 迁移到了容器中。这个容器很棒，但是……它有点像一个漂浮在大海中央的豪华起居室。\n容器提供了一个独立的环境，应用程序可以在这个环境中运行。但是这些孤立的容器常常需要被管理并与外面的世界连接。对于孤立的容器而言，共享文件系统、网络通信、调度、负载均衡和分发都是要面对的挑战。\n鲸鱼耸了耸肩。“对不起，孩子。”他说着，消失在海面下。就在 Phippy 甚至开始绝望时，一位驾驶着巨轮的船长出现在海平线上。这艘船由几十个绑在一起的木筏组成，但从外面来看，它就像一艘巨轮。 “你好呀，这位 PHP 应用程序朋友。我是 Kube 船长。”睿智的老船长说。\n“Kubernetes” 在希腊语中是船长的意思。我们可以从这个单词中得到 Cybernetic 和 Gubernatorial 这两个词组。Kubernetes 项目专注于构建一个健壮的平台，用于在生产环境中运行数千个容器。\n“我是 Phippy。”小应用程序说。\n“很高兴认识你。”船长一边说，一边在她身上贴上了一张标有姓名的标签。\nKubernetes 使用标签作为“名牌”来标识事物。它可以根据这些标签进行查询。标签是开放性的：你可以用他们来表示角色、稳定性或其他重要的属性。\n船长建议应用程序把她的容器搬到船上的一个船舱中。Phippy 很高兴地把她的容器搬到 Kube 船长巨轮的船舱内。Phippy 觉得这里像家一样。\n在 Kubernetes 中，Pod 代表一个可运行的工作单元。通常，你会在 Pod 中运行一个容器。但是对于一些容器紧密耦合的情况，你可以选择在同一个 Pod 中运行多个容器。Kubernetes 负责将你的 Pod 和网络以及 Kubernetes 的其余环境相连。\nPhippy 有一些不同寻常的兴趣，她很喜欢遗传学和绵羊。所以她问船长：“如果我想克隆我自己，是否可以根据需求克隆任意次数呢？”\n“这很容易。”船长说。船长把 Phippy 介绍给了 Replication Controller。\nReplication Controller 提供一种管理任意数量 Pod 的方法。一个 Replication Controller 包含一个 Pod 模板，该模板可以被复制任意次数。通过 Replication Controller，Kubernetes 将管理 Pod 的生命周期，包括伸缩、滚动更新和监控。\n无数个日夜，小应用程序在她的船舱中与她的复制品相处十分愉快。但与自己为伍并没有所说的那么好……即使你拥有 N 个自己的克隆体。 Kube 船长慈祥地笑了笑：“我正好有一样东西。” 他刚开口，在 Phippy 的 Replication Controller 和船的其他部分之间打开了一条隧道。Kube 船长笑着说：“即使你的复制品来了又去，这条隧道始终会留在这里，你可以通过它发现其他 Pod，其他 Pod 也可以发现你！”\n服务告知 Kubernetes 环境的其余部分（包括其他 Pod 和 Replication Controller）你的应用程序包含了哪些服务，当 Pod 来来往往，服务的 IP 地址和端口始终保持不变。其他应用程序可以通过 Kurbenetes 服务发现找到你的服务。\n多亏了这些服务，Phippy 开始探索船的其他部分。不久之后，Phippy 遇到了 Goldie。他们成了最好的朋友。有一天，Goldie 做了一件不同寻常的事。她送给 Phippy 一件礼物。Phippy 看了礼物一眼，悲伤的泪水夺眶而出。 “你为什么这么伤心呢？”Goldie 问道。 “我喜欢这个礼物，但我没有地方可以放它！”Phippy 抽噎道。 但 Goldie 知道该怎么做。“为什么不把它放入卷中呢？”\n卷表示容器可以访问和存储信息的位置。对于应用程序，卷显示为本地文件系统的一部分。但卷可以由本地存储、Ceph、Gluster、持久性块存储，以及其他存储后端支持。 Phippy 喜欢在 Kube 船长的船上生活，她很享受来自新朋友的陪伴（Goldie 的每个克隆人都同样令人愉悦）。但是，当她回想起在可怕的主机度过的日子，她想知道她是否也可以拥有一点自己的隐私。 “这听起来像是你所需要的，”Kube 船长说，“这是一个命名空间。”\n命名空间是 Kubernetes 内部的分组机制。服务、Pod、Replication Controller 和卷可以在命名空间内部轻松协作，但命名空间提供了与集群其他部分一定程度的隔离。 Phippy 与她的新朋友一起乘坐 Kube 船长的巨轮航行于大海之上。她经历了许多伟大的冒险，但最重要的是，Phippy 找到了自己的家。 所以 Phippy 从此过上了幸福的生活。\n","date":1562291458,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1562291458,"objectID":"ab7429a760c5aad064c3649f60037248","permalink":"https://guoxudong.io/post/the-childrens-illustrated-guide-to-kubernetes/","publishdate":"2019-07-05T09:50:58+08:00","relpermalink":"/post/the-childrens-illustrated-guide-to-kubernetes/","section":"post","summary":"献给所有试图向孩子们解释软件工程的家长。","tags":["翻译","kubernetes"],"title":"【转】Kubernetes 儿童插图指南","type":"post"},{"authors":["郭旭东"],"categories":["kustomize"],"content":"  只要仔细找，想要的轮子总会有的。 \u0026mdash; 某不知名 DevOps 工程师\n 感谢 kubernetes-dev 的 Mailing List ！早上在浏览邮件时发现了下面这封有趣的邮件：\n接触 Kubernetes 也有不短的时间了，也见证了 Kubernetes 干掉 Swarm 和 Mesos 成为容器编排领域的事实标准的过程。在享受 Kubernetes 及其生态圈带来的便利的同时也在为 Kubernetes 及 CNCF 项目进行贡献。而使用 kubectl、rancher 甚至是 kui 这些 CLI 和 UI 工具对 Kubernetes 集群进行操作和观察。\n虽然上面这些工具为操作 Kubernetes 集群带来了极大的便利，但是归根到底还是一些开源项目，并不能满足我们的全部需求。所以我们只能根据我们自己的需求和 Kubernetes 的 api-server 进行定制，但是由于 Kubernetes 的 api-server 比较复杂，短时间内并不是那么好梳理的。\nkubernetes-client/python 由于我们自研的 DevOps 平台是使用 python 开发的，所以我也基于 python 语言开发了一套 Kubernetes Client ，但总的来说由于 Kubernetes 的功能实在太多，而我的开发实践并不是很多，开发出来的功能只是差强人意。\n而 kubernetes-client/python 这个官方给出的轮子是真的香！\n安装方便 这个安装方式简单的令人发指，支持的 python 版本为 2.7 | 3.4 | 3.5 | 3.6 | 3.7 并且和所有 python 依赖包一样，只需要使用 pip 安装即可：\npip install kubernetes  简单示例 查看所有的 pod ：\n#!/usr/bin/env python #encoding: utf-8 #Author: guoxudong from kubernetes import client, config # Configs can be set in Configuration class directly or using helper utility config.load_kube_config() v1 = client.CoreV1Api() print(\u0026quot;Listing pods with their IPs:\u0026quot;) ret = v1.list_pod_for_all_namespaces(watch=False) for i in ret.items: print(\u0026quot;%s\\t%s\\t%s\u0026quot; % (i.status.pod_ip, i.metadata.namespace, i.metadata.name))  运行查看结果：\nListing pods with their IPs: 172.22.1.126\tkube-system\tcoredns-5975fdf55b-bqgkx 172.22.0.2\tkube-system\tcoredns-5975fdf55b-vxbb4 10.16.16.13\tkube-system\tflexvolume-9ccf7 10.16.16.15\tkube-system\tflexvolume-h5xn2 10.16.16.14\tkube-system\tflexvolume-kvn5x 10.16.16.17\tkube-system\tflexvolume-mf4zv 10.16.16.14\tkube-system\tkube-proxy-worker-7lpfz 10.16.16.15\tkube-system\tkube-proxy-worker-9wd9s 10.16.16.17\tkube-system\tkube-proxy-worker-phbbj 10.16.16.13\tkube-system\tkube-proxy-worker-pst5d 172.22.1.9\tkube-system\tmetrics-server-78b597d5bf-wdvqh 172.22.1.12\tkube-system\tnginx-ingress-controller-796ccc5d76-9jh5s 172.22.1.125\tkube-system\tnginx-ingress-controller-796ccc5d76-jwwwz 10.16.16.17\tkube-system\tterway-6mfs8 10.16.16.14\tkube-system\tterway-fz9ck 10.16.16.13\tkube-system\tterway-t9777 10.16.16.15\tkube-system\tterway-xbxlp 172.22.1.8\tkube-system\ttiller-deploy-5b5d8dd754-wpcrc ...  果然是一个好轮子，引入 kubeconfig 的方式及展示所有 namespace 的 pod 的方法封装的也十分简洁，是个非常漂亮的范例。建议可以看一下源码，肯定会有收获的！\n支持版本 client-python 遵循 semver 规范，所以在 client-python 的主要版本增加之前，代码将继续使用明确支持的 Kubernetes 集群版本。\n    Kubernetes 1.5 Kubernetes 1.6 Kubernetes 1.7 Kubernetes 1.8 Kubernetes 1.9 Kubernetes 1.10 Kubernetes 1.11 Kubernetes 1.12 Kubernetes 1.13 Kubernetes 1.14     client-python 1.0 ✓ - - - - - - - - -   client-python 2.0 + ✓ - - - - - - - -   client-python 3.0 + + ✓ - - - - - - -   client-python 4.0 + + + ✓ - - - - - -   client-python 5.0 + + + + ✓ - - - - -   client-python 6.0 + + + + + ✓ - - - -   client-python 7.0 + + + + + + ✓ - - -   client-python 8.0 + + + + + + + ✓ - -   client-python 9.0 + + + + + + + + ✓ -   client-python 10.0 + + + + + + + + + ✓   client-python HEAD + + + + + + + + + ✓    Mailing List 的重要性 这次的收获很大程度得益于 kubernetes-dev 的 Mailing List 也就是邮件列表。这种沟通方式在国内不是很流行，大家更喜欢使用 QQ 和微信这样的即时通讯软件进行交流，但是大多数著名开源项目都是主要使用 Mailing List 进行交流，交流的数量甚至比在 GitHub issue 中还多，在与 Apache 、 CNCF 项目开源的贡献者和维护者交流中得知了使用 Mailing List 主要考虑是一下几点：\n 这种异步的交流方式可以让更多关心该话题的开发人员一起加入到讨论中。 mailing list 是永久保留的，如果你对某个话题感兴趣，可以随时回复邮件，关注这个话题的开发者都会收到邮件，无论这个话题是昨天提出的，还是去年提出的，有助于解决一些陈年老 BUG （俗称技术债）。 即时通讯软件虽然很便利，但是问题很快会被评论顶掉，虽然诸如 slack 这样的工具解决了部分这方面的问题，但是还是不如 mailing list 好用。 并不是所有地区的开发者都有高速的宽带，性能优秀的PC，在地球上很多地区还是只能使用拨号上网，网速只有几kb/s，他们甚至 GitHub issue 都无法使用。但是你不能剥夺他们参与开源项目的权利，而 mailing list 是一种很好的交流方式。 通过 mailing list 可以很好掌握社区动态，效果明显好于 GitHub watch ，因为并不是项目的所有 commit 都是你关心的。  结语 如果你有志于参与到开源运动，在享受开源软件带来便利的同事，还想为开源软件做出自己的贡献，那么 mailing list 是你进入社区最好的选择。在 mailing list 中和来自世界各地志同道合的开发者交流中提升自己的能力，创造更大的价值，迈出你参与开源运动的第一步。\n","date":1562203001,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1562203001,"objectID":"d30474b6811d32daf1bf147a9c5e0eac","permalink":"https://guoxudong.io/post/kubernetes-client-python/","publishdate":"2019-07-04T09:16:41+08:00","relpermalink":"/post/kubernetes-client-python/","section":"post","summary":"从发现、使用 Kubernetes Client/Python 这个项目的过程，谈谈 mailing list 在开源项目中的重要性。","tags":["kubernetes","python","工具"],"title":"由一封邮件看 Mailing List 在开源项目中的重要性","type":"post"},{"authors":["郭旭东"],"categories":["kustomize"],"content":"  在以往的 pipeline 中，使用 kubectl 进行部署 Deployment 后无法检查 Deployment 是否部署成功，只能通过使用命令/脚本来手动检查 Deployment 状态，而 kubedog 这个小工具完美解决了这个问题，完善了 CI/CD 流水线的最后一步。\n KubeDog kubedog 是一个 lib 库和 CLI 小工具，允许在 CI/CD 部署 pipeline 中观察和跟踪 Kubernetes 资源。与 kustomize 配合，集成到 pipeline 之后，完美的解决了 CI/CD 的最后一步，完美的替代了之前不够灵活的脚本（好吧，其实我也开发了类似的小工具，但是有这么好用的轮子，拿来直接用何乐而不为呢？）。\nkubedog 提供了 lib 库和 CLI 小工具，这里由于是介绍 CI/CD 中的实践，所以只介绍其中的 rollout track 功能。 lib 库的使用和 CLI 的 follow 功能这里就不做介绍了，有兴趣的同学可以去 GitHub 上查看该项目的各种使用方式。\n集成 KubeDog 由于我司目前使用的是 drone 进行 CI ，每个 step 都是由一个 docker 制作的插件组成。我制作了一个包含 kubectl 、 kustomize 和 kubedog 的镜像。该镜像已上传 dockerhub ，需要的可以自行拉取使用 guoxudongdocker/kubectl ,而该插件的使用也在 GitHub 和 DockerHub 上查看。\n而集成方式也比较简单，直接将 kubectl 、 kustomize 和 kubedog 的可执行包下载到 /usr/local/bin 并赋予执行权限即可，下面就是 Dockerfile 文件：\nFROM alpine LABEL maintainer=\u0026quot;sunnydog0826@gmail.com\u0026quot; ENV KUBE_LATEST_VERSION=\u0026quot;v1.14.1\u0026quot; RUN apk add --update ca-certificates \\ \u0026amp;\u0026amp; apk add --update -t deps curl \\ \u0026amp;\u0026amp; curl -L https://storage.googleapis.com/kubernetes-release/release/${KUBE_LATEST_VERSION}/bin/linux/amd64/kubectl -o /usr/local/bin/kubectl \\ \u0026amp;\u0026amp; chmod +x /usr/local/bin/kubectl \\ \u0026amp;\u0026amp; curl -L https://github.com/kubernetes-sigs/kustomize/releases/download/v2.0.3/kustomize_2.0.3_linux_amd64 -o /usr/local/bin/kustomize \\ \u0026amp;\u0026amp; chmod +x /usr/local/bin/kustomize \\ \u0026amp;\u0026amp; curl -L https://dl.bintray.com/flant/kubedog/v0.2.0/kubedog-linux-amd64-v0.2.0 -o /usr/local/bin/kubedog \\ \u0026amp;\u0026amp; chmod +x /usr/local/bin/kubedog \\ \u0026amp;\u0026amp; apk del --purge deps \\ \u0026amp;\u0026amp; rm /var/cache/apk/* WORKDIR /root ENTRYPOINT [\u0026quot;kubectl\u0026quot;] CMD [\u0026quot;help\u0026quot;]  Kustomize 配合 KubeDog 使用 在镜像构建好之后就可以直接使用了，这里使用的是 DockerHub 的镜像仓库，这里建议将镜像同步到私有仓库，比如阿里云的容器镜像服务或者 Habor ，因为国内拉取 DockerHub 的镜像不太稳定，经常会拉取镜像失败或者访问超时，在 CI/CD 流水线中推荐使用更稳定镜像。\n以下是 .drone.yml 示例：\nkind: pipeline name: {your-pipeline-name} steps: - name: Kubernetes 部署 image: guoxudongdocker/kubectl volumes: - name: kube path: /root/.kube commands: - cd deploy/overlays/dev # 这里使用 kustomize ,详细使用方法请见 https://github.com/kubernetes-sigs/kustomize - kustomize edit set image {your-docker-registry}:${DRONE_BUILD_NUMBER} - kubectl apply -k . \u0026amp;\u0026amp; kubedog rollout track deployment {your-deployment-name} -n {your-namespace} -t {your-tomeout} ... volumes: - name: kube host: path: /tmp/cache/.kube # kubeconfig 挂载位置 trigger: branch: - master # 触发 CI 的分支  从上面的配置可见，在该 step 中执行了如下几步：\n 进入 patch 所在路径 使用了 Kustomize 命令 kustomize edit set image {your-docker-registry}:${DRONE_BUILD_NUMBER} 方式将前面 step 中构建好的镜像的 tag 插入到 patch 中 使用 kubectl apply -k . 进行 k8s 部署，要注意最后的那个 . 使用 kubedog 跟踪 Deployment 部署状态   命令解析：kubedog rollout track deployment {your-deployment-name} -n {your-namespace} -t {your-tomeout}\n deployment {your-deployment-name} : Deployment 的名称 -n {your-namespace} : Deployment 所在的 namespace -t {your-tomeout} : 超时时间，单位为秒，超时后会报错，这里请根据实际部署情况进行设置   结语 从 Kubernetes release v1.14 版本开始，kustomize 集成到 kubectl 中，越来越多 k8S 周边的小工具出现。这些小工具的出现帮助了 Kubernetes 的使用者来拉平 Kubernetes 的使用曲线，同时也标志着 K8S 的成熟，越来越多的开发人员基于使用 K8S 的痛点开发相关工具。套用一句今年 KubeCon 的 Keynote 演讲上，阿里云智能容器平台负责人丁宇的话： Kubernetes 正当时，云原生未来可期 。\n","date":1562138431,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1562138431,"objectID":"ab3f556204e3746f3958a13d19fda9df","permalink":"https://guoxudong.io/post/kustomize-5/","publishdate":"2019-07-03T15:20:31+08:00","relpermalink":"/post/kustomize-5/","section":"post","summary":"在以往的 pipeline 中，使用 kubectl 进行部署 Deployment 后无法检查 Deployment 是否部署成功，只能通过使用命令/脚本来手动检查 Deployment 状态，而 kubedog 这个小工具完美解决了这个问题，完善了 CI/CD 流水线的最后一步。","tags":["kubernetes","kustomize","工具"],"title":"使用 Kustomize 帮你管理 kubernetes 应用（五）：配合 kubedog 完善 CI/CD 的最后一步","type":"post"},{"authors":["郭旭东"],"categories":["kustomize"],"content":" 现象 在日常 CI/CD 流程中，已经将 Kustomize 集成到 pipeline 中使用，但是在对一个项目进行 Kustomize 改造时，将单个 deploy.yaml 拆分为了若干个 patch 以达到灵活 Kubernetes 部署的目的。但是在使用 kubectl apply -k . 命令进行部署的时候遇到了 error: failed to find an object with apps_v1_Deployment|myapp to apply the patch 的报错。\n解决之路 由于之前的使用中没有遇到此类报错，看报错信息像是 apiVersion 的问题，所以先检查了所有 patch 的 apiVersion ，但是并没有找到有什么问题。\nGoogle 搜索 对该报错进行了搜索，搜索到如下结果：\n？？？ 为何这个 issue 没有解决就被提出者关闭了？\n问题解决 在 Google 了一圈之后还是没有找到什么有营养的回答，问题又回到了原点\u0026hellip;只能对所有的 patch 的每个字符和每个配置逐一进行了检查。结果发现是 name 的内容 base 与 overlays 不同\u0026hellip; base 中是 name:myapp ，而 overlays 中是 name:my-app \u0026hellip;\n好吧，issue 关的是有道理的\u0026hellip;\n","date":1562132690,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1562132690,"objectID":"7e9caa8e104a2224b1fdbff449be4919","permalink":"https://guoxudong.io/post/kustomize-err-1/","publishdate":"2019-07-03T13:44:50+08:00","relpermalink":"/post/kustomize-err-1/","section":"post","summary":"解决使用 Kustomize 时遇到的报错： error: failed to find an object with apps_v1_Deployment|myapp to apply the patch ","tags":["kubernetes","kustomize","工具"],"title":"记一次使用 Kustomize 时遇到的愚蠢问题","type":"post"},{"authors":["郭旭东"],"categories":["程序员趣闻"],"content":" 前言 2019年6月24-26日，KubeCon + CloudNativeCon + Open Source Summit大会在上海世博中心举行。本次大会规模空前，预计有超过40个国家，3500多名云原生、开源领域的开发者参加，门票更是早早售罄。作为一名云原生应用的使用者与开发者，我也报名参与了这次大会。\n6月的上海已经入梅，潮湿的空气对于已经在上海生活好多年的我还是会感受到不适，但是这些也无法阻碍 KubeCon 带给我的兴奋与激动，况且这次是在家门口举行，3站地铁就能到达了世博中心。\n参与本次大会不仅仅是因为可以接触到最新的 Kubernetes \u0026amp; Cloud Native 实践，更是因为可以与很多神交已久的朋友会面，同时也可以与很多业界大牛面对面的交流，获取宝贵的经验。\n国内云原生开源力量强劲 会议第一天，先后参加了蚂蚁金服组织的 SOFAStack Workshop 和由 CNCF, VMware, 阿里云和 PingCAP 联合主办的中国原创CNCF项目社区沙龙，同时在午休时候短暂的旁听了华为主办的 Apache ServiceComb Meetup 。\n作为 ServiceMesher 社区的一员，与会议的组织者在社区中都很熟悉了，虽然没有注册 SOFAStack Workshop 会议，但也很自然的混进去了。作为金融级分布式框架 SOFAStack ，是用于快速构建金融级分布式架构的一套中间件，也是建立于蚂蚁金服海量金融场景锤炼出来的最佳实践。而此次的 Workshop 更是经过了精心的准备，准备了充足的材料，并在蚂蚁同学的帮助了使用 SOFAStack 实践了 使用 SOFAStack 快速构建微服务 、 SOFABoot 动态模块实践 、 使用 Seata 保障支付一致性 、 基于 Serverless 轻松构建云上应用 和 使用 CloudMesh 轻松实践 Service Mesh 五个demo。在 Service Mesh 方面，蚂蚁的同学异常的活跃，是 Istio 中文文档翻译和 ServiceMesher 社区的主要组织者。\n《未来架构-从服务化到云原生》作者敖小剑老师现场签名\n午饭后短暂旁听了华为主办的 Apache ServiceComb Meetup ，虽然时间短暂，只有不到1个小时，但是对于国内运作开源项目，尤其是 Apache 项目有个更深的理解，同时让我联想到了之前在 《从开源小白到 Apache Member ，阿里工程师的成长笔记》 阿里巴巴技术专家望陶成为的文章，中国的开源软件和开发者在开源领域起到越来越重要的作用。\n下午参加了由 CNCF, VMware, 阿里云和 PingCAP 联合主办的中国原创CNCF项目社区沙龙，聆听了 Harbor 、 Dragonfly 、 TiKV 三个中国原创 CNCF 项目的分享，同时 李响、Dan Kohn 等大佬也在会上发言表达了对国内 CNCF 开源项目的肯定及期待。而在会上也结识了阿里云容器镜像服务的开发小哥，作为阿里云的资深用户与开发小哥进行了交流，了解一些容器镜像服务方面的新功能，同时也反应了在使用中遇到的问题，总的来说收获颇丰。\n现场惊现 Linux 及 Git 创始人 Linus Torvalds 大会第二天的 Keynote 一直是 Linux 基金会宣传其理念，愿景以及赞助商进行市场宣传的重要活动。而当天最让人激动的就是 Linus 在 Keynote 后的一次嘉宾谈话，毫不夸张的说，我的工作就是 Linus 给的。而为了看到活的 Linus ，很多人一大早就在 red hall 的门前排起了长队，由于没有看好座位的分布，我只是找到了一个比较偏的位置，但是还是可以看的比较清楚。 Linus 本人还是十分幽默的，在谈话中提到了 Linux 5.1-rc6 的 release 计划，同时还询问现成有多少人是从事内核开发的，不过现场举手的同学并不多。\nKeynote 上还提到了中国在整个云原生运动中的巨大贡献，中国的 K8s contributors 已经在全球所有贡献者中排名第二，超过 10% 的 CNCF 会员来自中国，26%的 Kubernetes 的认证供应商来自中国，同时也公布了蚂蚁金服作为黄金会员加入 CNCF。\n平均每小时一个的分组会议，我的日程安排的满满的，但就这样还是出现了由于到晚了无法进入分组会议室的情况，注意这里不是因为到晚了不让进，而是进去都没有站的地方！可见人气之旺！让我有了像是上学时候穿梭在教学楼，赶人气高的选修课的错觉。\n而在赞助商展示区也有不少的收获，与rancher、kong、jenkins等开源软件的开发者进行了交流，同时也获得了不少小礼品。最大的收获就是在阿里云展台与张磊大神的合影。\n与张磊大神的合影\nServiceMesher 社区的壮大 平时都是在网上与社区的朋友们进行交流，讨论技术，交流经验。而 KubeCon 就变成了一场网友面基大会，见到了很多有过交流和帮助过我的朋友，包括 Jimmy 、 小剑 、秀龙老哥\u0026hellip;等等，同时也认识了不少新朋友。与去年11月的 KubeCon 相比，社区的朋友越来越多，在短短半年内 Service Mesh 相关书籍已经出版了4本，而作者都是社区成员，可见社区的活跃。\n写在最后 正如 KubeCon 第三天 Keynote 上，阿里云智能容器平台负责人丁宇的话：Kubernetes 正当时，云原生未来可期 。在 KubeCon 上看到云原生及开源软件的发展速度迅猛，各大厂商也都在最近几年组建了自己的开源团队，在使用开源软件获取便利的同时也在回馈社区，这也是让竞争对手共同为一款开源软件进行贡献的原因。相信随着开源运动在国内的深入，将会出现越来越多中国原创的开源项目，也会有更多的开发者加入到开源项目中，在贡献的同时提升自己的技术水平。\n","date":1562033898,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1562033898,"objectID":"1e931237635191ed55923a5c2ff5e9ea","permalink":"https://guoxudong.io/post/kubecon-2019/","publishdate":"2019-07-02T10:18:18+08:00","relpermalink":"/post/kubecon-2019/","section":"post","summary":"2019年6月24-26日，KubeCon + CloudNativeCon + Open Source Summit大会在上海世博中心举行。本次大会规模空前，预计有超过40个国家，3500多名云原生、开源领域的开发者参加，门票更是早早售罄。作为一名云原生应用的使用者与开发者，我也报名参与了这次大会。","tags":["云原生"],"title":"Kubecon 2019 见闻：云原生未来可期","type":"post"},{"authors":["郭旭东"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1561939200,"objectID":"711b3f39486ff5b2c784fcff46a4397a","permalink":"https://guoxudong.io/publication/wx_contact/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/wx_contact/","section":"publication","summary":"","tags":null,"title":"扫码关注","type":"publication"},{"authors":["郭旭东"],"categories":null,"content":" 微信 要添加微信，请注明姓名-公司信息\nGitHub https://github.com/sunny0826\n邮箱 sunnydog0826@gmail.com\n凯京技术团队 https://my.oschina.net/keking\n微信公众号 扫码关注\n","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1561939200,"objectID":"39411413e0aa0daf78edc55b6eb471fd","permalink":"https://guoxudong.io/publication/contact_me/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/contact_me/","section":"publication","summary":"微信 要添加微信，请注明姓名-公司信息 GitHub https://github.com/sunny0826 邮箱 sunnydog0826@gmail.com 凯京技术团队 https://my.oschina.net/keking 微信公众号 扫码关注","tags":null,"title":"联系方式","type":"publication"},{"authors":["郭旭东"],"categories":["kustomize"],"content":" 前言 在前面的文章中已经介绍了 kustomize 是什么，以及如何开始使用和如何简单的在 CI/CD 中使用，本篇文章将会介绍 kustomize 的核心文件 kustomization.yaml。\n另外，博主已经向 kustomize 贡献了中文文档，已被官方采纳，现在在 kustomize 中的 docs/zh 目录中就可看到，翻译的不好的地方欢迎指正。同时也在 GitHub 上新建了一个 名为 kustomize-lab 的 repo 用于演示 kustomize 的各种用法及技巧，本文中介绍的内容也会同步更新到该 repo 中，欢迎 fork、star、PR。\nkustomization.yaml 的作用  Kustomize 允许用户以一个应用描述文件 （YAML 文件）为基础（Base YAML），然后通过 Overlay 的方式生成最终部署应用所需的描述文件。\n 有前面的文章《使用 Kustomize 帮你管理 kubernetes 应用（二）： Kustomize 的使用方法》中已经介绍了，每个 base 或 overlays 中都必须要有一个 kustomization.yaml，这里我们看一下官方示例 helloWorld 中的 kustomization.yaml：\ncommonLabels: app: hello resources: - deployment.yaml - service.yaml - configMap.yaml  可以看到该项目中包含3个 resources ， deployment.yaml、service.yaml 、 configMap.yaml。\n. └── helloWorld ├── configMap.yaml ├── deployment.yaml ├── kustomization.yaml └── service.yaml  直接执行命令：\nkustomize build helloWorld  就可以看到结果了：\napiVersion: v1 data: altGreeting: Good Morning! enableRisky: \u0026quot;false\u0026quot; kind: ConfigMap metadata: labels: app: hello name: the-map image: caption: \u0026quot;Image from: [**Pexels**](https://www.pexels.com)\u0026quot; focal_point: \u0026quot;\u0026quot; preview_only: false --- apiVersion: v1 kind: Service metadata: labels: app: hello name: the-service spec: ports: - port: 8666 protocol: TCP targetPort: 8080 selector: app: hello deployment: hello type: LoadBalancer image: caption: \u0026quot;Image from: [**Pexels**](https://www.pexels.com)\u0026quot; focal_point: \u0026quot;\u0026quot; preview_only: false --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: hello name: the-deployment spec: replicas: 3 selector: matchLabels: app: hello template: metadata: labels: app: hello deployment: hello spec: containers: - command: - /hello - --port=8080 - --enableRiskyFeature=$(ENABLE_RISKY) env: - name: ALT_GREETING valueFrom: configMapKeyRef: key: altGreeting name: the-map - name: ENABLE_RISKY valueFrom: configMapKeyRef: key: enableRisky name: the-map image: monopole/hello:1 name: the-container ports: - containerPort: 8080  从上面的结果可以看大 kustomize 通过 kustomization.yaml 将3个 resources 进行了处理，给三个 resources 添加了共同的 labels app: hello 。这个示例展示了 kustomization.yaml 的作用：将不同的 resources 进行整合，同时为他们加上相同的配置。\n进阶使用 上面只不过是一个简单的示例，下面将结合实际情况分享一些比较实用的用法\n根据环境生成不同配置 在实际的使用中，使用最多的就是为不同的环境配置不同的 deploy.yaml，而使用 kustomize 可以把配置拆分为多个小的 patch ，然后通过 kustomize 来进行组合。而根据环境的不同，每个 patch 都可能不同，包括分配的资源、访问的方式、部署的节点都可以自由的定制。\n. ├── flask-env │ ├── README.md │ ├── base │ │ ├── deployment.yaml │ │ ├── kustomization.yaml │ │ └── service.yaml │ └── overlays │ ├── dev │ │ ├── healthcheck_patch.yaml │ │ ├── kustomization.yaml │ │ └── memorylimit_patch.yaml │ └── prod │ ├── healthcheck_patch.yaml │ ├── kustomization.yaml │ └── memorylimit_patch.yaml  这里可以看到配置分为了 base 和 overlays， overlays 则是继承了 base 的配置，同时添加了诸如 healthcheck 和 memorylimit 等不同的配置，那么我们分别看一下 base 和 overlays 中 kustomization.yaml 的内容\n base\ncommonLabels: app: test-cicd resources: - service.yaml - deployment.yaml   base 中的 kustomization.yaml 中定义了一些基础配置\n overlays\nbases: - ../../base patchesStrategicMerge: - healthcheck_patch.yaml - memorylimit_patch.yaml namespace: devops-dev   overlays 中的 kustomization.yaml 则是基于 base 新增了一些个性化的配置，来达到生成不同环境的目的。\n执行命令\nkustomize build flask-env/overlays/dev  结果\napiVersion: v1 kind: Service metadata: labels: app: test-cicd name: test-cicd namespace: devops-dev spec: ports: - name: http port: 80 targetPort: 80 selector: app: test-cicd type: ClusterIP image: caption: \u0026quot;Image from: [**Pexels**](https://www.pexels.com)\u0026quot; focal_point: \u0026quot;\u0026quot; preview_only: false --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: test-cicd name: test-cicd namespace: devops-dev spec: replicas: 1 selector: matchLabels: app: test-cicd template: metadata: labels: app: test-cicd version: 0.0.3 spec: containers: - env: - name: ENV value: dev image: guoxudongdocker/flask-python:latest imagePullPolicy: Always livenessProbe: httpGet: path: / port: 80 initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 20 name: test-cicd readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 20 resources: limits: cpu: 300m memory: 500Mi requests: cpu: 300m memory: 500Mi volumeMounts: - mountPath: /etc/localtime name: host-time imagePullSecrets: - name: registry-pull-secret volumes: - hostPath: path: /etc/localtime name: host-time  可以看到包括 replicas、limits、requests、env 等 dev 中个性的配置都已经出现在了生成的 yaml 中。由于篇幅有限，这里没有把所有的配置有罗列出来，需要的可以去 GitHub 上自取。\n结语 上面所有的 kustomize build dir/ 都可以使用 kubectl apply -k dir/ 实现，但是需要 v14.0 版以上的 kubectl，也就是说，其实我们在集成到 CI/CD 中的时候，甚至都不需要用来 kustomize 命令集，有 kubectl 就够了。\n由于篇幅有限，这里没法吧所有 kustomization.yaml 的用途都罗列出来，不过可以在官方文档中找到我提交的中文翻译版 kustomization.yaml，可以直接去官方 GitHub 查看。同时 kustomize-lab 会持续更行，敬请关注。\n","date":1558587012,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1558587012,"objectID":"22fb7732ca05c3c498912ca482816795","permalink":"https://guoxudong.io/post/kustomize-4/","publishdate":"2019-05-23T12:50:12+08:00","relpermalink":"/post/kustomize-4/","section":"post","summary":"本篇为系列文章第四篇，将简述 kustomize 的核心配置文件 kustomization.yaml","tags":["kubernetes","kustomize","工具"],"title":"使用 Kustomize 帮你管理 kubernetes 应用（四）：简述核心配置 kustomization.yaml","type":"post"},{"authors":["郭旭东"],"categories":["devops"],"content":" 前言 公司之前一直在使用 Jenkins 作为 CI/CD 工具， Jenkins 非常强大，它完成了几乎所有 CI/CD 的工作，并且应用于整个团队有好长一段时间了。但是随着公司推荐数字化、智慧化，以及服务容器化的推进， Jenkins 的一些弊端也凸显了出来：\n 重量级： Jenkins 功能十分齐全，几乎可以做所有的事情。但是这也是他的一个弊端，过于重量级，有时候往往一个小的修改需要改动许多地方，升级\\下载插件后需要进行重启等。 升级不易： 在一些安全 Jenkins 相关的安全漏洞被公开后，我们会对 Jenkins 进行升级，但这也不是一件容易的事。之前就出现过升级\\重启后，所有 job 丢失，虽然我们所有项目配置都是以 Jenkinsfile 的形式统一存储，但是每个 job 都需要重新重新创建，包括每个 job 的权限\u0026hellip;.._(´ཀ`」 ∠)_ 权限控制复杂： 这其实也是 Jenkins 的一大优势，可以精确控制每个用户的权限，但是需要花费更多时间去配置，时间长了也会出现权限混乱的问题。 UI 界面： 这个其实是吐槽最多的部分，虽然有诸如：Blue Ocean 这样的插件来展示 pipeline ，但是还是没有从根本改变它简陋的 UI 界面。  那么为什么选择使用 Drone 呢？\n其实在 GitHub 上提交 PR 后，大部分开源项目都会使用 travis-ci 对提交的代码进行 CI 及检查，而如果是 Kubernetes 相关的项目，则会使用 prow 进行 CI。但是 travis-ci 只能用于 GitHub ，在寻找类似项目的时候， Drone 进入了我的视野。\n大道至简。和 Jenkins 相比， Drone 就轻量的多了，从应用本身的安装部署到流水线的构建都简洁的多。由于是和源码管理系统相集成，所以 Drone 天生就省去了各种账户\\权限的配置，直接与 gitlab 、 github 、 Bitbucket 这样的源码管理系统操作源代码的权限一致。正如它官网上写的那样：\n Any Source Code Manager\nDrone integrates seamlessly with multiple source code management systems, including GitHub, GitHubEnterprise, Bitbucket, and GitLab.\nAny Platform\nDrone natively supports multiple operating systems and architectures, including Linux x64, ARM, ARM64 and Windows x64.\nAny Language\nDrone works with any language, database or service that runs inside a Docker container. Choose from thousands of public Docker images or provide your own.\n Drone 天生支持任何源码管理工具、任何平台和任何语言。\n而写这篇文章的目的，并不是要吹捧这个工具有多么的好用，而是要总结在搭建 drone 和使用时候需要的各种坑，帮助读者绕过这些坑。\n声明 鉴于在使用 Drone CI 中，遇到的各种坑都和 Drone 的版本有关，这里首先声明我使用的 Drone 版本为1.1，使用0.8版本的同学请绕道。\n搭建 Drone 这里要说的就是在使用 drone 中遇到的第一个坑，在最初正准备搭建 drone 的时候 Google 了很多相关的 blog ，大部分 blog （包括某些 medium.com 上面近期的英文 blog） 推荐的安装方式都是使用 docker-compose，而无一例外的都失败了\u0026hellip;走投无路之下，我回到了官网的文档，发现1.0之后许多参数都发生了变化，并且官方推荐使用 docker 的方式运行 Drone。\n所以在使用任何开源软件之前都要去阅读它的文档，不要跟着一篇 blog 就开始了（包括我的），这样会少踩很多坑！！！\n这里以 gitlab 为例，展示网上版本启动参数和实际参数的不同：\n   作用 各种blog 官网文档     设置 Drone 的管理员 DRONE_ADMIN=admin DRONE_USER_CREATE=username:admin,admin:true   设置GitLab的域名 DRONE_GITLAB_URL DRONE_SERVER_HOST   GitLab的Application中的key DRONE_GITLAB_CLIENT DRONE_GITLAB_CLIENT_ID   GitLab的Application中的secret DRONE_GITLAB_SECRET DRONE_GITLAB_CLIENT_SECRET   Drone 域名 DRONE_HOST DRONE_GITLAB_SERVER    上面只是列举了部分官方文档和网上流产版本的不同，所以在使用之前一定要仔细阅读官方文档。下附运行 drone 的命令：\ndocker run \\ --volume=/var/run/docker.sock:/var/run/docker.sock \\ --volume=/var/lib/drone:/data \\ --env=DRONE_GIT_ALWAYS_AUTH=false \\ --env=DRONE_GITLAB_SERVER={your-gitlab-url} \\ # gitlab 的 URL --env=DRONE_GITLAB_CLIENT_ID={your-gitlab-applications-id} \\ #GitLab的Application中的id --env=DRONE_GITLAB_CLIENT_SECRET={your-gitlab-applicati-secret} \\ #GitLab的Application中的secret --env=DRONE_SERVER_HOST={your-drone-url} \\ # drone 的URl --env=DRONE_SERVER_PROTO=http \\ --env=DRONE_TLS_AUTOCERT=false \\ --env=DRONE_USER_CREATE=username:{your-admin-username},admin:true \\ # Drone的管理员 --publish=8000:80 \\ --publish=443:443 \\ --restart=always \\ --detach=true \\ --name=drone \\ drone/drone:1.1  关于 gitlab Application 的配置和 Drone 其他参数含义请参考官方文档，这里只展示单节点办的运行方式。\n核心文件 .drone.yml 要使用 Drone 只需在项目根创建一个 .drone.yml 文件即可，这个是 Drone 构建脚本的配置文件，它随项目一块进行版本管理，开发者不需要额外再去维护一个配置脚本。其实现代 CI 程序都是这么做了，这个主要是相对于 Jekins 来说的。虽然 Jekins 也有插件支持，但毕竟还是需要配置。\n 值得注意的事这个文件时 .drone.yml，由于 Kubernetes 使用的多了，第一次创建了一个 .drone.yaml 文件，导致怎么都获取不到配置\u0026hellip;_(´ཀ`」 ∠)_\u0026hellip; YAML 工程师石锤了\u0026hellip;\n 这里放一个 Java 的 .drone.yml ，这个项目是 fork 别人的项目用作演示，记得要修改 deployment.yaml 中的镜像仓库地址修改为自己的私有仓库。\n示例项目源码：https://github.com/sunny0826/pipeline-example-maven\nkind: pipeline name: pipeline-example-maven steps: - name: Maven编译 image: maven:3-jdk-7 volumes: - name: cache path: /root/.m2 commands: - mvn clean install - name: 构建镜像 image: plugins/docker volumes: - name: docker path: /var/run/docker.sock settings: username: from_secret: docker_user password: from_secret: docker_pass repo: {your-repo} registry: {your-registry} tags: ${DRONE_BUILD_NUMBER} - name: Kubernetes 部署 image: guoxudongdocker/kubectl:v1.14.1 volumes: - name: kube path: /root/.kube commands: - sed -i \u0026quot;s/#Tag/${DRONE_BUILD_NUMBER}/g\u0026quot; deployment.yaml - kubectl apply -f deployment.yaml - name: 钉钉通知 image: guoxudongdocker/drone-dingtalk settings: token: from_secret: dingding type: markdown message_color: true message_pic: true sha_link: true when: status: [failure, success] volumes: - name: cache host: path: /tmp/cache/.m2 - name: kube host: path: /tmp/cache/.kube/.test_kube - name: docker host: path: /var/run/docker.sock trigger: branch: - master  值得注意的事：上面的这个 .drone.yml 文件将本地的.m2文件、kubeconfig文件、docker.sock 文件挂载到 pipeline 中以实现 maven 打包缓存，k8s 部署、docker 缓存的作用，以提高 CI 速度。而是用挂载需要管理员在项目 settings 中勾选 Trusted ，这个操作只能管理员进行，普通用户是看不到这个选项的。而管理员就是在docker运行时候 --env=DRONE_USER_CREATE=username:{your-admin-username},admin:true 设置的。\n而上传镜像和钉钉同时需要在 settings 设置中添加 secret\n docker_user：docker 仓库用户名 docker_pass：docker 仓库密码 dingding： 钉钉机器人 token   注意这里的钉钉 token 是 webhook 中 https://oapi.dingtalk.com/robot/send?access_token= 后这部分  构建结果 添加 .drone.yml 文件后，向 master 分支提交代码即可出发 CI 构建\nCI 结束后，会在钉钉机器人所在群收到通知\n插件支持 可以看到，每一步的镜像都是一个镜像，上面 pipeline 中的 Kubernetes 及钉钉通知插件就是我开发的，具体开发方法可以参考官方文档，而官方也提供了许多官方插件。\n 构建后部署：Kubernetes、helm、scp 构建后通知：钉钉 、Email、Slack、微信  后记 Drone 整体用起来还是很方便的，搭建、上手速度都很快，但是官方文档给的不够详实，而网上充斥着各种各样0.8版本的的实例，但是其实官网早就发布了1.0版本，而官方并没有 example 这样的示例项目，这样就又把本来降下来的学习曲线拉高了。许多坑都需要自己去趟，我在测试 drone 的时候，就构构建了上百次，不停的修改 .drone.yml ， commit 信息看起来是很恐怖的。后续抽空会向官方贡献 example 这样的 PR。\n","date":1558400340,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1558400340,"objectID":"67f81b7dad5f6e2c804955d275320995","permalink":"https://guoxudong.io/post/drone-ci/","publishdate":"2019-05-21T08:59:00+08:00","relpermalink":"/post/drone-ci/","section":"post","summary":"本文介绍一款轻量级的 CI 工具 Drone ，同时也介绍在实践中遇到的一些坑，帮助你快速搭建持续集成流水线。","tags":["devops","drone","工具"],"title":"轻量快速的 CI 工具 Drone","type":"post"},{"authors":null,"categories":["翻译"],"content":" 在当前的 IT 实践中，为了支持高效和快捷的软件开发，出现了巨大的转变：在单体应用的软件架构正在逐渐被微服务架构取代的情况下，开发、 QA 和运维团队为了摆脱了之前相互孤立的状况，开始相互关联并融合统一，我们将其称为DevOps。\n当今如果一个技术驱动型企业想要以客户为导向来进行快速的软件迭代，那么他们需要更快速的软件开发和交付周期。而这些需求直接导致了 DevOps 文化的核心 CI/CD 实践的诞生。\nCI 持续集成：一种专注于使发布更容易的软件开发实践。\nCD 持续交付：是持续集成的延伸，以确保您可以以可持续的方式快速向客户发布新的更改。\n早期整个 SDLC 都是线性和顺序的，这拉长了产品的发布周期。但随着快速变化的市场动态、激烈的竞争和多变的客户需求，这些都会导致公司无法继续使用原先开发流程。他们必须更贴近客户，需要不断创新以保持他们的参与。而 DevOps 为此提供了解决方案，并被技术驱动的公司广泛采用，用来改进其快速交付的流程和实践。\n那么让我们试着了解什么是 DevOps ？\nDevOps 定义：\n提到 DevOps 我经常引用 w.r.t 的话：\n 任何公司在学会使用最佳 DevOps 实践进行协作、统一和自动化所有开发和运维流程之前，都将无法扩展和维持。这是一种将每个人联系在一起实现共同目标的文化。\n 是时候让所有团队成员与他们的部门密切合作，采用工具和实践来高效地交付软件产品。\nAtlassian 将 DevOps 定义为：\nDevOps 是一组软件开发和运维团队之间的自动化流程实践，以便他们可以快速、可靠地构建，测试和发布软件。DevOps 的概念建立在为过去相对孤立的团队之间建立合作文化的基础上。其带来的好处包括更加可靠、更快的软件发布，快速解决关键问题的能力，以及更好地管理计划外的工作。\nAmazon 将 DevOps 定义为：\n文化理念、实践和工具的结合，提高了团队的高速交付应用程序和服务的能力：与使用单体应用的软件开发架构管理流程的团队相比，以更快的速度开发和改进产品。这种速度使团队能够更好地为客户服务，并在市场竞争中占据有利地位。\nMicrosoft 以更简化的方式定义 DevOps：\n“DevOps 可以为我们的终端客户持续提供价值的人员，流程和产品的结合。”\n我认为 DevOps 的最佳解释为：它是一种文化，是一种将人放在首位团队理念，为他们提供适宜的环境，使他们能够蓬勃发展，因此无论他们属于什么部门，都可以通过明确的流程进行协作和沟通，从而实现目标。\nDevOps 是如何工作的？ 如上所述，DevOps 没有任何固定的规则和实践，但它更像是通过来自不同部门具有不同的技能的团队在一起以实现预期的结果的文化。那么它实际上是如何工作的，让我通过下图简要解释一下：\n因此，开发人员，QA 和运维团队使用 CI/CD 实践来实现客户的预期目标。开发人员编写代码并将其提交到 GitHub 等源码控制工具。DevOps 工程师使用 CI 工具来提取代码，来进行自动化测试，并通过 CD 工具部署到处理生产或测试服务器。\n开发和运维人员一起工作，并使用各种工具进行 CI/CD 和监控，以快速响应客户需求并修复问题和错误。\nDevOps工具在DevOps周期的各个阶段： 如下所示： DevOps 工程师可以使用多种工具在整个 DevOps 生命周期的每个阶段获取期望的结果。\n计划： 您可以使用 Jira 或 Azure DevOps Board 以敏捷方式管理和规划您的工作。\n开发： 对于代码管理， Git 以分布式方式管理代码版本历史、分支、推送和拉取机制的首要工具。您还可以使用 Microsoft TFVC（Team Foundation Version Control），这是一个集中版本控制系统。\n测试： 要进行自动化测试，您可以使用 Selenium 、 JUnit 和 Apache JMeter 。\n集成： Jenkins 是目前最受欢迎的 CI 工具之一，它可以做到无缝地集成开发和运维流程。\n其他 CI 工具还有 Travis ＆ Bamboo。\n部署和配置管理： Docker 是最受欢迎和广泛使用的持续部署工具之一。它也是软件容器化工具。\n其他部署和配置管理工具还有 Kubernetes 、 Chef 、Ansible 和 Puppet。\nKubernetes 是一个开源容器管理（编排）工具。其容器管理职责包括容器部署，容器扩展和伸缩以及容器的负载均衡。\n监控： 将产品部署到正确的位置后，持续的监控就变得至关重要。 Nagios 、 Splunk 和 New Relics 是广泛使用的持续监控工具。\nDevOps 最佳实践： 正如文章开头所讨论的那样，为了使技术驱动的公司变得更加以客户为导向，他们需要将自己从单体应用的软件开发实践转变为为客户发布产品的敏捷方式。让我们试着了解他们需要采用的最佳 DevOps实践：\n 持续集成 持续交付 微服务 基础设施即代码 监控和日志 沟通与协作  让我简要解释一下\n1. 持续集成： Amazon 将 CI 定义为：\nDevOps 软件开发实践，开发人员定期将代码修改合并到中央存储库，然后运行自动构建和测试。持续集成通常是指软件发布过程的构建或集成阶段，并且需要自动化组件（例如，CI 或构建服务）。持续集成的目的是更快地发现和解决问题，提高软件质量，并减少验证和发布新软件更新所需的时间。\n2.持续交付： 持续交付是一种软件开发实践，其中开发人员完成的任何代码更改都会自动为发布到生产环境做好准备。\n通过在构建阶段之后将所有的代码更改部署到测试环境或生产环境，持续交付可在持续集成时进行扩展。\n3. 微服务：敏捷开发的架构 这是一种新的软件设计方法，您可以将单个应用程序拆分为一组小型服务/模块。与单体应用架构将所有前端和后端代码库以及数据库都全部部署在同一个服务器地址中相比，基于微服务架构的应用程序被分解为服务，其中每个服务器都在其中运行使用基于 HTTP 的应用程序编程接口（API），通过定义良好的接口使自己与其他服务进行通信。\n按照 Amazon 的介绍：：\n微服务是围绕业务能力构建的; 每项服务的范围都是一个简单的用途。您可以使用不同的框架或编程语言来编写微服务并将它们作为单个服务或一组服务独立部署。\n4. 基础设施即代码：IaC 是通过机器可读定义文件（代码库）管理和配置计算机数据中心的过程，而不是物理硬件配置或交互式配置工具。\nAmazon 定义 IaC 为：\n作为使用代码和软件开发技术（例如版本控制和持续集成）来配置和管理基础架构的实践。云服务的 API 驱动模型使开发人员和系统管理员能够以编程方式大规模地与基础架构交互，而不需要手动设置和配置资源。\n因此，开发人员可以使用基于代码的工具与基础架构进行交互，使其更像应用程序。这使得可以使用标准化模式快速部署基础架构和服务器，使用最新的补丁和版本进行更新，或以副本的方式进行复制部署。\n传统的服务（生命周期）自动化和配置管理工具用于完成IaC。现在企业也在使用连续配置自动化工具或独立的 IaC 框架，例如 Microsoft 的 PowerShell DSC 或 AWS CloudFormation。\n5. 监控与日志 公司可以通过监控指标和日志，来了解其应用程序和基础架构的运行情况。APM（Application performance management 应用程序性能管理）将 IT 指标转换为有意义的业务指标，致力于检测和诊断复杂的应用程序性能问题，以维持预期的服务等级。\n通过捕获、分类、分析应用程序和基础架构生成的数据和日志，团队可以了解更新是如何影响用户的，从而深入了解问题或报错的根本原因。\nwiki 中介绍：\n密切监控两组性能指标。第一组性能指标定义了应用程序终端用户所体验的性能。性能的一个示例是峰值负载下的平均响应时间，包括加载和响应时间。\n 负载是应用程序处理的事务量，例如，每秒事务数（tps）、每秒请求数、每秒页数。在没有被计算机的搜索、计算、传输等需求加载的情况下，大多数应用程序都足够快，这就是开发人员在开发过程中可能无法捕获性能问题的原因。 响应时间是应用程序在此类负载下响应用户操作所需的时间。  6. 沟通与协作 在我看来：\n团队中的 DevOps 作为一种实践取得成功，需要2个基本支柱：沟通和协作，才能非常有效地工作。如果没有这种感觉并理解紧密结合团队工作的重要性，那么采用 DevOps 最佳实践将非常困难。\n增加团队中的沟通和协作是 DevOps文化 的关键方面之一。有了这种文化，团队就会以良好的态度和动力聚集在一起，围绕信息共享建立强有力的文化规范，并通过沟通工具和应用促进沟通，使团队的所有部门能够更加紧密地协调共同的目标。\n为何选择DevOps？它的好处是什么？ 要了解 DevOps 提升的价值已经其如何被公司所采用：\n让我们看看由 veritis 带给您的以下信息图表。\n以上给出的图表清楚地阐述了 DevOps 实践的主要好处：\n速度 DevOps 促进团队的高速开发，以便您可以更快地为客户进行创新，更好地适应不断变化的市场，并在推动业务成果方面提高效率。\n快速交付 通过基于 CI/CD 的 DevOps 文化，缩短了应用程序发布周期，允许更快的客户反馈和有意义的创新在团队内的蓬勃发展。您可以更快地发布新功能并修复错误，更快地响应客户的需求并建立竞争优势。\n可靠性 DevOps 使您能够通过持续集成和持续交付等实践不断提高您的软件质量，以测试每项变更的功能和安全性。这造就了可靠和经过测试的应用程序和强大的基础设施的开发。DevOps 持续监控和记录实践可以帮助您实时了解软件的性能。\n文化 DevOps 培养了一种伟大的工作文化，在其文化模式下建立更有效的团队，强调所有权和责任等价值观。\n安全 通过采用 DevOps 模型，组织可以使用基础架即代码和策略即代码，在不牺牲安全性的情况下大规模定义和跟踪合规性。他们可以在保持控制和合规性的同时快速进步。\nDevOps 的挑战 在团队中实施 DevOps 文化并不容易。没有标准的规则可以参考，它更像是改变个人和团队的心态的游戏。这就像要求人们离开他们的舒适区。\n “当你试图在团队中带来任何相当大的变化时，一开始可能看起来很难，但当你有足够大的意愿时，就会发生变化，并渐进达成目标。”\n 因此，让我们看看采用 DevOps 作为文化的一些常见挑战。\n1. Dev Vs Ops 心态 由于长期的开发和运维团队一直在孤立地工作，完成不同的任务。所以他们经过精心调整，以不同的方式思考和行动。开发人员试图尽快创新并做出改变，运维人员则试图保持 100％ 的服务可用性。他们的目标和优先事项是不同的，所以如果我们必须将 DevOps 作为团队中的文化实践，那么如果他们的心态还是两个孤立的部分，那么 DevOps 必将黯然失色。\nDevOps 的实践就是将团队整合在一起，打破 IT 组织内部的孤岛。因此，将它们整合为统一单元以实现共同目标是任何公司在采用 DevOps 实践时需要克服的第一个障碍。\n2. 从传统基础设施转向微服务架构 多年来，这些公司一直存在遗留的基础设施，但如果他们必须快速创新，他们必须摆脱这种方法并采用更具可扩展性的微服务架构。将基础设施即代码与微服务一起使用是迈向持续创新未来的又一步。\n然而，将架构变为微服务架构系统存在很大的障碍。采用微服务架构需要采用最佳的 DevOps 实践以及 CI/CD 实践。这为团队带来了巨大的工作量和运维挑战，同时也增加了成本因素。\n确实，将开发与部署转变为现代的软件开发方案可能会很痛苦，但一旦采用就可以使您的团队变得更加高效与可扩展。\n3. 开发和运维工具集的冲突 开发团队的目标和指标完全不同，因此他们可能需要一个运维团队可能不需要的工具集。因此，必须将两个团队聚集在一起，以了解他们两者可以合作的位置，并整合对他们两者都有意义的工具，并统一他们可以监控的目标和指标。\n一些团队可能不愿意使用传统工具，这些工具不仅技术上较差，而且由于兼容性问题也会降低整个基础架构的速度。因此，请确保正在使用的工具与公司的产品愿景保持一致。\n总结 无论我们在公司方面与个人方面有多么不同，我们都必须摒弃差异并作为一个整体来实现客户需求和解决客户的问题。如果我们能够在我们团队的工作文化中吸收这种理念，那么 DevOps 将成为重视过程和收益的宝贵实践。\n","date":1557366911,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557366911,"objectID":"de0d1781bad24bf64c51ae06a72e064f","permalink":"https://guoxudong.io/post/all-about-devops-fundamentalsyou-ever-wanted-to-know/","publishdate":"2019-05-09T09:55:11+08:00","relpermalink":"/post/all-about-devops-fundamentalsyou-ever-wanted-to-know/","section":"post","summary":"在当前的 IT 实践中，为了支持高效和快捷的软件开发，已经出现了伟大的转变--在传统的软件架构正在逐渐被微服务架构取代的情况下，开发、 QA 和运维团队为了摆脱了之前相互孤立的状况，开始将开发与运维相互关联并融合统一，我们将其称为DevOps。","tags":["devops"],"title":"什么是 DevOps ？DevOps 是如何工作的？","type":"post"},{"authors":["郭旭东"],"categories":["kustomize"],"content":" 前言 首先明确软件版本，我这里使用的是 Jenkins ver. 2.121.3 ，这个版本比较老，其上安装 Kubernetes 插件所使用 kubectl 版本也比较老，无法使用 Kustomize 的 yaml 文件需要的 apiVersion: apps/v1 ，直接使用生成 deploy.yaml 文件会报错，所以这里选择了自己构建一个包含 kubectl 和 kustomize 的镜像，在镜像中使用 Kustomize 生成所需 yaml 文件并在 Kubernetes 上部署。\n软件版本    软件 版本     Jenkins 2.121.3   kubectl v1.14.1   kustomize v2.0.3    前期准备  Jenkins ：本篇使用 Jenkins 演示 CI/CD ，安装 Jenkins 就不在赘述，可以使用多种方法安装 Jenkins ，详细方法见官网。同时。 CI/CD 的工具有很多，这里为了省事使用笔者现有的 Jenkins 进行演示，不推荐使用同笔者一样的版本，请使用较新的版本；同时也可以使用其他 CI/CD 工具，这里推荐使用 drone。如果有更好的方案，欢迎交流，可以在关于中找到我的联系方式。 - Web 应用：这里使用 flask 写了一个简单的 web 应用，用于演示，同样以上传 dockerhub [```guoxudongdocker/flask-python```](https://hub.docker.com/r/guoxudongdocker/flask-python) ## 目录结构 首先看一下目录结构，目录中包括 ```Dockerfile``` 、 ```Jenkinsfile``` 、 Kustomize 要使用的 ```deploy``` 目录以及 web 应用目录。  bush\n  . ├── Dockerfile ├── Jenkinsfile ├── app │ ├── main.py │ └── uwsgi.ini └── deploy ├── base │ ├── deployment.yaml │ ├── kustomization.yaml │ └── service.yaml └── overlays ├── dev │ ├── healthcheck_patch.yaml │ ├── kustomization.yaml │ └── memorylimit_patch.yaml └── prod ├── healthcheck_patch.yaml ├── kustomization.yaml └── memorylimit_patch.yaml\n 这里可以看到 overlays 总共有两个子目录 `dev` 和 `prod` ，分别代表不同环境，在不同的环境中，应用不同的配置。 ## Jenkins 配置 Jenkins 的配置相对简单，只需要新建一个 pipeline 类型的 job ![WX20190506-180159](https://wx4.sinaimg.cn/large/ad5fbf65gy1g2rr57oixbj20tn0ogq6v.jpg) 增加参数化构建，**注**：参数化构建需要安装 Jenkins 插件 ![WX20190506-180918](https://ws4.sinaimg.cn/large/ad5fbf65gy1g2rrcb5ic9j21470q7mz8.jpg) 然后配置代码仓库即可 ![WX20190507-094958](https://ws3.sinaimg.cn/large/ad5fbf65gy1g2sij1xlb2j214w0nw0uw.jpg) ## Pipeline  groovy podTemplate(label: \u0026lsquo;jnlp-slave\u0026rsquo;, cloud: \u0026lsquo;kubernetes\u0026rsquo;, containers: [ containerTemplate( name: \u0026lsquo;jnlp\u0026rsquo;, image: \u0026lsquo;guoxudongdocker/jenkins-slave\u0026rsquo;, alwaysPullImage: true ), containerTemplate(name: \u0026lsquo;kubectl\u0026rsquo;, image: \u0026lsquo;guoxudongdocker/kubectl:v1.14.1\u0026rsquo;, command: \u0026lsquo;cat\u0026rsquo;, ttyEnabled: true), ], nodeSelector:\u0026lsquo;ci=jenkins\u0026rsquo;, volumes: [ hostPathVolume(mountPath: \u0026lsquo;/var/run/docker.sock\u0026rsquo;, hostPath: \u0026lsquo;/var/run/docker.sock\u0026rsquo;), hostPathVolume(mountPath: \u0026lsquo;/usr/bin/docker\u0026rsquo;, hostPath: \u0026lsquo;/usr/bin/docker\u0026rsquo;), hostPathVolume(mountPath: \u0026lsquo;/usr/local/jdk\u0026rsquo;, hostPath: \u0026lsquo;/usr/local/jdk\u0026rsquo;), hostPathVolume(mountPath: \u0026lsquo;/usr/local/maven\u0026rsquo;, hostPath: \u0026lsquo;/usr/local/maven\u0026rsquo;), secretVolume(mountPath: \u0026lsquo;/home/jenkins/.kube\u0026rsquo;, secretName: \u0026lsquo;devops-ctl\u0026rsquo;), ], ) { node(\u0026ldquo;jnlp-slave\u0026rdquo;){ stage(\u0026lsquo;Git Checkout\u0026rsquo;){ git branch: \u0026lsquo;${branch}\u0026rsquo;, url: \u0026lsquo;https://github.com/sunny0826/flask-python.git' } stage(\u0026lsquo;Build and Push Image\u0026rsquo;){ withCredentials([usernamePassword(credentialsId: \u0026lsquo;docker-register\u0026rsquo;, passwordVariable: \u0026lsquo;dockerPassword\u0026rsquo;, usernameVariable: \u0026lsquo;dockerUser\u0026rsquo;)]) { sh \u0026ldquo;\u0026rsquo; docker login -u ${dockerUser} -p ${dockerPassword} docker build -t guoxudongdocker/flask-python:${Tag} . docker push guoxudongdocker/flask-python:${Tag} \u0026ldquo;\u0026rsquo; } } stage(\u0026lsquo;Deploy to K8s\u0026rsquo;){ if (\u0026lsquo;true\u0026rsquo; == \u0026ldquo;${deploy}\u0026rdquo;) { container(\u0026lsquo;kubectl\u0026rsquo;) { sh \u0026ldquo;\u0026rsquo; cd deploy/base kustomize edit set image guoxudongdocker/flask-python:${Tag} \u0026ldquo;\u0026rsquo; echo \u0026ldquo;部署到 Kubernetes\u0026rdquo; if (\u0026lsquo;prod\u0026rsquo; == \u0026ldquo;${ENV}\u0026rdquo;) { sh \u0026ldquo;\u0026rsquo; # kustomize build deploy/overlays/prod | kubectl apply -f - kubectl applt -k deploy/overlays/prod \u0026ldquo;\u0026rsquo; }else { sh \u0026ldquo;\u0026rsquo; # kustomize build deploy/overlays/dev | kubectl apply -f - kubectl applt -k deploy/overlays/dev \u0026ldquo;\u0026rsquo; }\t} }else{ echo \u0026ldquo;跳过Deploy to K8s\u0026rdquo; }\n } }  }\n 这里要注意几点： - 拉取 git 中的代码需要在 jenkins 中配置凭据。 - 笔者的 jenkins 部署在 Kubernetes 上，要操作集群的话，需要将 kubeconfig 以 Secret 的形式挂载到 jenkins 所在 namespace。 - `jenkins-slave` 需要 Java 环境运行，所以要将宿主机的 `jdk` 挂载到 `jenkins-slave` 中。 - 同样的，宿主机中需要事先安装 `docker`。 - `docker-register` 为 dockerhub 的登录凭证，需要在 jenkins 中添加相应的凭证。 ## 演示 image: caption: \u0026quot;Image from: [**Pexels**](https://www.pexels.com)\u0026quot; focal_point: \u0026quot;\u0026quot; preview_only: false --- ### 开始构建 这里选择环境、分支，填入版本即可开始构建，**注意：**这里的版本将已 tag 的形式标记 docker 镜像。 ![WX20190507-095142](https://ws2.sinaimg.cn/large/ad5fbf65gy1g2sikst7tuj20ob0evabw.jpg) 这里就可以看到构建成功了 ![WX20190507-103721](https://ws2.sinaimg.cn/large/ad5fbf65ly1g2sjw9w22ej20v80km0w3.jpg) ### 查看结果 这里为了方便（其实就是懒），我就不给这个服务添加 ingress 来从外部访问了，这里使用 [KT](https://yq.aliyun.com/articles/690519) 打通本地和 k8s 集群网络来进行调试。 \u0026gt;为了简化在Kubernetes下进行联调测试的复杂度，云效在SSH隧道网络的基础上并结合Kubernetes特性构建了一款面向开发者的辅助工具kt 这里看到这个服务正常启动了 ![WX20190507-104154](https://ws2.sinaimg.cn/large/ad5fbf65ly1g2sk11dnzxj20av027jrn.jpg) ### 发布新版本 更新 web 服务并提交 ![WX20190507-104936](https://ws4.sinaimg.cn/large/ad5fbf65gy1g2sk94v1c5j209702vwej.jpg) 按照上面步骤在 jenkins 中重新构建，当然也可以配置钩子，每次代码提交后自动构建 ### 查看查看新版本 同上面一样，在构建成功后查看服务是否更新 ![WX20190507-105539](https://wx4.sinaimg.cn/large/ad5fbf65gy1g2skfczaz4j20by01smx7.jpg) 可以看到，版本已经更新了 ### 发布生产环境 这里模拟一下发布生产环境，假设生产环境是在 `devops-prod` 的 namespace 中，这里只做演示之用，真正的生产环境中，可能存在不止一个 k8s 集群，这时需要修改 Jenkinsfile 中的 `secretVolume` 来挂载不同 k8s 的 kubeconfig 来达到发布到不同集群的目的。当然，一般发布生产环境只需选择测试通过的镜像来发布即可，不需要在进行构建打包。 ![WX20190507-110730](https://ws3.sinaimg.cn/large/ad5fbf65gy1g2skrnbjyuj20fc0bjmxp.jpg) ### 查看生产版本 ![WX20190507-110850](https://ws1.sinaimg.cn/large/ad5fbf65ly1g2skt3rp4yj20aq010glj.jpg) ### 总结 上面的这些步骤简单的演示了使用 jenkins 进行 CI/CD 的流程，流程十分简单，这里仅供参考 ## Kustomize 的作用 那么， Kustomize 在整个流程中又扮演了一个什么角色呢？ ### 更新镜像 在 `jenkinsfile` 中可以看到， kustomize 更新了基础配置的镜像版本，这里我们之前一直是使用 `sed -i \u0026quot;s/#Tag/${Tag}/g\u0026quot; deploy.yaml` 来进行替换了，但是不同环境存在比较多的差异，需要替换的越来越多，导致 Jekninsfile 也越来越臃肿和难以维护。 kustomize 解决了这个问题。  bash kustomize edit set image guoxudongdocker/flask-python:${Tag}\n ### 环境区分 上面也提到了，不同的环境我们存在这许多差异，虽然看上去大致类似，但是很多细节都需要修改。这时 kustomize 就起到了很大的作用，不同环境相同的配置都放在 `base` 中，而差异就可以在 `overlays` 中实现。  bash . ├── base │ ├── deployment.yaml │ ├── kustomization.yaml │ └── service.yaml └── overlays ├── dev │ ├── healthcheck_patch.yaml │ ├── kustomization.yaml │ └── memorylimit_patch.yaml └── prod ├── healthcheck_patch.yaml ├── kustomization.yaml └── memorylimit_patch.yaml\n可以看到， `base` 中维护了项目共同的基础配置，如果有镜像版本等基础配置需要修改，可以使用 `kustomize edit set image ...` 来直接修改基础配置，而真正不同环境，或者不同使用情况的配置则在 `overlays` 中 以 patch 的形式添加配置。这里我的配置是 prod 环境部署的副本为2，同时给到的资源也更多，详情可以在 [Github](https://github.com/sunny0826/flask-python) 上查看。 ### 与 kubectl 的集成 在 jenkinsfile 中可以看到  bash\nkustomize build deploy/overlays/dev | kubectl apply -f - kubectl apply -k deploy/overlays/dev ```\n这两条命令的执行效果是一样的，在 kubectl v1.14.0 以上的版本中，已经集成了 kustomize ，可以直接使用 kubectl 进行部署。\n结语 这里只是对 kustomize 在 CI/CD 中简单应用的展示，只是一种比较简单和基础的使用，真正的 CI 流程要比这个复杂的多，这里只是为了演示 kustomize 的使用而临时搭建的。而 kustomize 还有很多黑科技的用法，将会在后续的文章中介绍。\n","date":1557132388,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557132388,"objectID":"bf98527e66030d5f83f2abbed286f159","permalink":"https://guoxudong.io/post/kustomize-3/","publishdate":"2019-05-06T16:46:28+08:00","relpermalink":"/post/kustomize-3/","section":"post","summary":"本篇为系列文章第三篇，使用 jenkins 发布一个简单的使用 flask 写的 web 项目，来演示在 CI/CD 流程中 Kustomize 的简单使用。","tags":["kubernetes","kustomize","工具"],"title":"使用 Kustomize 帮你管理 kubernetes 应用（三）：将 Kustomize 应用于 CI/CD","type":"post"},{"authors":null,"categories":["devops"],"content":" DevOps 不仅仅是开发和运营团队。如果您想要充分发挥出 DevOps 方法的敏捷性和响应力，则必须在应用的整个生命周期内同时兼顾 IT 安全性。\n为什么？以往，安全性会在开发的最后阶段由特定的团队来负责实现。当开发周期长达数月、甚至数年时，上述做法不存在任何问题；但是，这种做法现在已经行不通了。有效的 DevOps 可顺利推进快速频繁的开发周期（有时全程只有数周或数天），但是过时的安全措施会对此造成负面影响，即使对于最高效的 DevOps 计划也是如此。\n现在，安全防护在 DevOps 协作框架中属于共同责任，而且需要在整个周期中整合相应的安全功能。这是一个非常重要的理念。它还使得“DevSecOps”一词应运而生，以用于强调必须为 DevOps 计划打下扎实的安全基础。\nDevSecOps 意味着，从一开始就要考虑应用和基础架构的安全性；同时还要让某些安全网关实现自动化，以防止 DevOps 工作流程变慢。选择正确的工具来持续确保安全性有助于实现安全目标。但是，有效的 DevOps 安全防护需要的不仅是新工具。它建立在 DevOps 文化变革的基础上，以便尽早集成安全团队的工作。\nDevOps 安全性为内置特性 无论您将其称为“DevOps”还是“DevSecOps”，最好始终能在应用的整个生命周期内确保安全性。DevSecOps 关乎内置安全性，而不是应用和数据层面的安全性。如果将安全性问题留到开发流程的最后环节再加以考虑，那么采用 DevOps 方案的组织会发现自己的开发周期又变长了，而这是他们从一开始就想要避免的情况。\n在某种程度上，DevSecOps 强调，在 DevOps 计划刚启动时就要邀请安全团队来确保信息的安全性，并制定自动安全防护计划。它还强调，要帮助开发人员从代码层面确保安全性；在这个过程中，安全团队需要针对已知的威胁分享可见性信息、提供反馈并进行智能分析。这可能还包括为开发人员提供新的安全培训，因为 DevSecOps 并非始终着眼于较为传统的应用开发模式。\n那么，怎样才算是真正地实现了内置安全性？对于新手而言，优质的 DevSecOps 策略应能确定风险承受能力并进行风险/收益分析。在一个给定的应用中，需要配备多少个安全控制功能？对于不同的应用，上市速度又有多重要？自动执行重复任务是 DevSecOps 的关键所在，因为在管道中运行手动安全检查可能会非常耗时。\nDevOps 安全性可自动实现 企业应该：确保采用时间短、频率高的开发周期；采取安全措施，以最大限度地缩短运营中断时间；采用创新技术，如容器和微服务；同时，还要促使常见的孤立团队加强合作 — 这对所有企业来说都是一项艰巨的任务。上述所有举措都与人有关，而且企业内部需要协同合作；但是，自动化才是有助于在 DevSecOps 框架中实现这些人员变化的关键所在。\n那么，企业应该在哪些方面实现自动化？具体又该怎么做呢？红帽提供了相应的书面指南来帮助解答上述问题。企业应该退后一步，并着眼于整个开发和运营环境。其中涉及：源控制存储库；容器注册表；持续集成和持续部署 (CI/CD) 管道；应用编程接口 (API) 的管理、编排和发布自动化；以及运营管理和监控。\n全新的自动化技术已帮助企业提高了开发实践的敏捷性，还在推动采用新的安全措施方面起到了重要作用。但是，自动化并不是近年来 IT 领域发生的唯一变化。现在，对于大多数 DevOps 计划而言，容器和微服务等云原生技术也是一个非常重要的组成部分。所以，企业必须调整 DevOps 安全措施，以适应这些技术。\nDevOps 安全性适用于容器和微服务 可通过容器实现的规模扩展和基础架构动态性提升改变了许多组织开展业务的方式。因此，DevOps 安全性实践必须适应新环境并遵循特定于容器的安全准则。云原生技术不适合用来落实静态安全策略和检查清单。相反，组织必须在应用和基础架构生命周期的每个阶段确保持续安全并整合相应的安全功能。\nDevSecOps 意味着，要在应用开发的整个过程中确保安全性。要实现与管道的这种集成需要秉持一种全新的思维方式，就像使用新工具一样。考虑到这一点，DevOps 团队应该实现安全防护自动化，以保护整体环境和数据；同时实现持续集成/持续交付流程——可能还要确保容器中的微服务的安全性。\n   环境和数据安全性： CI/CD 流程安全性：     实现环境的标准化和自动化。\n每项服务都应具有最小的权限，以最大限度地减少未经授权的连接和访问。 集成适用于容器的安全性扫描程序。\n应在向注册表添加容器的过程中实现这一点。   实现用户身份和访问控制功能的集中化。\n由于要在多个点发起身份验证，因此严格的访问控制和集中式身份验证机制对于确保微服务安全性而言至关重要。 自动在 CI 过程中完成安全性测试。\n其中包括在构建过程中运行安全性静态分析工具；而且在构建管道中提取任何预构建容器映像时，都要进行扫描，以检查是否存在已知的安全漏洞。   使运行微服务的容器相互隔离并与网络隔离。\n这包括传输中和静止的数据，因为获取这两类数据是攻击者的高价值目标。 在验收测试流程中加入针对安全性功能的自动化测试。\n自动执行输入验证测试，并针对验证操作实现身份验证和授权功能的自动化。   加密应用与服务间的数据。\n具有集成式安全功能的容器编排平台有助于最大限度地降低发生未经授权访问的可能性。 自动执行安全性更新，\n例如针对已知漏洞打修补。通过 DevOps 实现这一点。这样，在创建记录在案的可跟踪更改日志时，管理员便无需登录生产系统。   引入安全的 API 网关。\n安全的 API 可提高授权和路由的可见性。通过减少公开的 API，组织可以减小攻击面。 实现系统和服务配置管理功能的自动化。\n这样可以确保遵守安全策略，避免出现人为错误。审核和补救操作也应实现自动化。    ","date":1556865434,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1556865434,"objectID":"ccc88bd829982ba3ee58672f1bca2a1e","permalink":"https://guoxudong.io/post/what-is-devsecops/","publishdate":"2019-05-03T14:37:14+08:00","relpermalink":"/post/what-is-devsecops/","section":"post","summary":"DevOps 不仅仅是开发和运营团队。如果您想要充分发挥出 DevOps 方法的敏捷性和响应力，则必须在应用的整个生命周期内同时兼顾 IT 安全性。","tags":["devops"],"title":"什么是 DevSecOps ?","type":"post"},{"authors":["郭旭东"],"categories":["Kubernetes"],"content":"该PPT 为 2019年4月26日 在云栖社区分享使用，这里留作展示和记录，下载地址可以参考下方链接。\n 由于图片资源位于 GitHub 上，国内访问可能会有些慢，建议下载观看。\nPPT 下载地址：https://yq.aliyun.com/articles/700084\n","date":1556621184,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1556621184,"objectID":"0d1cb10fa870ad734ab32c695f1484e7","permalink":"https://guoxudong.io/post/aliyun-share/","publishdate":"2019-04-30T18:46:24+08:00","relpermalink":"/post/aliyun-share/","section":"post","summary":"该PPT 为 2019年4月26日 在云栖社区分享使用，这里留作展示和记录。","tags":["Kubernetes","阿里云"],"title":"4月29日 云栖社区分享PPT -- 阿里云容器服务的优势与调优","type":"post"},{"authors":null,"categories":["翻译"],"content":"  读完 Kubernetes: Up and Running 后，我写下了这篇文章。旨在为那些认为文章 TL;DR 的人进行一些总结，这同时也是一种强迫自己检查所阅读内容的好方法。\n 基于 Google Borg 的开源系统 K8S( Kubernetes ) 是一个非常强大的容器编排调度系统。 其整个生态系统，包括：工具，模块，附加组件等，都是使用 Golang 语言编写的，这使得 K8S 及其周边生态系统基本上是面向 API 对象、运行速度非常快的二进制文件的集合，并且这些二进制文件都有很好的文档记录，易于编写和构建应用程序。\n在深入了解之前，我想先介绍一下 K8S 的竞争对手：ECS 、 Nomad 和 Mesos 。ECS 是 AWS 自己的业务编排解决方案，而最近 AWS 上也引入了一个托管的 K8S 系统 \u0026ndash; EKS 。两者都提供 FARGATE ，允许用户运行其应用并忽略其运行物理资源。\nK8S 作为一个开源系统，在采用量上毫无疑问是最大赢家，同时它也可以以托管形式在三个主要云提供商上提供服务。然而，它比其他系统更加复杂。K8S 可以处理几乎任何类型的容器化工作负载，但这并不意味着每个人都需要它。用户也可以选择其他解决方案，例如，单独部署在 AWS 上的互联网产品可以在生产环境很好的使用 ECS 而非 K8S。\n话虽如此，k8s也有其神奇之处 \u0026ndash; 它可以在任何地方部署，同时拥有一个活跃的社区和数百个核心开发人员，以及其广泛生态系统中的数千个其他开源贡献者。它快速、新颖、模块化和面向 API ，使其成为对于构建插件和服务非常友好的系统。\n话不多说，这里把 K8S 分为的11个部分介绍 1. Pods Pods 是 K8S 中创建或部署的最小基本单位。一个 pod 可以由多个容器组成，这些容器将形成一个部署在单个节点上的单元。一个 pod 包含一个容器之间共享的 IP。在微服务中， pod 将是执行某些后端工作或提供传入请求的微服务的单个实例。\n2. Nodes Node 就是服务器。它们是 K8S 部署其 pod 的“裸机”（也可以是虚拟机）。Nodes 为 K8S 提供可用的群集资源，以保持数据，运行作业，维护工作负载和创建网络路由。\n3. Labels \u0026amp; Annotations Labels 是 K8S 及其终端用户过滤和筛选系统中类似资源的方式，也是一个资源需要“访问”或与另一个资源关联的粘合剂。例如：一个 Service 想要开放 Deployment 的端口。无论是监控，记录，日志，测试，任何k8s资源都应添加 Labels 以供进一步检查。例如： app=worker ，一个给系统中所有工作 pod 的标签，稍后可以使用 kubectl 工具或 k8s api 使用 --selector 字段进行选择。\nAnnotations 与 Labels 非常类似，但是它通常以字符串的形式用于存储元数据，但他不能用于标识和选择对象，通常也不会被 Kubernetes 直接使用，其主要目的是方便工具或用户的阅读和查找等。\n4. 服务发现 作为编排调度器，控制不同工作负载的资源，K8S 管理 pods 、jobs 和其他任何需要网络通信的物理资源。 K8S 使用 etcd 管理这些。 etcd 是 K8S 的“内部”数据库， Master 节点使用它来知道一切资源都在哪里。K8S 还为您的服务提供了实时的 “服务发现” - 所有 Pod 都使用的自定义 DNS 服务（CoreDNS），您可以通过解析其他服务的名称来获取其 IP 地址和端口。它不需要任何设置，在 K8S 集群中“开箱即用”。\n5. ReplicaSets 虽然 pod 运行任务，但通常单个实例是不够的。出于对冗余和负载处理的考虑，需要进行复制容器，即“弹性缩放”。K8S 使用 ReplicaSet 来实现伸缩扩展。根据副本的数量来表示系统的期望状态，并且在任何给定时刻保持系统的当前状态。\n这也是配置自动扩展的地方，在系统负载高时创建新的副本，以及在不再需要这些资源来支持运行工作负载时减少扩展。简单的讲就是：少则增加，多增删除。\n6. DaemonSets 有时，某些应用程序在每个节点上只需要一个实例。最好的例子就是像 FileBeat 这样的日志采集组件。为了让 agent 从节点上收集日志，它需要运行在所有节点上，但只需要一个实例即可。为了创建满足上面需求的的工作负载，K8S 使用 DaemonSets 来完成这个工作。\n7. StatefulSets 虽然大多数微服务都是无状态的应用程序，但是还是有一部分并不是。有状态的工作负载需要由某种可靠的磁盘卷来支持。虽然应用程序容器本身可以是不变的，并且可以用更新的版本或更健康的实例来替换它们，但是即使使用其他副本也是需要持久化的数据。为此，StatefulSets 允许部署整个生命周期内需要运行在同一节点的应用程序。它还保留了它的 “名称” ; 容器内的 hostname 和整个集群中服务发现的名称。一个包含3个 ZooKeeper 的 StatefulSet 可以命名为 zk-1 ，zk-2 和 zk-3 还可以扩展为包含其他成员，如 zk-4 ， zk-5 等\u0026hellip; StatefulSets 还需要管理 PVC 。\n8. Jobs K8S 核心团队考察了绝大多数需要使用编排系统的应用程序。虽然大多数应用程序需要持续的正常运行时间来处理服务请求，例如 Web 服务，但有时也需要运行批量任务并在任务完成后进行清理。如果您愿意，可以使用小型无服务器环境。而在 K8S 中实现这一功能，可以使用 Job 资源。Jobs 正是听起来的那样，一个工作负载容器来完成特定的工作，并在成功后被销毁。一个很好的例子是设置一组 worker ，从要处理和存储的队列中读取任务。一旦队列为空，直到下一批准备好进行处理，都不再需要启动 worker。\n9. ConfigMaps \u0026amp; Secrets 如果您还不熟悉 Twelve-Factor App manifest 《十二要素应用》 ，可以点击链接了解一下。现代应用程序的一个关键概念是无环境，可通过注入的环境变量进行配置。应用程序应完全与其所在位置无关。ConfigMaps 在 K8S 中实现这一重要概念。其本质上是环境变量的 key-value 列表，这些变量被传递给正在运行的工作负载以确定不同的 runtime 行为。\nSecrets 与 ConfigMaps 类似，通过加密的方式防止密钥、密码、证书等敏感信息泄漏。\n 我个人认为在任何系统上使用密码的最佳选择是 Hashicorp 的 Vault 。请务必阅读我去年写的关于它的文章，关于 Vault 可以为你的产品提供的功能，以及我的一位同事写的另一篇更具技术性的[文章](https://medium.com/prodopsio/taking-your-hashicorp-vault-to-the-next-level-8549e7988b24）。\n 10. Deployments 为了使新版本快速替换原有的应用程序，我们希望将构建、测试和发布在一块来实现 short feedback loops 。K8S 使用 Deployments 来不断部署新软件，Deployments 是一组用来描述特定运行工作负载的元数据。例如：发布新版本，bug 修复，甚至是回滚（这是k8s的另一个内部选项）。\n在 K8S 中部署软件有两个主要策略：\n Replacement：将使用新副本替换您的整个工作负载，整个过程需要强制停机。\n RollingUpdate：k8s通过两种特定配置来实现使用新的 Pods 实例滚动更新：\n MaxAvailable ： 该设置表示在部署新版本时可用的工作负载的百分比（或确切数量），100％表示“我有2个容器，保持2个存活并在整个部署期间正常提供服务”。 MaxSurge ： 该设置表示升级期间总 Pod 数最多可以超出期望的百分比（或数量），100％表示“我有 X 个容器，再部署 X 个容器，然后开始推出旧容器”。   11. Storage K8S 在存储上添加了一层抽象，工作负载可以为不同的任务请求特定的存储，甚至可以管理持续的时间可以超过某个pod的生命周期。为了简短起见，我想向您介绍我最近发布的关于k8s存储的文章，特别是为什么它不能完全解决数据库部署等数据持久性要求。\n概念性理解 K8S 是根据一些指导方向设计和开发的，考虑到社区的性质，每个特征、概念和想法都被内置于系统中。此外，终端用户会以某种方式使用该系统，作为一个开源和免费的系统，不属于任何人，你可以用它做任何你想要做的事。\n面向 API ：系统中的每个部分都以一种可通过记录良好且可操作的 API 进行交互的方式进行构建。核心开发人员确保作为终端用户的您可以进行更改，查询和更新，用来提供更好的用户体验。\n工具友好 ： 作为上面一点的衍生，K8S 是热衷于在其 API 周围创建工具的。它将自身做为一个原始平台，以可定制的方式构建，以供其他人使用，并进一步开发用于不同的工具。有些已经变得非常有名并被广泛使用，如 Spinnaker ，Istio 和许多其他功工具。\n声明性状态 ： 鼓励用户使用具有声明性描述的系统而不是命令式描述。这意味着系统的状态和组件最好被描述为在某种版本控制（如 git ）中管理的代码，而不会因为某一处手动更改也对整体有影响。这样，k8s更容易灾难恢复 ，易于在团队之间分享和传递。\n最后 本文试图将重点放在 K8S 的介绍和主要概念上，当然，K8S 还有其他非常重要的领域，比如物理系统构建模块，如 kubelet， kube-proxy ， api-server 和终端操作工具：kubectl。我将在下一篇文章中讨论以及介绍这些很酷的功能。\n原文地址： https://medium.com/prodopsio/an-8-minute-introduction-to-k8s-94fda1fa5184\n","date":1556602692,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1556602692,"objectID":"6772c5d0c004c1d460fa857832b8afdc","permalink":"https://guoxudong.io/post/an-8-minute-introduction-to-k8s/","publishdate":"2019-04-30T13:38:12+08:00","relpermalink":"/post/an-8-minute-introduction-to-k8s/","section":"post","summary":"8分钟快速了解 Kubernetes 基本概念，快速入门 K8S。","tags":["翻译","kubernetes"],"title":"8分钟入门 K8S","type":"post"},{"authors":["郭旭东"],"categories":["程序员趣闻"],"content":" 马上就是五一假期了，而且今年的五一假期有4天！想必大家已经安排好是在家写代码还是出门去冒险了。不过在五一假期之前，我这里推荐一个好玩的又好用的软件给大家。\n想必大部分朋友和我一样在上周去看了复联4，其中钢铁侠战衣及设备各种炫酷又极具科技感的操作界面一定让你记忆犹新。很多朋友可能和我一样，都希望拥有一套这样的操作界面，这样不管是工作还是学习都会变得有趣而高效（主要是炫酷）。其实很早以前我就尝试写过，但是由于技术有限，写出来的工具都不是很符合我的要求，渐渐的也就都废弃了。而今天要介绍的这个软件，完全符合我的要求，高端大气上档次，并且还是开源的。\neDEX-UI eDEX-UI 是一个全屏且跨平台、可定制的终端模拟器，具有先进的监控和触摸屏支持。它的外观类似科幻的计算机界面。在保持未来感的外观和感觉的同时，它努力保持一定的功能水平并可用于现实场景，其更大的目标是将科幻用户体验纳入主流。\n特性  功能齐全的终端仿真器，带有选项卡、颜色、模拟鼠标，并支持 curses 和类似 curses的应用程序。 实时系统（CPU、RAM、进程）和网络（GeoIP、活动连接、传输速率）监控。 完全支持触摸屏，包括屏幕键盘。 具备跟随终端 CWD（当前工作目录）的目录查看器。 包括主题、屏幕键盘布局、CSS 注入等在内的高级自定义。 由才华横溢的声音设计师制作的可选音效，可实现最佳的好莱坞黑客氛围。  显示 这里我使用了 tron-disrupted 主题，还有多种主题可以选择\n可以看到这里的界面十分炫酷，可以为有些乏味的 shell 操作增添一抹乐趣\n配置 eDEX-UI 可以通过 settings.json 文件进行配置，配置包括执行的 shell 类型、工作目录、键盘类型、主题等\nsettings.json 在 Mac 系统中，存放在 /Users/guoxudong/Library/Application Support/eDEX-UI 中，默认的工作目录也是这个路径\n这里可以看到我选择使用 zsh 和 tron-disrupted 主题，并将工作目录改为了我的用户空间\n局限  目前看来该软件的全平台支持是不错的，同时还支持触摸屏操作，但是目前还未测试在 pad 上使用，测试之后会在后续文章中补充 CPU 占用过高，该软件 CPU 占用很高，如果是配置一般的电脑不建议让其作为终端常驻，偶尔拿出来玩玩即可  ","date":1556510147,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1556510147,"objectID":"d18c1494b219d348de7c4debaa808274","permalink":"https://guoxudong.io/post/edex-ui/","publishdate":"2019-04-29T11:55:47+08:00","relpermalink":"/post/edex-ui/","section":"post","summary":"eDEX-UI 是一个全屏且跨平台、可定制的终端模拟器，具有先进的监控和触摸屏支持。它的外观类似科幻的计算机界面。在保持未来感的外观和感觉的同时，它努力保持一定的功能水平并可用于现实场景，其更大的目标是将科幻用户体验纳入主流。","tags":["工具"],"title":"炫酷的终端软件 eDEX-UI","type":"post"},{"authors":["郭旭东"],"categories":["kubernetes"],"content":" 前言 选择阿里云的容器服务，主要原因是公司主要业务基本都在运行在阿里云上。相较自建 kubernetes 集群，容器服务的优势在于部署相对简单，与阿里云 VPC 完美兼容，网络的配置相对简单，而如果使用 kubeadm 安装部署 kubernetes 集群，除了众所周知的科学上网的问题，还有一系列的问题，包括 etcd 、 Scheduler 和 Controller-Manager 的高可用问题等。并且如果使用托管版的阿里云 kubernetes 容器服务，还会省掉3台 master 节点的钱，并且可能将 master 节点的运维问题丢给阿里云解决，并且其提供的 master 节点性能肯定会比自购的配置好，这点是阿里云容器服务的研发小哥在来我司交流时专门强调的。\n问题 前面吹了阿里云容器服务的优势，那这里就说说在实践中遇到的容器服务的问题：\n 在新建集群的时候需要选择相应的 VPC 并选择 Pod 和 Service 所在的网段，这两个网段不能和 Node 节点存在于同一网段，但是如果您在阿里云中存在不止一个 VPC （VPC的网段可以是 10.0.0.0/8，172.16-31.0.0/12-16，192.168.0.0/16 ），如果网段设置不对的话，就可能会使原本存在该网段的 ECS 失联，需要删除集群重新创建。如果删除失败的话，还需要手动删除路由表中的记录（别问我是怎么知道的）。\n 在使用容器服务创建集群后，会创建2个 SLB （之前是3个），一个是 SLB 是在 VPC 上并且绑定一个弹性IP（需要在创建的时候手动勾选创建弹性IP）用于 API Server，一个是经典网络的 SLB 使用提供给 Ingress 使用。但是这两个外网IP创建后的规格都是默认最大带宽、按流量收费，这个并不符合我们的要求，需要手动修改，然而这个修改都会在第二天才能生效。\n 容器服务创建集群后，Node 节点的名称会使{region-id}.{ECS-id}的形式，这个命名方式在集群监控，使用 kubectl 操作集群方面就显得比较反人类了，每次都要去查 ECS id 才能确定是哪个节点，而这个 Node 节点名称是不能修改的！\n  网段问题解决 这个比较好解决，甚至可以说不用解决，只要把网段规划好，不要出现网段冲突就好\nNode 节点名称无法修改问题解决 这个功能之前已有人在阿里聆听平台提出这个问题了，咨询了容器服务的研发小哥，得到的反馈是该功能已经在灰度测试了，相信很快就可以上线了。\n创建 SLB 规格问题解决 相较之前自动创建3个 SLB 的方式，目前的版本只会自动创建2个并且有一个是 VPC 内网+弹性IP的方式，已经进行了优化，但是 ingress 绑定的 SLB 还是经典网络类型，无法接入云防火墙并且规格也是不合适的。这里给出解决方案：\n方法一：使用 kubectl 配置 1. 创建新的 SLB  这里需要创建一个新的 SLB 用来代替自动创建的不符合要求的 SLB。这里可以先私网 SLB 先不绑定弹性IP。这里要注意的事，新建的 SLB 需要与 k8s集群处于同一 VPC 内，否则在后续会绑定失败。  查看新购买 SLB 的 ID   2. 在创建集群后重新绑定 ingress-controller 的 Service 首先需要使用 kubectl 或者直接在阿里云控制台操作，创建新的 nginx-ingress-svc\n# nginx ingress service apiVersion: v1 kind: Service metadata: name: nginx-ingress-lb-{new-name} namespace: kube-system labels: app: nginx-ingress-lb-{new-name} annotations: # set loadbalancer to the specified slb id service.beta.kubernetes.io/alicloud-loadbalancer-id: {SLB-ID} # set loadbalancer address type to intranet if using private slb instance #service.beta.kubernetes.io/alicloud-loadbalancer-address-type: intranet service.beta.kubernetes.io/alicloud-loadbalancer-force-override-listeners: 'true' #service.beta.kubernetes.io/alicloud-loadbalancer-backend-label: node-role.kubernetes.io/ingress=true spec: type: LoadBalancer # do not route traffic to other nodes # and reserve client ip for upstream externalTrafficPolicy: \u0026quot;Local\u0026quot; ports: - port: 80 name: http targetPort: 80 - port: 443 name: https targetPort: 443 selector: # select app=ingress-nginx pods app: ingress-nginx  创建成功后，可以进到 SLB 页面查看，可以看到 80 和 443 端口的监听已经被添加了 3. 绑定符合要求的弹性IP 确定 SLB 创建成功并且已经成功监听后，这里就可以为 SLB 绑定符合您需求的弹性IP了，这里我们绑定一个按宽带计费2M的弹性IP\n4. 验证连通性 到上面这步，我们的 ingress 入口 SLB 已经创建完成，这里我们验证一下是否联通。\n 在k8s集群中部署一个 nginx ，直接在阿里云容器服务控制台操作即可 这里创建 ingress 路由，注意：这里的域名需要解析到刚才创建的 SLB 绑定的弹性IP  访问该域名，显示 nginx 欢迎页，则证明修改成功   方法二： 使用阿里云容器服务控制台配置 1. 阿里云容器控制台创建新 service  在阿里云容器服务控制台：路由与负载均衡 \u0026ndash;\u0026gt; 服务 点击创建 选择 kube-system 命名空间 类型选中负载均衡 - 内网访问 关联 nginx-ingress-controller 并添加端口映射 点击创建  2. 进入负载均衡查看 SLB 是否创建 可见 SLB 已经成功创建\n3. 绑定符合要求的弹性IP 同方法一\n4.验证连通性 同方法一\n后续操作  在确定新的 SLB 创建成功后，就可以将容器服务自动创建的 SLB 释放了 删除 kube-system 中原本绑定的 Service （目前版本已经可以关联删除绑定的 SLB 了，不用分开操作） 这里别忘了，自动创建给API Server 的SLB还是按流量付费的，记得降配  后记 上面的这些问题和解决方案都属于临时方案，已在阿里的聆听平台提出了上面的问题，相信很快就会有所改进。总的来说，阿里云容器服务在提供优质的 kubernetes 功能，并且只收 ECS 的钱，对于想学习 kubernetes 又没有太多资金的同学也比较友好，直接买按量付费实例，测试完释放即可，不用购买 master 节点，十分良心！\n","date":1556202366,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1556202366,"objectID":"fe4313100487dc1237d95450e5cef8ab","permalink":"https://guoxudong.io/post/aliyun-k8s-perfect/","publishdate":"2019-04-25T22:26:06+08:00","relpermalink":"/post/aliyun-k8s-perfect/","section":"post","summary":"这里记录了在工作中遇到阿里云容器服务的调优优化方案，帮助您绕过阿里云容器服务中的一些坑，来使用更好更优质的阿里云容器服务。","tags":["阿里云","kubernetes","容器"],"title":"阿里云容器服务新建集群优化方案(更新版)","type":"post"},{"authors":["郭旭东"],"categories":["翻译"],"content":"  虽然 Kubernetes 赢得了容器之站，但是其仍然很难使用并且时长引起事故。\n 我想我应该给这篇文章做一点序言。 kubernetes 为许多应用程序提供新的 runtime ，如果使用得当，它可以成为一个强大的工具，并且可以将您冲复杂的开发生命周期中解放出来。然而在过去的几年里，我看到很多人和公司都会搭建他们的 Kubernetes ，但常常只是处于测试阶段，从未进入到生产。\nKubernetes 是如何运作的？ 粗略的讲， Kubernetes 或者 K8S 看起来十分简单。您运行的 Kubernetes 节点至少被分为两类：Master 和 Workers。Master 节点通常不运行任何真实的工作负载，那是 Workers 节点的工作。\nKubernetes 的 Master 节点包含一个名叫 API server 的组件，其提供的 API 可以通过 kubectl 调用。此外还包括一个 scheduler ，负责调度容器，决定容器运行在哪个节点。最后一个组件是 controller-manager ，它实际上是一组多个控制器，负责处理节点中断、复制、加入 services 和 pods ，并且处理授权相关内容。所有的数据都存储在 etcd 中，这是一个可信赖的分布式键值存储服务（包含一些非常酷的功能）。总而言之，Master 节点负责管理集群，这里没什么特别大的惊喜。\n另一方面， 真实的工作负载运行在 Worker 节点上。为此，它还包括许多组件。首先，Worker 节点上会运行 kubelet ，它是与该节点上的容器一起运行的 API ，负责与管控组件沟通，并按照管控组件指示管理 Worker 节点。另一个组件就是 kube-proxy ，其负责转发网络连接，根据您的配置运行容器。可能还有其他东西，如 kube-dns 或 gVisor。您还需要集成某种 overlay network 或底层网络设置，以便 Kubernetes 可以管理您的 pod 之间的网络。\n如果您想要一个更完整的概述，建议去看 Kelsey Hightowers 的 Kubernetes - The Hard Way。\n生产就绪的 Kubernetes 到目前为止，这听起来并不太糟糕。只是安装几个程序、配置、证书等。不要误会我的意思，这仍然是一个学习曲线，但这也不是系统管理员不能处理的问题。\n然而，简单地手动安装 Kubernetes 并不代表其已经完全准备就绪，所以让我们谈谈让这个东西运行起来所需的步骤。\n首先，安装。如果您想要某种自动安装，无论是使用 Ansible ， Terraform 还是其他工具。kops 可以帮助您解决这个问题，但是使用 kops 意味着您将不知道它是如何设置的，并且当您以后想要调试某些东西时可能会引起一些其他问题。应对此自动化进行测试，并定期进行检查。\n其次，您需要监控您的 Kubernetes 安装。所以您需要 Prometheus 、 Grafana 等工具。您是在 Kubernetes 里面运行它吗？ 如果您的 Kubernetes 有问题，那么您的监控是否会也会挂掉？ 或者您单独运行它？ 如果是，那么您在哪里运行它？\n另外值得注意的是备份。如果您的 Master 崩溃，数据无法恢复并且您需要重新配置系统上的所有 pod ，您会怎么做？您是否测试了再次运行 CI 系统中所有作业所需的时间？您有灾难恢复计划吗？\n现在，既然我们在谈论 CI 系统，那么您需要为您的镜像运行 Docker 镜像仓库。当然，您可以再次在 Kubernetes 上做，但如果 Kubernetes 崩溃\u0026hellip;\u0026hellip;您知道这个后果。当然，CI 系统与运行版本控制系统都有这个问题。理想情况下，这些系统是与生产环境隔离的，以便在系统出现问题时，至少可以访问 git ，来进行重新部署等操作。\n数据存储 最后，我们来谈谈最重要的部分：存储。Kubernetes 本身并不提供存储解决方案。当然，您可以将存储挂载到主机安装目录，但这既不推荐也不简单。\n基本上需要在 Kubernetes 下使用某种存储。例如，rook 使得运行 Ceph 作为底层块存储需求的变得相对简单，但我对 Ceph 的体验是它还有有很多地方需要调整，所以您绝不是只需点击下一步就能走出困境。\n调试 在与开发人员谈论 Kubernetes 时，一种常见的回答经常出现：在使用 Kubernetes 时，人们常常在调试应用程序时遇到问题。即使是一个例如容器未能启动的简单问题，也会引起混乱。\n当然，这是一个教育问题。在过去的几十年中，开发人员已经学会了调试的“经典”步骤：在 /vat/log/ 中查看日志等。但是对于容器，我们甚至不知道容器运行在哪个服务器上，因此它呈现出了一种范式转换。\n问题：复杂 您可能已经注意到我正在跳过共有云提供商给您的东西，即使它不是一个完整的托管 Kubernetes。当然，如果您使用托管的 Kubernetes 解决方案，这很好，除了调试之外，您不需要处理上面这些问题。\nKubernetes 拥有许多可以移动组件，但 Kubernetes 本身也并不能提供完整的解决方案。例如，RedHat OpenShift 可以，但它需要花钱，并且仍然需要添加自己的东西。\n现在Kubernetes正处于 Gartner hype cycle 的顶峰，每个人都想要它，但很少有人真正理解它。在接下来的几年里，不少公司必须意识到 Kubernetes 并不是银弹，而如何正确有效地使用它才是关键。\n我认为，如果您有能力将 Ops 团队专门用于为开发人员来维护底层平台，那么运行自己的 Kubernetes 是值得的。\n 本文作者：Janos Pasztor 2018-12-04\n 原文地址：https://pasztor.at/blog/kubernetes-is-hard\n","date":1556072326,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1556072326,"objectID":"5a57dad9090f8b3eedcec900bbd509a0","permalink":"https://guoxudong.io/post/kubernetes-is-har/","publishdate":"2019-04-24T10:18:46+08:00","relpermalink":"/post/kubernetes-is-har/","section":"post","summary":"虽然 Kubernetes 赢得了容器战争，但是其仍然很难使用并且引起很多事故。","tags":["翻译","kubernetes"],"title":"困难的 Kubernetes","type":"post"},{"authors":["郭旭东"],"categories":["程序员趣闻"],"content":" 这里的黑话，其实就是 GitHub 上的一些迷之缩写，这些歪果老司机们在 GitHub 上肆无忌惮的使用着他们的“黑话”，让我们这些非英语母语国家的新司机在 code review 时经常是一脸懵逼 + 黑人问好\u0026hellip;\nimage: caption: \u0026ldquo;Image from: Pexels\u0026ldquo; focal_point: \u0026ldquo;\u0026rdquo;\npreview_only: false   image: caption: \u0026ldquo;Image from: Pexels\u0026ldquo; focal_point: \u0026ldquo;\u0026rdquo;\npreview_only: false 记住下面这些“黑话”，以后我们也可以愉快的在 GitHub 上装逼了~\n PR: Pull Request. 拉取请求，给其他项目提交代码，这个是最为常见的缩写。 WIP: Work In Progress. 传说中提 PR 的最佳实践是，如果你有个改动很大的 PR，可以在写了一部分的情况下先提交，但是在标题里写上 WIP，以告诉项目维护者这个功能还未完成，方便维护者提前 review 部分提交的代码。 LGTM: Looks Good To Me. 已阅 代码已经过 review，可以合并。也可理解为 “OJBK”。 SGTM: Sounds Good To Me. 同上，也是已经通过了 review 的意思。 PTAL: Please Take A Look. 你来瞅瞅？用来提示别人来看一下。 TBR: To Be Reviewed. 提示维护者进行 review。 TL;DR: Too Long; Didn\u0026rsquo;t Read. 太长懒得看。也有很多文档在做简略描述之前会写这么一句，第一次看到的反应是：这TM是啥？ TBD: To Be Done(or Defined/Discussed/Decided/Determined). 根据语境不同意义有所区别，但一般都是还没搞定的意思。  参考 LGTM? 那些迷之缩写 - farer.org\n","date":1555895484,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1555895484,"objectID":"108f4208cf323fa50893a8f3ef6d6076","permalink":"https://guoxudong.io/post/cant/","publishdate":"2019-04-22T09:11:24+08:00","relpermalink":"/post/cant/","section":"post","summary":"这里总结了一些在 GitHub 上常见的，这些乱七八糟的缩写着实给很多新司机带来了许多困扰 :)","tags":["GitHub","翻译"],"title":"GitHub 黑话指南","type":"post"},{"authors":["郭旭东"],"categories":["翻译"],"content":" Docker 非常棒！ 它使软件开发者无需担心配置和依赖性，在任何地方打包，发送和运行他们的应用程序。而在与 kubernetes 相结合后，它使应用集群部署和管理变得更方便。这使得 Docker 深受软件开发者的喜爱，越来越多的开发者开始使用 Docker。\n那么 Docker 到底是什么？\n它是构建、测试、部署和发布容器化应用的平台。称其为平台是因为 Docker 其实是一套用于管理与容器相关的所有事物的工具。作为 Docker 的核心，接下来我们将深入探讨容器。\n什么是容器？ 容器提供了在计算机上的隔离环境中安装和运行应用程序的方法。在容器内运行的应用程序仅可使用于为该容器分配的资源，例如：CPU，内存，磁盘，进程空间，用户，网络，卷等。在使用有限的容器资源的同时，并不与其他容器冲突。您可以将容器视为简易计算机上运行应用程序的隔离沙箱。\n这个概念听起来很熟悉，有些类似于虚拟机。但它们有一个关键的区别：容器使用的一种非常不同的，轻量的技术来实现资源隔离。容器利用了底层 Linux 内核的功能，而不是虚拟机采用的 hypervisor 的方法。换句话说，容器调用 Linux 命令来分配和隔离出一组资源，然后在此空间中运行您的应用程序。我们快速来看下两个这样的功能：\n namespaces\n简单的讲就是，Linux namespace 允许用户在独立进程之间隔离 CPU 等资源。进程的访问权限及可见性仅限于其所在的 namespaces 。因此，用户无需担心在一个 namespace 内运行的进程与在另一个 namespace 内运行的进程冲突。甚至可以同一台机器上的不同容器中运行具有相同 PID 的进程。同样的，两个不同容器中的应用程序可以使用相同的端口。\n cgroups\ncgroups 允许对可用资源设置限制和约束。例如，您可以在一台拥有 16G 内存的计算机上创建一个 namespace ，限制其内部进程可用内存为 1GB。\n  到这，您可能已经猜到 Docker 的工作原理了。当您请求 Docker 运行容器时，Docker 会在您的计算机上设置一个资源隔离的环境。然后 Docker 会将打包的应用程序和关联的文件复制到 namespace 内的文件系统中，此时环境的配置就完成了。之后 Docker 会执行您指定的命令运行应用程序。\n简而言之，Docker 通过使用 Linux namespace 和 cgroup（以及其他一些命令）来协调配置容器，将应用程序文件复制到为容器分配的磁盘，然后运行启动命令。Docker 还附带了许多其他用于管理容器的工具，例如：列出正在运行的容器，停止容器，发布容器镜像等许多其他工具。\n与虚拟机相比，容器更轻量且速度更快，因为它利用了 Linux 底层操作系统在隔离的环境中运行。虚拟机的 hypervisor 创建了一个非常牢固的边界，以防止应用程序突破它，而容器的边界不那么强大。另一个区别是，由于 namespace 和 cgroups 功能仅在 Linux 上可用，因此容器无法在其他操作系统上运行。此时您可能想知道 Docker 如何在 macOS 或 Windows 上运行？ Docker 实际上使用了一个技巧，并在非 Linux 操作系统上安装 Linux 虚拟机，然后在虚拟机内运行容器。\n让我们利用目前为止学到的所有内容，从头开始创建和运行 Docker 容器。如果你还没有将 Docker 安装在你的机器上，可以参考这里安装 Docker 。在这个示例中，我们将创建一个 Docker 容器，下载一个用 C语言 写的 Web 服务，编译并运行它，然后使用浏览器访问这个 Web 服务。\n我们将从所有 Docker 项目开始的地方：创建一个 Dockerfile 开始。此文件描述了如何创建用于运行容器的 docker 镜像。既然我们还没有聊到镜像，那么让我们看一下镜像的官方定义：\n 镜像是一个可执行包，其包含运行应用程序所需的代码、运行时、库、环境变量和配置文件，容器是镜像的运行时实例。\n 简单的讲，当你要求 Docker 运行一个容器时，你必须给它一个包含如下内容的镜像：\n 包含应用程序及其所有依赖的文件系统快照。 容器启动时的运行命令。  在 Docker 的世界，使用别人的镜像作为基础镜像来创建自己的镜像是十分普遍的。例如，官方 reds Docker 镜像就是基于 Debian 文件系统快照（rootfs tarball），并安装在其上配置 Redis。\n在我们的示例中，我们选择 Alpine Linux 为基础镜像。当您在 Docker 中看到 “alpine” 时，它通常意味着一个精简的基本镜像。 Alpine Linux 镜像大小只有约为5 MB！\n在您的计算机创建一个新目录（例如 dockerprj ），然后新建一个 Dockerfile 文件。\numermansoor:dockerprj$ touch Dockerfile  将如下内容粘贴到 Dockerfile：\n# Use Alpine Linux rootfs tarball to base our image on FROM alpine:3.9 # Set the working directory to be '/home' WORKDIR '/home' # Setup our application on container's file system RUN wget http://www.cs.cmu.edu/afs/cs/academic/class/15213-s00/www/class28/tiny.c \\ \u0026amp;\u0026amp; apk add build-base \\ \u0026amp;\u0026amp; gcc tiny.c -o tiny \\ \u0026amp;\u0026amp; echo 'Hello World' \u0026gt;\u0026gt; index.html # Start the web server. This is container's entry point CMD [\u0026quot;./tiny\u0026quot;, \u0026quot;8082\u0026quot;] # Expose port 8082 EXPOSE 8082  这个 Dockerfile 包含创建镜像的内容说明。我们创建镜像基于 Alpine Linux（rootfs tarball），并将工作目录设置为 /home 。接下来下载，编译并创建了一个用C编写的简单 Web 服务器的可执行文件，然后指定在运行容器时要执行的命令，并将容器端口8082暴露给主机。\n现在，我们就可以构建镜像了。在 Dockerfile 的同级目录运行 docker build 命令：\numermansoor:dockerprj$ docker build -t codeahoydocker .  如果这个命令成功了，您将看到：\nSuccessfully tagged codeahoydocker:latest  此时我们的镜像就创建成功了，该镜像主要包括：\n 文件系统快照（Alpine Linux 和 我们安装的 Web 服务） 启动命令（./tiny 8092）  既然成功构建了镜像，那么我们可以使用如下命令运行容器。\numermansoor:dockerprj$ docker run -p 8082:8082 codeahoydocker:latest  让我们了解下这里发生了什么。\n通过 docker run 命令，我们请求 Docker 基于 codeahoydocker:latest 镜像创建和启动一个容器。-p 8082:8082 将本地的8082端口映射到容器的8082端口（容器内的 Web 服务器正在监听8082端口上的连接）。打开你的浏览器并访问 localhost:8082/index.html 。你将可以看到 Hello World 信息。\n最后我想补充一点，虽然 Docker 非常棒，而且对于大多数项目来说它是一个不错的选择，但我们并非处处都要使用它。在我的工作中，Docker 与 Kubernetes 结合使用，可以非常轻松地部署和管理后端微服务，我们不必为每个服务配置新的运行环境。另一方面，对于性能密集型应用程序，Docker 可能不是最佳选择。我经手的其中一个项目必须处理来自移动游戏客户端的 TCP 长连接（每台机器1000个），这时 Docker 网络出现了很多问题，导致无法将它用于该项目。\n希望上面这些内容有用。\n 这篇文章由 Umer Mansoor 撰写，可以在 Facebook 或 Twitter 上关注并留下评论。\n 原文地址： https://codeahoy.com/2019/04/12/what-are-containers-a-simple-guide-to-containerization-and-how-docker-works/\n","date":1555761290,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1555761290,"objectID":"dce85cd7440eb7c964881f0efb3d802d","permalink":"https://guoxudong.io/post/what-are-containers-a-simple-guide-to-containerization-and-how-docker-works/","publishdate":"2019-04-20T19:54:50+08:00","relpermalink":"/post/what-are-containers-a-simple-guide-to-containerization-and-how-docker-works/","section":"post","summary":"Docker 非常棒！ 它使软件开发者无需担心配置和依赖性，在任何地方打包，发送和运行他们的应用程序。而在与 kubernetes 相结合后，它使应用集群部署和管理变得更方便。这使得 Docker 深受软件开发者的喜爱，越来越多的开发者开始使用 Docker。","tags":["翻译","Docker","容器"],"title":"什么的容器？Docker 工作原理及容器化简易指南","type":"post"},{"authors":["郭旭东"],"categories":["kustomize"],"content":" 本文介绍使用和维护 Kustomize 的方法及步骤。\n定制配置 在这个工作流方式中，所有的配置文件（ YAML 资源）都为用户所有，存在于私有 repo 中。其他人是无法使用的。\n 创建一个目录用于版本控制\n我们希望将一个名为 ldap 的 Kubernetes 集群应用的配置保存在自己的 repo 中。 这里使用 git 进行版本控制。\ngit init ~/ldap  创建一个 base\nmkdir -p ~/ldap/base  在这个目录中创建并提交 kustomization.yaml 文件和一组资源，例如 deployment.yaml service.yaml 等。\n 创建 overlays\nmkdir -p ~/ldap/overlays/staging mkdir -p ~/ldap/overlays/production  每个目录都需要一个 kustomization.yaml 文件以及一个或多个 patch ，例如 healthcheck_patch.yaml memorylimit_patch.yaml 等。。\n production目录则可能会在deployment``` 中增加在副本数。\n 生成 variants\n运行 kustomize ，将生成的配置用于 kubernetes 应用部署\nkustomize build ~/ldap/overlays/staging | kubectl apply -f - kustomize build ~/ldap/overlays/production | kubectl apply -f -  在 kubernetes 1.14 版本， kustomize 已经集成到 kubectl 命令中，成为了其一个子命令，可使用 kubectl 来进行部署\nkubectl apply -k ~/ldap/overlays/staging kubectl apply -k ~/ldap/overlays/production   使用现成的配置 在这个工作流方式中，可从别人的 repo 中 fork kustomize 配置，并根据自己的需求来配置。\n 通过 fork/modify/rebase 等方式获得配置\n 将其克隆为你自己的 base\n在这个 bash 目录维护在一个 repo 中，在这个例子使用 ladp 的 repo\nmkdir ~/ldap git clone https://github.com/$USER/ldap ~/ldap/base cd ~/ldap/base git remote add upstream git@github.com:$USER/ldap  创建 overlays\n如上面的案例一样，创建并完善 overlays 目录中的内容\nmkdir -p ~/ldap/overlays/staging mkdir -p ~/ldap/overlays/production  用户可以将 overlays 维护在不同的 repo 中\n 生成 variants\nkustomize build ~/ldap/overlays/staging | kubectl apply -f - kustomize build ~/ldap/overlays/production | kubectl apply -f -  在 kubernetes 1.14 版本， kustomize 已经集成到 kubectl 命令中，成为了其一个子命令，可使用 kubectl 来进行部署\nkubectl apply -k ~/ldap/overlays/staging kubectl apply -k ~/ldap/overlays/production  （可选）更新 base 用户可以定期从上游 repo 中 rebase 他们的 base 以保证及时更新\ncd ~/ldap/base git fetch upstream git rebase upstream/master   参考  kustomize workflows - github.com  ","date":1555661102,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1555661102,"objectID":"257919151edb205a36c7fed9bcafc247","permalink":"https://guoxudong.io/post/kustomize-2/","publishdate":"2019-04-19T16:05:02+08:00","relpermalink":"/post/kustomize-2/","section":"post","summary":"本篇为系列文章第二篇，手把手教你使用 Kustomize 的两种方式。","tags":["kubernetes","kustomize","工具"],"title":"使用 Kustomize 帮你管理 kubernetes 应用（二）： Kustomize 的使用方法","type":"post"},{"authors":["郭旭东"],"categories":["问题解决"],"content":" 前言 Rancher 2.2.X 版本于3月底正式GA，新版本处理其他部分的优化以外，最大亮点莫过于本身集成了 Prometheus ，可以通过 Rancher 自带 UI 或者 Grafana 查看集群的实时监控，对所有监控进行了一次聚合，不用再和之前一样，每个集群都要安装一个 Prometheus 用于监控，而告警部分也可使用 Rancher 自带的通知组件进行告警。通知方式目前支持 Slack 、 邮件、 PagerDuty 、 Webhook 、 企业微信，由于我司办公使用钉钉，所以我们使用了 Webhook 的方式，告警触发后通知我们的消息服务，然后消息服务将其发送到钉钉进行告警。\n问题 Rancher 集成 Prometheus 后，监控方面变的十分强大，不用再徘徊于多个集群的 Grafana ，直接在 Rancher 上即可查看，非常方便\n但是在使用的时候，我发现了一个问题：就是在查看 工作负载和 Pod 的时候会显示 没有足够的数据绘制图表\n进入 Grafana 查看会发现，其实监控参数是存在的，但是没有采集到值，所以并没有展示出来。\n解决 在检查了配置后并没有找到原因，只好去 GitHub 上提一个 issue 来询问一下开发者或者其他用户有无遇到这个问题。\nRancher 官方的开发者还是十分负责的， GitHub 上用户名为 Logan 的官方小哥来我指导解决这个问题。\n小哥发现我是导入的集群，要我进入 Prometheus 查看，发现 cattle-prometheus/exporter-kube-state-cluster-monitoring 果然没有起来\n解决这个问题，需要在集群监控配置中添加一个高级选项，插入值为：exporter-kubelets.https=false\n点击保存，问题就解决了！\n后记 使用 Rancher 有半年，从2.0版本一直用到2.2版本，而18年分别在云栖大会和 KubeCon 上听了 Rancher 创始人梁胜博士的演讲。而从这一个小问题上就可以看到 Rancher 官方对每一个用户都是十分重视的。\n","date":1555580768,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1555580768,"objectID":"b286690e5f23f66c320001dd3f3334e7","permalink":"https://guoxudong.io/post/rancher-prometheus-fix-question/","publishdate":"2019-04-18T17:46:08+08:00","relpermalink":"/post/rancher-prometheus-fix-question/","section":"post","summary":"记一次在 Rancher 官方小哥帮助下解决 Rancher 问题的过程。","tags":["rancher","kubernetes","prometheus"],"title":"Rancher 2.2.1 解决工作负载监控为空问题","type":"post"},{"authors":["郭旭东"],"categories":["翻译"],"content":"  作者：Jeff Regan (Google), Phil Wittrock (Google) 2018-05-29\n 如果你在运行 kubernetes 集群，你可能会拷贝一些包含 kubernetes API 对象的 YAML 文件，并且根据你的需求来修改这些文件，通过这些 YAML 文件来定义你的 kubernetes 配置。\n但是这种方法存在很难找到配置的源头并对其进行改进。今天 Google 宣布推出 Kustomize ，一个作为 SIG-CLI 子项目的命令行工具。这个工具提供了一个全新的、纯粹的声明式的方法来定制 kubernetes 配置，遵循并利用我们熟悉且精心设计的 Kubernetes API。\n有这样一个常见的场景，在互联网上可以看到别人的 CMS（content management system，内容管理系统）的 kubernetes 配置，这个配置是一组包括 Kubernetes API 对象的 YAML 描述文件。然后，在您自己公司的某个角落，您找到一个你非常了解的数据库，希望用它来该 CMS 的数据。\n你希望同时使用它们，此外，你希望自定义配置文件以便你的资源实例在集群中显示，并通过添加一个标签来区分在同一集群中做同样事情的其他资源。同时也希望为其配置适当的 CPU 、内存和副本数。\n此外，你还想要配置整个配置的多种变化：一个专门用于测试和实验的小服务实例（就计算资源而言），或更大的用于对外提供服务的生产级别的服务实例。同时，其他的团队也希望拥有他们自己的服务实例。\n定制就是复用 kubernetes 的配置并不是代码（是使用 YAML 描述的 API 对象，严格来说应该是数据），但是配置的生命周期与代码的生命周期有许多相似之处。\n你需要在版本控制中保留配置。所有者的配置不必与使用者的配置相同。配置可以作为整体的一部分。而用户希望为在不同的情况下复用这些配置。\n与代码复用相同，一种复用配置的方法是简单的全部拷贝并进行自定义。像代码一样，切断与源代码的联系使得从改进变的十分困难。许多团队和环境都使用这种方法，每个团队和环境都拥有自己的配置，这使得简单的升级变得十分棘手。\n另一种复用方法是将源代码抽象为参数化模板。使用一个通过执行脚本来替换所需参数的模板处理工具生成配置，通过为同一模板设置不同的值来达到复用的目的。而这种方式面临的问题是模板和参数文件并不在 kubernetes API 资源的规范中，这种方式必定是一种包装了 kubernetes API 的新东西、新语言。虽然这种方式很强大，但是也带来了学习成本和安装工具的成本。不同的团队需要不同的更改，因此几乎所有可以包含在 YAML 文件中的规范都会需要抽象成参数。\n自定义配置的新选择 kustomize 中工具的声明与规范是由名为 kustomization.yaml 的文件定义。\nkustomize 将会读取声明文件和 Kubernetes API 资源文件，将其组合然后将完整的资源进行标准化的输出。输出的文本可以被其他工具进一步处理，或者直接通过 kubectl 应用于集群。\n例如，如果 kustomization.yaml 文件包括：\ncommonLabels: app: hello resources: - deployment.yaml - configMap.yaml - service.yaml  确保这三个文件与 kustomization.yaml 位于同一目录下，然后运行：\nkustomize build  将创建包含三个资源的 YAML 流，其中 app: hello 为每个资源共同的标签。\n同样的，你可以使用 commonAnnotations 字段给所有资源添加注释， namePrefix 字段为所有的资源添加共同的前缀名。这些琐碎而有常见的定制只是一个开始。\n一个更常见的例子是，你需要为一组相同资源设置不同的参数。例如：开发、演示和生产的参数。\n为此，Kustomize 允许用户以一个应用描述文件 （YAML 文件）为基础（Base YAML），然后通过 Overlay 的方式生成最终部署应用所需的描述文件。两者都是由 kustomization 文件表示。基础（Base）声明了共享的内容（资源和常见的资源配置），Overlay 则声明了差异。\n这里是一个目录树，用于管理集群应用程序的 演示 和 生产 配置参数：\nsomeapp/ ├── base/ │ ├── kustomization.yaml │ ├── deployment.yaml │ ├── configMap.yaml │ └── service.yaml └── overlays/ ├── production/ │ └── kustomization.yaml │ ├── replica_count.yaml └── staging/ ├── kustomization.yaml └── cpu_count.yaml  someapp/base/kustomization.yaml 文件指定了公共资源和常见自定义配置（例如，它们一些相同的标签，名称前缀和注释）。\nsomeapp/overlays/production/kustomization.yaml 文件的内容可能是：\ncommonLabels: env: production bases: - ../../base patches: - replica_count.yaml  这个 kustomization 指定了一个 patch 文件 replica_count.yaml ，其内容可能是：\napiVersion: apps/v1 kind: Deployment metadata: name: the-deployment spec: replicas: 100  patch 是部分的资源声明，在这个例子中是 Deployment 的补丁 someapp/base/deployment.yaml ，仅修改了副本数用以处理生产流量。\n该补丁不仅仅是一个无上下文 {parameter name，value} 元组。其作为部分 deployment spec，可以通过验证，即使与其余配置隔离读取，也具有明确的上下文和用途。\n要为生产环境创建资源，请运行：\nkustomize build someapp/overlays/production  运行结果将作为一组完整资源打印到标准输出，并准备应用于集群。可以用类似的命令定义演示环境的配置。\n综上所述 使用 kustomize ，您可以仅使用 Kubernetes API 资源文件就可以管理任意数量的 Kubernetes 定制配置。kustomize 的每个产物都是纯 YAML 的，每个都可以进行验证和运行的。kustomize 鼓励通过 fork/modify/rebase 这样的工作流来管理海量的应用描述文件。\n尝试hello world示例，开始使用 kustomize 吧！有关的反馈与讨论，可以通过加入邮件列表或提 issue，欢迎提交PR。\n译者按 随着 kubernetes 1.14 的发布，kustomize 被集成到 kubectl 中，用户可以利用 kubectl apply -k dir/ 将指定目录的 kustomization.yaml 提交到集群中。\n原文链接 https://kubernetes.io/blog/2018/05/29/introducing-kustomize-template-free-configuration-customization-for-kubernetes/\n","date":1555320201,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1555320201,"objectID":"cfb7bbfe49edd6e9e1ac2e746913d5ff","permalink":"https://guoxudong.io/post/introducing-kustomize-template-free-configuration-customization-for-kubernetes/","publishdate":"2019-04-15T17:23:21+08:00","relpermalink":"/post/introducing-kustomize-template-free-configuration-customization-for-kubernetes/","section":"post","summary":"本文介绍了 Kubernetes 原生的应用管理工具 Kustomize。","tags":["翻译","kustomize","kubernetes","工具"],"title":"Kustomize: 无需模板定制你的 kubernetes 配置","type":"post"},{"authors":["郭旭东"],"categories":["kustomize"],"content":" 初识 Kustomize 第一次听说 Kustomize 其实是在 kubernetes 1.14 发布时候，它被集成到 kubectl 中，成为了一个子命令，但也只是扫了一眼，并没有深究。真正让我注意到它，并主动开始了解其功能和使用方法的，是张磊大神在云栖社区发表的一篇文章《从Kubernetes 1.14 发布，看技术社区演进方向》，他在文中是这么说的：\n Kustomize 允许用户以一个应用描述文件 （YAML 文件）为基础（Base YAML），然后通过 Overlay 的方式生成最终部署应用所需的描述文件，而不是像 Helm 那样只提供应用描述文件模板，然后通过字符替换（Templating）的方式来进行定制化。\n 这不正我在苦苦寻找的东西嘛！自从公司确定了应用容器化的方案，至今已有半年多了，这期间我们的服务一个接一个的实现了容器化，部署到了 kubernetes 集群中。kubernetes 集群也有原先了1个测试集群，几个节点，发展到了如今的多个集群，几十个节点。而在推进容器化的过程中，每个服务都对对应多个应用描述文件（ YAML 文件），而根据环境的不同，又配置了多套的应用描述文件。随着服务越部越多，应用描述文件更是呈爆炸式的增长。\n感谢 devops 文化，它是我不需要为每个应用去写 YAML 文件，各个应用的开发组承担了这一工作，我只需要为他们提供基础模板即可。但应用上线后出现的 OOM 、服务无法拉起等 YAML 文件配置有误导致的问题接踵而至，使得我必须要深入各个服务，为他们配置符合他们配置。虽然也使用了 helm ，但是其只提供应用描述文件模板，在不同环境拉起一整套服务会节省很多时间，而像我们这种在指定环境快速迭代的服务，并不会减少很多时间。针对这种情况，我已经计划要自己开发一套更符合我们工作这种场景的应用管理服务，集成在我们自己的 devops 平台中。\n这时 Kustomize 出现了，我明锐的感觉到 Kustomize 可能就是解决我现阶段问题的一剂良药。\n什么是 Kustomize ？  Kubernetes native configuration management Kustomize introduces a template-free way to customize application configuration that simplifies the use of off-the-shelf applications. Now, built into kubectl as apply -k.\nkustomize 允许用户以一个应用描述文件 （YAML 文件）为基础（Base YAML），然后通过 Overlay 的方式生成最终部署应用所需的描述文件。而其他用户可以完全不受影响的使用任何一个 Base YAML 或者任何一层生成出来的 YAML 。这使得每一个用户都可以通过类似fork/modify/rebase 这样 Git 风格的流程来管理海量的应用描述文件。这种 PATCH 的思想跟 Docker 镜像是非常相似的，它可以规避“字符替换”对应用描述文件的入侵，也不需要用户学习额外的 DSL 语法（比如 Lua）。\n 而其成为 kubectl 子命令则代表这 kubectl 本身的插件机制的成熟，未来可能有更多的工具命令集成到 kubectl 中。拿张磊大神的这张图不难看出，在 kubernetes 原生应用管理系统中，应用描述文件在整个应用管理体系中占据核心位置，通过应用描述文件可以组合和编排多种 kubernetes API 资源，kubernetes 通过控制器来保证集群中的资源与应用状态与描述文件完全一致。\nKustomize 不像 Helm 那样需要一整套独立的体系来完成管理应用，而是完全采用 kubernetes 的设计理念来完成管理应用的目的。同时使用起来也更加的得心应手。\n参考  Kustomize - kustomize.io\n 从Kubernetes 1.14 发布，看技术社区演进方向 - yq.aliyun.com\n  ","date":1555306379,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1555306379,"objectID":"78fd9fb10ddd154bd4ad042fc0a6867a","permalink":"https://guoxudong.io/post/kustomize-1/","publishdate":"2019-04-15T13:32:59+08:00","relpermalink":"/post/kustomize-1/","section":"post","summary":"本篇为系列文章第一篇，介绍我对 Kustomize 的了解过程以及 Kustomize 是什么，为什么它能解决我的燃眉之急。","tags":["kubernetes","kustomize","工具"],"title":"使用 Kustomize 帮你管理 kubernetes 应用（一）：什么是 Kustomize ？","type":"post"},{"authors":["郭旭东"],"categories":["服务网格"],"content":" 所谓边车模式（ Sidecar pattern ），也译作挎斗模式，是分布式架构中云设计模式的一种。因为其非常类似于生活中的边三轮摩托车而得名。该设计模式通过给应用程序加上一个“边车”的方式来拓展应用程序现有的功能。这种设计模式出现的很早，实现的方式也多种多样。现在这个模式更是随着微服务的火热与 Service Mesh 的逐渐成熟而进入人们的视野。\n什么是边车模式 在 Azure Architecture Center 的云设计模式中是这么介绍边车模式的：\n Deploy components of an application into a separate process or container to provide isolation and encapsulation.\n\u0026mdash; Sidecar pattern\n 这里要注意的是： 这里的 Sidecar 是分布式架构中云设计模式的一种，与我们目前在使用的 Istio 或 Linkerd 中的 Sidecar 是设计与实现的区别，后文中提到的边车模式均是指这种设计模式，请勿与 Istio 或 其他 Service Mesh 软件 中的 Sidecar 混淆。\n边车模式是一种分布式架构的设计模式。如上图所示，边车就是加装在摩托车旁来达到拓展功能的目的，比如行驶更加稳定，可以拉更多的人和货物，坐在边车上的人可以给驾驶员指路等。边车模式通过给应用服务加装一个“边车”来达到控制和逻辑的分离的目的。\n比如日志记录、监控、流量控制、服务注册、服务发现、服务限流、服务熔断等在业务服务中不需要实现的控制面功能，可以交给“边车”，业务服务只需要专注实现业务逻辑即可。如上图那样，应用服务你只管开好你的车，打仗的事情就交给边车上的代理就好。这与分布式和微服务架构完美契合，真正的实现了控制和逻辑的分离与解耦。\n边车模式设计 在设计上边车模式与网关模式有类似之处，但是其粒度更细。其为每个服务都配备一个“边车”，这个“边车“可以理解为一个 agent ，这个服务所有的通信都是通过这个 agent 来完成的，这个 agent 同服务一起创建，一起销毁。像服务注册、服务发现、监控、流量控制、日志记录、服务限流和服务服务熔断等功能完全可以做成标准化的组件和模块，不需要在单独实现其功能来消耗业务开发的精力和时间来开发和调试这些功能，这样可以开发出真正高内聚低耦合的软件。\n这里有两种方法来实现边车模式：\n 通过 SDK 、 Lib 等软件包的形式，在开发时引入该软件包依赖，使其与业务服务集成起来。\n这种方法可以与应用密切集成，提高资源利用率并且提高应用性能。但是这种方法是对代码有侵入的，受到编程语言和软件开发人员水平的限制，但当该依赖有 bug 或者需要升级时，业务代码需要重新编译和发布。同时，如果该依赖宣布停止维护或者闭源，那么会给该服务带来不小的影响。\n 以 Sidecar 的形式，在运维的时候与应用服务集成在一起。\n这种方式对应用服务没有侵入性，不受编程语言和开发人员水平的限制，做到了控制与逻辑分开部署。但是会增加应用延迟，并且管理和部署的复杂度会增加。\n  边车模式解决了什么问题 边车模式在概念上是比较简单的，但是在实践中还是要了解边车模式到底解决了什么问题，我们为什么要使用边车模式？\n 控制与逻辑分离的问题\n边车模式是基于将控制与逻辑分离和解耦的思想，通俗的讲就是让专业的人做专业的事，业务代码只需要关心其复杂的业务逻辑，其他的事情”边车“会帮其处理，从这个角度看，可能叫跟班或者秘书模式也不错 :)\n日志记录、监控、流量控制、服务注册、服务发现、服务限流、服务熔断、鉴权、访问控制和服务调用可视化等，这些功能从本质上和业务服务的关系并不大，而传统的软件工程在开发层面完成这些功能，这导致了各种各样维护上的问题。\n就好像一个厨师不是必须去关心食材的产地、饭店的选址、是给大厅的客人上菜还是给包房的客人上菜\u0026hellip;他只需要做好菜就好，虽然上面的这些事他都可以做。而传统的软件工程就像是一个小饭店的厨师，他即是老板又是厨师，既需要买菜又需要炒菜，所有的事情都要他一个人做，如果客人一多，就会变的手忙脚乱；而控制与逻辑分离的软件，其逻辑部分就像是高档酒店的厨师，他只需要将菜做好即可，其他的事情由像”边车“这样的成员帮其处理。\n 解决服务之间调用越来越复杂的问题\n随着分布式架构越来越复杂和微服务越拆越细，我们越来越迫切的希望有一个统一的控制面来管理我们的微服务，来帮助我们维护和管理所有微服务，这时传统开发层面上的控制就远远不够了。而边车模式可以很好的解决这个问题。\n  从边车模式到 Service Mesh 边车模式有效的分离了系统控制和业务逻辑，可以将所有的服务进行统一管理，让开发人员更专注于业务开发，显著的提升开发效率。而遵循这种模式进行实践从很早以前就开始了，开发人员一直试图将上文中我们提到的功能（如：流量控制、服务注册、服务发现、服务限流、服务熔断等）提取成一个标准化的 Sidecar ，通过 Sidecar 代理来与其他系统进行交互，这样可以大大简化业务开发和运维。而随着分布式架构和微服务被越来越多的公司和开发者接受并使用，这一需求日益凸显。\n这就是 Service Mesh 服务网格诞生的契机，它是 CNCF（Cloud Native Computing Foundation，云原生基金会）目前主推的新一代微服务架构。 William Morgan 在 What\u0026rsquo;s a service mesh? And why do I need one? 【译文】中解释了什么是 Service Mesh 。\nService Mesh 有如下几个特点：\n 应用程序间通讯的中间层 轻量级网络代理 应用程序无感知 解耦应用程序的重试/超时、监控、追踪和服务发现  Service Mesh 将底层那些难以控制的网络通讯统一管理，诸如：流量管控，丢包重试，访问控制等。而上层的应用层协议只需关心业务逻辑即可。Service Mesh 是一个用于处理服务间通信的基础设施层，它负责为构建复杂的云原生应用传递可靠的网络请求。\n你真的需要 Service Mesh 吗？ 正如 NGINX 在其博客上发表的一篇文章名叫 Do I Need a Service Mesh?  【译文】 的文章中提到：\n As the complexity of the application increases, service mesh becomes a realistic alternative to implementing capabilities service-by-service.\n随着应用程序复杂性的增加，服务网格将成为实现服务到服务的能力的现实选择。\n 随着我们的微服务越来越细分，我们所要管理的服务正在成倍的增长着，Kubernetes 提供了丰富的功能，使得我们可以快速的部署和调度这些服务，同时也提供了我们熟悉的方式来实现那些复杂的功能，但是当临界点到来时，可能就是我们真正要去考虑使用 Service Mesh 的时候了。\n参考  Sidecar pattern ： https://docs.microsoft.com/en-us/azure/architecture/patterns/sidecar\n What\u0026rsquo;s a service mesh? And why do I need one?： https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/\n Do I Need a Service Mesh?：https://www.nginx.com/blog/do-i-need-a-service-mesh/\n  ","date":1554876205,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1554876205,"objectID":"488197fd3c04b7422c3fe16c678cc7cc","permalink":"https://guoxudong.io/post/sidercar-to-servicemesh/","publishdate":"2019-04-10T14:03:25+08:00","relpermalink":"/post/sidercar-to-servicemesh/","section":"post","summary":"谈谈从边车模式这一分布式架构的设计模式到 Service Mesh 的演变。","tags":["Service Mesh"],"title":"从边车模式到 Service Mesh","type":"post"},{"authors":["郭旭东"],"categories":["翻译"],"content":" DevOps 是什么？ “DevOps” 这个词是 development 和 operations 这两个词的组合。它是一种促进开发和运维团队之间的协作，以自动化和可重复的方式更快地将代码部署到生产中的文化。\nDevOps 帮助团体提高软件和服务的交付速度。它使团队能够更好地为客户服务，并提高在市场中的竞争力。\n简而言之， DevOps 可以定义为通过更好的沟通和协作，使开发和运维保持一致。\n本手册中，您将学到：\n DevOps 是什么？ 为什么需要 DevOps ？ DevOps 与传统运维有什么不同？ 为什么使用 DevOps ？ DevOps 的生命周期 DevOps 的工作流 DevOps 与敏捷有什么不同？ DevOps VS Agile DevOps 原则 谁可以做 DevOps 工程师？ DevOps 工程师的角色、职责和技能 DevOps 工程师可以挣多少钱？ DevOps 培训认证 DevOps 自动化工具 DevOps 的未来是怎样的？ 总结  为什么需要 DevOps ？  在实行 DevOps 之前，开发和运维团队是完全孤立的。 测试和部署是设计在构建之后完成的独立活动。因此，他们比实际构建周期消耗更多时间。 在不使用 DevOps 的情况下，团队成员将大量时间花在测试，部署和设计上，而不是构建项目。 手动部署代码会导致生产中出现人为错误。 开发和运维团队都有各自的时间表，时间的不同步导致生产交付进一步延误。  提高软件交付率是业务方最迫切的需求。根据 Forrester Consulting Study 统计，只有17％的团队可以足够快地交付软件。更是证明了这一痛点。\nDevOps 与传统运维有什么不同？ 让我们将传统软件瀑布开发模型与 DevOps 进行比较，以了解 DevOps 带来的变化。\n我们假设有一个应用程序计划在2周内上线，代码完成80％。该应用程序是一个新的发布，从购买服务器开始\n   瀑布式开发 DevOps     订购新服务器后，开发团队需要进行测试。运维团队根据需求文档开始部署基础设施。 订购新服务器后，开发和运维团队根据需求文档共同调试部署新服务器。这样开发人员可以更好地了解服务器的基础架构。   关于故障转移，冗余策略，数据中心位置和存储要求的规划存在偏差，因为开发人员对应用程序有深入了解，但他们无法提供任何协助。 由于开发人员的加入，有关故障转移，冗余策略，灾难恢复，数据中心位置和存储要求的规划非常准确。   运维团队对开发团队的进展一无所知。只能根据运维团队理解制定监控计划。 在 DevOps 中，运维团队完全了解开发人员的进展。通过互动，共同制定满足运维和业务需求的监控计划。他们还使用应用程序性能监视（APM）工具以优化应用。   在上线之前，压力测试使应用程序崩溃。发布延迟了。 在上线之前，压力测试使应用程序有点慢。开发团队迅速解决了瓶颈问题。该应用程序按时发布。    为什么使用 DevOps ？ DevOps 允许敏捷开发团队实施持续集成和持续交付。这有助于他们更快地将产品推向市场。\n其他重要原因是：\n 可预测性： DevOps 可以显着降低新版本的故障率。 自愈性： 可以随时将应用回滚到较早的版本。 可维护性： 在新版本崩溃或当前系统不可用的情况下，可以毫不费力地进行恢复。 上线时间： DevOps 通过简化软件交付流程将上线时间缩短至50％。对于互联网和移动应用时间更短。 更高的质量： DevOps 帮助团队提高应用程序开发的质量。 降低风险： DevOps 在软件交付的生命周期中包含安全检查。它有助于减少整个软件生命周期中的安全风险。 弹性： 软件系统的运行状态更稳定，更安全，更改是可审计的。 成本效益： DevOps 在软件开发过程中提供了成本效益，这始终是互联网公司管理层所期望的。 将大的代码库分成小块： DevOps 是基于敏捷编程方法的。因此，它允许将大的代码库分解为更小且易于管理的块。  什么时候使用 DevOps ？ DevOps 应该用于大型分布式应用程序，例如电子商务站点或托管在云平台上的应用程序。\n什么时候不使用 DevOps？ 它不应该用于关键任务应用程序，如银行，电力设施和其他敏感数据站点。此类应用程序需要对生产环境进行严格的访问控制，详细的变更管理策略，完善的数据中心访问控制策略。\nDevOps 的生命周期 DevOps 是开发和运维之间的深度集成。在不了解 DevOps 生命周期的情况下，是无法真正理解 DevOps 的。\n以下是有关 DevOps生命周期的简要信息：\n 开发\n在此阶段，整个开发过程分为小的开发周期。这有利于 DevOps 团队加快软件开发和交付过程。\n 测试\nQA 团队使用 Selenium 等自动化测试工具来识别和修复新代码中的错误。\n 集成\n在此阶段，新功能与主分支代码集成，并进行测试。只有持续集成和测试才能实现持续交付。\n 部署\n在此阶段，部署过程持续进行。它的执行方式是任何时候在代码中进行的任何更改都不应影响高流量网站的运行。\n 监测\n在此阶段，运维团队将负责处理不合适的系统行为或生产中发现的错误。\n  DevOps 的工作流 工作流允许排列和分离用户最需要的任务。它还能够在配置任务时反应其最理想过程。\nDevOps 与敏捷有什么不同？ DevOps VS Agile 这是一个典型的IT流程\n敏捷解决了客户和开发人员沟通中的问题\nDevOps 解决了开发人员运维人员沟通中的问题\n   敏捷 DevOps     强调打破开发人员和管理层之间的障碍。 DevOps 是关于软件开发和运维团队的。   解决客户需求与开发团队之间的距离。 解决开发和运维团队之间的距离。   重点关注功能和非功能准备。 它侧重于运维和业务准备。   敏捷开发主要涉及公司对开发方式的思考。 DevOps 强调以最可靠和最安全的方式部署软件，而这些方式并不总是最快的。   敏捷开发非常注重培训所有团队成员，使他们拥有各种相同的技能。因此，当出现问题时，任何团队成员都可以在没有团队领导的情况下从别的成员那里获得帮助。 DevOps 在开发和运维团队之间传播技能，并保持一致的沟通。   敏捷开发管理 “sprint” ，意味着时间更短（不到一个月），并且在此期间将产生和发布多个功能。 DevOps 努力争取主要版本的稳定可靠，而不是更小和更频繁的发布版本。    DevOps 原则 这里有六个在采用 DevOps 时必不可少的原则：\n 以客户为中心： DevOps 团队必须以客户为中心，因为是他们不断向我的产品和服务投资。 端到端的责任： DevOps 团队需要在产品的整个生命周期提供性能支持。这提高了产品的水平和质量。 持续改进： DevOps 文化专注于持续改进，以尽量减少浪费。它不断加快产品或服务改进的速度。 自动化一切： 自动化是 DevOps 流程的重要原则。这不仅适用于软件开发，同时也适用于整个基础架构环境。 作为一个团队工作： 在 DevOps 文化角色中，设计人员，开发人员和测试人员已经定义。他们所需要做的就是作为一个团队完成合作。 监控和测试所有内容： DevOps 团队拥有强大的监控和测试程序是非常重要的。  谁可以做 DevOps 工程师？ DevOps 工程师是一名IT专业人员，他与软件开发人员，系统运维人员和其他IT人员一起管理代码发布。DevOps 应具备与开发，测试和运维团队进行沟通和协作的硬技能和软技能。\nDevOps 方法需要对代码版本进行频繁的增量更改，这意味着频繁的部署和测试方案。尽管 DevOps 工程师需要偶尔从头开始编码，但重要的是他们应该具备软件开发语言的基础知识。\nDevOps 工程师将与开发团队的工作人员一起解决连接代码的元素（如库或软件开发工具包）所需的编码和脚本。\nDevOps 工程师的角色、职责和技能 DevOps 工程师负责软件应用程序平台的生产和持续维护。\n以下是 DevOps 工程师的一些角色，职责和技能：\n 能够跨平台和应用程序域执行系统故障排除和问题解决。 通过开放的，标准的平台有效管理项目。 提高项目可见性和可追溯性。 通过协作提高开发质量并降低开发成本。 分析、设计和评估自动化脚本和系统。 通过使用最佳的云安全解决方案确保系统的安全。 DevOps 工程师应该具备问题解决者和快速学习者的软技能。  DevOps 工程师可以挣多少钱？ DevOps 是最热门的IT专业之一。这就是为什么那里都有很多机会的原因。因此，即使是初级DevOps工程师的薪酬水平也相当高。在美国，初级DevOps工程师的平均年薪为78,696美元。\nDevOps 培训认证 DevOps 培训认证可以帮助任何渴望成为 DevOps 工程师职业的人。认证可从 Amazon web services 、 Red Hat 、 Microsoft Academy 、 DevOps Institute 获得。\nAWS Certified DevOps Engineer\n此 DevOps 工程师证书将测试您如何使用最常见的 DevOps 模式在 AWS 上开发，部署和维护应用程序。它还会评估 DevOps 方法的核心原则。\n该认证有两个必要条件：认证费用为300美元，持续时间为170分钟。\nRed Hat Certification\n红帽为 DevOps 专业人士提供不同级别的认证，如下所示:\n Red Hat Certificate of Expertise in Platform-as-a-Service Red Hat Certificate of Expertise in Containerized Application Development Red Hat Certificate of Expertise in Ansible Automation Red Hat Certificate of Expertise in Configuration Management Red Hat Certificate of Expertise in Container Administration  Devops Institute\nDevops Institute是围绕新兴 DevOps 实践的全球学习社区。该组织正在为 DevOps 能力资格设置质量标准。Devops Institute目前提供三个课程和认证。\n公司提供的认证课程有：\n DevOps Foundation DevOps Foundation Certified Certified Agile Service Manager Certified Agile Process Owner DevOps Test Engineering Continuous Delivery Architecture DevOps Leader DevSecOps Engineering  DevOps 自动化工具 所有测试流程自动化并对其进行配置以实现至关重要的速度和灵活性。此过程称为 DevOps 自动化。\n维护庞大的IT基础架构的大型 DevOps 团队面临的困难可以简要分为六个不同的类别。\n 基础设施自动化 配置管理 部署自动化 性能管理 日志管理 监测  让我们看看每个类别中的工具以及它们如何解决痛点：\n基础设施自动化 亚马逊网络服务（AWS）：作为云服务，您无需建立实际的数据中心。此外，它们易于按需扩展。没有前期硬件成本。它可以配置为自动根据流量配置更多服务器。\n配置管理 Chef：它是一个有用的 DevOps 工具，用于提升速度，规模和一致性。它可用于简化复杂任务并执行配置管理。使用此工具，DevOps 团队可以避免在一万台服务器上进行更改。相反，只需要在一个地方进行更改，这些更改会自动反映在其他服务器中。\n部署自动化 Jenkins：该工具有助于持续集成和测试。通过在部署构建后快速查找问题，更​​轻松地集成项目更改。\n日志管理 Splunk：可以解决在一个地方聚合，存储和分析所有日志的问题的工具。\n性能管理 App Dynamic：它是一个 DevOps 工具，提供实时性能监控。此工具收集的数据可帮助开发人员在发生问题时进行调试。\n监控 Nagios：在基础架构和相关服务出现故障时通知相关人员也很重要。Nagios 就是这样一种工具，它可以帮助 DevOps 团队找到并纠正问题。\nDevOps 的未来是怎样的？  团队将代码部署周期转换为数周和数月，而不是数年。 很快就会看到，DevOps 工程师可以比企业中的任何其他人更多地接近和管理终端用户。 DevOps 正在成为IT人员的重要技能。例如，Linux 招聘进行的一项调查发现，25％的受访者的求职者寻求 DevOps 工作。 DevOps 和持续交付将继续存在。因为公司需要发展，他们别无选择，只能改变。然而，DevOps 概念的主流化则需要5到10年。  总结  DevOps 是一种促进开发和运维团队之间的协作，以自动化和可重复的方式更快地将代码部署到生产中的文化。 在 DevOps 出现之前运维和开发团队完全独立。 手动部署代码会导致生产中出现人为错误。 在旧的软件开发流程中，运维团队不了解开发团队的进度。因此，运维团队只能根据他们自己的理解制定了基础设施的购买和监控计划。 在 DevOps 流程中，运维团队充分了解开发人员的进度。采购和监控计划准确无误。 DevOps 提供可维护性，可预测性，更高质量的代码和更准确的上线时间。 敏捷流程侧重于功能和非功能准备，而 DevOps 则侧重于IT基础架构方面。 DevOps 生命周期包括开发，测试，集成，部署和监控。 DevOps 工程师将与开发团队工作人员合作，以解决编码和脚本编写需求。 DevOps 工程师应该具备问题解决者的软技能，并且是一个快速学习者。 DevOps 认证可从 Amazon web services，Red Hat，Microsoft Academy，DevOps Institute 获得 DevOps 可帮助团队将代码部署周期转换为数周和数月，而不是数年。  原文链接 https://www.guru99.com/devops-tutorial.html\n","date":1554787316,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1554787316,"objectID":"b238927f28f34b86efe319037ab06e47","permalink":"https://guoxudong.io/post/devops-tutorial/","publishdate":"2019-04-09T13:21:56+08:00","relpermalink":"/post/devops-tutorial/","section":"post","summary":"“DevOps”这个词是 “development” 和 “operations”这两个词的组合。它是一种促进开发和运维团队之间的协作，以自动化和可重复的方式更快地将代码部署到生产中的文化。","tags":["devops","翻译"],"title":"Devops入门手册","type":"post"},{"authors":["郭旭东"],"categories":["部署安装"],"content":"  Rancher 不仅可以在任何云提供商的任何地方部署 Kubernetes 集群，而且还将它们集中在集中式的身份验证和访问控制之下。由于它与资源的运行位置无关，因此您可以轻松地在不同的环境部署你的 kubernetes 集群并操作他们。 Rancher 不是将部署几个独立的 Kubernetes 集群，而是将它们统一为一个单独的托管Kubernetes Cloud。\n 前言 目前我们使用的是 rancher 2.1.1版本，在去年 rancher 发布 v2.1.* 版本的时候做过一次升级，当时遇到了很多问题，虽然都一一解决，但是并没有有效的记录下来，这里在升级 v2.2.* 版本的时候做一个记录以便在今后升级的时候的提供参考作用。\n升级前的准备  首先查看当前 rancher 版本，记下这个版本号后面需要使用。查看方式就是登陆 rancher 在左下角就可以看到当前版本号，我们这里使用的v2.1.1版本。 打开官方文档，这里推荐对照官方文档进行升级，一般官方文档都会及时更新并提供最佳升级方法，而一般的博客会因为其写作时间、使用版本、部署环境的不同有所偏差。官方文档： https://www.cnrancher.com/docs/rancher/v2.x/cn/upgrades/single-node-upgrade/  升级  首先获取正在运行的 rancher 容器 ID,由以下命令可知 RANCHER_CONTAINER_ID 为 83167cb60134\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 83167cb60134 rancher/rancher:latest \u0026quot;entrypoint.sh\u0026quot; 4 months ago Up 4 months 0.0.0.0:80-\u0026gt;80/tcp, 0.0.0.0:443-\u0026gt;443/tcp priceless_newton  停止该容器\n$ docker stop {RANCHER_CONTAINER_ID}  创建正在运行的 Rancher Server 容器的数据卷容器，将在升级中使用，这里命名为 rancher-data 容器。\n 替换{RANCHER_CONTAINER_ID}为上一步中的容器ID。 替换{RANCHER_CONTAINER_TAG}为你当前正在运行的Rancher版本，如上面的先决条件中所述。\n$ docker create --volumes-from {RANCHER_CONTAINER_ID} --name rancher-data rancher/rancher:{RANCHER_CONTAINER_TAG}   备份 rancher-data 数据卷容器\n如果升级失败，可以通过此备份还原Rancher Server，容器命名:rancher-data-snapshot-.\n 替换{RANCHER_CONTAINER_ID}为上一步中的容器ID。 替换{CURRENT_VERSION}为当前安装的Rancher版本的标记。 替换{RANCHER_CONTAINER_TAG}为当前正在运行的Rancher版本，如先决条件中所述 。\n$ docker create --volumes-from {RANCHER_CONTAINER_ID} --name rancher-data-snapshot-{CURRENT_VERSION} rancher/rancher:{RANCHER_CONTAINER_TAG}   拉取Rancher的最新镜像,这里确保有外网，可能拉取到新的镜像，如果没有外网，这里就需要将镜像上传到私有镜像仓库，将拉取地址设置为私有镜像仓库即可\n$ docker pull rancher/rancher:latest  通过 rancher-data 数据卷容器启动新的 Rancher Server 容器。\n这里要注意到，我们这是使用的是独立容器+外部七层负载均衡，是通过阿里云SLB进行SSL证书认证，需要在启动的时候增加--no-cacerts\n$ docker run -d --volumes-from rancher-data --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher:latest --no-cacerts   升级过程会需要一定时间，不要在升级过程中终止升级，强制终止可能会导致数据库迁移错误。\n升级 Rancher Server后， server 容器中的数据会保存到 rancher-data 容器中，以便将来升级。\n 删除旧版本 Rancher Server 容器\n如果你只是停止以前的Rancher Server容器(并且不删除它),则旧版本容器可能随着主机重启后自动运行，导致容器端口冲突。\n 升级成功\n访问 rancher 可以看到右下角版本已经完成更新。\n  ","date":1554002135,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1554002135,"objectID":"69746155b644a99ee7a09912d6465296","permalink":"https://guoxudong.io/post/rancher-update-2.2.1/","publishdate":"2019-03-31T11:15:35+08:00","relpermalink":"/post/rancher-update-2.2.1/","section":"post","summary":"Rancher 不仅可以在任何云提供商的任何地方部署 Kubernetes 集群，而且还将它们集中在集中式的身份验证和访问控制之下。由于它与资源的运行位置无关，因此您可以轻松地在不同的环境部署你的 kubernetes 集群并操作他们。 Rancher 不是将部署几个独立的 Kubernetes 集群，而是将它们统一为一个单独的托管Kubernetes Cloud。","tags":["rancher","容器","kubernetes"],"title":"单节点版rancher升级指南","type":"post"},{"authors":["郭旭东"],"categories":["翻译"],"content":"  作者：William Morgan 发表于2017年4月25日，2018年11月26日有所修改。\n Service Mesh 是一个专门使服务与服务之间的通信变得安全、快速和可靠的的基础设施。如果你正在在构建一个云原生（ Cloud Native ）应用，那么你一定需要 Service Mesh 。\n在过去的一年中， Service Mesh 成为了云原生技术栈的关键组件。像 Paypal , Ticketmaster 和 Credit Karma 这样的大厂，已经将 Service Mesh 加入到他们的全部应用中。并且在2017年1月，开源的 Service Mesh 软件 Linkerd 加入云原生基金会（ CNCF ），成为云原生基金会（ CNCF ）的官方项目。但是什么是真正的 Service Mesh ？它又为何突然变的如此重要？\n在这篇文章，我会讲解 Service Mesh 的定义，并通过应用服务架构过去十年的发展追溯其起源。并将 Service Mesh 与其他相似的概念（包括 API 网关，边缘代理以及 ESB （enterprise service bus））进行区分。最终，将会描述 Service Mesh 的发展方向，以及随着云原生概念的普及，Service Mesh 发生的变化。\n什么是 Service Mesh Service Mesh 这个服务网络专注于处理服务和服务间的通讯。其主要负责构造一个稳定可靠的服务通讯的基础设施，并让整个架构更为的先进和 Cloud Native。在工程中，Service Mesh 基本来说是一组轻量级的服务代理和应用逻辑的服务在一起，并且对于应用服务是透明的。\nService Mesh 作为独立层的概念与云原生应用的兴起有关。在云原生模式，单个应用可能有数百个服务组成，每个服务又可能有上千个实例，而每个实例都有可能被像 kubernetes 这样的服务调度器不断调度从而不断变化状态。而这些复杂的通信又普遍是服务运行时行为的一部分，这时确保端到端的通信的性能和可靠性就变的至关重要。\nService Mesh 就是一个网络模型吗？ Service Mesh 是一个位于 TCP/IP 上的抽象层的网络模型。它假定底层 L3/L4 网络存在并且能够从一点向另一点传输数据。（它还假定这个网络和环境的其他方面一样不可靠，所以 Service Mesh 也必须能够处理网络故障。）\n在某些方面，Service Mesh 就像是网络七层模型中的第四层 TCP 协议。其把底层的那些非常难控制的网络通讯方面的控制面的东西都管了（比如：丢包重传、拥塞控制、流量控制），而更为上面的应用层的协议，只需要关心自己业务应用层上的事了。\n与 TCP 不同的是， Service Mesh 想要达成的目的不仅仅是正常的网络通讯。它为应用提供了统一的，可视化的以及可控制的控制平面。Service Mesh 是要将服务间的通信从无法发现和控制的基础设施中分离出来，并对其进行监控、管理和控制。\nService Mesh 实际上做了什么？ 在云原生应用中传递可靠的请求是十分复杂的。而 Linkerd 提供了服务熔断、重试、负载均衡、熔断降级等功能，通过其强大的功能来管理那些必须运行在复杂环境中的服务。\n这里列举一个通过 Linkerd 向服务发出请求的简单流程：\n 通过 Linkerd 的动态路由规则来确定打算连接哪个服务。这个请求是要路由到生产环境还是演示环境？是请求本地数据中心的服务还是云上的服务？是请求正在测试的最新版的服务还是已经在生产中经过验证的老版本？所有的这些路由规则都是动态配置的，可以全局应用也可以部分应用。 找到正确的目的服务后， Linkerd 从一个或几个相关的服务发现端点检索实例池。如果这些信息与 Linkerd 的服务发现信息不同， Linkerd 会决定信任哪些信息来源。 Linkerd 会根据观察到的最近的响应延迟来选择速度最快的实例。 Linkerd 发送请求给这个实例，记录延迟和响应类型。 如果这个实例挂了、无响应或者无法处理请求， Linkerd 会再另一个实例上重试这个请求。（但只有在请求是幂等的时候） 如果一个实例一直请求失败， Linkerd 会将其移出定时重试的负载均衡池。 如果请求超时， Linkerd 会主动将请求失效，而不是进一步重试从而增加负载。 Linkerd 会记录指标和分布式的追踪上述行为的各个方面，将他们保存在集中的指标系统中。  以上只是简化版的介绍， Linkerd 还可以启动和重试 TLS ，执行协议升级，动态切换流量，甚至在故障之后数据中心的切换。 值得注意的是，这些功能旨在为每个实例和应用程序提供弹性伸缩。而大规模的分布式系统（无论是如何构建的）都有一个共同特点：都会因为许多小的故障，而升级为全系统灾难性的故障。Service Mesh 则被设计为通过快速的失效和减少负载来保护整个系统免受这样灾难性的故障。\n为什么 Service Mesh 是必要的？ Service Mesh 本质上并不是什么新技术，而是功能所在位置的转变。Web 应用需要管理复杂的服务通信，Service Mesh 模式的起源和演变过程可以追溯到15年前。\n参考2000年左右中型 Web 应用的典型三层架构，在这个架构中，应用被分为三层：应用逻辑、web 服务逻辑、存储逻辑。层之间的通信虽然复杂，但是毕竟范围有限，最多只有2跳。这里并不是 “Mesh” 的，但在每层中处理跳转的代码是存在通信逻辑的。\n当这种架构向更大规模发展的时候，这种通信方式就无以为继了。像 Google , Netflix , 和 Twitte ，在面临巨大的请求流量的时候，他们的实现了云原生应用的前身：应用被分割成了许多服务（现在称作“微服务”），这些服务组成了一种网格结构。在这些系统中，通用通信层突然兴起，表现为“胖客户端”的形式 - Twitter 的 Finagle, Netflix 的 Hystrix 和 Google 的 Stubby 都是很典型的例子。\n现在看来，像 Finagle 、Stubby 和 Hystrix 这样的库就是最早的 Service Mesh。虽然它们是为特定环境、语言和框架定制了，但都是作为基础设施专门用于管理服务间的通信，并（在 Finagle 和 Hystrix 开源的情况下）在其他公司的应用中被使用。\n这三个组件都有应用自适应机制，以便在负载中进行拓展，并处理在云环境中的部分故障。但是对于数百个服务或数千个实例，以及不时需要重新调度的业务层实例，单个请求通过的调用链可能变的非常复杂，而且服务可能由不同的语言编写，这时基于库的解决方案可能就不再适用了。\n服务通信的复杂性和重要性导致我们急需一个专门的基础设施层来处理服务间的通信，该层需要与业务代码解耦，并且具有捕获底层环境的动态机制。这就是 Service Mesh 。\nService Mesh 的未来 Service Mesh 在云生态下迅速的成长，并且有着令人激动的未来等待探索。对无服务器计算（Serverless， 例如 Amazon 的 Lambda）适用的 Service Mesh 网络模型，在云生态系统中角色的自然拓展。Service Mesh 可能成为服务身份和访问策略这些在云原生领域还是比较新的技术的基础。最后，Service Mesh ，如之前的TCP / IP，将推进加入到底层的基础架构中。就像 Linkerd 是由像 Finagle 这样的系统发展而来，Service Mesh 将作为单独的用户空间代理添加到云原生技术栈中继续发展。\n结语 Service Mesh 是云原生技术栈的关键技术。Linkerd 成立仅1年就成为了云原生基金会（CNCF）的一部分，拥有蓬勃发展的社区和贡献者。使用者从像 Monzo 这样颠覆英国银行业的创业公司，到像 Paypal、 Ticketmaster 和 Credit Karma 这样的互联网大厂，再到像 Houghton Mifflin Harcourt 这样经营了数百年的公司。\n使用者和贡献者每天都在 Linkerd 社区展示 Service Mesh 创造的价值。我们将致力于打造这一令人惊叹的产品，并继续发展壮大我们的社区，加入我们吧！\n原文链接 https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/\n","date":1553509040,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1553509040,"objectID":"eb4a0539fb3ed49ca0b08950721945a5","permalink":"https://guoxudong.io/post/whats-a-service-mesh-and-why-do-i-need-one/","publishdate":"2019-03-25T18:17:20+08:00","relpermalink":"/post/whats-a-service-mesh-and-why-do-i-need-one/","section":"post","summary":"Service Mesh 是一个专门用于使服务与服务之间的通信变得安全、快速和可靠的的基础设施。如果你正在在构建一个云原生（ Cloud Native ）应用，那么 Service Mesh 是你需要的。","tags":["翻译","service mesh"],"title":"Service Mesh是什么，我们又为什么需要它","type":"post"},{"authors":["郭旭东"],"categories":["istio"],"content":" 前言 之前介绍了 Istio 和 Service Mesh 能给我们带来什么，我们为什么要用 Istio ，但大家对 Istio 的认识可能还没有那么深刻。正如Linux 的创始人 Linus Torvalds 的那句话：Talk is cheap. Show me the code. 这里我们部署一个demo，由四个单独的微服务构成（注意这里的四个微服务是由不同的语言编写的），用来演示多种 Istio 特性。这个应用模仿在线书店的一个分类，显示一本书的信息。页面上会显示一本书的描述，书籍的细节（ISBN、页数等），以及关于这本书的一些评论。\nBookinfo 应用 Bookinfo 应用分为四个单独的微服务：\n - ```details``` ：这个微服务包含了书籍的信息。 - ```reviews``` ：这个微服务包含了书籍相关的评论。它还会调用 ratings 微服务。 - ```ratings``` ：ratings 微服务中包含了由书籍评价组成的评级信息。 这里主要使用```reviews```来演示 Istio 特性，```reviews``` 微服务有 3 个版本： - v1 版本不会调用 ```ratings``` 服务。 - v2 版本会调用 ```ratings``` 服务，并使用 1 到 5 个黑色星形图标来显示评分信息。 - v3 版本会调用 ```ratings``` 服务，并使用 1 到 5 个红色星形图标来显示评分信息。 下图展示了这个应用的端到端架构。 ![Istio 注入之前的 Bookinfo 应用](https://istio.io/docs/examples/bookinfo/noistio.svg) \u0026lt;center\u0026gt;Istio 注入之前的 Bookinfo 应用\u0026lt;/center\u0026gt; Bookinfo 是一个异构应用，几个微服务是由不同的语言编写的。这些服务对 Istio **并无依赖**，但是构成了一个有代表性的服务网格的例子：它由多个服务、多个语言构成，并且 reviews 服务具有多个版本。 ## 部署应用 这里 Istio 的安装部署就不在赘述了。 值得注意的是：如果使用的是**阿里云**容器服务安装的 Istio ，需要在 ```容器服务```-```市场```-```应用目录``` 中选择 ```gateway``` 进行安装，这里提供了多种 ```gateway``` ，我们选择 ```istio-ingressgateway```，选择直接安装的话会默认创建 ```LoadBalancer``` 类型的Service，会自动创建一个经典网络SLB，这里是可以调整的，会在后续的文章中进行详细讲解，这里不做赘述。 在 Istio 中运行这一应用，无需对应用自身做出任何改变。我们只要简单的在 Istio 环境中对服务进行配置和运行，具体一点说就是把 Envoy sidecar 注入到每个服务之中。这个过程所需的具体命令和配置方法由运行时环境决定，而部署结果较为一致，如下图所示： ![Bookinfo 应用](https://istio.io/docs/examples/bookinfo/withistio.svg) \u0026lt;center\u0026gt;Bookinfo 应用\u0026lt;/center\u0026gt; 所有的微服务都和 Envoy sidecar 集成在一起，被集成服务所有的出入流量都被 sidecar 所劫持，这样就为外部控制准备了所需的 Hook，然后就可以利用 Istio 控制平面为应用提供服务路由、遥测数据收集以及策略实施等功能。 ### 下载安装 到 GitHub 中 istio 的 [release](https://github.com/istio/istio/releases) 中下载相应版本的 istio 包，下载后将 ```bin``` 目录配置到环境变量 ```PATH``` 中 ```export PATH=\u0026quot;/istio/bin:$PATH\u0026quot;``` ，这里我们使用的是 ```istio 1.0.5``` 版本 Bookinfo 这个应用就在 ```samples/```目录下 ## 在 阿里云容器服务（kubernetes） 中运行 启动应用容器，这里提供两种注入方法：**手工注入**和**自动注入** - 自动注入 需要修改 namespace ，为其添加 label 标签，这样所以在这个 namespace 中创建的应用都会被自动注入 sidecar  bash $ kubectl label namespace {inject-namespace} istio-injection=enabled $ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml\n - 手工注入 需要使用 istioctl 命令生成注入后应用的配置，然后在部署应用  bash $ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml | kubectl apply -f -\n 由于是测试，这里我们使用手工注入的方法。 上面的命令会启动全部的四个服务，其中也包括了 ```reviews``` 服务的三个版本（```v1```、```v2``` 以及 ```v3```）  bash\n  $ istioctl kube-inject -f bookinfo.yaml | kubectl apply -f - service/details created deployment.extensions/details-v1 configured service/ratings created deployment.extensions/ratings-v1 created service/reviews created deployment.extensions/reviews-v1 created deployment.extensions/reviews-v2 created deployment.extensions/reviews-v3 created service/productpage created deployment.extensions/productpage-v1 created $ kubectl get po NAME READY STATUS RESTARTS AGE details-v1-8685d68cf9-8fwdb 2\u0026frasl;2 Running 0 1h productpage-v1-5fd9fddc97-tx88z 2\u0026frasl;2 Running 0 1h ratings-v1-7c4d756c55-cn76d 2\u0026frasl;2 Running 0 1h reviews-v1-5d868db586-w28q5 2\u0026frasl;2 Running 0 1h reviews-v2-787647c7d9-7sc52 2\u0026frasl;2 Running 0 1h reviews-v3-6964c86584-8728m 2\u0026frasl;2 Running 0 1h $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.11.224.17  9080/TCP 1h productpage ClusterIP 10.11.16.86  9080/TCP 1h ratings ClusterIP 10.11.244.59  9080/TCP 1h reviews ClusterIP 10.11.162.37  9080/TCP 1h\n 可以看到 Bookinfo 应用已经正常运行 ### 指定 ingress 和 IP 的端口 1. 为为应用程序定义入口网关： ```bash $ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml ``` 2. 确认网关创建完成 ```bash $ kubectl get gateway NAME AGE bookinfo-gateway 1h ``` 3. 快速查询访问地址，这里的是之前在阿里云上创建的 ```LoadBalancer``` 类型的 Service ```bash $ kubectl get svc istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.11.18.83 xxx.xxx.xxx.xxx 80:xxx/TCP,443:xxx/TCP 2h ``` ### 查看效果 访问 http://{EXTERNAL-IP}/productpage 注意：这里最后不能有/，否则将找不到页面 ![image](http://wx4.sinaimg.cn/large/ad5fbf65ly1g1ad2jg6p3j21g90mxgo7.jpg) 多次刷新浏览器，将在 ```productpage``` 中看到评论的不同的版本，它们会按照 round robin（红星、黑星、没有星星）的方式展现，这三个展示分来来自```v1```、```v2```和```v3```版本，因为还没有使用 Istio 来控制版本的路由，所以这里显示的是以轮询的负载均衡算法进行展示。 ### 请求路由 BookInfo示例部署了三个版本的reviews服务，因此需要设置一个缺省路由。否则当多次访问该应用程序时，会发现有时输出会包含带星级的评价内容，有时又没有。出现该现象的原因是当没有为应用显式指定缺省路由时，Istio会将请求随机路由到该服务的所有可用版本上。 在使用 Istio 控制 Bookinfo 版本路由之前，你需要在目标规则中定义好可用的版本 。 运行以下命令为 Bookinfo 服务创建的默认的目标规则： - 如果不需要启用双向TLS，请执行以下命令： ```bash $ kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml ``` - 如果需要启用双向 TLS，请执行以下命令： ```bash $ kubectl apply -f samples/bookinfo/networking/destination-rule-all-mtls.yaml ``` 等待几秒钟，等待目标规则生效。你可以使用以下命令查看目标规则： ```bash kubectl get destinationrules NAME AGE details 28s productpage 28s ratings 28s reviews 28s ``` ### 将所有微服务的缺省版本设置为v1 通过运行如下命令，将所有微服务的缺省版本设置为v1：  bash $ kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml\n 可以通过下面的命令来显示所有已创建的路由规则：  bash $ kubectl get virtualservices NAME AGE bookinfo 33m details 8s productpage 8s ratings 8s reviews 8s\n 显示已创建的详细路由规划：  bash $ kubectl get virtualservices -o yaml\n 由于路由规则是通过异步方式分发到代理的，过一段时间后规则才会同步到所有pod上。因此需要等几秒钟后再尝试访问应用。 在浏览器中打开 Bookinfo 应用程序的URL: http://{EXTERNAL-IP}/productpage。 ![image](http://wx4.sinaimg.cn/large/ad5fbf65ly1g1adqyf9dej21g70oitbd.jpg) 可以看到 Bookinfo 应用程序的 ```productpage``` 页面，显示的内容中不包含带星的评价信息，这是因为 ```reviews:v1``` 服务不会访问ratings服务。 ### 将来自特定用户的请求路由到reviews:v2 本例中，首先使用 Istio 将100%的请求流量都路由到了 Bookinfo 服务的```v1```版本；然后再设置了一条路由规则，路由规则基于请求的 header（例如一个用户cookie）选择性地将特定的流量路由到了 ```reviews``` 服务的```v2```版本。 通过运行如下命令，把来自测试用户\u0026quot;jason\u0026quot;的请求路由到 ```reviews:v2 ```，以启用ratings服务。  bash $ kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml\n 通过如下命令确认规则是否创建：  bash $ kubectl get virtualservice reviews -o yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026ldquo;apiVersion\u0026rdquo;:\u0026ldquo;networking.istio.io/v1alpha3\u0026rdquo;,\u0026ldquo;kind\u0026rdquo;:\u0026ldquo;VirtualService\u0026rdquo;,\u0026ldquo;metadata\u0026rdquo;:{\u0026ldquo;annotations\u0026rdquo;:{},\u0026ldquo;name\u0026rdquo;:\u0026ldquo;reviews\u0026rdquo;,\u0026ldquo;namespace\u0026rdquo;:\u0026ldquo;default\u0026rdquo;},\u0026ldquo;spec\u0026rdquo;:{\u0026ldquo;hosts\u0026rdquo;:[\u0026ldquo;reviews\u0026rdquo;],\u0026ldquo;http\u0026rdquo;:[{\u0026ldquo;match\u0026rdquo;:[{\u0026ldquo;headers\u0026rdquo;:{\u0026ldquo;end-user\u0026rdquo;:{\u0026ldquo;exact\u0026rdquo;:\u0026ldquo;jason\u0026rdquo;}}}],\u0026ldquo;route\u0026rdquo;:[{\u0026ldquo;destination\u0026rdquo;:{\u0026ldquo;host\u0026rdquo;:\u0026ldquo;reviews\u0026rdquo;,\u0026ldquo;subset\u0026rdquo;:\u0026ldquo;v2\u0026rdquo;}}]},{\u0026ldquo;route\u0026rdquo;:[{\u0026ldquo;destination\u0026rdquo;:{\u0026ldquo;host\u0026rdquo;:\u0026ldquo;reviews\u0026rdquo;,\u0026ldquo;subset\u0026rdquo;:\u0026ldquo;v1\u0026rdquo;}}]}]}} creationTimestamp: \u0026ldquo;2019-03-21T06:01:10Z\u0026rdquo; generation: 1 name: reviews namespace: default resourceVersion: \u0026ldquo;62486214\u0026rdquo; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/virtualservices/reviews uid: b9e41681-4b9e-11e9-a679-00163e045478 spec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v1\n 确认规则已创建之后，在浏览器中打开BookInfo应用程序的URL: http://{EXTERNAL-IP}/productpage。 以\u0026quot;jason\u0026quot;用户登录 ```productpage``` 页面，您可以在每条评价后面看到星级信息。 这里登录用户名为 ```jason``` ，密码随便输入即可 ![image](http://wx4.sinaimg.cn/large/ad5fbf65ly1g1adtjugp3j21gb0iygoa.jpg) ### 流量转移 除了基于内容的路由，Istio还支持基于权重的路由规则。 首先，将所有微服务的缺省版本设置为v1：  bash $ kubectl replace -f samples/bookinfo/networking/virtual-service-all-v1.yaml\n 其次，使用下面的命令把50%的流量从reviews:v1转移到reviews:v3:  bash $ kubectl replace -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml\n 在浏览器中多次刷新productpage页面，大约有50%的几率会看到页面中出现带红星的评价内容。 说明： 注意该方式和使用容器编排平台的部署特性来进行版本迁移是完全不同的。容器编排平台使用了实例scaling来对流量进行管理。而通过Istio，两个版本的reviews服务可以独立地进行扩容和缩容，并不会影响这两个版本服务之间的流量分发。 如果觉得 ```reviews：v3``` 微服务已经稳定，你可以通过以下命令， 将 ```virtual service``` 100％的流量路由到 ```reviews：v3```，从而实现一个灰度发布的功能。  bash $ kubectl replace -f samples/bookinfo/networking/virtual-service-reviews-v3.yaml ```\n在华为云（CCE）上运行 华为云率先将 Istio 作为产品投入到公有云中进行商业应用，开通方式十分简单，只要在华为云CCE上创建集群，然后申请 Istio 公测即可。\n为了方便测试 Bookinfo 应用在华为云上提供了一键体验应用，点击即可省去刚刚那一系列的 kubectl 操作\n一键创建体验应用\n点击灰度发布即可\n创建金丝雀发布\n选择灰度发布的组件\n填写版本号\n选择镜像版本\n版本创建完成后配置灰度策略\n选择相应策略，策略下发即可\n总的来说，华为云的 Istio 确实已经是商业化应用，这里只是展示了部分灰度发布的功能。其他比如流量治理，流量监控等功能还没展示，这些功能做的十分细致，值得尝试。\n参考 在Kubernetes上基于Istio实现Service Mesh智能路由\n基于ISTIO服务网格的灰度发布\n","date":1553132538,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1553132538,"objectID":"895c4b9783d98e5b4153fd4e54b471b3","permalink":"https://guoxudong.io/post/istio-bookinfo-demo/","publishdate":"2019-03-21T09:42:18+08:00","relpermalink":"/post/istio-bookinfo-demo/","section":"post","summary":"正如Linux 的创始人 Linus Torvalds 的那句话：Talk is cheap. Show me the code. 这里我们部署一个demo，由四个单独的微服务构成（注意这里的四个微服务是由不同的语言编写的），用来演示多种 Istio 特性。","tags":["istio","service mesh","阿里云","华为云"],"title":"Istio初探之Bookinfo样例部署","type":"post"},{"authors":["郭旭东"],"categories":["istio"],"content":" 前言  公司于18年10月正式确认服务容器化，到18年12月4日第一个服务正式部署到生产环境kubernetes集群，再到如今已有23个服务完成了生产环境容器化的切换，更多的服务在测试环境容器化部署随时可以切换到生产环境。目前新项目的开发，大部分都直接在测试环境容器化部署，不再需要新购ECS搭建测试环境。随着容器化的深入，服务间的通信和联系变的更加复杂，其中通信的可视化、流量的控制和服务质量的评估问题日益凸显，成为了微服务方案的短板。这个时候Service mesh就进入了我们的视野。\nService mesh是什么 **这里注意：**```istio```只是```Service mesh```服务网格的一种。 ### 服务网格的特点 服务网格有如下几个特点： - 应用程序间通讯的中间层 - 轻量级网络代理 - 应用程序无感知 - 解耦应用程序的重试/超时、监控、追踪和服务发现 目前两款流行的服务网格开源软件 [Linkerd](https://linkerd.io) 和 [Istio](https://Istio.io) 都可以直接在 Kubernetes 中集成，其中 Linkerd 已经成为 CNCF 成员，Istio 在 2018年7月31日宣布 [1.0](https://istio.io/zh/blog/2018/announcing-1.0/)。 ### 服务网格的发展历史 * Spring Cloud Spring Cloud 诞生于2015年，Spring Cloud 最早在功能层面为微服务治理定义了一系列标准特性，比如：智能路由、服务熔断、服务注册于发现等这些名词我最早看到都是在 Sprint Cloud 相关文章中。同时也有一些缺点，比如：需要在代码级别对诸多组件进行控制，并且都依赖于 Java 的实现，这与微服务的多语言协作背道而驰；没有对资源的调度、devops等提供相关支持，需要借助平台来完成；众所周知的Eureka闭源等。 * Linkerd Service mesh 这个命名就是来源于Linkerd。Linkerd 很好地结合了 kubernetes 所提供的功能，于2017年加入CNCF。 * Istio 2017年5月， Google、 IBM 和 Lyft 宣布了Istio的诞生。一经发布，便立即获得Red Hat、F5等大厂响应，社区活跃度高涨，很快超越了 Linkerd，成为了 Service mesh 的代表产品。 * 国内服务网格 这里不得不提的是国内服务网格的兴起，在 Service mesh 概念具体定义以前，国内的许多厂商就已经开始了微服务进程，同时在做自己的微服务治理产品。而在 Service mesh 概念普及之后，厂商意识到了自己产品也具有 Service mesh 的特点，将自己的服务治理平台进行了改造和完善，推出了自己的 Service mesh 产品。例如，微博、腾讯和华为都有自己的服务网格产品，华为更是已经将产品投入到公有云中进行商业应用。蚂蚁金服的 SOFAMesh 则是针对大流量的生产场景，在 Istio 的架构基础上进行修改并推广。 ## Istio又是什么  Istio``` 提供了一个完整的解决方案，通过为整个服务网格提供行为洞察和操作控制来满足微服务应用程序的多样化需求。Istio 允许您连接、保护、控制和观测服务。在较高的层次上，Istio 有助于降低这些部署的复杂性，并减轻开发团队的压力。它是一个完全开源的服务网格，可以透明地分层到现有的分布式应用程序上。它也是一个平台，包括允许它集成到任何日志记录平台、遥测或策略系统的 API。Istio 的多样化功能集使您能够成功高效地运行分布式微服务架构，并提供保护、连接和监控微服务的统一方法。\nIstio的架构 Istio总的来说由两部分组成：控制平面和数据平面\n  数据平面由一组以 sidecar 方式部署的智能代理（Envoy）组成。sidecar通过注入的方式和业务容器共存于一个 Pod 中，会劫业务容器的流量，接受控制面组件的控制，可以调节和控制微服务及 Mixer 之间所有的网络通信。 控制平面是 Istio 的核心，负责管理和配置代理来路由流量。此外控制平面配置 Mixer 以实施策略和收集遥测数据。  下图显示了构成每个面板的不同组件： 图片为原创，转载请标记出处https://blog.maoxianplay.com/\nEnvoy Istio 使用 Envoy 代理的扩展版本，Envoy 是以 C++ 开发的高性能代理，用于调解服务网格中所有服务的所有入站和出站流量。Envoy 的许多内置功能被 Istio 发扬光大，例如：\n 动态服务发现 负载均衡 TLS 终止 HTTP/2 \u0026amp; gRPC 代理 熔断器 健康检查、基于百分比流量拆分的灰度发布 故障注入 丰富的度量指标  Envoy 被部署为 sidecar，和对应服务在同一个 Kubernetes pod 中。这允许 Istio 将大量关于流量行为的信号作为属性提取出来，而这些属性又可以在 Mixer 中用于执行策略决策，并发送给监控系统，以提供整个网格行为的信息。\nSidecar 代理模型还可以将 Istio 的功能添加到现有部署中，而无需重新构建或重写代码。可以阅读更多来了解为什么我们在设计目标中选择这种方式。\nMixer Mixer 是一个独立于平台的组件，负责在服务网格上执行访问控制和使用策略，并从 Envoy 代理和其他服务收集遥测数据。代理提取请求级属性，发送到 Mixer 进行评估。\nMixer 中包括一个灵活的插件模型，使其能够接入到各种主机环境和基础设施后端，从这些细节中抽象出 Envoy 代理和 Istio 管理的服务。\nPilot Pilot 为 Envoy sidecar 提供服务发现功能，为智能路由（例如 A/B 测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能。它将控制流量行为的高级路由规则转换为特定于 Envoy 的配置，并在运行时将它们传播到 sidecar。\nPilot 将平台特定的服务发现机制抽象化并将其合成为符合 Envoy 数据平面 API 的任何 sidecar 都可以使用的标准格式。这种松散耦合使得 Istio 能够在多种环境下运行（例如，Kubernetes、Consul、Nomad），同时保持用于流量管理的相同操作界面。\nCitadel Citadel 通过内置身份和凭证管理赋能强大的服务间和最终用户身份验证。可用于升级服务网格中未加密的流量，并为运维人员提供基于服务标识而不是网络控制的强制执行策略的能力。从 0.5 版本开始，Istio 支持基于角色的访问控制，以控制谁可以访问您的服务，而不是基于不稳定的三层或四层网络标识。\nGalley(1.1版本新增) Galley 代表其他的 Istio 控制平面组件，用来验证用户编写的 Istio API 配置。随着时间的推移，Galley 将接管 Istio 获取配置、 处理和分配组件的顶级责任。它将负责将其他的 Istio 组件与从底层平台（例如 Kubernetes）获取用户配置的细节中隔离开来。\n结语 在云原生架构下，容器的使用给予了异构应用程序的更多可行性，Kubernetes 增强了应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，Istio可以使开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情而专注于程序开发，赋予开发者更多的创造性。\n参考  Istio 官方文档 - istio.io 《深入浅出istio》  ","date":1553044348,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1553044348,"objectID":"97d5b460929f3318418cbd4a948da365","permalink":"https://guoxudong.io/post/istio-share/","publishdate":"2019-03-20T09:12:28+08:00","relpermalink":"/post/istio-share/","section":"post","summary":"在云原生架构下，容器的使用给予了异构应用程序的更多可行性，Kubernetes 增强了应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，Istio可以使开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情而专注于程序开发，赋予开发者更多的创造性。","tags":["istio","service mesh"],"title":"浅析ServiceMesh \u0026 Istio","type":"post"},{"authors":["郭旭东"],"categories":["服务网格"],"content":" 本文转自《Istio 服务网格进阶实战》\n什么是服务网格？ Service mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。\n服务网格是用于处理服务间通信的专用基础设施层。它负责通过包含现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，服务网格通常通过一组轻量级网络代理来实现，这些代理与应用程序代码一起部署，而不需要感知应用程序本身。—— Willian Morgan Buoyant CEO\n服务网格（Service Mesh）这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。\n服务网格的特点 服务网格有如下几个特点：\n 应用程序间通讯的中间层 轻量级网络代理 应用程序无感知 解耦应用程序的重试/超时、监控、追踪和服务发现  目前两款流行的服务网格开源软件 Linkerd 和 Istio 都可以直接在 Kubernetes 中集成，其中 Linkerd 已经成为 CNCF 成员，Istio 在 2018年7月31日宣布 1.0。\n理解服务网格 如果用一句话来解释什么是服务网格，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用服务网格也就无须关心服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给服务网格就可以了。\nPhil Calçado 在他的这篇博客 Pattern: Service Mesh 中详细解释了服务网格的来龙去脉：\n 从最原始的主机之间直接使用网线相连 网络层的出现 集成到应用程序内部的控制流 分解到应用程序外部的控制流 应用程序的中集成服务发现和断路器 出现了专门用于服务发现和断路器的软件包/库，如 Twitter 的 Finagle 和 Facebook 的 Proxygen，这时候还是集成在应用程序内部 出现了专门用于服务发现和断路器的开源软件，如 Netflix OSS、Airbnb 的 synapse 和 nerve 最后作为微服务的中间层服务网格出现  服务网格的架构如下图所示：\n图片来自：Pattern: Service Mesh\n服务网格作为 sidecar 运行，对应用程序来说是透明，所有应用程序间的流量都会通过它，所以对应用程序流量的控制都可以在 serivce mesh 中实现。\n服务网格如何工作？ 下面以 Istio 为例讲解服务网格如何在 Kubernetes 中工作。\n Istio 将服务请求路由到目的地址，根据其中的参数判断是到生产环境、测试环境还是 staging 环境中的服务（服务可能同时部署在这三个环境中），是路由到本地环境还是公有云环境？所有的这些路由信息可以动态配置，可以是全局配置也可以为某些服务单独配置。 当 Istio 确认了目的地址后，将流量发送到相应服务发现端点，在 Kubernetes 中是 service，然后 service 会将服务转发给后端的实例。 Istio 根据它观测到最近请求的延迟时间，选择出所有应用程序的实例中响应最快的实例。 Istio 将请求发送给该实例，同时记录响应类型和延迟数据。 如果该实例挂了、不响应了或者进程不工作了，Istio 将把请求发送到其他实例上重试。 如果该实例持续返回 error，Istio 会将该实例从负载均衡池中移除，稍后再周期性得重试。 如果请求的截止时间已过，Istio 主动失败该请求，而不是再次尝试添加负载。 Istio 以 metric 和分布式追踪的形式捕获上述行为的各个方面，这些追踪信息将发送到集中 metric 系统。  为何使用服务网格？ 服务网格并没有给我们带来新功能，它是用于解决其他工具已经解决过的问题，只不过这次是在云原生的 Kubernetes 环境下的实现。\n在传统的 MVC 三层 Web 应用程序架构下，服务之间的通讯并不复杂，在应用程序内部自己管理即可，但是在现今的复杂的大型网站情况下，单体应用被分解为众多的微服务，服务之间的依赖和通讯十分复杂，出现了 twitter 开发的 Finagle、Netflix 开发的 Hystrix 和 Google 的 Stubby 这样的 “胖客户端” 库，这些就是早期的服务网格，但是它们都仅适用于特定的环境和特定的开发语言，并不能作为平台级的服务网格支持。\n在云原生架构下，容器的使用给予了异构应用程序的更多可行性，Kubernetes 增强的应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，同时开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情而专注于程序开发，赋予开发者更多的创造性。\n参考  WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? - buoyant.io Istio: A service mesh for AWS ECS - medium.com 初次了解 Istio - istio.io Application Network Functions With ESBs, API Management, and Now.. Service Mesh? - blog.christianposta.com Pattern: Service Mesh - philcalcado.com Envoy 官方文档中文版 - servicemesher.com Istio 官方文档 - istio.io servicemesher/awesome-servicemesh - github.com  ","date":1552983176,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1552983176,"objectID":"a084ed7181e88533bb63bd4fa8b7a6d3","permalink":"https://guoxudong.io/post/istio-servicemesh/","publishdate":"2019-03-19T16:12:56+08:00","relpermalink":"/post/istio-servicemesh/","section":"post","summary":"现在Service Mesh很热，但是到底什么事Service Mesh，Service Mesh能为我们做什么，我们为什么要用Service Mesh。","tags":["Service Mesh"],"title":"Service Mesh是什么","type":"post"},{"authors":["郭旭东"],"categories":["kubernetes"],"content":" 前言  随着微服务和容器化的深入人心，以及kubernetes已经成为容器编排领域的事实标准，越来越多的公司将自己的服务迁移到kubernetes集群中。而随着kubernetes集群的增加，集群管理的问题就凸显出来，不同的环境存在不同的集群，不同的业务线不同的集群，甚至有些开发人员都有自己的集群。诚然，如果集群是使用公有云如阿里云或华为云的容器服务，可以登录其控制台进行集群管理；或者使用rancher这用的多集群管理工具进行统一的管理。但是在想操作istio特有的容器资源，或者想使用istioctl的时候，或者像我一样就是想使用kubectl命令的同学，这个时候多集群的切换就显的十分重要了。\n简介 ## 原理 使用```kubeconfig```文件，您可以组织您的群集，用户和名称空间。 还可以定义上下文以快速轻松地在群集和名称空间之间切换。 ### 上下文(Context)  kubeconfig```文件中的上下文元素用于以方便的名称对访问参数进行分组。 每个上下文有三个参数：集群，命名空间和用户。 默认情况下，kubectl命令行工具使用当前上下文中的参数与集群进行通信。可以使用下面的命令设置上下文：\n kubectl config use-context  配置内容 kubectl config view   如果设置了--kubeconfig标志，则只使用指定的文件。该标志只允许有一个实例。 如果环境变量KUBECONFIG存在，那么就使用该环境变量KUBECONFIG里面的值，如果不存在该环境变量KUBECONFIG，那么默认就是使用$HOME/.kube/config文件。  kubeconfig内容 从下面kubeconfig文件的配置来看集群、用户、上下文、当前上下文的关系就比较明显了。\napiVersion: v1 kind: Config preferences: {} clusters: - cluster: name: {cluster-name} users: - name: {user-name} contexts: - context: cluster: {cluster-name} user: {user-name} name: {context-name} current-context: {context-name}  为何要自动合并 在日常的工作中，如果我们需要操作多个集群，会得到多个kubeconfig配置文件。一般的kubeconfig文件都是yaml格式的，但是也有少部分的集群kubeconfig时已json文件的形式给出的（比如华为云的=。=），比如我们公司再阿里云、华为云和自建环境上均存在kubernetes集群，平时操作要在多集群之间切换，这也就催生了我写这个工具（其实就是一个脚本）的动机。\n自动合并生成kubeconfig 众所周知，yaml是一种直观的能够被电脑识别的数据序列化格式，是一个可读性高并且容易被人类阅读的语言和json相比（没有格式化之前）可读性更强。而我这个工具并不是很关心kubeconfig的格式，只要将想要合并的kubeconfig放入指定文件即可。\nGitHub：https://github.com/sunny0826/mergeKubeConfig\n适用环境  需要在终端使用命令行管理多集群 kubernetes集群中安装了istio，需要使用istioctl命令，但是集群节点并没有安装istioctl，需要在本地终端操作 不愿频繁编辑.kube目录中的config文件的同学  准备工作  Python环境：2.7或者3均可 需要依赖包：PyYAML  开始使用  安装依赖：\npip install PyYAML  运行脚本\n 默认运行方式，kubeconfig文件放入configfile文件,注意删掉作为示例的两个文件\npython merge.py  自定义kubeconfig文件目录\npython merge.py -d {custom-dir}    运行后操作  将生成的config文件放入.kube目录中\ncp config ~/.kube  查看所有的可使用的kubernetes集群角色\nkubectl config get-contexts  更多关于kubernetes配置文件操作\nkubectl config --help  切换kubernetes配置\nkubectl config use-context {your-contexts}   结语 在使用kubernetes初期，在多集群之间我一直是频繁的切换.kube/config文件来达到切换操作集群的目的。这也导致了我的.kube目录中存在这多个类似于al_test_config.bak、al_prod_config.bak、hw_test_config.bak的文件，本地环境已经自建环境，在集群切换的时候十分头疼。而后来使用--kubeconfig来进行切换集群，虽然比之前的方法要方便很多，但是并不十分优雅。这个简单的小工具一举解决了我的文件，对于我这个kubectl重度依赖者来说十分重要。\n","date":1552790702,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1552790702,"objectID":"b56bee2ceb79fd69e0f7dd288430f84d","permalink":"https://guoxudong.io/post/merge-kubeconfig/","publishdate":"2019-03-17T10:45:02+08:00","relpermalink":"/post/merge-kubeconfig/","section":"post","summary":"随着kubernetes集群的增加，集群管理的问题就凸显出来，不同的环境存在不同的集群，不同的业务线不同的集群，甚至有些开发人员都有自己的集群。这里介绍一款工具来自动合并Kubeconfig，实现多k8s集群切换。","tags":["kubernetes","工具"],"title":"自动合并Kubeconfig，实现多k8s集群切换","type":"post"},{"authors":["郭旭东"],"categories":["系统"],"content":" 简介  Small. Simple. Secure.Alpine Linux is a security-oriented, lightweight Linux distribution based on musl libc and busybox.\nAlpine Linux 是一个社区开发的面向安全应用的轻量级Linux发行版。 Alpine 的意思是“高山的”，它采用了musl libc和busybox以减小系统的体积和运行时资源消耗，同时还提供了自己的包管理工具apk。\n 适用环境 由于其小巧、安全、简单以及功能完备的特点，被广泛应用于众多Docker容器中。我司目前使用的基础镜像均是基于该系统，dockerhub上有提供各种语言的基础镜像.如：node:8-alpine、python:3.6-alpine，同时也可以基于alpine镜像制作符合自己需求的基础镜像。\n简单的镜像构建示例 这里提供一个python3的基础镜像的Dockerfile，get-pip.py可在 https://pip.pypa.io/en/latest/installing/ 下载。\nFROM alpine MAINTAINER guoxudong@keking.cn # 拷贝安装pip的脚本 COPY get-pip.py /get-pip.py # 设置alpine的镜像地址为阿里云的地址 RUN echo \u0026quot;https://mirrors.aliyun.com/alpine/v3.6/main/\u0026quot; \u0026gt; /etc/apk/repositories \\ # 安装依赖包 \u0026amp;\u0026amp; apk update \\ \u0026amp;\u0026amp; apk add --no-cache bash \\ # libevent-dev libxml2-dev libffi libxml2 libxslt libxslt-dev \\ python3 gcc g++ python3-dev python-dev linux-headers libffi-dev openssl-dev \\ # 由于通过apk安装的pip总是基于python2.7的版本，不符合项目要求，此处使用get-pip.py的方式 #安装基于python3.6的pip \u0026amp;\u0026amp; python3 /get-pip.py \\ # 删除不必要的脚本 \u0026amp;\u0026amp; cd .. \\ \u0026amp;\u0026amp; rm -f /get-pip.py \\ # 此环境专用做运行django项目，因此移除不必要的工具，减少空间 # \u0026amp;\u0026amp; pip uninstall -y pip setuptools wheel \\ # 最后清空apk安装时产生的无用文件 \u0026amp;\u0026amp; rm -rf /var/cache/apk/*  对比：同样版本的python，对比镜像大小，可见使用alpine的优势\n~ docker images | grep python python 3.4 ccbffa0d70d9 2 months ago 922MB alpine-python3 latest 69e41b673a50 2 months ago 297MB  apk包管理  镜像源配置\n官方镜像源列表：http://dl-cdn.alpinelinux.org/alpine/MIRRORS.txt\n MIRRORS.txt中是当前Alpine官方提供的镜像源（Alpine安装的时候系统自动选择最佳镜像源）\n 国内镜像源\n 清华TUNA镜像源：https://mirror.tuna.tsinghua.edu.cn/alpine/ 中科大镜像源：http://mirrors.ustc.edu.cn/alpine/ 阿里云镜像源：http://mirrors.aliyun.com/alpine/  镜像源配置\n这里推荐使用阿里云镜像源，由于公司应用都是部署在阿里云上，使用阿里云镜像源会快很多\n$ vi /etc/apk/repositories # 将这两行插入到repositories文件开头 http://mirrors.aliyun.com/alpine/v3.9/main http://mirrors.aliyun.com/alpine/v3.9/community # 后面是原有的默认配置 http://dl-cdn.alpinelinux.org/alpine/v3.8/main http://dl-cdn.alpinelinux.org/alpine/v3.8/community  apk包管理命令\n这里介绍一些常用的操作apk包管理命令\n  bash\n  bash-4.3# apk \u0026ndash;help apk-tools 2.10.0, compiled for x86_64.\nInstalling and removing packages: add Add PACKAGEs to \u0026lsquo;world\u0026rsquo; and install (or upgrade) them, while ensuring that all dependencies are met del Remove PACKAGEs from \u0026lsquo;world\u0026rsquo; and uninstall them\nSystem maintenance: fix Repair package or upgrade it without modifying main dependencies update Update repository indexes from all remote repositories upgrade Upgrade currently installed packages to match repositories cache Download missing PACKAGEs to cache and/or delete unneeded files from cache\nQuerying information about packages: info Give detailed information about PACKAGEs or repositories list List packages by PATTERN and other criteria dot Generate graphviz graphs policy Show repository policy for packages\nRepository maintenance: index Create repository index file from FILEs fetch Download PACKAGEs from global repositories to a local directory verify Verify package integrity and signature manifest Show checksums of package contents\nUse apk  \u0026ndash;help for command-specific help. Use apk \u0026ndash;help \u0026ndash;verbose for a full command listing.\nThis apk has coffee making abilities.\n - ```apk info``` 列出所有已安装的软件包 - ```apk apk update``` 更新最新本地镜像源 - ```apk upgrade``` 升级软件 - ```apk search``` 搜索可用软件包，**搜索之前最好先更新镜像源**  bash $ apk search #查找所以可用软件包 $ apk search -v #查找所以可用软件包及其描述内容 $ apk search -v \u0026lsquo;acf*\u0026rsquo; #通过软件包名称查找软件包 $ apk search -v -d \u0026lsquo;docker\u0026rsquo; #通过描述文件查找特定的软件包\n - ```apk add``` 从仓库中安装最新软件包，并自动安装必须的依赖包,也可以从第三方仓库添加软件包  bash $ apk add curl busybox-extras #软件以空格分开这里，这里列举我们用的最多的curl和telnet bash-4.3# apk add \u0026ndash;no-cache curl bash-4.3# apk add mongodb \u0026ndash;update-cache \u0026ndash;repository http://mirrors.ustc.edu.cn/alpine/v3.6/main/ \u0026ndash;allow-untrusted #从指定镜像源拉取\n 安装指定版本软件包  bash bash-4.3# apk add mongodb=4.0.5-r0 bash-4.3# apk add \u0026lsquo;mongodb4.0.5\u0026rsquo;\n 升级指定软件包  bash bash-4.3# apk add \u0026ndash;upgrade busybox #升级指定软件包 ```\n注：安装之前最好修改本地镜像源，更新镜像源，搜索软件包是否存在，选择合适岸本在进行安装。\n apk del 卸载并删除指定软件包   结语 Alpine以其小巧、简单在docker容器中得到了广泛的应用。但是Alpine Linux使用了musl，可能和其他Linux发行版使用的glibc实现会有些不同。这里主要介绍了它的基础用法，但是足以满足日常运维需要。毕竟在kubernetes集群中操作容器内环境较直接在虚拟机或者物理机上操作更为复杂，由于缩减的容器的大小，导致和CentOS或Ubuntu相比缺少许多功能。而缺少的这些功能又不想在基础镜像中安装导致容器变大，这个时候就可以在容器运行后，根据实际需要安装即可。\n参考文档 https://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management\n","date":1552614782,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1552614782,"objectID":"ebd9e58b0de01d4ad9db3158fb114c74","permalink":"https://guoxudong.io/post/alpine-linux/","publishdate":"2019-03-15T09:53:02+08:00","relpermalink":"/post/alpine-linux/","section":"post","summary":"Alpine以其小巧、简单在docker容器中得到了广泛的应用。但是Alpine Linux使用了musl，可能和其他Linux发行版使用的glibc实现会有些不同。这里主要介绍了它的基础用法，但是足以满足日常运维需要。","tags":["Linux","Docker","容器"],"title":"Alpine Linux详解","type":"post"},{"authors":["郭旭东"],"categories":["istio"],"content":" istio应用部署样例 该实例为一套istio服务上线流程：注入-\u0026gt;部署-\u0026gt;创建目标规则-\u0026gt;创建默认路由。就大多数istio服务网格应用均可基于这一流程上线。\n部署istio istio有多种部署方式，阿里云、华为云等云服务商均提供一键安装，同时也可以通过GitHub下载release包，使用install/kubernetes/istio-demo.yaml部署，或者使用helm部署。这里采用阿里云容器服务一键部署istio。\n部署两个版本的服务 这里选择一个简单的Python项目作为服务端，这里使用崔秀龙老哥的flaskapp服务，该服务的作用就是提供2个url路径：\n 一个是/env，用户获取容器中的环境变量，例如 http://flaskapp/env/version 另一个是/fetch ，用于获取在参数url中指定的网址的内容，例如 http://flaskapp/fetch?url=http://weibo.com  创建2个Deployment，分别命名为 flaskapp-v1 和 flaskapp-v2 ，同时创建一个 Service ,将其命名为flaskapp。代码文件为 flaskapp.istio.yaml。\nimage: caption: \u0026quot;Image from: [**Pexels**](https://www.pexels.com)\u0026quot; focal_point: \u0026quot;\u0026quot; preview_only: false --- apiVersion: v1 kind: Service metadata: name: flaskapp labels: app: flaskapp spec: selector: app: flaskapp ports: - name: http port: 80 image: caption: \u0026quot;Image from: [**Pexels**](https://www.pexels.com)\u0026quot; focal_point: \u0026quot;\u0026quot; preview_only: false --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: flaskapp-v1 spec: replicas: 1 template: metadata: labels: app: flaskapp version: v1 spec: containers: - name: flaskapp image: dustise/flaskapp imagePullPolicy: IfNotPresent env: - name: version value: v1 image: caption: \u0026quot;Image from: [**Pexels**](https://www.pexels.com)\u0026quot; focal_point: \u0026quot;\u0026quot; preview_only: false --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: flaskapp-v2 spec: replicas: 1 template: metadata: labels: app: flaskapp version: v2 spec: containers: - name: flaskapp image: dustise/flaskapp imagePullPolicy: IfNotPresent env: - name: version value: v2  注意\n 两个版本Deployment的镜像一致，但是使用了不同的version标签区分，分别为 v1 和 v2 。实际环境中的镜像是不同的 在两个Deployment中都有一个名为version的环境变量，分别为 v1 和 v2 。这里设置是为了方便后续区分服务。 两个Deployment中都使用了 app 和 version 标签，在 istio 网格应用中通常会使用这两个标签作为应用和版本的标识。 Service 中的 Selector 仅使用了一个 app 标签，这意味着该 Service 对两个 Deployment 都是有效的。 将在 Service 中定义的端口根据 istio 规范命名为http。  istio注入并部署服务端\n$ istioctl kube-inject -f flask.istio.yaml | kubectl apply -f - service/flaskapp created deployment.extensions/flaskapp-v1 created deployment.extensions/flaskapp-v2 created  在rancher查看注入情况\n这里也可以使用kubectl describe po flaskapp-v1-7d4f9b8459-2ncnf命令查看Pod容器，这里可以看到Pod中多了一个容器，名为istio-proxy，这就表示注入成功了。而前面istio-init的初始化容器，这个容器是用于初始化劫持的。\n部署客户端 这里的客户端是一个安装了测试工具的镜像，测试的内容可以在容器内通过shell完成。代码文件为 sleep.istio.yaml。\nimage: caption: \u0026quot;Image from: [**Pexels**](https://www.pexels.com)\u0026quot; focal_point: \u0026quot;\u0026quot; preview_only: false --- apiVersion: v1 kind: Service metadata: name: sleep labels: app: sleep version: v1 spec: selector: app: sleep version: v1 ports: - name: ssh port: 80 image: caption: \u0026quot;Image from: [**Pexels**](https://www.pexels.com)\u0026quot; focal_point: \u0026quot;\u0026quot; preview_only: false --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: sleep spec: replicas: 1 template: metadata: labels: app: sleep version: v1 spec: containers: - name: sleep image: dustise/sleep imagePullPolicy: IfNotPresent  istio注入并部署客户端\n$ istioctl kube-inject -f sleep.istio.yaml | kubectl apply -f - service/sleep created deployment.extensions/sleep created  sleep应用的Pod进入Running状态就可以进行验证了\n验证服务 直接在sleep容器中执行命令行\n$ for i in `seq 10`;do http --body http://flaskapp/env/version;done v1 v2 ... v1  该命令使用一个for循环，重复访问 http://flaskapp/env/version ，查看内容，结果为 v1 和 v2 随机出现，各占一半。出现 v1 和 v2 版本轮流调用的效果，达到了基本的负载均衡的功能。\n创建目标规则 目标规则代码 flaskapp-destinationrule.yaml\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: flaskapp spec: host: flaskapp subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2  部署目标规则（这里使用kubectl和istioctl均可）\n$ kubectl apply -f flaskapp-destinationrule.yaml Created config destination-rule/default/flaskapp at revision 59183403  创建默认路由 默认路由代码 flaskapp-default-vs-v2.yaml\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: flaskapp-default-v2 spec: hosts: - flaskapp http: - route: - destination: host: flaskapp subset: v2  部署默认路由\n$ kubectl apply -f flaskapp-default-vs-v2.yaml Created config virtual-service/default/flaskapp-default-v2 at revision 59185583  验证路由规则是否生效 再次在sleep容器中执行命令，查看新定义的流量管理规则是否生效\n$ for i in `seq 10`;do http --body http://flaskapp/env/version;done v2 v2 v2 v2 v2 v2 v2 v2 v2 v2  这里就可以看到，设置的默认路由已经生效了，多次重复访问，返回的内容都是来自环境变量 version 设置为 v2 的版本，也就是v2版本。\nkiali查看调用情况 可以看到流量都进入了v2版本中\n小结 这里实现了一个极简的istio应用，可以帮助新手快速入门，官网提供的Bookinfo应用较为复杂。这里提供的小例子更为简洁易懂，非常利于入门。\n参考  《深入浅出Istio》 \u0026mdash; 崔秀龙  ","date":1552463143,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1552463143,"objectID":"12e4e0fecb1ce1147c771738c6b397f2","permalink":"https://guoxudong.io/post/istio-demo/","publishdate":"2019-03-13T15:45:43+08:00","relpermalink":"/post/istio-demo/","section":"post","summary":"极简的istio样例部署，可以帮助新手快速入门，相较官方提供的Bookinfo应用更容易上手。","tags":["istio","service mesh","阿里云"],"title":"阿里云环境Istio初探","type":"post"},{"authors":["郭旭东"],"categories":["devops"],"content":" 简介 image: caption: \u0026ldquo;Image from: Pexels\u0026ldquo; focal_point: \u0026ldquo;\u0026rdquo;\npreview_only: false jenkins pipeline Jenkins Pipeline (或简称为 \u0026ldquo;Pipeline\u0026rdquo; )是一套插件，将持续交付的实现和实施集成到 Jenkins 中。\n持续交付Pipeline自动化的表达了这样一种流程：将基于版本控制管理的软件持续的交付到您的用户和消费者手中。\nJenkins Pipeline 提供了一套可扩展的工具，用于将“简单到复杂”的交付流程实现为“持续交付即代码”。 Jenkins Pipeline 的定义通常被写入到一个文本文件（称为 Jenkinsfile ）中，该文件可以被检入到项目的源代码控制库中。\n摘自Jenkins官方文档\nSonarQube SonarQube is an open source platform to perform automatic reviews with static analysis of code to detect bugs, code smells and security vulnerabilities on 25+ programming languages including Java, C#, JavaScript, TypeScript, C/C++, COBOL and more.\nSonarQube是一个开源的平台，以执行与代码的静态分析，自动审查，可以检测在25+的编程语言如Java，C＃，JavaScript，TypeScript，C/C++，COBOL等的代码缺陷和安全漏洞。\nOWASP OWASP，全称是：Open Web Application Security Project，翻译为中文就是：开放式Web应用程序安全项目，是一个非营利组织，不附属于任何企业或财团，这也是该组织可以不受商业控制地进行安全开发及安全普及的重要原因，详细介绍。OWASP Dependency-Check，它识别项目依赖关系，并检查是否存在任何已知的、公开的、漏洞，基于OWASP Top 10 2013。\n场景 在devops理念中，CI/CD毫无疑问是最重要的一环，而代码质量检查则是CI中必不可少的一步。在敏捷开发的思想下，代码的迭代周期变短，交付速度提升，这个时候代码的质量就很难保证，测试只能保证功能完整与可用，而代码的质量纯靠review的话效率又很低，这个时候sonar就可以很好的帮助开发自动化检测代码质量，降低bug数量，也可以根据扫描结果养成良好的编程习惯，同时也可以减少测试的工作量，真正提升整个团队效率，实现devops理念。\n前提 jenkins、sonarqube服务已经搭建完成，jenkins安装sonar插件SonarQube Scanner for Jenkins，jenkins、sonarqube安装Dependency-Check插件OWASP Dependency-Check Plugin\n版本：jenkins2.166，sonarqube6.7.6\n配置  下载安装jenkins插件\n[系统管理]-[插件管理]-[可选插件]-[SonarQube Scanner for Jenkins]\n SonarQube生成token，这个token不会显示第二次，所以一定要记住\n SonarQube配置Dependency-Check\n[配置]-[Dependency-Check]\n[注意：]这里去掉 ${WORKSPACE}/，否则将报\n[INFO] Dependency-Check XML report does not exists. Please check property sonar.dependencyCheck.reportPath:/data/jenkinsHome/workspace/xxx/${WORKSPACE}/dependency-check-report.xml   在pom.xml文件中添加\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.sonarsource.scanner.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;sonar-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.6.0.1398\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt;  配置jenkins\n[系统管理]-[系统设置]-[SonarQube servers]\n sonar添加webhook\n在代码扫描成功后，扫描结果需要回调jenkins，添加的Jenkins的webhook结构为：http://[jenkins_url]/sonarqube-webhook/\n[配置]-[web回调接口]-[URL]\n 编辑jenkins pipeline\n在jenkinsfile文件中添加配置\nstage('依赖安全检查') { steps{ dependencyCheckAnalyzer datadir: '', hintsFile: '', includeCsvReports: false, includeHtmlReports: true, includeJsonReports: false, includeVulnReports: true, isAutoupdateDisabled: false, outdir: '', scanpath: '', skipOnScmChange: false, skipOnUpstreamChange: false, suppressionFile: '', zipExtensions: '' } } stage('静态代码检查') { steps { echo \u0026quot;starting codeAnalyze with SonarQube......\u0026quot; withSonarQubeEnv('sonar') { //注意这里withSonarQubeEnv()中的参数要与之前SonarQube servers中Name的配置相同 withMaven(maven: 'M3') { sh \u0026quot;mvn clean package -Dmaven.test.skip=true sonar:sonar -Dsonar.projectKey={项目key} -Dsonar.projectName={项目名称} -Dsonar.projectVersion={项目版本} -Dsonar.sourceEncoding=UTF-8 -Dsonar.exclusions=src/test/** -Dsonar.sources=src/ -Dsonar.java.binaries=target/classes -Dsonar.host.url={SonarQube地址} -Dsonar.login={SonarQube的token}\u0026quot; } } script { timeout(1) { //这里设置超时时间1分钟，不会出现一直卡在检查状态 //利用sonar webhook功能通知pipeline代码检测结果，未通过质量阈，pipeline将会fail def qg = waitForQualityGate('sonar') //注意：这里waitForQualityGate()中的参数也要与之前SonarQube servers中Name的配置相同 if (qg.status != 'OK') { error \u0026quot;未通过Sonarqube的代码质量阈检查，请及时修改！failure: ${qg.status}\u0026quot; } } } } }  参数解释：\n sonar.projectKey：项目key (必填项) sonar.projectName：项目名称（必填项） sonar.projectVersion：项目版本（必填项） sonar.sources：源码位置(相对路径） sonar.java.binaries：编译后的class位置（必填项，相对路径同上） sonar.exclusions：排除的扫描的文件路径 sonar.host.url：SonarQube地址 sonar.login：SonarQube生成的token   运行 执行jenkins构建，构建成功后会显示如下，则证明sonar代码扫描成功且通过代码质量阈检查 查看sonar报告，这里有两种方式\n 可直接登录SonarQube查看报告  也可直接在jenkins页面点击SonarQube图标进入，点击以下标记均可进去   其他 问题一：无法扫描代码，错误提示 hudson.remoting.ProxyException: hudson.AbortException: SonarQube installation defined in this job (sonar) does not match any configured installation. Number of installations that can be configured: 1. If you want to reassign jobs to a different SonarQube installation, check the documentation under https://redirect.sonarsource.com/plugins/jenkins.html at hudson.plugins.sonar.SonarInstallation.checkValid(SonarInstallation.java:94) at hudson.plugins.sonar.SonarBuildWrapper.setUp(SonarBuildWrapper.java:67) at org.jenkinsci.plugins.workflow.steps.CoreWrapperStep$Execution.start(CoreWrapperStep.java:80) at org.jenkinsci.plugins.workflow.cps.DSL.invokeStep(DSL.java:268) Caused: hudson.remoting.ProxyException: org.codehaus.groovy.runtime.InvokerInvocationException: hudson.AbortException: SonarQube installation defined in this job (sonar) does not match any configured installation. Number of installations that can be configured: 1. If you want to reassign jobs to a different SonarQube installation, check the documentation under https://redirect.sonarsource.com/plugins/jenkins.html at org.jenkinsci.plugins.workflow.cps.CpsStepContext.replay(CpsStepContext.java:499) at org.jenkinsci.plugins.workflow.cps.DSL.invokeStep(DSL.java:295) at org.jenkinsci.plugins.workflow.cps.DSL.invokeStep(DSL.java:207) at org.jenkinsci.plugins.workflow.cps.DSL.invokeDescribable(DSL.java:395) at org.jenkinsci.plugins.workflow.cps.DSL.invokeMethod(DSL.java:179) at org.jenkinsci.plugins.workflow.cps.CpsScript.invokeMethod(CpsScript.java:122) at sun.reflect.GeneratedMethodAccessor1200.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93) at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325) at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1213) at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1022) at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:42) at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48) at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113) at org.kohsuke.groovy.sandbox.impl.Checker$1.call(Checker.java:157) at org.kohsuke.groovy.sandbox.GroovyInterceptor.onMethodCall(GroovyInterceptor.java:23) at org.jenkinsci.plugins.scriptsecurity.sandbox.groovy.SandboxInterceptor.onMethodCall(SandboxInterceptor.java:155) at org.kohsuke.groovy.sandbox.impl.Checker$1.call(Checker.java:155) at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:159) at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:129) at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:129) at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:129) at com.cloudbees.groovy.cps.sandbox.SandboxInvoker.methodCall(SandboxInvoker.java:17) Caused: hudson.remoting.ProxyException: java.lang.IllegalArgumentException: Failed to prepare withSonarQubeEnv step at org.jenkinsci.plugins.workflow.cps.DSL.invokeDescribable(DSL.java:397) at org.jenkinsci.plugins.workflow.cps.DSL.invokeMethod(DSL.java:179) at org.jenkinsci.plugins.workflow.cps.CpsScript.invokeMethod(CpsScript.java:122) at sun.reflect.GeneratedMethodAccessor1200.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93) at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325) at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1213) at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1022) at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:42) at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48) at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113) at org.kohsuke.groovy.sandbox.impl.Checker$1.call(Checker.java:157) at org.kohsuke.groovy.sandbox.GroovyInterceptor.onMethodCall(GroovyInterceptor.java:23) at org.jenkinsci.plugins.scriptsecurity.sandbox.groovy.SandboxInterceptor.onMethodCall(SandboxInterceptor.java:155) at org.kohsuke.groovy.sandbox.impl.Checker$1.call(Checker.java:155) at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:159) at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:129) at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:129) at org.kohsuke.groovy.sandbox.impl.Checker.checkedCall(Checker.java:129) at com.cloudbees.groovy.cps.sandbox.SandboxInvoker.methodCall(SandboxInvoker.java:17) at WorkflowScript.run(WorkflowScript:27) at ___cps.transform___(Native Method) at com.cloudbees.groovy.cps.impl.ContinuationGroup.methodCall(ContinuationGroup.java:57) at com.cloudbees.groovy.cps.impl.FunctionCallBlock$ContinuationImpl.dispatchOrArg(FunctionCallBlock.java:109) at com.cloudbees.groovy.cps.impl.FunctionCallBlock$ContinuationImpl.fixArg(FunctionCallBlock.java:82) at sun.reflect.GeneratedMethodAccessor249.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.cloudbees.groovy.cps.impl.ContinuationPtr$ContinuationImpl.receive(ContinuationPtr.java:72) at com.cloudbees.groovy.cps.impl.ClosureBlock.eval(ClosureBlock.java:46) at com.cloudbees.groovy.cps.Next.step(Next.java:83) at com.cloudbees.groovy.cps.Continuable$1.call(Continuable.java:174) at com.cloudbees.groovy.cps.Continuable$1.call(Continuable.java:163) at org.codehaus.groovy.runtime.GroovyCategorySupport$ThreadCategoryInfo.use(GroovyCategorySupport.java:122) at org.codehaus.groovy.runtime.GroovyCategorySupport.use(GroovyCategorySupport.java:261) at com.cloudbees.groovy.cps.Continuable.run0(Continuable.java:163) at org.jenkinsci.plugins.workflow.cps.SandboxContinuable.access$101(SandboxContinuable.java:34) at org.jenkinsci.plugins.workflow.cps.SandboxContinuable.lambda$run0$0(SandboxContinuable.java:59) at org.jenkinsci.plugins.scriptsecurity.sandbox.groovy.GroovySandbox.runInSandbox(GroovySandbox.java:121) at org.jenkinsci.plugins.workflow.cps.SandboxContinuable.run0(SandboxContinuable.java:58) at org.jenkinsci.plugins.workflow.cps.CpsThread.runNextChunk(CpsThread.java:182) at org.jenkinsci.plugins.workflow.cps.CpsThreadGroup.run(CpsThreadGroup.java:332) at org.jenkinsci.plugins.workflow.cps.CpsThreadGroup.access$200(CpsThreadGroup.java:83) at org.jenkinsci.plugins.workflow.cps.CpsThreadGroup$2.call(CpsThreadGroup.java:244) at org.jenkinsci.plugins.workflow.cps.CpsThreadGroup$2.call(CpsThreadGroup.java:232) at org.jenkinsci.plugins.workflow.cps.CpsVmExecutorService$2.call(CpsVmExecutorService.java:64) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at hudson.remoting.SingleLaneExecutorService$1.run(SingleLaneExecutorService.java:131) at jenkins.util.ContextResettingExecutorService$1.run(ContextResettingExecutorService.java:28) at jenkins.security.ImpersonatingExecutorService$1.run(ImpersonatingExecutorService.java:59) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Finished: FAILURE  原因：withSonarQubeEnv()中的参数与之前SonarQube servers中Name的配置不同，导致没有找到找到SonarQube\n问题二：SonarQube的token配置不对，导致无法连接sonar [ERROR] Failed to execute goal org.sonarsource.scanner.maven:sonar-maven-plugin:3.6.0.1398:sonar (default-cli) on project callcenter: Not authorized. Please check the properties sonar.login and sonar.password. -\u0026gt; [Help 1] [ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch. [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles: [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException [Pipeline] } [withMaven] artifactsPublisher - Archive artifact pom.xml under cn/keking/callcenter/callcenter/0.0.1-SNAPSHOT/callcenter-0.0.1-SNAPSHOT.pom [withMaven] artifactsPublisher - Archive artifact target/callcenter-0.0.1-SNAPSHOT.jar under cn/keking/callcenter/callcenter/0.0.1-SNAPSHOT/callcenter-0.0.1-SNAPSHOT.jar [withMaven] artifactsPublisher - Archive artifact target/callcenter-0.0.1-SNAPSHOT-api.jar under cn/keking/callcenter/callcenter/0.0.1-SNAPSHOT/callcenter-0.0.1-SNAPSHOT-api.jar [withMaven] junitPublisher - Archive test results for Maven artifact cn.keking.callcenter:callcenter:jar:0.0.1-SNAPSHOT generated by maven-surefire-plugin:test (default-test): target/surefire-reports/*.xml [withMaven] junitPublisher - Jenkins JUnit Attachments Plugin not found, can't publish test attachments.Recording test results None of the test reports contained any result [withMaven] Jenkins Task Scanner Plugin not found, don't display results of source code scanning for 'TODO' and 'FIXME' in pipeline screen. [withMaven] Publishers: Pipeline Graph Publisher: 1 ms, Generated Artifacts Publisher: 891 ms, Junit Publisher: 4 ms, Dependencies Fingerprint Publisher: 5 ms [Pipeline] // withMaven [Pipeline] } WARN: Unable to locate 'report-task.txt' in the workspace. Did the SonarScanner succedeed?  原因：sonar.login的token配置不正确或者没有配置\n问题三：jenkins pipeline在SonarQube回调时显示超时 [Pipeline] waitForQualityGate Checking status of SonarQube task 'AWlX97LSgWqXn-z33SO5' on server 'sonar' SonarQube task 'AWlX97LSgWqXn-z33SO5' status is 'IN_PROGRESS' Cancelling nested steps due to timeout  原因：SonarQube没有配置webhook回调，导致请求超时，按照步骤4配置webhook即可解决\n问题四：sonar找不到Dependency-Check XML [INFO] Sensor Dependency-Check [dependencycheck] [INFO] Process Dependency-Check report [INFO] Dependency-Check XML report does not exists. Please check property sonar.dependencyCheck.reportPath:/data/jenkinsHome/workspace/xxx/${WORKSPACE}/dependency-check-report.xml [INFO] Analysis skipped/aborted due to missing report file [INFO] Dependency-Check HTML report does not exists. Please check property sonar.dependencyCheck.htmlReportPath:/data/jenkinsHome/workspace/xxx/${WORKSPACE}/dependency-check-report.html [INFO] HTML-Dependency-Check report does not exist.  原因：SonarQube配置Dependency-Check插件有误，按照上文配置即可\n结语 sonar与jenkins集成的方式还有很多，不止pipeline+maven这一种，还有配置在jenkins构建任务中、直接使用sonar脚本等方法。采用这样方法，一方面是配置相对简单，不需要每个构建任务都进行配置，只需要将jenkinsfile中拷入相应代码并修改几个参数即可。同时可以在静态代码扫描期间完整maven打包，减少持续集成的时间。\n","date":1551921279,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1551921279,"objectID":"655d46de27435ce51770143eb117231f","permalink":"https://guoxudong.io/post/sonar-pipline/","publishdate":"2019-03-07T09:14:39+08:00","relpermalink":"/post/sonar-pipline/","section":"post","summary":"在devops理念中，CI/CD毫无疑问是最重要的一环，而代码质量检查则是CI中必不可少的一步。在敏捷开发的思想下，代码的迭代周期变短，交付速度提升，这个时候代码的质量就很难保证，测试只能保证功能完整与可用，而代码的质量纯靠review的话效率又很低，这个时候sonar...","tags":["devops","jenkins","sonar"],"title":"Jenkins Pipeline集成Sonar进行代码质量检测","type":"post"},{"authors":["郭旭东"],"categories":["kubernetes"],"content":" 前言 随着公司容器化的深入，越来越多的服务陆续迁移到kubernetes集群中，有些问题在测试环境并未凸显，但是在生产环境中这些问题就显得格外的扎眼。这里就对实践中kubernetes集群中的7层负载均衡器ingress遇到的问题进行总结。\nHTTP(S)负载均衡器-ingress Ingress是kubernetes API的标准资源类型之一，其本质就是一组基于DNS名称(host)或URL路径把请求转发至指定的Service资源的规则，用于将集群外的请求流量转发至集群内部完成服务发布。\nIngress控制器(Ingress Controller)可以由任何具有反向代理(HTTP/HTTPS)功能的服务程序实现，如Nginx、Envoy、HAProxy、Vulcand和Traefik等。Ingress控制器本身也作为Pod对象与被代理的运行为Pod资源的应用运行于同一网络中。我们在这里选择了NGINX Ingress Controller，由于对NGINX的配置较为熟悉，同时我们使用的kubernetes是阿里云的容器服务，构建集群的时候，容器服务会自带NGINX Ingress Controller。\n根据实际情况Ingress调优 1. 解决400 Request Header Or Cookie Too Large问题 image: caption: \u0026ldquo;Image from: Pexels\u0026ldquo; focal_point: \u0026ldquo;\u0026rdquo;\npreview_only: false 现象 微信小程序需要调用后端接口，需要在header中传一段很长的token参数，直接使用浏览器访问该端口可以访问通，但是在加上token访问之后，会报“400 Request Header Or Cookie Too Large”\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;400 Request Header Or Cookie Too Large\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt; \u0026lt;h1\u0026gt;400 Bad Request\u0026lt;/h1\u0026gt; \u0026lt;/center\u0026gt; \u0026lt;center\u0026gt;Request Header Or Cookie Too Large\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt; \u0026lt;center\u0026gt;nginx/1.15.6\u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  问题定位 直接修改Service使用nodeport的形式访问，则没有报错，初步定位需要在ingress中nginx配置客户端的请求头，进入Ingress Controller的Pod查询配置，果然是请求头空间不足。\n$ cat nginx.conf | grep client_header_buffer_size client_header_buffer_size 1k; $ cat nginx.conf | grep large_client_header_buffers large_client_header_buffers 4 8k;  解决方法 在ingress中添加注释\nnginx.ingress.kubernetes.io/server-snippet: client_header_buffer_size 2046k;  image: caption: \u0026ldquo;Image from: Pexels\u0026ldquo; focal_point: \u0026ldquo;\u0026rdquo;\npreview_only: false Server snippet\nUsing the annotation nginx.ingress.kubernetes.io/server-snippet it is possible to add custom configuration in the server configuration block. 该注释是将自定义配置加入nginx的server配置中\nimage: caption: \u0026ldquo;Image from: Pexels\u0026ldquo; focal_point: \u0026ldquo;\u0026rdquo;\npreview_only: false 2. 解决请求超时问题 现象 有一个数据导出功能，需要将大量数据进行处理，然后以Excel格式返回，在导出一个大约3W条数据的时候，出现访问超时情况。\n解决方法 调整proxy_read_timeout，连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理 在ingress中添加注释\nnginx.ingress.kubernetes.io/proxy-read-timeout: 600   这里需要注意的事该注释的value需要时number类型，不能加s，否则将不生效\n 3. 增加白名单 现象 在实际的使用中，会有一部分应用需要设置只可以在办公场地的网络使用，之前使用阿里云 SLB 的时候可以针对端口进行访问控制，但是现在走 ingress ，都是从80 or 443端口进，所以需要在 ingress 设置\n解决方法  Whitelist source range\nYou can specify allowed client IP source ranges through the nginx.ingress.kubernetes.io/whitelist-source-range annotation. The value is a comma separated list of CIDRs, e.g. 10.0.0.0/24,172.10.0.1.\n 在 ingress 里配置 nginx.ingress.kubernetes.io/whitelist-source-range ，如有多个ip段，用逗号分隔即可\nnginx.ingress.kubernetes.io/whitelist-source-range: 10.0.0.0/24  如果想全局适用，可以在阿里云 SLB 里操作，也可以将该配置加入到 NGINX ConfigMap 中。\nimage: caption: \u0026ldquo;Image from: Pexels\u0026ldquo; focal_point: \u0026ldquo;\u0026rdquo;\npreview_only: false 根据工作中遇到的实际问题，持续更新中\u0026hellip;\n总结 使用NGINX ingress controller的好处就是对于nginx配置相对比较熟悉，性能也不差。相关nginx配置的对应的ingress可以在 https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/ 上查到。\n","date":1551854525,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1551854525,"objectID":"395e2ad81521e8cc5dc441a894e886b6","permalink":"https://guoxudong.io/post/k8s-ingress-config/","publishdate":"2019-03-06T14:42:05+08:00","relpermalink":"/post/k8s-ingress-config/","section":"post","summary":"随着公司容器化的深入，越来越多的服务陆续迁移到kubernetes集群中，有些问题在测试环境并未凸显，但是在生产环境中这些问题就显得格外的扎眼。这里就对实践中kubernetes集群中的7层负载均衡器ingress遇到的问题进行总结。","tags":["kubernetes","容器"],"title":"解决kubernetes中ingress-nginx配置问题","type":"post"},{"authors":["郭旭东"],"categories":["kubernetes"],"content":" 根据Pod对象的requests和limits属性，kubernetes将Pod对象归类到BestEffort、Burstable和Guaranteed三个服务质量（Quality of Service，QoS）类别。\n  Guaranteed  cpu:requests=limits memory:requests=limits 这类Pod具有最高优先级  Burstable  至少一个容器设置了cpu或内存资源的requests 这类Pod具有中等优先级  BestEffort  未有任何一个容器设置requests或limits属性 这类Pod具有最低优先级   同级别优先级的Pod资源在OOM时，与自身的requests属性相比，其内存占用比例最大的Pod对象将被首先杀死。如上图同属Burstable类别的Pod A将先于Pod B被杀死，虽然其内存用量小，但与自身的requests值相比，它的占用比例95%要大于Pod B的80%。\n","date":1551698293,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1551698293,"objectID":"624af9b10deca3a288bfa7770b4a3f65","permalink":"https://guoxudong.io/post/k8s-qos/","publishdate":"2019-03-04T19:18:13+08:00","relpermalink":"/post/k8s-qos/","section":"post","summary":"根据Pod对象的requests和limits属性，kubernetes将Pod对象归类到BestEffort、Burstable和Guaranteed三个服务质量（Quality of Service，QoS）类别。","tags":["kubernetes"],"title":"Pod质量服务类别(QoS)","type":"post"},{"authors":["郭旭东"],"categories":["部署安装"],"content":" 前言  目前的项目日志都是通过Logtail直接采集，投递到OSS持久化，同时可以通过阿里云日志服务、devops自建平台进行查看（虽然大部分人是直接登录ECS查看=。=）， 在开始进行容器化之后，同样遇到日志的问题，目前的解决方案是阿里云日志服务持久化和展现格式化后的日志、使用rancher查看实时日志， 但是之前由于rancher平台出现一些问题，导致不能及时查看日志的情况，在这个背景下对阿里云日志服务采集k8s日志和livetail进行搭建并调研此方案是否可行。\n 简介（转自阿里云官方文档） 日志服务（Log Service，简称 LOG）是针对日志类数据的一站式服务，在阿里巴巴集团经历大量大数据场景锤炼而成。您无需开发就能快捷完成日志数据采集、消费、投递以及查询分析等功能，提升运维、运营效率，建立 DT 时代海量日志处理能力。\nkubernetes日志采集组件安装 安装Logtail  进入阿里云容器服务找到集群id  通过ssh登录master节点，或者任意安装了kubectl并配置了该集群kubeconfig的服务器\n 执行命令，将${your_k8s_cluster_id}替换为集群id\nwget http://logtail-release-cn-hangzhou.oss-cn-hangzhou.aliyuncs.com/kubernetes/alicloud-log-k8s-install.sh -O alicloud-log-k8s-install.sh; chmod 744 ./alicloud-log-k8s-install.sh; sh ./alicloud-log-k8s-install.sh ${your_k8s_cluster_id}   Project k8s-log-${your_k8s_cluster_id}下会自动创建名为config-operation-log的Logstore，用于存储alibaba-log-controller的运行日志。请勿删除此Logstore，否则无法为alibaba-log-controller排查问题。 若您需要将日志采集到已有的Project，请执行安装命令sh ./alicloud-log-k8s-install.sh${your_k8s_cluster_id} ${your_project_name} ，并确保日志服务Project和您的Kubernetes集群在同一地域。  该条命令其实就是执行了一个shell脚本，使用helm安装了采集kubernetes集群日志的组件\n#!/bin/bash if [ $# -eq 0 ] ; then echo \u0026quot;[Invalid Param], use sudo ./install-k8s-log.sh {your-k8s-cluster-id}\u0026quot; exit 1 fi clusterName=$(echo $1 | tr '[A-Z]' '[a-z]') curl --connect-timeout 5 http://100.100.100.200/latest/meta-data/region-id if [ $? != 0 ]; then echo \u0026quot;[FAIL] ECS meta server connect fail, only support alibaba cloud k8s service\u0026quot; exit 1 fi regionId=`curl http://100.100.100.200/latest/meta-data/region-id` aliuid=`curl http://100.100.100.200/latest/meta-data/owner-account-id` helmPackageUrl=\u0026quot;http://logtail-release-$regionId.oss-$regionId.aliyuncs.com/kubernetes/alibaba-cloud-log.tgz\u0026quot; wget $helmPackageUrl -O alibaba-cloud-log.tgz if [ $? != 0 ]; then echo \u0026quot;[FAIL] download alibaba-cloud-log.tgz from $helmPackageUrl failed\u0026quot; exit 1 fi project=\u0026quot;k8s-log-\u0026quot;$clusterName if [ $# -ge 2 ]; then project=$2 fi echo [INFO] your k8s is using project : $project helm install alibaba-cloud-log.tgz --name alibaba-log-controller \\ --set ProjectName=$project \\ --set RegionId=$regionId \\ --set InstallParam=$regionId \\ --set MachineGroupId=\u0026quot;k8s-group-\u0026quot;$clusterName \\ --set Endpoint=$regionId\u0026quot;-intranet.log.aliyuncs.com\u0026quot; \\ --set AlibabaCloudUserId=\u0026quot;:\u0026quot;$aliuid \\ --set LogtailImage.Repository=\u0026quot;registry.$regionId.aliyuncs.com/log-service/logtail\u0026quot; \\ --set ControllerImage.Repository=\u0026quot;registry.$regionId.aliyuncs.com/log-service/alibabacloud-log-controller\u0026quot; installRst=$? if [ $installRst -eq 0 ]; then echo \u0026quot;[SUCCESS] install helm package : alibaba-log-controller success.\u0026quot; exit 0 else echo \u0026quot;[FAIL] install helm package failed, errno \u0026quot; $installRst exit 0 fi  命令执行后，会在kubernetes集群中的每个节点运行一个日志采集的pod：logatail-ds，该pod位于kube-system\n 安装完成后，可使用以下命令来查看pod状态，若状态全部成功后，则表示安装完成\n$ helm status alibaba-log-controller LAST DEPLOYED: Thu Nov 22 15:09:35 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/ServiceAccount NAME SECRETS AGE alibaba-log-controller 1 6d ==\u0026gt; v1beta1/CustomResourceDefinition NAME AGE aliyunlogconfigs.log.alibabacloud.com 6d ==\u0026gt; v1beta1/ClusterRole alibaba-log-controller 6d ==\u0026gt; v1beta1/ClusterRoleBinding NAME AGE alibaba-log-controller 6d ==\u0026gt; v1beta1/DaemonSet NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE logtail-ds 16 16 16 16 16 \u0026lt;none\u0026gt; 6d ==\u0026gt; v1beta1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE alibaba-log-controller 1 1 1 1 6d ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE logtail-ds-2fqs4 1/1 Running 0 6d logtail-ds-4bz7w 1/1 Running 1 6d logtail-ds-6vg88 1/1 Running 0 6d logtail-ds-7tp6v 1/1 Running 0 6d logtail-ds-9575c 1/1 Running 0 6d logtail-ds-bgq84 1/1 Running 0 6d logtail-ds-kdlhr 1/1 Running 0 6d logtail-ds-lknxw 1/1 Running 0 6d logtail-ds-pdxfk 1/1 Running 0 6d logtail-ds-pf4dz 1/1 Running 0 6d logtail-ds-rzsnw 1/1 Running 0 6d logtail-ds-sqhbv 1/1 Running 0 6d logtail-ds-vvtwn 1/1 Running 0 6d logtail-ds-wwmhg 1/1 Running 0 6d logtail-ds-xbp4j 1/1 Running 0 6d logtail-ds-zpld9 1/1 Running 0 6d alibaba-log-controller-85f8fbb498-nzhc8 1/1 Running 0 6d   配置日志组件展示  在集群内安装好日志组件后，登录阿里云日志服务控制台，就会发现有一个新的project，名称为k8s-log-{集群id}  创建Logstore  数据导入  选择数据类型中选择docker标准输出  数据源配置，这里可以使用默认的  选择数据源  配置好之后等待1-2分钟，日志就会进来了  为了快速查询和过滤，需要配置索引  添加容器名称、命名空间、pod名称作为索引（后续使用livetail需要）  这样就完成了一个k8s集群日志采集和展示的基本流程了\n  livetail功能使用 背景介绍 在线上运维的场景中，往往需要对日志队列中进入的数据进行实时监控，从最新的日志数据中提取出关键的信息进而快速地分析出异常原因。在传统的运维方式中，如果需要对日志文件进行实时监控，需要到服务器上对日志文件执行命令tail -f，如果实时监控的日志信息不够直观，可以加上grep或者grep -v进行关键词过滤。日志服务在控制台提供了日志数据实时监控的交互功能LiveTail，针对线上日志进行实时监控分析，减轻运维压力。\n使用方法  这里选择来源类型为kubernetes，命名空间、pod名称、容器名称为上一步新建的3个索引的内容，过滤关键字的功劳与tail命令后加的grep命令是一样的，用于关键词过滤  点击开启livetail，这时就有实时日志展示出来了   以上就是阿里云livetail日志服务功能\n","date":1550124426,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1550124426,"objectID":"5d98828078eb8a1d588f28fbd4a208cd","permalink":"https://guoxudong.io/post/dashboard-k8s/","publishdate":"2019-02-14T14:07:06+08:00","relpermalink":"/post/dashboard-k8s/","section":"post","summary":"阿里云日志服务采集k8s日志和livetail功能调研","tags":["阿里云","日志","kubernetes"],"title":"阿里云日志服务采集k8s日志并实现livetail功能","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides academia\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://guoxudong.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using academia's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["郭旭东"],"categories":["问题解决"],"content":"  新版监控大屏于18年最后一天正式上线，之后陆续进行了几次优化和修改，最近发现一个比较大的bug，就是监控显示的时间轴不对，显示的就是和目前的时间相差8小时，这就引出了docker中的时区问题\n 问题的原因 默认的情况，在K8S里启动一个容器，该容器的设置的时区是UTC0，但是对用户而言，主机环境并不在UTC0。我们在UTC8。如果不把容器的时区和主机主机设置为一致，则在查找日志等时候将非常不方便，也容易造成误解。但是K8S以及Docker容器没有一个简便的设置/开关在系统层面做配置。都需要我们从单个容器入手做设置，具体有两个方法：\n 直接修改镜像的时间设置，好处是应用部署时无需做特殊设置，但是需要手动构建Docker镜像。 部署应用时，单独读取主机的“/etc/localtime”文件，即创建pod时同步时区，无需修改镜像，但是每个应用都要单独设置。  问题的解决 这里我们选择第二种方法，即修改部署应用的yaml文件，创建pod时同步时区\napiVersion: extensions/v1beta1 kind: Deployment metadata: name: myweb spec: replicas: 2 template: metadata: labels: app: myweb spec: containers: - name: myweb image: nginx:apline ports: - containerPort: 80 #挂载到pod中 volumeMounts: - name: host-time mountPath: /etc/localtime #需要被挂载的宿主机的时区文件 volumes: - name: host-time hostPath: path: /etc/localtime  效果对比 修改时区前 修改时区后 ","date":1548850693,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1548850693,"objectID":"724a6c0104bf30ec9864841a9ae9bd82","permalink":"https://guoxudong.io/post/pod-timezone/","publishdate":"2019-01-30T20:18:13+08:00","relpermalink":"/post/pod-timezone/","section":"post","summary":"解决容器中时区问题。","tags":["kubernetes"],"title":"kubernetes中pod同步时区问题","type":"post"},{"authors":["郭旭东"],"categories":["kubernetes"],"content":"  devops平台率先在公司内使用kubernetes集群提供后端服务，但是由于之前一直处于探索阶段，所以使用的事http的方式提供后端服务，但是在开发统一入口后，出现了访问HTTPS页面的跨域问题，由此引出了后端服务配置SSL证书的问题\n 使用rancher配置SSL证书 下载SSL证书文件 首先需要获得SSL证书文件，可以直接在阿里云SSL证书管理控制台下载\n选中需要下载证书，选择下载nginx证书 将证书上传项目 打开rancher，选择要使用证书的项目，点击资源中的证书\n将证书上传项目 打开rancher，选择要使用证书的项目，点击资源中的证书 添加证书，点击从文件上传 上传证书文件中的秘钥和证书，点击保存即可\n使用yaml上传证书 这个证书的原理其实是在相应的命名空间创建了一个包含证书信息的secrets\napiVersion: v1 data: tls.crt: {私钥} tls.key: {证书} kind: Secret metadata: name: keking-cn namespace: devops-plat type: kubernetes.io/tls  在kubernetes上运行该yaml即可\nrancher中证书绑定 选中需要绑定证书的ingress，点击编辑，选中证书，保存即可（由于ingress-controller中没有绑定默认证书，所以这里不能选中默认） 保存完毕，证书即可生效\n","date":1546090093,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1546090093,"objectID":"3b233c793e2ed8e0e963331c3a77c714","permalink":"https://guoxudong.io/post/https-ingress/","publishdate":"2018-12-29T21:28:13+08:00","relpermalink":"/post/https-ingress/","section":"post","summary":"devops平台率先在公司内使用kubernetes集群提供后端服务，但是由于之前一直处于探索阶段，所以使用的事http的方式提供后端服务，但是在开发统一入口后，出现了访问HTTPS页面的跨域问题，由此引出了后端服务配置SSL证书的问题。","tags":["kubernetes","阿里云","rancher"],"title":"为ingress配置SSL证书，实现HTTPS访问","type":"post"},{"authors":["郭旭东"],"categories":["问题解决"],"content":"  近期由于公司需要将部署在ucloud上的rancher迁移到阿里云上，所以将部署到阿里云的图中遇到的问题和踩到的坑在这里进行记录。\n 无法删除namespace 在安装新环境的rancher之前，需要将kubernetes集群中cattle-system ns下面的cluster-agent和node-agent干掉，这里我选择直接删除cattle-system这个命名空间\nkubectl delete ns cattle-system  然而问题来了，在删除命名空间之后，这个命名空间并没有立刻被删除，而是一直处于Terminating状态，这里我专门写了一篇文章解决这个问题，这里就不再赘述\n阿里云证书配置 由于之前使用的ucloud的机器进行测试，使用默认自签名证书并没有使用SSL证书，所以在配置证书这里遇到的许多的问题\n首先根据官方文档使用权威CA机构颁发的证书，这里使用的是本公司自己的证书\n获取证书方法： 点击下载证书，选择nginx证书下载 之后将下载的证书上传到rancher所在服务器，并配置好数据卷挂载\n将下面代码的挂载地址指向证书文件，运行代码\n$ docker run -d --restart=unless-stopped \\ -p 80:80 -p 443:443 \\ -v /root/var/log/auditlog:/var/log/auditlog \\ -e AUDIT_LEVEL=3 \\ -v /etc/your_certificate_directory/fullchain.pem:/etc/rancher/ssl/cert.pem \\ -v /etc/your_certificate_directory/privkey.pem:/etc/rancher/ssl/key.pem \\ rancher/rancher:latest --no-cacerts  之后会自动冲dockerhub上拉取最新的rancher进行进行安装，之后使用命令\ndocker ps  查看容器是否在运行，如果运行正常，则后端的配置就完成了\n划重点：这是是在后端配置了证书，所以在阿里云的配置上要使用四层TCP监听\n这个地方可是坑了我许久，我一直在前端配置https七层监听，导致一直无法正常访问，一度已经到了怀疑人生的地步=。=\n之后就是简单的阿里云SLB配置四层TCP监听，这里也就不再赘述了\nk8s集群导入rancher 前后端都准备就绪，现在就可以访问rancher了，访问rancher根据页面提示进行基本配置，登录后选择添加集群\n选择导入现有集群 为集群创建一个rancher中的名称，然后根据提示将命令拷贝到k8s集群所在宿主机执行即可，注意：这里由于配置了证书，所以选择有证书，不绕过证书的那个命令执行，之后就可看到集群数据导入中 等待几秒即可开心的使用rancher了！\n关于rancher部署后访问集群api超时问题 经过排查，原因是阿里云在容器服务对外连接处设置了TLS双向认证，导致rancher的外网ip经常性的被拦截，导致超时\n解决办法：\n对k8s集群中rancher的cattle-cluster-agent传递内网参数，将其配置为内网连接，就可以正常访问了\nkubectl -n cattle-system patch deployments cattle-cluster-agent --patch '{ \u0026quot;spec\u0026quot;: { \u0026quot;template\u0026quot;: { \u0026quot;spec\u0026quot;: { \u0026quot;hostAliases\u0026quot;: [{ \u0026quot;hostnames\u0026quot;:[\u0026quot;rancher.keking.cn\u0026quot;], #rancher的域名 \u0026quot;ip\u0026quot;: \u0026quot;10.0.0.219\u0026quot; #rancher部署地址 }] } } } }'  ","date":1543487293,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1543487293,"objectID":"483d7eaa973ba9d89a2186ac056e2a42","permalink":"https://guoxudong.io/post/install-rancher/","publishdate":"2018-11-29T18:28:13+08:00","relpermalink":"/post/install-rancher/","section":"post","summary":"近期由于公司需要将部署在ucloud上的rancher迁移到阿里云上，所以将部署到阿里云的图中遇到的问题和踩到的坑在这里进行记录。","tags":["容器","阿里云","rancher"],"title":"阿里云部署rancher2.1采坑记","type":"post"},{"authors":["郭旭东"],"categories":["问题解决"],"content":"  近期由于公司需要将部署在ucloud上的rancher迁移到阿里云上，所以需要将原有Rancher依赖的namespace（cattle-system）删除，但在删除中出现了删除的namespace一直处于Terminating状态的情况\n 解决方案 运行命令：\nkubectl edit namespaces cattle-system  可以看到namespaces的yaml配置：\n将finalizer的value删除，这里将其设置为[]\n保存即可看到该namespace已被删除\n","date":1542363493,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1542363493,"objectID":"fca2e4368aeab4e02ada1c55350b0100","permalink":"https://guoxudong.io/post/k8s-d-n/","publishdate":"2018-11-16T18:18:13+08:00","relpermalink":"/post/k8s-d-n/","section":"post","summary":"kubernetes解决删除的namespace一直处于Terminating状态的情况。","tags":["kubernetes"],"title":"Kubernetes删除一直处于Terminating状态的namespace","type":"post"},{"authors":["郭旭东"],"categories":["kubernetes"],"content":"  随着2017年AWS，Azure和阿里云相继在其原有容器服务上新增了对kubernetes的支持，而Docker官网也在同年10月宣布同时支持Swarm好kubernetes容器编排系统。kubernetes俨然已成为容器编排领域事实上的标准，而2018年更是各大公司相继将服务迁移到kubernetes上，而kubernetes则以惊人更新速度，保持着每个季度发布一个大版本的速度高速发展着。\n kubernetes特征 kubernetes是一种在一组主机上运行和协同容器化应用程序的系统，旨在提供可预测性、可拓展性与高可用性的方法来完全管理容器化应用和服务的生命周期平台。用户可以定义应用程序的运行方式，以及与其他应用程序或外部世界交互的途径，并能实现服务的扩容和缩容，执行平滑滚动更新，以及在不同版本的应用程序之间调度流量以测试功能或回滚有问题的部署。kubernetes提供了接口和可组合帆软平台原语，使得用户能够以高度的灵活性和可靠性定义及管理应用程序。\nkubernetes组件及网络通信 kubernetes集群的客户端可以分为两类：API Server客户端和应用程序（运行为Pod中的容器）客户端。  第一类客户端通常包含用户和Pod对象两种，它们通过API Server访问kubernetes集群完成管理任务，例如，管理集群上的各种资源对象。 第二类客户端一般也包含人类用户和Pod对象两种，它们的访问目标是Pod上运行于容器中的应用程序提供的各种具体的服务，如redis或nginx等，不过，这些访问请求通常要经由Service或Ingress资源对象进行。另外，第二类客户端的访问目标对象的操作要经由第一类客户端创建和配置完成后才进行。\n访问API Server时，人类用户一般借助于命令行工具kubectl或图形UI（例如kubernetes dashboard）进行，也通过编程接口进行访问，包括REST API。访问Pod中的应用时，其访问方式要取决于Pod中的应用程序，例如，对于运行Nginx容器的Pod来说，其最常用工具就是浏览器。\n管理员（开发人员或运维人员）使用kubernetes集群的常见操作包括通过控制器创建Pod，在Pod的基础上创建Service供第二类客户端访问，更新Pod中的应用版本（更新和回滚）以及对应用规模进行扩容或缩容等，另外还有集群附件管理、存储卷管理、网络及网络策略管理、资源管理和安全管理等。\n  ","date":1538540293,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1538540293,"objectID":"aeb247588889fea5c6b82b8d280c985c","permalink":"https://guoxudong.io/post/k8s-topo/","publishdate":"2018-10-03T12:18:13+08:00","relpermalink":"/post/k8s-topo/","section":"post","summary":"随着2017年AWS，Azure和阿里云相继在其原有容器服务上新增了对kubernetes的支持，而Docker官网也在同年10月宣布同时支持Swarm好kubernetes容器编排系统。kubernetes俨然已成为容器编排领域事实上的标准，而2018年更是各大公司相继将服务迁移到kubernetes上，而kubernetes则以惊人更新速度，保持着每个季度发布一个大版本的速度高速发展着。","tags":["kubernetes"],"title":"kubernetes集群概述","type":"post"},{"authors":["郭旭东"],"categories":["问题解决"],"content":"  精简Docker镜像的好处很多，不仅可以节省存储空间和带宽，还能减少安全隐患。优化镜像大小的手段多种多样，因服务所使用的基础开发语言不同而有差异。本文将介绍精简Docker镜像的几种通用方法。\n 精简Docker镜像大小的必要性 Docker镜像由很多镜像层（Layers）组成（最多127层），镜像层依赖于一系列的底层技术，比如文件系统(filesystems)、写时复制(copy-on-write)、联合挂载(union mounts)等技术，你可以查看Docker社区文档以了解更多有关Docker存储驱动的内容，这里就不再赘述技术细节。总的来说，Dockerfile中的每条指令都会创建一个镜像层，继而会增加整体镜像的尺寸。\n下面是精简Docker镜像尺寸的好处：\n 减少构建时间 减少磁盘使用量 减少下载时间 因为包含文件少，攻击面减小，提高了安全性 提高部署速度  五点建议减小Docker镜像尺寸\n一、优化基础镜像 优化基础镜像的方法就是选用合适的更小的基础镜像，常用的 Linux 系统镜像一般有 Ubuntu、CentOs、Alpine，其中Alpine更推荐使用。大小对比如下：\nguoxudong@ubuntu ~/s\u0026gt; docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 74f8760a2a8b 8 days ago 82.4MB alpine latest 11cd0b38bc3c 2 weeks ago 4.41MB centos 7 49f7960eb7e4 7 weeks ago 200MB debian latest 3bbb526d2608 8 days ago 101MB guoxudong@ubuntu ~/s\u0026gt;  Alpine是一个高度精简又包含了基本工具的轻量级Linux发行版，基础镜像只有4.41M，各开发语言和框架都有基于Alpine制作的基础镜像，强烈推荐使用它。Alpine镜像各个语言和框架支持情况，可以参考《优化Docker镜像、加速应用部署》。 查看上面的镜像尺寸对比结果，你会发现最小的镜像也有4.41M，那么有办法构建更小的镜像吗？答案是肯定的，例如 gcr.io/google_containers/pause-amd64:3.1 镜像仅有742KB。为什么这个镜像能这么小？在为大家解密之前，再推荐两个基础镜像：\n scratch镜像\nscratch是一个空镜像，只能用于构建其他镜像，比如你要运行一个包含所有依赖的二进制文件，如Golang程序，可以直接使用scratch作为基础镜像。现在给大家展示一下上文提到的Google pause镜像Dockerfile：\nFROM scratch ARG ARCH ADD bin/pause-${ARCH} /pause ENTRYPOINT [\u0026quot;/pause\u0026quot;]  Google pause镜像使用了scratch作为基础镜像，这个镜像本身是不占空间的，使用它构建的镜像大小几乎和二进制文件本身一样大，所以镜像非常小。当然在我们的Golang程序中也会使用。对于一些Golang/C程序，可能会依赖一些动态库，你可以使用自动提取动态库工具，比如ldd、linuxdeployqt等提取所有动态库，然后将二进制文件和依赖动态库一起打包到镜像中。\n busybox镜像\nscratch是个空镜像，如果希望镜像里可以包含一些常用的Linux工具，busybox镜像是个不错选择，镜像本身只有1.16M，非常便于构建小镜像。\n  二、串联 Dockerfile 指令 大家在定义Dockerfile时，如果太多的使用RUN指令，经常会导致镜像有特别多的层，镜像很臃肿，而且甚至会碰到超出最大层数（127层）限制的问题，遵循 Dockerfile 最佳实践，我们应该把多个命令串联合并为一个 RUN（通过运算符\u0026amp;\u0026amp;和/ 来实现），每一个 RUN 要精心设计，确保安装构建最后进行清理，这样才可以降低镜像体积，以及最大化的利用构建缓存。\n下面是一个优化前Dockerfile：\nFROM ubuntu ENV VER 3.0.0 ENV TARBALL http://download.redis.io/releases/redis-$VER.tar.gz # ==\u0026gt; Install curl and helper tools... RUN apt-get update RUN apt-get install -y curl make gcc # ==\u0026gt; Download, compile, and install... RUN curl -L $TARBALL | tar zxv WORKDIR redis-$VER RUN make RUN make install #... # ==\u0026gt; Clean up... WORKDIR / RUN apt-get remove -y --auto-remove curl make gcc RUN apt-get clean RUN rm -rf /var/lib/apt/lists/* /redis-$VER #... CMD [\u0026quot;redis-server\u0026quot;]  构建镜像，名称叫 test/test:0.1。\n我们对Dockerfile做优化，优化后Dockerfile：\nFROM ubuntu ENV VER 3.0.0 ENV TARBALL http://download.redis.io/releases/redis-$VER.tar.gz RUN echo \u0026quot;==\u0026gt; Install curl and helper tools...\u0026quot; \u0026amp;\u0026amp; \\ apt-get update \u0026amp;\u0026amp; \\ apt-get install -y curl make gcc \u0026amp;\u0026amp; \\ echo \u0026quot;==\u0026gt; Download, compile, and install...\u0026quot; \u0026amp;\u0026amp; \\ curl -L $TARBALL | tar zxv \u0026amp;\u0026amp; \\ cd redis-$VER \u0026amp;\u0026amp; \\ make \u0026amp;\u0026amp; \\ make install \u0026amp;\u0026amp; \\ echo \u0026quot;==\u0026gt; Clean up...\u0026quot; \u0026amp;\u0026amp; \\ apt-get remove -y --auto-remove curl make gcc \u0026amp;\u0026amp; \\ apt-get clean \u0026amp;\u0026amp; \\ rm -rf /var/lib/apt/lists/* /redis-$VER #... CMD [\u0026quot;redis-server\u0026quot;]  构建镜像，名称叫 test/test:0.2。\n对比两个镜像大小：\nroot@k8s-master:/tmp/iops# docker images REPOSITORY TAG IMAGE ID CREATED SIZE test/test 0.2 58468c0222ed 2 minutes ago 98.1MB test/test 0.1 e496cf7243f2 6 minutes ago 307MB root@k8s-master:/tmp/iops#  可以看到，将多条RUN命令串联起来构建的镜像大小是每条命令分别RUN的三分之一。\n提示：为了应对镜像中存在太多镜像层，Docker 1.13版本以后，提供了一个压扁镜像功能，即将 Dockerfile 中所有的操作压缩为一层。这个特性还处于实验阶段，Docker默认没有开启，如果要开启，需要在启动Docker时添加-experimental 选项，并在Docker build 构建镜像时候添加 \u0026ndash;squash 。我们不推荐使用这个办法，请在撰写 Dockerfile 时遵循最佳实践编写，不要试图用这种办法去压缩镜像。\n三、使用多阶段构建 Dockerfile中每条指令都会为镜像增加一个镜像层，并且你需要在移动到下一个镜像层之前清理不需要的组件。实际上，有一个Dockerfile用于开发（其中包含构建应用程序所需的所有内容）以及一个用于生产的瘦客户端，它只包含你的应用程序以及运行它所需的内容。这被称为“建造者模式”。Docker 17.05.0-ce版本以后支持多阶段构建。使用多阶段构建，你可以在Dockerfile中使用多个FROM语句，每条FROM指令可以使用不同的基础镜像，这样您可以选择性地将服务组件从一个阶段COPY到另一个阶段，在最终镜像中只保留需要的内容。\n下面是一个使用 COPY --from 和 FROM ... AS ... 的Dockerfile：\n# Compile FROM golang:1.9.0 AS builder WORKDIR /go/src/v9.git...com/.../k8s-monitor COPY . . WORKDIR /go/src/v9.git...com/.../k8s-monitor RUN make build RUN mv k8s-monitor /root # Package # Use scratch image FROM scratch WORKDIR /root/ COPY --from=builder /root . EXPOSE 8080 CMD [\u0026quot;/root/k8s-monitor\u0026quot;]  构建镜像，你会发现生成的镜像只有上面COPY 指令指定的内容，镜像大小只有2M。这样在以前使用两个Dockerfile（一个Dockerfile用于开发和一个用于生产的瘦客户端），现在使用多阶段构建就可以搞定。\n四、构建业务服务镜像技巧 Docker在build镜像的时候，如果某个命令相关的内容没有变化，会使用上一次缓存（cache）的文件层，在构建业务镜像的时候可以注意下面两点：\n 不变或者变化很少的体积较大的依赖库和经常修改的自有代码分开； 因为cache缓存在运行Docker build命令的本地机器上，建议固定使用某台机器来进行Docker build，以便利用cache。  下面是构建Spring Boot应用镜像的例子，用来说明如何分层。其他类型的应用，比如Java WAR包，Nodejs的npm 模块等，可以采取类似的方式。\n 在Dockerfile所在目录，解压缩maven生成的jar包\n$ unzip \u0026lt;path-to-app-jar\u0026gt;.jar -d app  Dockerfile 我们把应用的内容分成4个部分COPY到镜像里面：其中前面3个基本不变，第4个是经常变化的自有代码。最后一行是解压缩后，启动spring boot应用的方式。\nFROM openjdk:8-jre-alpine LABEL maintainer \u0026quot;opl-xws@xiaomi.com\u0026quot; COPY app/BOOT-INF/lib/ /app/BOOT-INF/lib/ COPY app/org /app/org COPY app/META-INF /app/META-INF COPY app/BOOT-INF/classes /app/BOOT-INF/classes EXPOSE 8080 CMD [\u0026quot;/usr/bin/java\u0026quot;, \u0026quot;-cp\u0026quot;, \u0026quot;/app\u0026quot;, \u0026quot;org.springframework.boot.loader.JarLauncher\u0026quot;]   这样在构建镜像时候可大大提高构建速度。\n五、其他优化办法  RUN命令中执行apt、apk或者yum类工具技巧，如果在RUN命令中执行apt、apk或者yum类工具，可以借助这些工具提供的一些小技巧来减少镜像层数量及镜像大小。举几个例子：\n 在执行apt-get install -y 时增加选项— no-install-recommends ，可以不用安装建议性（非必须）的依赖，也可以在执行apk add 时添加选项\u0026ndash;no-cache 达到同样效果； 执行yum install -y 时候， 可以同时安装多个工具，比如yum install -y gcc gcc-c++ make \u0026hellip;。将所有yum install 任务放在一条RUN命令上执行，从而减少镜像层的数量； 组件的安装和清理要串联在一条指令里面，如 apk \u0026ndash;update add php7 \u0026amp;\u0026amp; rm -rf /var/cache/apk/* ，因为Dockerfile的每条指令都会产生一个文件层，如果将apk add \u0026hellip;和 rm -rf \u0026hellip; 命令分开，清理无法减小apk命令产生的文件层的大小。 Ubuntu或Debian可以使用 rm -rf /var/lib/apt/lists/* 清理镜像中缓存文件；CentOS等系统使用yum clean all 命令清理。  压缩镜像\n  Docker 自带的一些命令还能协助压缩镜像，比如 export 和 import\ndocker run -d test/test:0.2 $ docker export 747dc0e72d13 | docker import - test/test:0.3  使用这种方式需要先将容器运行起来，而且这个过程中会丢失镜像原有的一些信息，比如：导出端口，环境变量，默认指令。\n查看这两个镜像history信息，如下，可以看到test/test:0.3 丢失了所有的镜像层信息：\nroot@k8s-master:/tmp/iops# docker history test/test:0.3 IMAGE CREATED CREATED BY SIZE COMMENT 6fb3f00b7a72 15 seconds ago 84.7MB Imported from - root@k8s-master:/tmp/iops# docker history test/test:0.2 IMAGE CREATED CREATED BY SIZE COMMENT 58468c0222ed 2 hours ago /bin/sh -c #(nop) CMD [\u0026quot;redis-server\u0026quot;] 0B 1af7ffe3d163 2 hours ago /bin/sh -c echo \u0026quot;==\u0026gt; Install curl and helper... 15.7MB 8bac6e733d54 2 hours ago /bin/sh -c #(nop) ENV TARBALL=http://downlo... 0B 793282f3ef7a 2 hours ago /bin/sh -c #(nop) ENV VER=3.0.0 0B 74f8760a2a8b 8 days ago /bin/sh -c #(nop) CMD [\u0026quot;/bin/bash\u0026quot;] 0B \u0026lt;missing\u0026gt; 8 days ago /bin/sh -c mkdir -p /run/systemd \u0026amp;\u0026amp; echo 'do... 7B \u0026lt;missing\u0026gt; 8 days ago /bin/sh -c sed -i 's/^#\\s*\\(deb.*universe\\)$... 2.76kB \u0026lt;missing\u0026gt; 8 days ago /bin/sh -c rm -rf /var/lib/apt/lists/* 0B \u0026lt;missing\u0026gt; 8 days ago /bin/sh -c set -xe \u0026amp;\u0026amp; echo '#!/bin/sh' \u0026gt; /... 745B \u0026lt;missing\u0026gt; 8 days ago /bin/sh -c #(nop) ADD file:5fabb77ea8d61e02d... 82.4MB root@k8s-master:/tmp/iops#  社区里还有很多压缩工具，比如Docker-squash ，用起来更简单方便，并且不会丢失原有镜像的自带信息，大家有兴趣可以试试。\n","date":1538051293,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1538051293,"objectID":"959436d56c8f6a395bab0e76d949754d","permalink":"https://guoxudong.io/post/image-size/","publishdate":"2018-09-27T20:28:13+08:00","relpermalink":"/post/image-size/","section":"post","summary":"精简Docker镜像的好处很多，不仅可以节省存储空间和带宽，还能减少安全隐患。优化镜像大小的手段多种多样，因服务所使用的基础开发语言不同而有差异。本文将介绍精简Docker镜像的几种通用方法。","tags":["容器"],"title":"精简docker镜像","type":"post"},{"authors":["郭旭东"],"categories":["问题解决"],"content":" 现象 启动docker容器\ndocker run –name [CONTAINER_NAME] [CONTAINER_ID]  查看容器运行状态\ndocker ps -a  发现刚刚启动的mydocker容器已经退出\n原因 docker容器的主线程（dockfile中CMD执行的命令）结束，容器会退出\n解决办法  可以使用交互式启动\ndocker run -i [CONTAINER_NAME or CONTAINER_ID]  上面的不太友好，建议使用后台模式和tty选项\ndocker run -dit [CONTAINER_NAME or CONTAINER_ID]  Docker 容器在后台以守护态（Daemonized）形式运行，可以通过添加 -d 参数来实现\n$ sudo docker run -d ubuntu:14.04 /bin/sh -c \u0026quot;while true; do echo hello world; sleep 1; done\u0026quot;  在脚本最后一行添加tail -f /dev/null，这个命令永远完成不了，所以该脚本一直不会执行完，所以该容器永远不会退出。\n   TIPs:退出时，使用[ctrl + D]，这样会结束docker当前线程，容器结束，可以使用 [ctrl + P] [ctrl + Q] 退出而不终止容器运行\n如下命令，会在指定容器中执行指定命令， [ctrl+D] 退出后不会终止容器运行\ndocker默认会把容器内部pid=1的作为默认的程序\n ","date":1538047623,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1538047623,"objectID":"461f3c5322cfd166c9de23b8fc70ccf9","permalink":"https://guoxudong.io/post/docker-quit/","publishdate":"2018-09-27T19:27:03+08:00","relpermalink":"/post/docker-quit/","section":"post","summary":"解决容器启动退出问题。","tags":["容器","Linux"],"title":"Docker容器启动退出解决方案","type":"post"},{"authors":["郭旭东"],"categories":["kubernetes"],"content":" 背景  自从微服务（Microservice）的出现，出于业务的需要，IT应用模型不断的变革。开发模式从瀑布式到敏捷开发；开发、运维和测试互相配合的devops思想；应用程序架构从单体模型到分层模型再到微服务；部署方式也从面向物理机到虚拟键再到容器；应用程序的基础架构从自建机房到托管再到云计算，等等。这些变革使得IT技术应用的效率大大提升，同时却以更低的成本交付更高质量的产品。\n尤其是以Docker为代表的容器技术的出现，终结了devops中交付和部署环节因环节、配置及程序本身的不同而造成的动辄几种甚至十几种部署配置的困境，将它们统一在容器镜像（image）之上。这就是我在工作中遇到最先遇到的困境，同时也是我开始研究容器技术的契机。\n如今，越来越多的企业或组织开始开始选择以镜像文件为交付载体。容器镜像之内直接包含了应用程序及其依赖的系统环境、库、基础程序等，从而能够在容器引擎上直接运行。\n 容器技术概述 容器是一种轻量级、可移植、自包含的软件打包技术，它使得应用程序可以在几乎任何地方以相同的方式运行。\n容器有应用程序本身和它的环境依赖（库和其他应用程序）两部分组成，并在宿主机（Host）操作系统的用户空间中运行，但与操作系统的其他进程互相隔离，他们的实现机制有别于VMWare、KVM、Xen等实现方案的虚拟化技术。容器与虚拟机的对比关系如下图 由于同一个宿主机上的所有容器都共享其底层操作系统（内核空间），这就使得容器在体积上要比传统的虚拟机小很多。另外，启动容器无须启动整个操作系统，所以容器部署和启动的速度更快，开销更小，也更容易迁移。事实上，容器赋予了应用程序超强的可移植能力。\n容器技术的优势  开发方面：“一次构建、到处运行”（Build Once, Run Anywhere）。容器意味着环境隔离和可重复性，开发人员只需为应用创建一个运行环境，并将其打包成容器便可在各种部署环境上运行，并与它所在的宿主机环境隔离。 运维方面：“一次配置，运行所以”（Configure Once, Run Anything）。一旦配置好标准的容器运行时环境，服务器就可以运行任何容器，这使得运维人员的工作变得更高效、一致和可重复。容器消除了开发、测试、生产环境的不一致性。  ","date":1535625922,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1535625922,"objectID":"92820ba0ae82f98eb11e3d5854f631a3","permalink":"https://guoxudong.io/post/con-in/","publishdate":"2018-08-30T18:45:22+08:00","relpermalink":"/post/con-in/","section":"post","summary":"自从微服务（Microservice）的出现，出于业务的需要，IT应用模型不断的变革。开发模式从瀑布式到敏捷开发；开发、运维和测试互相配合的devops思想；应用程序架构从单体模型到分层模型再到微服务；部署方式也从面向物理机到虚拟键再到容器；应用程序的基础架构从自建机房到托管再到云计算，等等。这些变革使得IT技术应用的效率大大提升，同时却以更低的成本交付更高质量的产品。","tags":["容器","云原生"],"title":"容器技术概述","type":"post"},{"authors":["郭旭东"],"categories":["部署安装"],"content":" 前言  最近由于公司业务发展到了瓶颈，原有的技术架构已经逐渐无法满足业务开发和测试的需求，出现了应用测试环境搭建复杂，有许多套（真的很多很多）应用环境，应用在持续集成/持续交付也遇到了很大的困难，经过讨论研究决定对应用和微服务进行容器化，这就是我首次直面docker和k8s的契机.\n Kubernetes 介绍 Kubernetes 是 Google 团队发起的开源项目，它的目标是管理跨多个主机的容器，提供基本的部署，维护以及运用伸缩，主要实现语言为 Go 语言。 Kubernetes的特点：\n 易学：轻量级，简单，容易理解 便携：支持公有云，私有云，混合云，以及多种云平台 可拓展：模块化，可插拔，支持钩子，可任意组合 自修复：自动重调度，自动重启，自动复制  准备工作 注：以下操作都是在root权限下执行的\n 安装docker-ce，这里使用docker-ce-17.09.0.c版本，安装方法见之前的教程 安装Kubeadm\n#安装 Kubeadm 首先我们要配置好阿里云的国内源，执行如下命令： cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 EOF #之后，执行以下命令来重建yum缓存： yum -y install epel-releaseyum clean all yum makecache  接下来需要安装指定版本的Kubeadm（这里要安装指定版本，因为后续依赖的镜像由于有墙无法拉取，这里我们只有指定版本的镜像），注意：这里是安装指定版本的Kubeadm，k8s的版本更新之快完全超出你的想象！\nyum -y install kubelet-1.11.0-0 yum -y install kubeadm-1.11.0-0 yum -y install kubectl-1.11.0-0 yum -y install kubernetes-cni #执行命令启动Kubeadm服务： systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet  配置 Kubeadm 所用到的镜像 这里是重中之重，因为在国内的原因，无法访问到 Google 的镜像库，所以我们需要执行以下脚本来从 Docker Hub 仓库中获取相同的镜像，并且更改 TAG 让其变成与 Google 拉去镜像一致。\n新建一个 Shell 脚本，填入以下代码之后保存\n#docker.sh #!/bin/bash images=(kube-proxy-amd64:v1.11.0 kube-scheduler-amd64:v1.11.0 kube-controller-manager-amd64:v1.11.0 kube-apiserver-amd64:v1.11.0 etcd-amd64:3.2.18 coredns:1.1.3 pause-amd64:3.1 kubernetes-dashboard-amd64:v1.8.3 k8s-dns-sidecar-amd64:1.14.9 k8s-dns-kube-dns-amd64:1.14.9 k8s-dns-dnsmasq-nanny-amd64:1.14.9 ) for imageName in ${images[@]} ; do docker pull keveon/$imageName docker tag keveon/$imageName k8s.gcr.io/$imageName docker rmi keveon/$imageName done # 个人新加的一句，V 1.11.0 必加 docker tag da86e6ba6ca1 k8s.gcr.io/pause:3.1  保存后使用chmod命令赋予脚本执行权限\nchmod -R 777 ./docker.sh  执行脚本拉取镜像\nsh docker.sh #这里就开始了漫长的拉取镜像之路  关闭掉swap\nsudo swapoff -a #要永久禁掉swap分区，打开如下文件注释掉swap那一行 # sudo vi /etc/stab  关闭SELinux的\n# 临时禁用selinux # 永久关闭 修改/etc/sysconfig/selinux文件设置 sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinux # 这里按回车，下面是第二条命令 setenforce 0  关闭防火墙\nsystemctl disable firewalld.service \u0026amp;\u0026amp; systemctl stop firewalld.service  配置转发参数\n# 配置转发相关参数，否则可能会出错 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 vm.swappiness=0 EOF # 这里按回车，下面是第二条命令 sysctl --system  这里就完成了k8s集群搭建的准备工作，集群搭建的话以上操作结束后将操作完的系统制作成系统镜像，方便集群搭建\n  正式安装 以下的操作都只在主节点上进行：\n初始化镜像\nkubeadm init --kubernetes-version=v1.11.0 --pod-network-cidr=10.10.0.0/16 #这里填写集群所在网段  之后的输出会是这样：\nI0712 10:46:30.938979 13461 feature_gate.go:230] feature gates: \u0026amp;{map[]} [init] using Kubernetes version: v1.11.0 [preflight] running pre-flight checks I0712 10:46:30.961005 13461 kernel_validator.go:81] Validating kernel version I0712 10:46:30.961061 13461 kernel_validator.go:96] Validating kernel config [WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 18.03.1-ce. Max validated version: 17.03 [WARNING Hostname]: hostname \u0026quot;g2-apigateway\u0026quot; could not be reached [WARNING Hostname]: hostname \u0026quot;g2-apigateway\u0026quot; lookup g2-apigateway on 100.100.2.138:53: no such host [preflight/images] Pulling images required for setting up a Kubernetes cluster [preflight/images] This might take a minute or two, depending on the speed of your internet connection [preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot; [kubelet] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot; [preflight] Activating the kubelet service [certificates] Generated ca certificate and key. [certificates] Generated apiserver certificate and key. [certificates] apiserver serving cert is signed for DNS names [g2-apigateway kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.16.8.62] [certificates] Generated apiserver-kubelet-client certificate and key. [certificates] Generated sa key and public key. [certificates] Generated front-proxy-ca certificate and key. [certificates] Generated front-proxy-client certificate and key. [certificates] Generated etcd/ca certificate and key. [certificates] Generated etcd/server certificate and key. [certificates] etcd/server serving cert is signed for DNS names [g2-apigateway localhost] and IPs [127.0.0.1 ::1] [certificates] Generated etcd/peer certificate and key. [certificates] etcd/peer serving cert is signed for DNS names [g2-apigateway localhost] and IPs [172.16.8.62 127.0.0.1 ::1] [certificates] Generated etcd/healthcheck-client certificate and key. [certificates] Generated apiserver-etcd-client certificate and key. [certificates] valid certificates and keys now exist in \u0026quot;/etc/kubernetes/pki\u0026quot; [kubeconfig] Wrote KubeConfig file to disk: \u0026quot;/etc/kubernetes/admin.conf\u0026quot; [kubeconfig] Wrote KubeConfig file to disk: \u0026quot;/etc/kubernetes/kubelet.conf\u0026quot; [kubeconfig] Wrote KubeConfig file to disk: \u0026quot;/etc/kubernetes/controller-manager.conf\u0026quot; [kubeconfig] Wrote KubeConfig file to disk: \u0026quot;/etc/kubernetes/scheduler.conf\u0026quot; [controlplane] wrote Static Pod manifest for component kube-apiserver to \u0026quot;/etc/kubernetes/manifests/kube-apiserver.yaml\u0026quot; [controlplane] wrote Static Pod manifest for component kube-controller-manager to \u0026quot;/etc/kubernetes/manifests/kube-controller-manager.yaml\u0026quot; [controlplane] wrote Static Pod manifest for component kube-scheduler to \u0026quot;/etc/kubernetes/manifests/kube-scheduler.yaml\u0026quot; [etcd] Wrote Static Pod manifest for a local etcd instance to \u0026quot;/etc/kubernetes/manifests/etcd.yaml\u0026quot; [init] waiting for the kubelet to boot up the control plane as Static Pods from directory \u0026quot;/etc/kubernetes/manifests\u0026quot; [init] this might take a minute or longer if the control plane images have to be pulled [apiclient] All control plane components are healthy after 41.001672 seconds [uploadconfig] storing the configuration used in ConfigMap \u0026quot;kubeadm-config\u0026quot; in the \u0026quot;kube-system\u0026quot; Namespace [kubelet] Creating a ConfigMap \u0026quot;kubelet-config-1.11\u0026quot; in namespace kube-system with the configuration for the kubelets in the cluster [markmaster] Marking the node g2-apigateway as master by adding the label \u0026quot;node-role.kubernetes.io/master=''\u0026quot; [markmaster] Marking the node g2-apigateway as master by adding the taints [node-role.kubernetes.io/master:NoSchedule] [patchnode] Uploading the CRI Socket information \u0026quot;/var/run/dockershim.sock\u0026quot; to the Node API object \u0026quot;g2-apigateway\u0026quot; as an annotation [bootstraptoken] using token: o337m9.ceq32wg9g2gro7gx [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstraptoken] creating the \u0026quot;cluster-info\u0026quot; ConfigMap in the \u0026quot;kube-public\u0026quot; namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026quot;kubectl apply -f [podnetwork].yaml\u0026quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 10.10.207.253:6443 --token t69z6h.lr2etdbg9mfx5r15 --discovery-token-ca-cert-hash sha256:90e3a748c0eb4cb7058f3d0ee8870ee5d746214ab0589b5e841fd5d68fec8f00  这里注意最后一行：\nkubeadm join 10.10.207.253:6443 --token t69z6h.lr2etdbg9mfx5r15 --discovery-token-ca-cert-hash sha256:90e3a748c0eb4cb7058f3d0ee8870ee5d746214ab0589b5e841fd5d68fec8f00  证明集群主节点安装成功，这里要记得保存这条命令，以便之后各个节点加入集群\n配置kubetl认证信息\nexport KUBECONFIG=/etc/kubernetes/admin.conf # 如果你想持久化的话，直接执行以下命令【推荐】 echo \u0026quot;export KUBECONFIG=/etc/kubernetes/admin.conf\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile  安装flanel网络\nmkdir -p /etc/cni/net.d/ cat \u0026lt;\u0026lt;EOF\u0026gt; /etc/cni/net.d/10-flannel.conf { \u0026quot;name\u0026quot;: \u0026quot;cbr0\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;flannel\u0026quot;, \u0026quot;delegate\u0026quot;: { \u0026quot;isDefaultGateway\u0026quot;: true } } EOF mkdir /usr/share/oci-umount/oci-umount.d -p mkdir /run/flannel/ cat \u0026lt;\u0026lt;EOF\u0026gt; /run/flannel/subnet.env FLANNEL_NETWORK=10.244.0.0/16 FLANNEL_SUBNET=10.244.1.0/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true EOF  最后需要新建一个flannel.yml文件：\nimage: caption: \u0026quot;Image from: [**Pexels**](https://www.pexels.com)\u0026quot; focal_point: \u0026quot;\u0026quot; preview_only: false --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel rules: - apiGroups: - \u0026quot;\u0026quot; resources: - pods verbs: - get - apiGroups: - \u0026quot;\u0026quot; resources: - nodes verbs: - list - watch - apiGroups: - \u0026quot;\u0026quot; resources: - nodes/status verbs: - patch image: caption: \u0026quot;Image from: [**Pexels**](https://www.pexels.com)\u0026quot; focal_point: \u0026quot;\u0026quot; preview_only: false --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system image: caption: \u0026quot;Image from: [**Pexels**](https://www.pexels.com)\u0026quot; focal_point: \u0026quot;\u0026quot; preview_only: false --- apiVersion: v1 kind: ServiceAccount metadata: name: flannel namespace: kube-system image: caption: \u0026quot;Image from: [**Pexels**](https://www.pexels.com)\u0026quot; focal_point: \u0026quot;\u0026quot; preview_only: false --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flannel data: cni-conf.json: | { \u0026quot;name\u0026quot;: \u0026quot;cbr0\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;flannel\u0026quot;, \u0026quot;delegate\u0026quot;: { \u0026quot;isDefaultGateway\u0026quot;: true } } net-conf.json: | { \u0026quot;Network\u0026quot;: \u0026quot;10.10.0.0/16\u0026quot;, #这里换成集群所在的网段 \u0026quot;Backend\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;vxlan\u0026quot; } } image: caption: \u0026quot;Image from: [**Pexels**](https://www.pexels.com)\u0026quot; focal_point: \u0026quot;\u0026quot; preview_only: false --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: kube-flannel-ds namespace: kube-system labels: tier: node app: flannel spec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: amd64 tolerations: - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.9.1-amd64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conf volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.9.1-amd64 command: [ \u0026quot;/opt/bin/flanneld\u0026quot;, \u0026quot;--ip-masq\u0026quot;, \u0026quot;--kube-subnet-mgr\u0026quot; ] securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg  执行：\nkubectl create -f ./flannel.yml  默认情况下，master节点不参与工作负载，但如果希望安装出一个all-in-one的k8s环境，则可以执行以下命令：\n让master节点成为一个node节点：\nkubectl taint nodes --all node-role.kubernetes.io/master-  查看节点信息：\nkubectl get nodes  会看到如下的输出：\nNAME STATUS ROLES AGE VERSION k8s-master Ready master 18h v1.11.0  以下是节点配置\n在配置好主节点之后，就可以配置集群的其他节点了，这里建议直接安装之前做好准备工作的系统镜像 进入节点机器之后，直接执行之前保存好的命令\nkubeadm join 10.10.207.253:6443 --token t69z6h.lr2etdbg9mfx5r15 --discovery-token-ca-cert-hash sha256:90e3a748c0eb4cb7058f3d0ee8870ee5d746214ab0589b5e841fd5d68fec8f00  执行完后会看到：\n[preflight] running pre-flight checks [WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs_wrr ip_vs_sh ip_vs ip_vs_rr] or no builtin kernel ipvs support: map[ip_vs_rr:{} ip_vs_wrr:{} ip_vs_sh:{} nf_conntrack_ipv4:{} ip_vs:{}] you can solve this problem with following methods: 1. Run 'modprobe -- ' to load missing kernel modules; 2. Provide the missing builtin kernel ipvs support I0725 09:59:27.929247 10196 kernel_validator.go:81] Validating kernel version I0725 09:59:27.929356 10196 kernel_validator.go:96] Validating kernel config [discovery] Trying to connect to API Server \u0026quot;10.10.207.253:6443\u0026quot; [discovery] Created cluster-info discovery client, requesting info from \u0026quot;https://10.10.207.253:6443\u0026quot; [discovery] Requesting info from \u0026quot;https://10.10.207.253:6443\u0026quot; again to validate TLS against the pinned public key [discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \u0026quot;10.10.207.253:6443\u0026quot; [discovery] Successfully established connection with API Server \u0026quot;10.10.207.253:6443\u0026quot; [kubelet] Downloading configuration for the kubelet from the \u0026quot;kubelet-config-1.11\u0026quot; ConfigMap in the kube-system namespace [kubelet] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot; [kubelet] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot; [preflight] Activating the kubelet service [tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap... [patchnode] Uploading the CRI Socket information \u0026quot;/var/run/dockershim.sock\u0026quot; to the Node API object \u0026quot;k8s-node1\u0026quot; as an annotation This node has joined the cluster: * Certificate signing request was sent to master and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the master to see this node join the cluster.  这里就表示执行完毕了，可以去主节点执行命令：\nkubectl get nodes  可以看到节点已加入集群：\nNAME STATUS ROLES AGE VERSION k8s-master Ready master 20h v1.11.0 k8s-node1 Ready \u0026lt;none\u0026gt; 20h v1.11.0 k8s-node2 Ready \u0026lt;none\u0026gt; 20h v1.11.0  这期间可能需要等待一段时间，状态才会全部变为ready\nkubernetes-dashboard安装 详见：kubernetes安装dashboard\n采坑指南 有时会出现master节点一直处于notready的状态，这里可能是没有启动flannel，只需要按照上面的教程配置好flannel，然后执行：\nkubectl create -f ./flannel.yml  ","date":1534248423,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1534248423,"objectID":"f47500b86b1d8019a23c252f4c415470","permalink":"https://guoxudong.io/post/install-k8s/","publishdate":"2018-08-14T20:07:03+08:00","relpermalink":"/post/install-k8s/","section":"post","summary":"最近由于公司业务发展到了瓶颈，原有的技术架构已经逐渐无法满足业务开发和测试的需求，出现了应用测试环境搭建复杂，有许多套（真的很多很多）应用环境，应用在持续集成/持续交付也遇到了很大的困难，经过讨论研究决定对应用和微服务进行容器化，这就是我首次直面docker和k8s的契机。","tags":["容器","kubernetes"],"title":"centos7.2 安装k8s v1.11.0","type":"post"},{"authors":["郭旭东"],"categories":["部署安装"],"content":" 前言  在使用centos7，并使用荫安装搬运工的时候，往往不希望安装最新版本的搬运工，而是希望安装与自己熟悉或者当前业务环境需要的版本，例如目前Kubernetes支持的最新搬运工版本为v17.03，所以就产生了安装指定版本码头工人的需求。\n 安装步骤 # 安装依赖包 yum install -y yum-utils device-mapper-persistent-data lvm2 # 添加Docker软件包源 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo #关闭测试版本list（只显示稳定版） sudo yum-config-manager --enable docker-ce-edge sudo yum-config-manager --enable docker-ce-test # 更新yum包索引 yum makecache fast #NO.1 直接安装Docker CE （will always install the highest possible version，可能不符合你的需求） yum install docker-ce #NO.2 指定版本安装 yum list docker-ce --showduplicates|sort -r #找到需要安装的 yum install docker-ce-17.09.0.ce -y #启动docker systemctl start docker \u0026amp; systemctl enable docker  采坑指南  当然本着万事皆有坑的原则，这里也是有坑的，在安装中也是会遇到如下的问题\n 在执行一下命令的时候：\nyum install docker-ce-17.03.0.ce -y  会出现如下的报错：\n--\u0026gt; Finished Dependency Resolution Error: Package: docker-ce-17.03.0.ce-1.el7.centos.x86_64 (docker-ce-stable) Requires: docker-ce-selinux \u0026gt;= 17.03.0.ce-1.el7.centos Available: docker-ce-selinux-17.03.0.ce-1.el7.centos.noarch (docker-ce-stable) docker-ce-selinux = 17.03.0.ce-1.el7.centos Available: docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch (docker-ce-stable) docker-ce-selinux = 17.03.1.ce-1.el7.centos Available: docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch (docker-ce-stable) docker-ce-selinux = 17.03.2.ce-1.el7.centos You could try using --skip-broken to work around the problem You could try running: rpm -Va --nofiles --nodigest  在出现这个问题之后，需要执行以下命令：\n#要先安装docker-ce-selinux-17.03.2.ce，否则安装docker-ce会报错 yum install https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm #然后再安装 docker-ce-17.03.2.ce，就能正常安装 yum install docker-ce-17.03.2.ce-1.el7.centos  ","date":1534248321,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1534248321,"objectID":"26a8efe61d330e4635ae4d1e83bc5664","permalink":"https://guoxudong.io/post/install-docker/","publishdate":"2018-08-14T20:05:21+08:00","relpermalink":"/post/install-docker/","section":"post","summary":"在使用CentOS7，并使用荫安装搬运工的时候，往往不希望安装最新版本的搬运工，而是希望安装与自己熟悉或者当前业务环境需要的版本，例如目前Kubernetes支持的最新搬运工版本为v17.03，所以就产生了安装指定版本码头工人的需求。","tags":["容器","Linux"],"title":"centos7安装指定版本的docker","type":"post"},{"authors":null,"categories":null,"content":" 分享二维码     微信分享   扫描分享 扫描二维码，分享精彩博文！\n         function getQueryVariable(variable) { var query = window.location.search.substring(1); var vars = query.split(\"\u0026\"); for (var i = 0; i  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"5cd44e2cce277be566a0bb929960415b","permalink":"https://guoxudong.io/share/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/share/","section":"","summary":" 分享二维码     微信分享   扫描分享 扫描二维码，分享精彩博文！\n         function getQueryVariable(variable) { var query = window.location.search.substring(1); var vars = query.split(\"\u0026\"); for (var i = 0; i  ","tags":null,"title":"","type":"page"}]