<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on GuoXD Blog·郭旭东的博客</title>
    <link>https://guoxudong.io/categories/kubernetes/</link>
    <description>Recent content in kubernetes on GuoXD Blog·郭旭东的博客</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>zh-Hans</language>
    <copyright>&lt;a rel=&#39;license&#39; href=&#39;http://creativecommons.org/licenses/by-nc/4.0/&#39; target=&#39;_blank&#39;&gt;知识共享署名-非商业性使用 4.0 国际许可协议&lt;/a&gt;</copyright>
    <lastBuildDate>Mon, 02 Mar 2020 15:29:16 +0800</lastBuildDate>
    
	    <atom:link href="https://guoxudong.io/categories/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>修改 Nginx Ingress 日志打印格式</title>
      <link>https://guoxudong.io/post/nginx-ingress-log-aliyun/</link>
      <pubDate>Mon, 02 Mar 2020 15:29:16 +0800</pubDate>
      
      <guid>https://guoxudong.io/post/nginx-ingress-log-aliyun/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;最近接到一个需求，需要展示 ingress 上面的访问日志，由于我们的业务系统都部署在 Kubernetes 上面，通过 ingress 进行访问，所以这里的访问日志，其实就是我们全部业务系统的访问日志。&lt;/p&gt;

&lt;p&gt;日志采集方面，阿里云天生就提供了 nginx-ingress 日志和采集和展示，本身提供很多不错的基于 ingress 日志数据的图表与分析。如果你使用的是阿里云 ACK 容器服务，那么极端推荐使用，配置方法见官方文档：&lt;a href=&#34;https://help.aliyun.com/document_detail/86532.html。&#34; target=&#34;_blank&#34;&gt;https://help.aliyun.com/document_detail/86532.html。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva2.sinaimg.cn/large/ad5fbf65gy1gcfmo5d410j21970nzwjg.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;让人头秃的是，我们这次不但要采集 ingress 日志上比较常规的 &lt;code&gt;url&lt;/code&gt; &lt;code&gt;client_ip&lt;/code&gt; &lt;code&gt;method&lt;/code&gt; &lt;code&gt;status&lt;/code&gt; 等字段，还要采集我们系统在 &lt;code&gt;Request Headers&lt;/code&gt; 里面自定义的参数，这些参数是默认的 ingress 并不展示的，所以需要我们进行调整。&lt;/p&gt;

&lt;h2 id=&#34;开始&#34;&gt;开始&lt;/h2&gt;

&lt;p&gt;首先明确需要调整的组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nginx-ingress&lt;/code&gt; 的 ConfigMap：用于打印自定义日志字段&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AliyunLogConfig&lt;/code&gt;：这个是阿里云日志服务的 CRD 扩展，需要在这个里面加入新增的字段名和修改后的正则表达式&lt;/li&gt;
&lt;li&gt;在日志服务控制台，添加新增字段的指定字段查询&lt;/li&gt;
&lt;li&gt;新增展示仪表盘&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;调整-ingress-日志输出&#34;&gt;调整 ingress 日志输出&lt;/h3&gt;

&lt;p&gt;我们 ingress 组件使用的是 &lt;code&gt;nginx-ingress-container&lt;/code&gt;，这里要调整日志输出格式，老规矩，直接官方文档：&lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/。&#34; target=&#34;_blank&#34;&gt;https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;从文档可见，只需要调整 &lt;code&gt;ingress-nginx&lt;/code&gt; 的 ConfigMap &lt;code&gt;nginx-configuration&lt;/code&gt; data 中的 &lt;code&gt;log-format-upstream&lt;/code&gt; 字段即可。&lt;/p&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;知识点：&lt;/p&gt;

&lt;p&gt;官方文档里面给的说明不是很详细，没有提到 &lt;code&gt;Request Headers&lt;/code&gt; 里自定义的字段应该怎么表示（也有可能是我眼瘸没看见），但经过我多次试验发现，&lt;code&gt;Request Headers&lt;/code&gt; 里的字段在 &lt;code&gt;log-format-upstream&lt;/code&gt; 中应该使用 &lt;code&gt;$http_{your field}&lt;/code&gt; 表示，比如 &lt;code&gt;$http_cookie&lt;/code&gt;；而带 &lt;code&gt;-&lt;/code&gt; 的字段则需要将 &lt;code&gt;-&lt;/code&gt; 改为 &lt;code&gt;_&lt;/code&gt;，并且使用小写，比如 &lt;code&gt;app-Id&lt;/code&gt; 就应使用 &lt;code&gt;$http_app_id&lt;/code&gt; 表示。&lt;/p&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;修改 ConfigMap，&lt;code&gt;ingress-controller&lt;/code&gt; 将进行热更新，看到如下日志，就证明配置已完成更新，接下来就可以看到你自定义字段的值已经打印出来了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;I0302 08:20:58.393365 9 controller.go:200] Backend successfully reloaded.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;调整阿里云日志组件配置&#34;&gt;调整阿里云日志组件配置&lt;/h3&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    执行下面的步骤请确保已经按照&lt;a href=&#34;https://help.aliyun.com/document_detail/86532.html&#34; target=&#34;_blank&#34;&gt;官方文档&lt;/a&gt;正确部署阿里云日志服务在您的 K8S 集群之后，并且已达到要求的版本。
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;日志已经成功打印了，接下来就是调整日志采集的字段了，这里只需要调整日志服务 CRD 的扩展配置即可。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl edit AliyunLogConfig k8s-nginx-ingress
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在修改配置之前，推荐先去 &lt;a href=&#34;https://regex101.com/&#34; target=&#34;_blank&#34;&gt;https://regex101.com/&lt;/a&gt; 验证正则表达式是否正确，将调整过的正则表达式和 &lt;code&gt;ingress-controller&lt;/code&gt; 打印的日志贴入下图指定位置，就可以看出正则表达式是否正确。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax1.sinaimg.cn/large/ad5fbf65gy1gcfo9lxuc6j21gv0juwka.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后将添加的字段名称（这个名称将作为 key 在日志服务中展示，可以与 header 中的字段不同）和正则表达式贴入如下 CRD 中。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: log.alibabacloud.com/v1alpha1
kind: AliyunLogConfig
metadata:
  # your config name, must be unique in you k8s cluster
  name: k8s-nginx-ingress
spec:
  # logstore name to upload log
  logstore: nginx-ingress
  # product code, only for k8s nginx ingress
  productCode: k8s-nginx-ingress
  # logtail config detail
  logtailConfig:
    inputType: plugin
    # logtail config name, should be same with [metadata.name]
    configName: k8s-nginx-ingress
    inputDetail:
      plugin:
        inputs:
        - type: service_docker_stdout
          detail:
            IncludeLabel:
              io.kubernetes.container.name: nginx-ingress-controller
            Stderr: false
            Stdout: true
        processors:
        - type: processor_regex
          detail:
            KeepSource: false
            Keys:
            - client_ip
            - x_forward_for
            - remote_user
            - time
            - method
            - url
            - version
            - status
            - body_bytes_sent
            - http_referer
            - http_user_agent
            - request_length
            - request_time
            - proxy_upstream_name
            - upstream_addr
            - upstream_response_length
            - upstream_response_time
            - upstream_status
            - req_id
            - host
            - #需要添加的字段名称
            - ...
            NoKeyError: true
            NoMatchError: true
            Regex: #修改后的正则表达式
            SourceKey: content
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;日志控制台新增字段&#34;&gt;日志控制台新增字段&lt;/h3&gt;

&lt;p&gt;如果上面的操作无误的话，日志服务中就会展示您添加的字段了，如果配置有误，所有的自定义字段都会不显示，只会显示保留字段名称。&lt;/p&gt;

&lt;p&gt;添加指定字段查询，就可以快速查看添加的字段了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva3.sinaimg.cn/large/ad5fbf65gy1gcfohy9fv4j21460gxtc6.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;新增展示仪表盘&#34;&gt;新增展示仪表盘&lt;/h3&gt;

&lt;p&gt;日志既然已经取到了，那么展示就很容易了，直接在查询栏中输入分析语句，日志服务支持 SQL 聚合日志，并直接生成统计图表，点击添加到仪表盘可以就可以添加到现有仪表盘或者新建一个仪表盘。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva2.sinaimg.cn/large/ad5fbf65gy1gcfos33c23j219a0nuae3.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;成果&#34;&gt;成果&lt;/h2&gt;

&lt;p&gt;之后进行一些微调，添加过滤栏，由于这里统计的是登录用户，你甚至都可以添加一个词云来看看哪些用于使用系统比较频繁。当然，想添加什么都看您的喜好，日志在你手里，想怎么分析都可以。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva4.sinaimg.cn/large/ad5fbf65gy1gcfowk10tjj21970ns79i.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;本次实现的功能并不是什么高深的功能，只不过是一个简单的访问日志记录和展示，相信每个系统其实都有一套这种功能。但是这种实现方式在我看来优点更多：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;无代码：全程没有写一行代码，如果有的话，也就是业务需要统一 &lt;code&gt;Request Headers&lt;/code&gt; 里面的字段。&lt;/li&gt;
&lt;li&gt;配置简单：只需要修改 nginx ConfigMap 中的一个字段，并在 CRD 中添加字段名称和正在表达式，唯一的难度可能就是正则表达式。&lt;/li&gt;
&lt;li&gt;配置快：整体的配置时间很短，加上查文档和调整图表也不过半天的时间，肯定比 &lt;code&gt;提需求-评估-开发-测试-验收&lt;/code&gt; 全流程走一遍，前端后端撕一遍要快的多的多的多。&lt;/li&gt;
&lt;li&gt;高度定制：可以根据自己的喜好，随意定制图表。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;最近发现阿里云日志服务是一个宝藏产品，从安全到 k8s 业务，从成本控制到疫情动态，日志服务真的就是把所有没有前端开发资源的服务都帮了一把。
&amp;mdash; 摘自本人朋友圈&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Kubecm：管理你的 kubeconfig</title>
      <link>https://guoxudong.io/post/kubecm/</link>
      <pubDate>Mon, 09 Dec 2019 10:07:46 +0800</pubDate>
      
      <guid>https://guoxudong.io/post/kubecm/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;该项目脱胎于 &lt;a href=&#34;https://github.com/sunny0826/mergeKubeConfig&#34; target=&#34;_blank&#34;&gt;mergeKubeConfig&lt;/a&gt; 项目，最早写该项目的目的是在一堆杂乱无章的 kubeconfig 中自由的切换。随着需要操作的 Kubernetes 集群越来越多，在不同的集群之间切换也越来越麻烦，而操作 Kubernetes 集群的本质不过是通过 &lt;code&gt;kubeconfig&lt;/code&gt; 访问 Kubernetes 集群的 API Server，以操作 Kubernetes 的各种资源，而 &lt;code&gt;kubeconfig&lt;/code&gt; 不过是一个 yaml 文件，用来保存访问集群的密钥，最早的 &lt;a href=&#34;https://github.com/sunny0826/mergeKubeConfig&#34; target=&#34;_blank&#34;&gt;mergeKubeConfig&lt;/a&gt; 不过是一个操作 yaml 文件的 Python 脚本。而随着 golang 学习的深入，也就动了重写这个项目的念头，就这样 &lt;a href=&#34;https://github.com/sunny0826/kubecm&#34; target=&#34;_blank&#34;&gt;kubecm&lt;/a&gt; 诞生了。&lt;/p&gt;

&lt;h2 id=&#34;kubecm&#34;&gt;kubecm&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/sunny0826/kubecm&#34; target=&#34;_blank&#34;&gt;kubecm&lt;/a&gt; 由 golang 编写，支持 &lt;code&gt;Mac&lt;/code&gt; &lt;code&gt;Linux&lt;/code&gt; 和 &lt;code&gt;windows&lt;/code&gt; 平台，&lt;code&gt;delete&lt;/code&gt; &lt;code&gt;rename&lt;/code&gt; &lt;code&gt;switch&lt;/code&gt; 提供比较实用的交互式的操作，目前的功能包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;add ：添加新的 &lt;code&gt;kubeconfig&lt;/code&gt; 到 &lt;code&gt;$HOME/.kube/config&lt;/code&gt; 中&lt;/li&gt;
&lt;li&gt;completion ：命令行自动补全功能&lt;/li&gt;
&lt;li&gt;delete：删除已有的 &lt;code&gt;context&lt;/code&gt; ，提供交互式和指定删除两种方式&lt;/li&gt;
&lt;li&gt;merge：将指定目录中的 &lt;code&gt;kubeconfig&lt;/code&gt; 合并为一个 &lt;code&gt;kubeconfig&lt;/code&gt; 文件&lt;/li&gt;
&lt;li&gt;rename：重名指定的 &lt;code&gt;context&lt;/code&gt;，提供交互式和指定重命名两种方式&lt;/li&gt;
&lt;li&gt;switch：交互式切换 &lt;code&gt;context&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/sunny0826/kubecm&#34; target=&#34;_blank&#34;&gt;kubecm&lt;/a&gt; 支持 &lt;code&gt;Mac&lt;/code&gt; &lt;code&gt;Linux&lt;/code&gt; 和 &lt;code&gt;windows&lt;/code&gt; 平台，安装方式也比较简单：&lt;/p&gt;

&lt;h4 id=&#34;macos&#34;&gt;MacOS&lt;/h4&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    使用 &lt;code&gt;brew&lt;/code&gt; 或者直接下载二进制可执行文件
  &lt;/div&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install sunny0826/tap/kubecm
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;linux&#34;&gt;Linux&lt;/h4&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    下载二进制可执行文件
  &lt;/div&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# linux x86_64
curl -Lo kubecm.tar.gz https://github.com/sunny0826/kubecm/releases/download/v${VERSION}/kubecm_${VERSION}_Linux_x86_64.tar.gz
tar -zxvf kubecm.tar.gz kubecm
cd kubecm
sudo mv kubecm /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;windows&#34;&gt;Windows&lt;/h4&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    下载二进制可执行文件，并将文件移动到 &lt;code&gt;$PATH&lt;/code&gt; 中即可
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;命令行自动补全&#34;&gt;命令行自动补全&lt;/h2&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;a href=&#34;https://github.com/sunny0826/kubecm&#34; target=&#34;_blank&#34;&gt;kubecm&lt;/a&gt; 提供了和 &lt;a href=&#34;https://github.com/kubernetes/kubectl&#34; target=&#34;_blank&#34;&gt;kubectl&lt;/a&gt; 一样的 completion 命令行自动补全功能（支持 bash/zsh）
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;以 &lt;code&gt;zsh&lt;/code&gt; 为例，在 &lt;code&gt;$HOME/.zshrc&lt;/code&gt; 中添加&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-vim&#34;&gt;source &amp;lt;(kubecm completion zsh)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后使用 &lt;code&gt;source&lt;/code&gt; 命令，使其生效&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-zsh&#34;&gt;source $HOME/.zshrc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后，在输入 &lt;code&gt;kubecm&lt;/code&gt; 后按 &lt;kbd&gt;tab&lt;/kbd&gt; 键，就可以看到命令行自动补全的内容&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva2.sinaimg.cn/large/ad5fbf65gy1g9qa0yy3bvj21co0f2hdt.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;操作-kubeconfig&#34;&gt;操作 kubeconfig&lt;/h2&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;a href=&#34;https://github.com/sunny0826/kubecm&#34; target=&#34;_blank&#34;&gt;kubecm&lt;/a&gt; 可以实现 &lt;code&gt;kubeconfig&lt;/code&gt; 的查看、添加、删除、合并、重命名和切换
  &lt;/div&gt;
&lt;/div&gt;

&lt;h4 id=&#34;查看&#34;&gt;查看&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 查看 $HOME/.kube/config 中所有的 context
kubecm
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;添加&#34;&gt;添加&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 添加 example.yaml 到 $HOME/.kube/config.yaml，该方式不会覆盖源 kubeconfig，只会在当前目录中生成一个 config.yaml 文件
kubecm add -f example.yaml

# 功能同上，但是会将 example.yaml 中的 context 命名为 test
kubecm add -f example.yaml -n test

# 添加 -c 会覆盖源 kubeconfig
kubecm add -f example.yaml -c
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;删除&#34;&gt;删除&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 交互式删除
kubecm delete
# 删除指定 context
kubecm delete my-context
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;合并&#34;&gt;合并&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 合并 test 目录中的 kubeconfig,该方式不会覆盖源 kubeconfig，只会在当前目录中生成一个 config.yaml 文件
kubecm merge -f test 

# 添加 -c 会覆盖源 kubeconfig
kubecm merge -f test -c
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;重命名&#34;&gt;重命名&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 交互式重命名
kubecm rename
# 将 dev 重命名为 test
kubecm rename -o dev -n test
# 重命名 current-context 为 dev
kubecm rename -n dev -c
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;切换默认-namespace&#34;&gt;切换默认 namespace&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 交互式切换 namespace
kubecm namespace
# 或者
kubecm ns
# 切换默认 namespace 为 kube-system
kubecm ns kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;效果展示&#34;&gt;效果展示&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;Interaction.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;视频介绍&#34;&gt;视频介绍&lt;/h2&gt;


&lt;div style=&#34;position: relative; width: 100%; height: 0; padding-bottom: 75%;&#34;&gt;
    &lt;iframe src=&#34;//player.bilibili.com/player.html?aid=88259938&amp;amp;cid=150776221&amp;amp;page=1&#34; scrolling=&#34;no&#34; border=&#34;0&#34; frameborder=&#34;no&#34; framespacing=&#34;0&#34; allowfullscreen=&#34;true&#34;style=&#34;position: absolute; width: 100%; height: 100%; left: 0; top: 0;&#34;&gt; &lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/sunny0826/kubecm&#34; target=&#34;_blank&#34;&gt;kubecm&lt;/a&gt; 项目的初衷为学习 golang 并熟悉 client-go 的使用，随着使用的深入，断断续续增加了不少功能，开发出了一个看上去还算正规的项目。总的来说都是根据自己的喜好来开发的业余项目，欢迎各位通过 &lt;a href=&#34;https://github.com/sunny0826/kubecm/issues/new&#34; target=&#34;_blank&#34;&gt;ISSUE&lt;/a&gt; 来进行交流和讨论。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>小工具介绍：KubeWatch</title>
      <link>https://guoxudong.io/post/kubewatch/</link>
      <pubDate>Wed, 04 Dec 2019 17:09:51 +0800</pubDate>
      
      <guid>https://guoxudong.io/post/kubewatch/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;这次要介绍一个 Kubernetes 资源观测工具，实时监控 Kubernetes 集群中各种资源的新建、更新和删除，并实时通知到各种协作软件/聊天软件，目前支持的通知渠道有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;slack&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hipchat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mattermost&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;flock&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;webhook&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我这边开发了钉钉的通知渠道，但是在上游 &lt;a href=&#34;https://github.com/bitnami-labs/kubewatch/issues/198&#34; target=&#34;_blank&#34;&gt;ISSUE#198&lt;/a&gt; 中提出的贡献请求并没有得到回应，所以这边只能 fork 了代码，然后自己进行了开发，以支持钉钉通知。&lt;/p&gt;

&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;

&lt;p&gt;这里推荐使用 helm 进行安装，快速部署&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm install kubewatch stable/kubewatch \
--set rbac.create=true \
--set slack.channel=&#39;#YOUR_CHANNEL&#39; \
--set slack.token=&#39;xoxb-YOUR_TOKEN&#39; \
--set resourcesToWatch.pod=true \
--set resourcesToWatch.daemonset=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想使用钉钉通知，则可以在 &lt;a href=&#34;https://github.com/sunny0826/kubewatch-chat&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; 上拉取我的代码，代码中包含 helm chart 包，可直接进行安装&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/sunny0826/kubewatch-chat.git
cd kubewatch-chat
helm install kubewatch kubewatch \
--set dingtalk.sign=&amp;quot;XXX&amp;quot; \
--set dingtalk.token=&amp;quot;XXXX-XXXX-XXXX&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;钉钉配置&#34;&gt;钉钉配置&lt;/h2&gt;

&lt;p&gt;在钉钉中创建 &lt;code&gt;智能群助手&lt;/code&gt; ，之后&lt;/p&gt;

&lt;h3 id=&#34;获取-token&#34;&gt;获取 token&lt;/h3&gt;

&lt;p&gt;复制的 webhook 中 &lt;code&gt;https://oapi.dingtalk.com/robot/send?access_token={YOUR_TOKEN}&lt;/code&gt;, &lt;code&gt;{YOUR_TOKEN}&lt;/code&gt; 就是要填入的 token。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva4.sinaimg.cn/large/ad5fbf65gy1g9ku2hvs16j20ep05smxk.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;安全设置&#34;&gt;安全设置&lt;/h2&gt;

&lt;p&gt;钉钉智能群助手在更新后新增了安全设置，提供三种验证方式 &lt;code&gt;自定义关键词&lt;/code&gt; &lt;code&gt;加签&lt;/code&gt; &lt;code&gt;IP地址（段）&lt;/code&gt;，这里推荐使用 &lt;code&gt;IP地址（段）的方式&lt;/code&gt;，直接将 Kubernetes 集群的出口 IP 填入设置即可。同时也提供了 &lt;code&gt;加签&lt;/code&gt; 的方式，拷贝秘钥，将其填入 &lt;code&gt;dingtalk.sign&lt;/code&gt; 中。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/ad5fbf65gy1g9ku6qjwy2j20fo077glw.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;项目配置&#34;&gt;项目配置&lt;/h2&gt;

&lt;p&gt;编辑 &lt;code&gt;kubewatch/value.yaml&lt;/code&gt; ，修改配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;## Global Docker image parameters
## Please, note that this will override the image parameters, including dependencies, configured to use the global value
## Current available global Docker image parameters: imageRegistry and imagePullSecrets
##
# global:
#   imageRegistry: myRegistryName
#   imagePullSecrets:
#     - myRegistryKeySecretName

slack:
  enabled: false
  channel: &amp;quot;&amp;quot;
  token: &amp;quot;xoxb&amp;quot;

hipchat:
  enabled: false
  # room: &amp;quot;&amp;quot;
  # token: &amp;quot;&amp;quot;
  # url: &amp;quot;&amp;quot;
mattermost:
  enabled: false
  # channel: &amp;quot;&amp;quot;
  # url: &amp;quot;&amp;quot;
  # username: &amp;quot;&amp;quot;
flock:
  enabled: false
  # url: &amp;quot;&amp;quot;
webhook:
  enabled: false
  # url: &amp;quot;&amp;quot;
dingtalk:
  enabled: true
  token: &amp;quot;&amp;quot;
  sign: &amp;quot;&amp;quot;

# namespace to watch, leave it empty for watching all.
namespaceToWatch: &amp;quot;&amp;quot;

# Resources to watch
resourcesToWatch:
  deployment: true
  replicationcontroller: false
  replicaset: false
  daemonset: false
  services: false
  pod: true
  job: false
  persistentvolume: false

image:
  registry: docker.io
#  repository: bitnami/kubewatch
  repository: guoxudongdocker/kubewatch-chart
#  tag: 0.0.4-debian-9-r405
  tag: latest
  pullPolicy: Always
  ## Optionally specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ##
  # pullSecrets:
  #   - myRegistryKeySecretName

## String to partially override kubewatch.fullname template (will maintain the release name)
##
# nameOverride:

## String to fully override kubewatch.fullname template
##
# fullnameOverride:

rbac:
  # If true, create &amp;amp; use RBAC resources
  #
  create: true

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

resources: {}
  # limits:
  #   cpu: 100m
  #   memory: 300Mi
  # requests:
  #   cpu: 100m
  #   memory: 300Mi

# Affinity for pod assignment
# Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
# affinity: {}

# Tolerations for pod assignment
# Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: []

# Node labels for pod assignment
# Ref: https://kubernetes.io/docs/user-guide/node-selection/
nodeSelector: {}

podAnnotations: {}
podLabels: {}
replicaCount: 1

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 &lt;code&gt;value.yaml&lt;/code&gt; 安装&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/sunny0826/kubewatch-chat.git
cd kubewatch-chat
helm install my-release -f kubewatch/values.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;slack-配置&#34;&gt;Slack 配置&lt;/h2&gt;

&lt;p&gt;Slack 为 kubewatch 默认的通知软件，这里就不简介 Slack 的安装和注册，直接从创建 APP 开始&lt;/p&gt;

&lt;h3 id=&#34;创建一个-app&#34;&gt;创建一个 APP&lt;/h3&gt;

&lt;p&gt;进去创建 &lt;a href=&#34;https://api.slack.com/apps&#34; target=&#34;_blank&#34;&gt;APP 页面&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/ad5fbf65gy1g9kum3x5npj21h40p6tdx.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;选择 &lt;code&gt;App Name&lt;/code&gt; 和 &lt;code&gt;Development Slack Workspace&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/ad5fbf65gy1g9kupp0av1j210c0uejvj.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;添加-bot-用户&#34;&gt;添加 Bot 用户&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax3.sinaimg.cn/large/ad5fbf65gy1g9kuszmgggj21n4156gu2.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;添加-app-到-workspace&#34;&gt;添加 App 到 Workspace&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://tva3.sinaimg.cn/large/ad5fbf65gy1g9kuyzwzetj21qu0wmq9n.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;获取-bot-token&#34;&gt;获取 Bot-token&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax3.sinaimg.cn/large/ad5fbf65gy1g9kv06dva8j21s60uajxf.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;通知效果&#34;&gt;通知效果&lt;/h2&gt;

&lt;p&gt;在 Slack 中，&lt;code&gt;创建&lt;/code&gt; &lt;code&gt;更新&lt;/code&gt; &lt;code&gt;删除&lt;/code&gt; 分别以绿、黄和红色代表&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax1.sinaimg.cn/large/ad5fbf65gy1g9kv23nvmoj213c0mewj4.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在钉钉中，我进行了汉化&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax3.sinaimg.cn/large/ad5fbf65gy1g9kv5fppglj20dd08zdgs.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax4.sinaimg.cn/large/ad5fbf65gy1g9kv5uuxn4j20ea08fgmk.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;对于 kubewatch 我们这里主要用作监控各种 CronJob 的定时触发状态，已经 ConfigMap 和 Secrets 的状态变化，同时也观察 HPA 触发的弹性伸缩的状态，可以实时观测到业务高峰的到来，是一个不错的小工具。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Velero 进行集群备份与迁移</title>
      <link>https://guoxudong.io/post/aliyun-velero/</link>
      <pubDate>Wed, 13 Nov 2019 09:13:22 +0800</pubDate>
      
      <guid>https://guoxudong.io/post/aliyun-velero/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;在近日的一个风和日丽的下午，正在快乐的写 bug 时，突然间钉钉就被 call 爆了，原来是 k8s 测试集群的一个 namespace 突然不见了。这个 namespace 里面有 60 多个服务，瞬间全部没有了……虽然得益于我们的 CI/CD 系统，这些服务很快都重新部署并正常运行了，但是如果在生产环境，那后果就是不可想象的了。在排查这个问题发生的原因的同时，集群资源的灾备和恢复功能就提上日程了，这时 Velero 就出现了。&lt;/p&gt;

&lt;h2 id=&#34;velero&#34;&gt;Velero&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/vmware-tanzu/velero&#34; target=&#34;_blank&#34;&gt;Velero&lt;/a&gt; 是 VMWare 开源的 k8s 集群备份、迁移工具。可以帮助我们完成 k8s 的例行备份工作，以便在出现上面问题的时候可以快速进行恢复。同时也提供了集群迁移功能，可以将 k8s 资源迁移到其他 k8s 集群的功能。Velero 将集群资源保存在对象存储中，默认情况下可以使用 &lt;a href=&#34;https://velero.io/docs/v1.1.0/aws-config&#34; target=&#34;_blank&#34;&gt;AWS&lt;/a&gt;、&lt;a href=&#34;https://velero.io/docs/v1.1.0/azure-config&#34; target=&#34;_blank&#34;&gt;Azure&lt;/a&gt;、&lt;a href=&#34;https://velero.io/docs/v1.1.0/gcp-config&#34; target=&#34;_blank&#34;&gt;GCP&lt;/a&gt; 的对象存储，同时也给出了插件功能用来拓展其他平台的存储，这里我们用到的就是阿里云的对象存储 OSS，阿里云也提供了 Velero 的插件，用于将备份存储到 OSS 中。下面我就介绍一下如何在阿里云容器服务 ACK 使用 Velero 完成备份和迁移。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Velero 地址：&lt;a href=&#34;https://github.com/vmware-tanzu/velero&#34; target=&#34;_blank&#34;&gt;https://github.com/vmware-tanzu/velero&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ACK 插件地址：&lt;a href=&#34;https://github.com/AliyunContainerService/velero-plugin&#34; target=&#34;_blank&#34;&gt;https://github.com/AliyunContainerService/velero-plugin&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;下载-velero-客户端&#34;&gt;下载 Velero 客户端&lt;/h3&gt;

&lt;p&gt;Velero 由客户端和服务端组成，服务器部署在目标 k8s 集群上，而客户端则是运行在本地的命令行工具。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;前往 &lt;a href=&#34;https://github.com/vmware-tanzu/velero/releases&#34; target=&#34;_blank&#34;&gt;Velero 的 Release 页面&lt;/a&gt; 下载客户端，直接在 GitHub 上下载即可&lt;/li&gt;
&lt;li&gt;解压 release 包&lt;/li&gt;
&lt;li&gt;将 release 包中的二进制文件 &lt;code&gt;velero&lt;/code&gt; 移动到 &lt;code&gt;$PATH&lt;/code&gt; 中的某个目录下&lt;/li&gt;
&lt;li&gt;执行 &lt;code&gt;velero -h&lt;/code&gt; 测试&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;创建-oss-bucket&#34;&gt;创建 OSS bucket&lt;/h3&gt;

&lt;p&gt;创建一个 OSS bucket 用于存储备份文件，这里也可以用已有的 bucket，之后会在 bucket 中创建 &lt;code&gt;backups&lt;/code&gt;、&lt;code&gt;metadata&lt;/code&gt;、&lt;code&gt;restores&lt;/code&gt;三个目录，这里建议在已有的 bucket 中创建一个子目录用于存储备份文件。&lt;/p&gt;

&lt;p&gt;创建 OSS 的时候一定要选对区域，要和 ACK 集群在同一个区域，存储类型和读写权限选择&lt;strong&gt;标准存储&lt;/strong&gt;和&lt;strong&gt;私有&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva3.sinaimg.cn/wap720/ad5fbf65gy1g8w7t8c4xbj21021d8thq.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;创建阿里云-ram-用户&#34;&gt;创建阿里云 RAM 用户&lt;/h3&gt;

&lt;p&gt;这里需要创建一个阿里云 RAM 的用户，用于操作 OSS 以及 ACK 资源。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;新建权限策略&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax4.sinaimg.cn/large/ad5fbf65gy1g8w80cjiv2j21uo18cag8.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;策略内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;Version&amp;quot;: &amp;quot;1&amp;quot;,
    &amp;quot;Statement&amp;quot;: [
        {
            &amp;quot;Action&amp;quot;: [
                &amp;quot;ecs:DescribeSnapshots&amp;quot;,
                &amp;quot;ecs:CreateSnapshot&amp;quot;,
                &amp;quot;ecs:DeleteSnapshot&amp;quot;,
                &amp;quot;ecs:DescribeDisks&amp;quot;,
                &amp;quot;ecs:CreateDisk&amp;quot;,
                &amp;quot;ecs:Addtags&amp;quot;,
                &amp;quot;oss:PutObject&amp;quot;,
                &amp;quot;oss:GetObject&amp;quot;,
                &amp;quot;oss:DeleteObject&amp;quot;,
                &amp;quot;oss:GetBucket&amp;quot;,
                &amp;quot;oss:ListObjects&amp;quot;
            ],
            &amp;quot;Resource&amp;quot;: [
                &amp;quot;*&amp;quot;
            ],
            &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;新建用户&lt;/p&gt;

&lt;p&gt;在新建用户的时候要选择 &lt;code&gt;编程访问&lt;/code&gt;，来获取 &lt;code&gt;AccessKeyID&lt;/code&gt; 和 &lt;code&gt;AccessKeySecret&lt;/code&gt;，这里请创建一个新用于用于备份，不要使用老用户的 AK 和 AS。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax2.sinaimg.cn/large/ad5fbf65gy1g8w8h4ek4uj21h40ue785.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;部署服务端&#34;&gt;部署服务端&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;拉取 &lt;a href=&#34;https://github.com/AliyunContainerService/velero-plugin&#34; target=&#34;_blank&#34;&gt;Velero 插件&lt;/a&gt; 到本地&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/AliyunContainerService/velero-plugin
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置修改&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;修改 &lt;code&gt;install/credentials-velero&lt;/code&gt; 文件，将新建用户中获得的 &lt;code&gt;AccessKeyID&lt;/code&gt; 和 &lt;code&gt;AccessKeySecret&lt;/code&gt; 填入，这里的 OSS EndPoint 为之前 OSS 的访问域名（&lt;strong&gt;注：这里需要选择外网访问的 EndPoint。&lt;/strong&gt;）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax2.sinaimg.cn/large/ad5fbf65gy1g8w8xd1sgzj21c20cm75z.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ALIBABA_CLOUD_ACCESS_KEY_ID=&amp;lt;ALIBABA_CLOUD_ACCESS_KEY_ID&amp;gt;
ALIBABA_CLOUD_ACCESS_KEY_SECRET=&amp;lt;ALIBABA_CLOUD_ACCESS_KEY_SECRET&amp;gt;
ALIBABA_CLOUD_OSS_ENDPOINT=&amp;lt;ALIBABA_CLOUD_OSS_ENDPOINT&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改 &lt;code&gt;install/01-velero.yaml&lt;/code&gt;，将 OSS 配置填入：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;---
apiVersion: velero.io/v1
kind: BackupStorageLocation
metadata:
labels:
    component: velero
name: default
namespace: velero
spec:
config: {}
objectStorage:
    bucket: &amp;lt;ALIBABA_CLOUD_OSS_BUCKET&amp;gt;  # OSS bucket 名称
    prefix: &amp;lt;OSS_PREFIX&amp;gt;    # bucket 子目录
provider: alibabacloud
---
apiVersion: velero.io/v1
kind: VolumeSnapshotLocation
metadata:
labels:
    component: velero
name: default
namespace: velero
spec:
config:
    region: &amp;lt;REGION&amp;gt;    # 地域，如果是华东2（上海），则为 cn-shanghai
provider: alibabacloud
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;k8s 部署 Velero 服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 新建 namespace
kubectl create namespace velero
# 部署 credentials-velero 的 secret
kubectl create secret generic cloud-credentials --namespace velero --from-file cloud=install/credentials-velero
# 部署 CRD
kubectl apply -f install/00-crds.yaml
# 部署 Velero
kubectl apply -f install/01-velero.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;测试 Velero 状态&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ velero version
Client:
    Version: v1.1.0
    Git commit: a357f21aec6b39a8244dd23e469cc4519f1fe608
Server:
    Version: v1.1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到 Velero 的客户端和服务端已经部署成功。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;服务端清理&lt;/p&gt;

&lt;p&gt;在完成测试或者需要重新安装时，执行如下命令进行清理：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl delete namespace/velero clusterrolebinding/velero
kubectl delete crds -l component=velero
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;备份测试&#34;&gt;备份测试&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;velero-plugin&lt;/code&gt; 项目中已经给出 &lt;code&gt;example&lt;/code&gt; 用于测试备份。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;部署测试服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -f examples/base.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对 &lt;code&gt;nginx-example&lt;/code&gt; 所在的 namespace 进行备份&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero backup create nginx-backup --include-namespaces nginx-example --wait
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;模拟 namespace 被误删&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl delete namespaces nginx-example
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用 Velero 进行恢复&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero restore create --from-backup nginx-backup --wait
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;集群迁移&#34;&gt;集群迁移&lt;/h3&gt;

&lt;p&gt;迁移方法同备份，在备份后切换集群，在新集群恢复备份即可。&lt;/p&gt;

&lt;h3 id=&#34;高级用法&#34;&gt;高级用法&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定时备份&lt;/p&gt;

&lt;p&gt;对集群资源进行定时备份，则可在发生意外的情况下，进行恢复（默认情况下，备份保留 30 天）。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 每日1点进行备份
velero create schedule &amp;lt;SCHEDULE NAME&amp;gt; --schedule=&amp;quot;0 1 * * *&amp;quot;
# 每日1点进行备份，备份保留48小时
velero create schedule &amp;lt;SCHEDULE NAME&amp;gt; --schedule=&amp;quot;0 1 * * *&amp;quot; --ttl 48h
# 每6小时进行一次备份
velero create schedule &amp;lt;SCHEDULE NAME&amp;gt; --schedule=&amp;quot;@every 6h&amp;quot;
# 每日对 web namespace 进行一次备份
velero create schedule &amp;lt;SCHEDULE NAME&amp;gt; --schedule=&amp;quot;@every 24h&amp;quot; --include-namespaces web
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;定时备份的名称为：&lt;code&gt;&amp;lt;SCHEDULE NAME&amp;gt;-&amp;lt;TIMESTAMP&amp;gt;&lt;/code&gt;，恢复命令为：&lt;code&gt;velero restore create --from-backup &amp;lt;SCHEDULE NAME&amp;gt;-&amp;lt;TIMESTAMP&amp;gt;&lt;/code&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;备份删除&lt;/p&gt;

&lt;p&gt;直接执行命令进行删除&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero delete backups &amp;lt;BACKUP_NAME&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;备份资源查看&lt;/p&gt;

&lt;p&gt;备份查看&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero backup get
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看定时备份&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero schedule get
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看可恢复备份&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;velero restore get
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;备份排除项目&lt;/p&gt;

&lt;p&gt;可为资源添加指定标签，添加标签的资源在备份的时候被排除。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 添加标签
kubectl label -n &amp;lt;ITEM_NAMESPACE&amp;gt; &amp;lt;RESOURCE&amp;gt;/&amp;lt;NAME&amp;gt; velero.io/exclude-from-backup=true
# 为 default namespace 添加标签
kubectl label -n default namespace/default velero.io/exclude-from-backup=true
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;问题汇总&#34;&gt;问题汇总&lt;/h3&gt;

&lt;h4 id=&#34;时区问题&#34;&gt;时区问题&lt;/h4&gt;

&lt;p&gt;进行定时备份时，发现备份使用的事 UTC 时间，并不是本地时间，经过排查后发现是 &lt;code&gt;velero&lt;/code&gt; 镜像的时区问题，在调整后就会正常定时备份了，这里我重新调整了时区，直接调整镜像就好，修改 &lt;code&gt;install/01-velero.yaml&lt;/code&gt; 文件，将镜像替换为 &lt;code&gt;registry-vpc.cn-shanghai.aliyuncs.com/keking/velero:latest&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;image:
  caption: &amp;quot;Image from: [**Pexels**](https://www.pexels.com)&amp;quot;
  focal_point: &amp;quot;&amp;quot;
  preview_only: false
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: velero
  namespace: velero
spec:
  replicas: 1
  selector:
    matchLabels:
      deploy: velero
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: &amp;quot;8085&amp;quot;
        prometheus.io/scrape: &amp;quot;true&amp;quot;
      labels:
        component: velero
        deploy: velero
    spec:
      serviceAccountName: velero
      containers:
      - name: velero
        # sync from gcr.io/heptio-images/velero:latest
        image: registry-vpc.cn-shanghai.aliyuncs.com/keking/velero:latest   # 修复时区后的镜像
        imagePullPolicy: IfNotPresent
        command:
          - /velero
        args:
          - server
          - --default-volume-snapshot-locations=alibabacloud:default
        env:
          - name: VELERO_SCRATCH_DIR
            value: /scratch
          - name: ALIBABA_CLOUD_CREDENTIALS_FILE
            value: /credentials/cloud
        volumeMounts:
          - mountPath: /plugins
            name: plugins
          - mountPath: /scratch
            name: scratch
          - mountPath: /credentials
            name: cloud-credentials
      initContainers:
      - image: registry.cn-hangzhou.aliyuncs.com/acs/velero-plugin-alibabacloud:v1.2
        imagePullPolicy: IfNotPresent
        name: velero-plugin-alibabacloud
        volumeMounts:
        - mountPath: /target
          name: plugins
      volumes:
        - emptyDir: {}
          name: plugins
        - emptyDir: {}
          name: scratch
        - name: cloud-credentials
          secret:
            secretName: cloud-credentials

&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;版本问题&#34;&gt;版本问题&lt;/h4&gt;

&lt;p&gt;截止发稿时，Velero 已经发布了 v1.2.0 版本，目前 ACK 的 Velero 的插件还未升级。&lt;/p&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;近日正好有 k8s 集群服务迁移服务的需求，使用 Velero 完成了服务的迁移，同时也每日进行集群资源备份，其能力可以满足容器服务的灾备和迁移场景，实测可用，现已运行在所有的 k8s 集群。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Grafana 展示阿里云监控指标</title>
      <link>https://guoxudong.io/post/aliyun-cms-grafana/</link>
      <pubDate>Thu, 07 Nov 2019 11:08:36 +0800</pubDate>
      
      <guid>https://guoxudong.io/post/aliyun-cms-grafana/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;对于阿里云用户来说，阿里云监控是一个很不错的产品，首先它在配额内使用是免费的！免费的！免费的！重要的事情说三遍。他的功能类似于 zabbix，但是比 zabbix 提供了更多的监控项，基本上在云上使用的资源都可以通过云监控来实时监控。而它提供的开箱即用方式，天然集成云资源，并提供多种告警方式，免去了监控与告警系统搭建与维护的繁琐，并且减少了资源的消耗，比购买 ECS 自己搭建 zabbix 要少消耗很多资源。同时阿里云监控和阿里云其他服务一样，也提供了比较完整的 OpenApi 以及各种语言的 sdk，可以基于阿里云的 OpenApi 将其与自己的系统集成。我们之前也是这么做的，但是随着监控项的增加，以及经常需要在办公场地监控投屏的专项监控页，光凭我们的运维开发工程师使用 vue 写速度明显跟不上，而且页面的美观程度也差很多。&lt;/p&gt;

&lt;h3 id=&#34;手写前端-vs-grafana&#34;&gt;手写前端 VS Grafana&lt;/h3&gt;

&lt;p&gt;手写前端虽然可定制化程度更高，但是需要消耗大量精力进行调试，对于运维人员，哪怕是运维开发也是吃不消的（前端小哥哥和小姐姐是不会来帮你的，下图就是我去年拿 vue 写的伪 Grafana 展示页面，花费了大约一周时间在调整这些前端元素）。
&lt;img src=&#34;https://tva4.sinaimg.cn/large/ad5fbf65gy1g8pfrw1licj22ye1gg4qp.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Grafana 则标准化程度很高，展示也更加符合大众审美，某些定制化需求可以通过自定义 DataSource 或者 AJAX 插件的 iframe 模式完成。开发后端 DataSource 肯定就没有前端调整 css 那么痛苦和耗时了，整体配置开发一个这样的页面可能只消耗一人天就能完成。而在新产品上线时，构建一个专项监控展示页面速度就更快了，几分钟内就能完成。
&lt;img src=&#34;https://tva4.sinaimg.cn/large/ad5fbf65gy1g8pfvp0keej22yc1g2khm.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;关于阿里云监控&#34;&gt;关于阿里云监控&lt;/h2&gt;

&lt;p&gt;云监控（CloudMonitor）是一项针对阿里云资源和互联网应用进行监控的服务。&lt;/p&gt;

&lt;p&gt;云监控为云上用户提供开箱即用的企业级开放型一站式监控解决方案。涵盖 IT 设施基础监控，外网网络质量拨测监控，基于事件、自定义指标、日志的业务监控。为您全方位提供更高效、更全面、更省钱的监控服务。通过提供跨产品、跨地域的应用分组管理模型和报警模板，帮助您快速构建支持几十种云产品、管理数万实例的高效监控报警管理体系。通过提供 Dashboard，帮助您快速构建自定义业务监控大盘。使用云监控，不但可以帮助您提升您的系统服务可用时长，还可以降低企业 IT 运维监控成本。&lt;/p&gt;

&lt;p&gt;云监控服务可用于收集获取阿里云资源的监控指标或用户自定义的监控指标，探测服务可用性，以及针对指标设置警报。使您全面了解阿里云上的资源使用情况、业务的运行状况和健康度，并及时收到异常报警做出反应，保证应用程序顺畅运行。&lt;/p&gt;

&lt;h2 id=&#34;关于-grafana&#34;&gt;关于 Grafana&lt;/h2&gt;

&lt;p&gt;Grafana 是一个跨平台的开源的度量分析和可视化工具，可以通过将采集的数据查询然后可视化的展示，并及时通知。由于云监控的 Grafana 还没有支持告警，所以我们这里只用了 Grafana 的可视化功能，而告警本身就是云监控自带的，所以也不需要依赖 Grafana 来实现。而我们的 Prometheus 也使用了 Grafana 进行数据可视化，所以有现成的 Grafana-Server 使用。&lt;/p&gt;

&lt;h2 id=&#34;阿里云监控对接-grafana&#34;&gt;阿里云监控对接 Grafana&lt;/h2&gt;

&lt;p&gt;首先 Grafana 服务的部署方式这里就不做介绍了，请使用较新版本的 Grafana，最好是 5.5.0+。后文中也有我开源的基于阿里云云监控的 Grafana 的 helm chart，可以使用 helm 安装，并会直接导入云监控的指标，这个会在后文中介绍。&lt;/p&gt;

&lt;h3 id=&#34;安装阿里云监控插件&#34;&gt;安装阿里云监控插件&lt;/h3&gt;

&lt;p&gt;进入插件目录进行安装&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /var/lib/grafana/plugins/
git clone https://github.com/aliyun/aliyun-cms-grafana.git 
service grafana-server restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是使用 docker 或者部署在 k8s 集群，这里也可以使用环境变量在 Grafana 部署的时候进行安装&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;...
spec:
  containers:
  - env:
    - name: GF_INSTALL_PLUGINS  # 多个插件请使用,隔开
      value: grafana-simple-json-datasource,https://github.com/aliyun/aliyun-cms-grafana/archive/master.zip;aliyun-cms-grafana
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;您也可以下载 aliyun-cms-grafana.zip 插件解压后，上传服务器的 Grafana 的 plugins 目录下，重启 grafana-server 即可。&lt;/p&gt;

&lt;h3 id=&#34;配置云监控-datasource&#34;&gt;配置云监控 DataSource&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Grafana 启动后，进入 &lt;code&gt;Configuration&lt;/code&gt; 页面，选择 &lt;code&gt;DataSource&lt;/code&gt; Tab 页，单击右上方的&lt;code&gt;Add data source&lt;/code&gt;，添加数据源。&lt;/li&gt;
&lt;li&gt;选中&lt;code&gt;CMS Grafana Service&lt;/code&gt;，单击&lt;code&gt;select&lt;/code&gt;。
&lt;img src=&#34;https://tvax2.sinaimg.cn/large/ad5fbf65gy1g8ph0ukr0pj21nm0jk76m.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;li&gt;填写配置项，URL 根据云监控所在地域填写，并且填写阿里云账号的 accessKeyId 和 accessSecret，完成后单击&lt;code&gt;Save&amp;amp;Test&lt;/code&gt;。
&lt;img src=&#34;https://tvax3.sinaimg.cn/large/ad5fbf65gy1g8ph4bg2bij218m194n9f.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;创建-dashboard&#34;&gt;创建 Dashboard&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;单击 &lt;code&gt;Create&lt;/code&gt; -&amp;gt; &lt;code&gt;Dashboard&lt;/code&gt; -&amp;gt; &lt;code&gt;Add Query&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;配置图标，数据源选择之前添加的 &lt;code&gt;CMS Grafana Service&lt;/code&gt;，然后文档中的配置项填入指标即可（这里要注意的是，云监控 API 给返回的只有实例 ID，并没有自定义的实例名称，这里需要手动将其填入 &lt;code&gt;Y - column describe&lt;/code&gt; 中；而且只支持输入单个 Dimension，若输入多个，默认选第一个，由于这些问题才有了后续我开发的 &lt;code&gt;cms-grafana-builder&lt;/code&gt; 的动机）。
&lt;img src=&#34;https://tva4.sinaimg.cn/large/ad5fbf65gy1g8phck0irbj22ye13in79.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;li&gt;配置参考 &lt;a href=&#34;https://help.aliyun.com/document_detail/28619.html&#34; target=&#34;_blank&#34;&gt;云产品监控项&lt;/a&gt;，
&lt;img src=&#34;https://tva2.sinaimg.cn/large/ad5fbf65gy1g8phg832uvj21a40vo793.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;使用-helm-chart-的方式部署-grafana&#34;&gt;使用 helm chart 的方式部署 Grafana&lt;/h2&gt;

&lt;p&gt;项目地址：&lt;a href=&#34;https://github.com/sunny0826/cms-grafana-builder&#34; target=&#34;_blank&#34;&gt;https://github.com/sunny0826/cms-grafana-builder&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;cms-grafana-builder&#34;&gt;cms-grafana-builder&lt;/h3&gt;

&lt;p&gt;由于上文中的问题，我们需要手动选择每个实例 ID 到 Dimension 中，并且还要讲该实例的名称键入 &lt;code&gt;Y - column describe&lt;/code&gt; 中，十分的繁琐，根本不可能大批量的输入。&lt;/p&gt;

&lt;p&gt;这就是我开发这个 Grafana 指标参数生成器的原因，起初只是一个 python 脚本，用来将我们要监控的指标组装成一个 Grafana 可以使用 json 文件，之后结合 Grafana 的容器化部署方法，将其做成了一个 helm chart。可以在启动的时候自动将需要的参数生成，并且每日会对所有指标进行更新，这样就不用每次新购或者释放掉资源后还需要再跑一遍脚本。&lt;/p&gt;

&lt;h3 id=&#34;部署&#34;&gt;部署&lt;/h3&gt;

&lt;p&gt;只需要将项目拉取下来运行 &lt;code&gt;helm install&lt;/code&gt; 命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm install my-release kk-grafana-cms \
--namespace {your_namespace} \
--set access_key_id={your_access_key_id} \
--set access_secret={your_access_secret} \
--set region_id={your_aliyun_region_id} \
--set password={admin_password}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多详情见 &lt;a href=&#34;https://github.com/sunny0826/cms-grafana-builder&#34; target=&#34;_blank&#34;&gt;github README&lt;/a&gt;，欢迎提 issue 交流。&lt;/p&gt;

&lt;h3 id=&#34;指标选择&#34;&gt;指标选择&lt;/h3&gt;

&lt;p&gt;在部署成功后，可修改 ConfigMap：&lt;code&gt;grafana-cms-metric&lt;/code&gt;，然后修改对应的监控指标项。&lt;/p&gt;

&lt;h3 id=&#34;效果&#34;&gt;效果&lt;/h3&gt;

&lt;p&gt;ECS:
&lt;img src=&#34;https://tvax1.sinaimg.cn/large/ad5fbf65gy1g8pi9toh3dj21gv0pldyf.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;RDS:
&lt;img src=&#34;https://tva2.sinaimg.cn/large/ad5fbf65gy1g8pi9o91ejj21h80q316p.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;EIP:
&lt;img src=&#34;https://tva4.sinaimg.cn/large/ad5fbf65gy1g8pi9i9if3j21h70q3aif.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Redis:
&lt;img src=&#34;https://tvax1.sinaimg.cn/large/ad5fbf65gy1g8pi8ss733j21h30pz7b6.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;为了满足公司需求，后续还开发 DataSource 定制部分，用于公司监控大屏的展示，这部分是另一个项目，不在这个项目里，就不细说了，之后有机会总结后再进行分享。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>阿里云产品夜谈-容器服务交流</title>
      <link>https://guoxudong.io/post/aliyun-product-meetup/</link>
      <pubDate>Mon, 30 Sep 2019 09:32:35 +0800</pubDate>
      
      <guid>https://guoxudong.io/post/aliyun-product-meetup/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://tvax4.sinaimg.cn/large/ad5fbf65gy1g7hb4iwdpvj213i0vs4qq.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;9月25日晚受邀来到阿里云飞天园区参加阿里云MVP产品夜谈，在会上遇到了容器服务团队的负责人易立，并就容器服务进行了交流。此次参加夜谈的除了来自全球各地的阿里云MVP，还有来自安全团队、容器团队、AIoT 团队、大数据团队、数据库团队、人工智能团队、中间件团队、搜索引擎&amp;amp;智能推荐团队的负责人&amp;amp;产品经理。各个参会的MVP可以根据自己的研究方向或者感兴趣的方向选择，直接与团队负责人面对面交流，获取阿里云产品的最新信息，并提出使用意见，促进产品的发展。由于主要从事云原生&amp;amp;容器方面的工作，我选择了容器团队，与阿里云容器服务团队负责人易立就容器服务进行交流，本文记录了部分交流内容。&lt;/p&gt;

&lt;h2 id=&#34;容器服务交流&#34;&gt;容器服务交流&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://tva3.sinaimg.cn/large/ad5fbf65gy1g7hdbw7rwij21zk13ax6s.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;关于集群版本-集群升级&#34;&gt;关于集群版本&amp;amp;集群升级&lt;/h3&gt;

&lt;p&gt;众所周知，Kubernetes 以非常稳定的3个月发布一个版本速度在高速迭代这，Kubernetes v1.16.0 也即将 release ，但是目前 ACK 的 Kubernetes 版本依旧为 v1.12.6-aliyun.1 已落后官方4个大版本。得到的回复是新版本 1.14 已经上线，下周就可以升级了，1.14 版本之前已经上线，只不过一直在灰度测试，下周（2019年9月29日）就全面放开升级了。截止写这篇文章的时候，我们的多个 Kubernetes 集群已成功升级到 v1.14.6-aliyun.1 ，虽然在升级的时候出现了一点小问题，但是最后还是顺利解决了。&lt;/p&gt;

&lt;p&gt;然后就是集群升级的问题，集群升级的时候会建议对所有节点打快照，确保节点安全，但是如果在节点升级当中失败，就会出现一半为新版本节点，一半为旧版本节点的问题。我们的一个节点升级失败，就出现了上述问题，最后还是将该节点容器驱散，并将该节点移出集群才解决了升级问题。希望集群升级提供整体状态保存&amp;amp;回退功能，确保如果升级失败（或者出现新旧版本不兼容问题）的时候可以安全回退到之前版本。&lt;/p&gt;

&lt;h3 id=&#34;关于容器服务前端展示&#34;&gt;关于容器服务前端展示&lt;/h3&gt;

&lt;p&gt;ACK 的 WEB 界面相对简陋，一直以来都是对 Kubernetes Dashboard 进行了简单的包装，和其他公有云相比确实不如。不过这也不是容器服务独有的问题，阿里云你产品众多，大部分都有这样的问题。与易立交流得知，容器服务团队目前主要的任务还是确保 Kubernetes 集群的安全稳定运行，他们在安全和可用性上花费的大量精力，貌似并没有拿到什么前端开发资源。我注意到像费用中心、日志服务等产品都有了新版页面，这里希望能容器服务页面也能尽快改版，提高页面操作的便捷和美观。&lt;/p&gt;

&lt;h3 id=&#34;关于授权管理&#34;&gt;关于授权管理&lt;/h3&gt;

&lt;p&gt;一直以来容器服务都有授权管理功能，后来都基于RAM重新做了授权管理功能。但是RAM权限管理策略十分复杂，配置起来也很麻烦，不同的策略结构和语法学习起来非常困难。在配置和管理起来非常困难，我们只能把所有权限收回，每项权限都要根据需求提工单来进行配置，还时长会出现配置不生效的问题。而且这个问题一提出，就引起了大家的共鸣，后了解得知，为了安全合规的要求，操作便捷和安全合规没法兼顾。这里希望授权管理上能在确保合规的同时，能提升RAM操作的便捷性。&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://tvax2.sinaimg.cn/large/ad5fbf65gy1g7hdrlln6vj21rm0ycwov.jpg&#34; alt=&#34;image&#34; /&gt;
关于容器服务的交流主要是以上几点，其他的还包括监控、存储和 CI/CD 方面进行了交流，同时也获得了不少建议。当面给阿里云提需求的机会并不多，我也是抓住机会，把日常使用 ACK 的问题汇总之后一股脑的丢了出去。有类似需求的同学可以在&lt;a href=&#34;https://connect.aliyun.com&#34; target=&#34;_blank&#34;&gt;阿里云的聆听平台&lt;/a&gt;上给阿里云提交建议，以我的经验，合理的需求会很快审核通过并排期开发，换句话说就是“人人都可以是阿里云的产品经理”。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>解决 Nginx-Ingress 重定向失败问题</title>
      <link>https://guoxudong.io/post/nginx-ingress-error/</link>
      <pubDate>Fri, 16 Aug 2019 11:15:37 +0800</pubDate>
      
      <guid>https://guoxudong.io/post/nginx-ingress-error/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;最近对公司 Kubernetes 集群的 &lt;code&gt;nginx-ingress-controller&lt;/code&gt; 进行了升级，但是升级后却出现了大问题，之前所有采用 &lt;code&gt;nginx.ingress.kubernetes.io/rewrite-target: /&lt;/code&gt; 注释进行重定向的 Ingress 路由全部失效了，但是那些直接解析了域名，没有进行重定向的却没有发生这个问题。&lt;/p&gt;

&lt;h2 id=&#34;问题分析&#34;&gt;问题分析&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;首先检查对应服务健康状态，发现所有出问题的服务的状态均正常，同时受影响的之后 http 调用，而 RPC 调用却不受影响，这时问题就定位到了 ingress。&lt;/li&gt;
&lt;li&gt;然后检查 nginx-ingress-controller ，发现 nginx-ingress-controller 的状态也是正常的，路由也是正常的。&lt;/li&gt;
&lt;li&gt;最后发现受影响的只有添加了重定向策略的 ingress 。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;问题解决&#34;&gt;问题解决&lt;/h2&gt;

&lt;p&gt;问题已经定位，接下来就是着手解决问题，这时候值得注意的就是之前进行了什么变更：升级了 nginx-ingress-controller 版本！看来问题就出现在新版本上，那么就打开官方文档：&lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/examples/rewrite/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.github.io/ingress-nginx/examples/rewrite/&lt;/a&gt; 看一下吧。&lt;/p&gt;

&lt;h3 id=&#34;attention&#34;&gt;Attention&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Starting in Version 0.22.0, ingress definitions using the annotation &lt;code&gt;nginx.ingress.kubernetes.io/rewrite-target&lt;/code&gt; are not backwards compatible with previous versions. In Version 0.22.0 and beyond, any substrings within the request URI that need to be passed to the rewritten path must explicitly be defined in a &lt;a href=&#34;https://www.regular-expressions.info/refcapture.html&#34; target=&#34;_blank&#34;&gt;capture group&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;文档上给出了非常明显的警告⚠️：从 V0.22.0 版本开始将不再兼容之前的入口定义，再查看一下我的 nginx-ingress-controller 版本，果然问题出现来这里。&lt;/p&gt;

&lt;h3 id=&#34;note&#34;&gt;Note&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.regular-expressions.info/refcapture.html&#34; target=&#34;_blank&#34;&gt;Captured groups&lt;/a&gt; are saved in numbered placeholders, chronologically, in the form &lt;code&gt;$1&lt;/code&gt;, &lt;code&gt;$2&lt;/code&gt; &amp;hellip; &lt;code&gt;$n&lt;/code&gt;. These placeholders can be used as parameters in the &lt;code&gt;rewrite-target&lt;/code&gt; annotation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;示例&#34;&gt;示例&lt;/h3&gt;

&lt;p&gt;到这里问题已经解决了，在更新了 ingress 的配置之后，之前所有无法重定向的服务现在都已经可以正常访问了。修改见如下示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ echo &#39;
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  name: rewrite
  namespace: default
spec:
  rules:
  - host: rewrite.bar.com
    http:
      paths:
      - backend:
          serviceName: http-svc
          servicePort: 80
        path: /something(/|$)(.*)
&#39; | kubectl create -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;解决这个问题的实际时间虽然不长，但是着实让人出了一身冷汗，同时也给了我警示：变更有风险，升级需谨慎。在升级之前需要先浏览新版本的升级信息，同时需要制定完善的回滚策略，确保万无一失。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>阿里云 ACK 挂载 NAS 数据卷</title>
      <link>https://guoxudong.io/post/nas-k8s/</link>
      <pubDate>Mon, 08 Jul 2019 15:09:56 +0800</pubDate>
      
      <guid>https://guoxudong.io/post/nas-k8s/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;今天接到一个将 NAS 数据卷挂载到 Kubernetes 集群的需求，需要将一个 NAS 数据卷挂载到集群中。这一很简单的操作由于好久没有操作了，去翻看了一下官方文档，发现官方文档还在停留在去年7月份&amp;hellip;为了防止之后还有相似情况的发生，这里将所有操作做一个简单记录。&lt;/p&gt;

&lt;h2 id=&#34;购买存储包-创建文件系统&#34;&gt;购买存储包（创建文件系统）&lt;/h2&gt;

&lt;p&gt;在挂载 NAS 之前，首先要先购买 NAS 文件存储，这里推荐购买存储包，100G 的 SSD 急速型一年只需1400多，而容量型只要279，对于我这种只有少量 NAS 存储需求的人来说是是靠谱的，因为我只需要5G的左右的存储空间，SSD 急速型 NAS 一年只要18块，完美。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g4sglwrx0gj22wa09gae4.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;选择想要创建 NAS 所在 VPC 和 区域&lt;/p&gt;

&lt;h2 id=&#34;添加挂载点&#34;&gt;添加挂载点&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;点击添加挂载点
&lt;img src=&#34;https://wx3.sinaimg.cn/large/ad5fbf65gy1g4sgp0dos2j22ky0iowkr.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;选择 VPC 网络、交换机和权限组
&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g4sgpwqrgoj20xu0vowib.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;linux-挂载-nas-数据卷&#34;&gt;Linux 挂载 NAS 数据卷&lt;/h2&gt;

&lt;p&gt;在挂载点创建成功后，就可以将 NAS 数据卷挂载到 Linux 系统，这里以 CentOS 为例：&lt;/p&gt;

&lt;h3 id=&#34;安装-nfs-客户端&#34;&gt;安装 NFS 客户端&lt;/h3&gt;

&lt;p&gt;如果 Linux 系统要挂载 NAS ，首先需要安装 NFS 客户端&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo yum install nfs-utils
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;挂载-nfs-文件系统&#34;&gt;挂载 NFS 文件系统&lt;/h3&gt;

&lt;p&gt;这里阿里云早就进行了优化，点击创建的文件系统，页面上就可以 copy 挂载命令。页面提供了挂载地址的 copy 和挂载命令的 copy 功能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx2.sinaimg.cn/large/ad5fbf65gy1g4sh2i33wnj22w40yyn55.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;挂载命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo mount -t nfs -o vers=4,minorversion=0,noresvport xxxxx.cn-shanghai.nas.aliyuncs.com:/ /mnt
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;查看挂载结果&#34;&gt;查看挂载结果&lt;/h3&gt;

&lt;p&gt;直接在挂载数据卷所在服务上执行命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;df -h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就可以看到结果：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/ad5fbf65gy1g4sh6xwyt8j20lj0850tq.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-集群挂载-nas-数据卷&#34;&gt;Kubernetes 集群挂载 NAS 数据卷&lt;/h2&gt;

&lt;p&gt;K8S 的持久数据卷挂载大同小异，流程都是：&lt;strong&gt;创建PV&lt;/strong&gt; -&amp;gt; &lt;strong&gt;创建PVC&lt;/strong&gt; -&amp;gt; &lt;strong&gt;使用PVC&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;下面就简单介绍在阿里云上的操作：&lt;/p&gt;

&lt;h3 id=&#34;创建存储卷-pv&#34;&gt;创建存储卷（PV）&lt;/h3&gt;

&lt;p&gt;首先要创建存储卷，选择 &lt;strong&gt;容器服务&lt;/strong&gt; -&amp;gt; &lt;strong&gt;存储卷&lt;/strong&gt; -&amp;gt; &lt;strong&gt;创建&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这里要注意的是：&lt;strong&gt;挂载点域名使用上面面的挂载地址&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g4shuiiyyqj20hc0hp0tz.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;创建存储声明-pvc&#34;&gt;创建存储声明（PVC）&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;选择 NAS&lt;/strong&gt; -&amp;gt; &lt;strong&gt;已有存储卷&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;选择刚才创建的存储卷&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws4.sinaimg.cn/large/ad5fbf65gy1g4shv5vs1kj20hx0bvt9g.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;使用pvc&#34;&gt;使用PVC&lt;/h3&gt;

&lt;p&gt;使用的方法这里就不做详细介绍了，相关文章也比较多，这里就只记录 Deployment 中使用的 yaml 片段：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;...
volumeMounts:
- mountPath: /data      # 挂载路径
    name: volume-nas-test
...
volumes:
- name: volume-nas-test
persistentVolumeClaim:
    claimName: nas-test     # PVC 名称
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;这里只是做一个简单的记录，仅适用于阿里云 ACK 容器服务，同时也是 ACK 的一个简单应用。由于不经常对数据卷进行操作，这里做简单的记录，防止以后使用还要再看一遍文档。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>4月29日 云栖社区分享PPT -- 阿里云容器服务的优势与调优</title>
      <link>https://guoxudong.io/post/aliyun-share/</link>
      <pubDate>Tue, 30 Apr 2019 18:46:24 +0800</pubDate>
      
      <guid>https://guoxudong.io/post/aliyun-share/</guid>
      <description>&lt;p&gt;该PPT 为 2019年4月26日 在云栖社区分享使用，这里留作展示和记录，下载地址可以参考下方链接。&lt;/p&gt;

&lt;iframe src=&#34;https://guoxudong.io/aliyun-share/index.html&#34; style=&#34;width: 100%;height:600px;&#34; frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;由于图片资源位于 GitHub 上，国内访问可能会有些慢，建议下载观看。&lt;/p&gt;

&lt;p&gt;PPT 下载地址：&lt;a href=&#34;https://yq.aliyun.com/articles/700084&#34; target=&#34;_blank&#34;&gt;https://yq.aliyun.com/articles/700084&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>阿里云容器服务新建集群优化方案(更新版)</title>
      <link>https://guoxudong.io/post/aliyun-k8s-perfect/</link>
      <pubDate>Thu, 25 Apr 2019 22:26:06 +0800</pubDate>
      
      <guid>https://guoxudong.io/post/aliyun-k8s-perfect/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;选择阿里云的&lt;code&gt;容器服务&lt;/code&gt;，主要原因是公司主要业务基本都在运行在阿里云上。相较自建 kubernetes 集群，容器服务的优势在于部署相对简单，与阿里云 VPC 完美兼容，网络的配置相对简单，而如果使用 &lt;code&gt;kubeadm&lt;/code&gt; 安装部署 kubernetes 集群，除了众所周知的科学上网的问题，还有一系列的问题，包括 &lt;code&gt;etcd&lt;/code&gt; 、 &lt;code&gt;Scheduler&lt;/code&gt; 和 &lt;code&gt;Controller-Manager&lt;/code&gt; 的高可用问题等。并且如果使用托管版的阿里云 kubernetes 容器服务，还会省掉3台 master 节点的钱，并且可能将 master 节点的运维问题丢给阿里云解决，并且其提供的 master 节点性能肯定会比自购的配置好，这点是阿里云容器服务的研发小哥在来我司交流时专门强调的。&lt;/p&gt;

&lt;h2 id=&#34;问题&#34;&gt;问题&lt;/h2&gt;

&lt;p&gt;前面吹了阿里云容器服务的优势，那这里就说说在实践中遇到的容器服务的问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在新建集群的时候需要选择相应的 VPC 并选择 &lt;code&gt;Pod&lt;/code&gt; 和 &lt;code&gt;Service&lt;/code&gt; 所在的网段，这两个网段不能和 Node 节点存在于同一网段，但是如果您在阿里云中存在不止一个 VPC （VPC的网段可以是 10.0.0.0/8，172.16-31.0.0/12-16，192.168.0.0/16 ），如果网段设置不对的话，就可能会使原本存在该网段的 ECS 失联，需要删除集群重新创建。如果删除失败的话，还需要手动删除路由表中的记录（&lt;strong&gt;别问我是怎么知道的&lt;/strong&gt;）。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在使用容器服务创建集群后，会创建2个 SLB （之前是3个），一个是 SLB 是在 VPC 上并且绑定一个弹性IP（需要在创建的时候手动勾选创建弹性IP）用于 API Server，一个是经典网络的 SLB 使用提供给 Ingress 使用。但是这两个外网IP创建后的规格都是默认最大带宽、按流量收费，这个并不符合我们的要求，需要手动修改，&lt;del&gt;然而这个修改都会在第二天才能生效&lt;/del&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;容器服务创建集群后，Node 节点的名称会使&lt;code&gt;{region-id}.{ECS-id}&lt;/code&gt;的形式，这个命名方式在集群监控，使用 &lt;code&gt;kubectl&lt;/code&gt; 操作集群方面就显得比较反人类了，每次都要去查 &lt;code&gt;ECS id&lt;/code&gt; 才能确定是哪个节点，而这个 Node 节点名称是不能修改的！&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;网段问题解决&#34;&gt;网段问题解决&lt;/h2&gt;

&lt;p&gt;这个比较好解决，甚至可以说不用解决，只要把网段规划好，不要出现网段冲突就好&lt;/p&gt;

&lt;h2 id=&#34;node-节点名称无法修改问题解决&#34;&gt;Node 节点名称无法修改问题解决&lt;/h2&gt;

&lt;p&gt;这个功能之前已有人在阿里聆听平台提出这个问题了，咨询了容器服务的研发小哥，得到的反馈是该功能已经在灰度测试了，相信很快就可以上线了。&lt;/p&gt;

&lt;h2 id=&#34;创建-slb-规格问题解决&#34;&gt;创建 SLB 规格问题解决&lt;/h2&gt;

&lt;p&gt;相较之前自动创建3个 SLB 的方式，目前的版本只会自动创建2个并且有一个是 VPC 内网+弹性IP的方式，已经进行了优化，但是 ingress 绑定的 SLB 还是经典网络类型，无法接入云防火墙并且规格也是不合适的。这里给出解决方案：&lt;/p&gt;

&lt;h3 id=&#34;方法一-使用-kubectl-配置&#34;&gt;方法一：使用 &lt;code&gt;kubectl&lt;/code&gt; 配置&lt;/h3&gt;

&lt;h4 id=&#34;1-创建新的-slb&#34;&gt;1. 创建新的 SLB&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;这里需要创建一个新的 SLB 用来代替自动创建的不符合要求的 SLB。这里可以先私网 SLB 先不绑定弹性IP。&lt;strong&gt;&lt;em&gt;这里要注意的事，新建的 SLB 需要与 k8s集群处于同一 VPC 内，否则在后续会绑定失败&lt;/em&gt;&lt;/strong&gt;。
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1ma5lxgvdj21ws0s6qa5.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;li&gt;查看新购买 SLB 的 ID
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1ma8zuq1gj20sa0hoq4b.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;2-在创建集群后重新绑定-ingress-controller-的-service&#34;&gt;2. 在创建集群后重新绑定 &lt;code&gt;ingress-controller&lt;/code&gt; 的 &lt;code&gt;Service&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;首先需要使用 &lt;code&gt;kubectl&lt;/code&gt; 或者直接在阿里云控制台操作，创建新的 &lt;code&gt;nginx-ingress-svc&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# nginx ingress service
apiVersion: v1
kind: Service
metadata:
name: nginx-ingress-lb-{new-name}
namespace: kube-system
labels:
    app: nginx-ingress-lb-{new-name}
annotations:
    # set loadbalancer to the specified slb id
    service.beta.kubernetes.io/alicloud-loadbalancer-id: {SLB-ID}
    # set loadbalancer address type to intranet if using private slb instance
    #service.beta.kubernetes.io/alicloud-loadbalancer-address-type: intranet
    service.beta.kubernetes.io/alicloud-loadbalancer-force-override-listeners: &#39;true&#39;
    #service.beta.kubernetes.io/alicloud-loadbalancer-backend-label: node-role.kubernetes.io/ingress=true
spec:
type: LoadBalancer
# do not route traffic to other nodes
# and reserve client ip for upstream
externalTrafficPolicy: &amp;quot;Local&amp;quot;
ports:
- port: 80
    name: http
    targetPort: 80
- port: 443
    name: https
    targetPort: 443
selector:
    # select app=ingress-nginx pods
    app: ingress-nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建成功后，可以进到 SLB 页面查看，可以看到 &lt;code&gt;80&lt;/code&gt; 和 &lt;code&gt;443&lt;/code&gt; 端口的监听已经被添加了
    &lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1maej57c1j21ru0rwq8b.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-绑定符合要求的弹性ip&#34;&gt;3. 绑定符合要求的弹性IP&lt;/h4&gt;

&lt;p&gt;确定 SLB 创建成功并且已经成功监听后，这里就可以为 SLB 绑定符合您需求的弹性IP了，这里我们绑定一个按宽带计费2M的弹性IP&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1mak2r0p3j207k07mq33.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;4-验证连通性&#34;&gt;4. 验证连通性&lt;/h4&gt;

&lt;p&gt;到上面这步，我们的 ingress 入口 SLB 已经创建完成，这里我们验证一下是否联通。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在k8s集群中部署一个 &lt;code&gt;nginx&lt;/code&gt; ，直接在阿里云容器服务控制台操作即可
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1mant7ec6j21s40qegpr.jpg&#34; alt=&#34;image&#34; /&gt;
这里创建 ingress 路由，&lt;strong&gt;注意：这里的域名需要解析到刚才创建的 SLB 绑定的弹性IP&lt;/strong&gt;
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1maqf7gdjj21ns0kymz8.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;访问该域名，显示 &lt;code&gt;nginx&lt;/code&gt; 欢迎页，则证明修改成功
&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g1mat8srhnj21ak0hmact.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;方法二-使用阿里云容器服务控制台配置&#34;&gt;方法二： 使用阿里云容器服务控制台配置&lt;/h3&gt;

&lt;h4 id=&#34;1-阿里云容器控制台创建新-service&#34;&gt;1. 阿里云容器控制台创建新 &lt;code&gt;service&lt;/code&gt;&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;在阿里云容器服务控制台：&lt;code&gt;路由与负载均衡&lt;/code&gt; &amp;ndash;&amp;gt; &lt;code&gt;服务&lt;/code&gt; 点击&lt;code&gt;创建&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;选择 &lt;code&gt;kube-system&lt;/code&gt; 命名空间&lt;/li&gt;
&lt;li&gt;类型选中&lt;code&gt;负载均衡&lt;/code&gt; - &lt;code&gt;内网访问&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;关联 &lt;code&gt;nginx-ingress-controller&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;并添加端口映射&lt;/li&gt;
&lt;li&gt;点击创建&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g2g4fwfgevj20i50hsgmp.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;2-进入负载均衡查看-slb-是否创建&#34;&gt;2. 进入负载均衡查看 SLB 是否创建&lt;/h4&gt;

&lt;p&gt;可见 SLB 已经成功创建&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wx4.sinaimg.cn/large/ad5fbf65gy1g2g4pb1d45j215303c74r.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-绑定符合要求的弹性ip-1&#34;&gt;3. 绑定符合要求的弹性IP&lt;/h4&gt;

&lt;p&gt;同方法一&lt;/p&gt;

&lt;h4 id=&#34;4-验证连通性-1&#34;&gt;4.验证连通性&lt;/h4&gt;

&lt;p&gt;同方法一&lt;/p&gt;

&lt;h3 id=&#34;后续操作&#34;&gt;后续操作&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;在确定新的 SLB 创建成功后，就可以将容器服务自动创建的 SLB 释放了&lt;/li&gt;
&lt;li&gt;删除 &lt;code&gt;kube-system&lt;/code&gt; 中原本绑定的 &lt;code&gt;Service&lt;/code&gt; &lt;strong&gt;（目前版本已经可以关联删除绑定的 SLB 了，不用分开操作）&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;这里别忘了，自动创建给API Server 的SLB还是按流量付费的，记得降配&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;上面的这些问题和解决方案都属于临时方案，已在阿里的聆听平台提出了上面的问题，相信很快就会有所改进。总的来说，阿里云容器服务在提供优质的 kubernetes 功能，并且只收 ECS 的钱，对于想学习 kubernetes 又没有太多资金的同学也比较友好，直接买按量付费实例，测试完释放即可，不用购买 master 节点，十分良心！&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>自动合并Kubeconfig，实现多k8s集群切换</title>
      <link>https://guoxudong.io/post/merge-kubeconfig/</link>
      <pubDate>Sun, 17 Mar 2019 10:45:02 +0800</pubDate>
      
      <guid>https://guoxudong.io/post/merge-kubeconfig/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;随着微服务和容器化的深入人心，以及kubernetes已经成为容器编排领域的事实标准，越来越多的公司将自己的服务迁移到kubernetes集群中。而随着kubernetes集群的增加，集群管理的问题就凸显出来，不同的环境存在不同的集群，不同的业务线不同的集群，甚至有些开发人员都有自己的集群。诚然，如果集群是使用公有云如阿里云或华为云的容器服务，可以登录其控制台进行集群管理；或者使用rancher这用的多集群管理工具进行统一的管理。但是在想操作&lt;code&gt;istio&lt;/code&gt;特有的容器资源，或者想使用&lt;code&gt;istioctl&lt;/code&gt;的时候，或者像我一样就是想使用&lt;code&gt;kubectl&lt;/code&gt;命令的同学，这个时候多集群的切换就显的十分重要了。&lt;/p&gt;

&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-kubectl```命令行工具通过```kubeconfig```文件的配置来选择集群以及集群的API&#34;&gt;
## 原理
使用```kubeconfig```文件，您可以组织您的群集，用户和名称空间。 还可以定义上下文以快速轻松地在群集和名称空间之间切换。

### 上下文(Context) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kubeconfig```文件中的上下文元素用于以方便的名称对访问参数进行分组。 每个上下文有三个参数：集群，命名空间和用户。 默认情况下，kubectl命令行工具使用当前上下文中的参数与集群进行通信。可以使用下面的命令设置上下文：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;配置内容&#34;&gt;配置内容&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;kubectl config view
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;如果设置了&lt;code&gt;--kubeconfig&lt;/code&gt;标志，则只使用指定的文件。该标志只允许有一个实例。&lt;/li&gt;
&lt;li&gt;如果环境变量&lt;code&gt;KUBECONFIG&lt;/code&gt;存在，那么就使用该环境变量&lt;code&gt;KUBECONFIG&lt;/code&gt;里面的值，如果不存在该环境变量&lt;code&gt;KUBECONFIG&lt;/code&gt;，那么默认就是使用&lt;code&gt;$HOME/.kube/config&lt;/code&gt;文件。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;kubeconfig-内容&#34;&gt;&lt;code&gt;kubeconfig&lt;/code&gt;内容&lt;/h3&gt;

&lt;p&gt;从下面kubeconfig文件的配置来看集群、用户、上下文、当前上下文的关系就比较明显了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Config
preferences: {}

clusters:
- cluster:
name: {cluster-name}

users:
- name: {user-name}

contexts:
- context:
    cluster: {cluster-name}
    user: {user-name}
name: {context-name}

current-context: {context-name}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;为何要自动合并&#34;&gt;为何要自动合并&lt;/h2&gt;

&lt;p&gt;在日常的工作中，如果我们需要操作多个集群，会得到多个kubeconfig配置文件。一般的kubeconfig文件都是yaml格式的，但是也有少部分的集群kubeconfig时已json文件的形式给出的（比如华为云的=。=），比如我们公司再阿里云、华为云和自建环境上均存在kubernetes集群，平时操作要在多集群之间切换，这也就催生了我写这个工具（其实就是一个脚本）的动机。&lt;/p&gt;

&lt;h2 id=&#34;自动合并生成kubeconfig&#34;&gt;自动合并生成kubeconfig&lt;/h2&gt;

&lt;p&gt;众所周知，yaml是一种直观的能够被电脑识别的数据序列化格式，是一个可读性高并且容易被人类阅读的语言和json相比（没有格式化之前）可读性更强。而我这个工具并不是很关心kubeconfig的格式，只要将想要合并的kubeconfig放入指定文件即可。&lt;/p&gt;

&lt;p&gt;GitHub：&lt;a href=&#34;https://github.com/sunny0826/mergeKubeConfig&#34; target=&#34;_blank&#34;&gt;https://github.com/sunny0826/mergeKubeConfig&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;适用环境&#34;&gt;适用环境&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;需要在终端使用命令行管理多集群&lt;/li&gt;
&lt;li&gt;kubernetes集群中安装了istio，需要使用&lt;code&gt;istioctl&lt;/code&gt;命令，但是集群节点并没有安装&lt;code&gt;istioctl&lt;/code&gt;，需要在本地终端操作&lt;/li&gt;
&lt;li&gt;不愿频繁编辑.kube目录中的config文件的同学&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;准备工作&#34;&gt;准备工作&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Python环境：2.7或者3均可&lt;/li&gt;
&lt;li&gt;需要依赖包：&lt;code&gt;PyYAML&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;开始使用&#34;&gt;开始使用&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;安装依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install PyYAML
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;运行脚本&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;默认运行方式，kubeconfig文件放入&lt;code&gt;configfile&lt;/code&gt;文件,注意删掉作为示例的两个文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python merge.py
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;自定义kubeconfig文件目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python merge.py -d {custom-dir}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;运行后操作&#34;&gt;运行后操作&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;将生成的config文件放入.kube目录中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp config ~/.kube
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看所有的可使用的kubernetes集群角色&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config get-contexts
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;更多关于kubernetes配置文件操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config --help
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;切换kubernetes配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context {your-contexts}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;在使用kubernetes初期，在多集群之间我一直是频繁的切换&lt;code&gt;.kube/config&lt;/code&gt;文件来达到切换操作集群的目的。这也导致了我的&lt;code&gt;.kube&lt;/code&gt;目录中存在这多个类似于&lt;code&gt;al_test_config.bak&lt;/code&gt;、&lt;code&gt;al_prod_config.bak&lt;/code&gt;、&lt;code&gt;hw_test_config.bak&lt;/code&gt;的文件，本地环境已经自建环境，在集群切换的时候十分头疼。而后来使用&lt;code&gt;--kubeconfig&lt;/code&gt;来进行切换集群，虽然比之前的方法要方便很多，但是并不十分优雅。这个简单的小工具一举解决了我的文件，对于我这个&lt;code&gt;kubectl&lt;/code&gt;重度依赖者来说十分重要。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>解决kubernetes中ingress-nginx配置问题</title>
      <link>https://guoxudong.io/post/k8s-ingress-config/</link>
      <pubDate>Wed, 06 Mar 2019 14:42:05 +0800</pubDate>
      
      <guid>https://guoxudong.io/post/k8s-ingress-config/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;随着公司容器化的深入，越来越多的服务陆续迁移到kubernetes集群中，有些问题在测试环境并未凸显，但是在生产环境中这些问题就显得格外的扎眼。这里就对实践中kubernetes集群中的7层负载均衡器ingress遇到的问题进行总结。&lt;/p&gt;

&lt;h2 id=&#34;http-s-负载均衡器-ingress&#34;&gt;HTTP(S)负载均衡器-ingress&lt;/h2&gt;

&lt;p&gt;Ingress是kubernetes API的标准资源类型之一，其本质就是一组基于DNS名称(host)或URL路径把请求转发至指定的Service资源的规则，&lt;strong&gt;用于将集群外的请求流量转发至集群内部完成服务发布&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;Ingress控制器(Ingress Controller)可以由任何具有反向代理(HTTP/HTTPS)功能的服务程序实现，如Nginx、Envoy、HAProxy、Vulcand和Traefik等。Ingress控制器本身也作为Pod对象与被代理的运行为Pod资源的应用运行于同一网络中。我们在这里选择了NGINX Ingress Controller，由于对NGINX的配置较为熟悉，同时我们使用的kubernetes是阿里云的容器服务，构建集群的时候，容器服务会自带NGINX Ingress Controller。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://wx2.sinaimg.cn/large/ad5fbf65ly1g0t3yj7wecj20w50doab9.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;根据实际情况ingress调优&#34;&gt;根据实际情况Ingress调优&lt;/h2&gt;

&lt;h3 id=&#34;1-解决400-request-header-or-cookie-too-large问题&#34;&gt;1. 解决400 Request Header Or Cookie Too Large问题&lt;/h3&gt;

&lt;p&gt;image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;h4 id=&#34;现象&#34;&gt;现象&lt;/h4&gt;

&lt;p&gt;微信小程序需要调用后端接口，需要在header中传一段很长的token参数，直接使用浏览器访问该端口可以访问通，但是在加上token访问之后，会报“400 Request Header Or Cookie Too Large”&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;html&amp;gt;
    &amp;lt;head&amp;gt;
        &amp;lt;title&amp;gt;400 Request Header Or Cookie Too Large&amp;lt;/title&amp;gt;
    &amp;lt;/head&amp;gt;
    &amp;lt;body&amp;gt;
        &amp;lt;center&amp;gt;
            &amp;lt;h1&amp;gt;400 Bad Request&amp;lt;/h1&amp;gt;
        &amp;lt;/center&amp;gt;
        &amp;lt;center&amp;gt;Request Header Or Cookie Too Large&amp;lt;/center&amp;gt;
        &amp;lt;hr&amp;gt;
        &amp;lt;center&amp;gt;nginx/1.15.6&amp;lt;/center&amp;gt;
    &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;问题定位&#34;&gt;问题定位&lt;/h4&gt;

&lt;p&gt;直接修改Service使用nodeport的形式访问，则没有报错，初步定位需要在ingress中nginx配置客户端的请求头，进入Ingress Controller的Pod查询配置，果然是请求头空间不足。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat nginx.conf | grep client_header_buffer_size
    client_header_buffer_size       1k;
$ cat nginx.conf | grep large_client_header_buffers
    large_client_header_buffers     4 8k;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;解决方法&#34;&gt;解决方法&lt;/h4&gt;

&lt;p&gt;在ingress中添加注释&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;nginx.ingress.kubernetes.io/server-snippet: client_header_buffer_size 2046k;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false-1&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Server snippet&lt;/strong&gt;&lt;br&gt;Using the annotation &lt;code&gt;nginx.ingress.kubernetes.io/server-snippet&lt;/code&gt; it is possible to add custom configuration in the server configuration block.
&lt;br&gt;该注释是将自定义配置加入nginx的server配置中&lt;/p&gt;

&lt;p&gt;image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false-2&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;h3 id=&#34;2-解决请求超时问题&#34;&gt;2. 解决请求超时问题&lt;/h3&gt;

&lt;h4 id=&#34;现象-1&#34;&gt;现象&lt;/h4&gt;

&lt;p&gt;有一个数据导出功能，需要将大量数据进行处理，然后以Excel格式返回，在导出一个大约3W条数据的时候，出现访问超时情况。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws2.sinaimg.cn/mw690/ad5fbf65ly1g0ubdwwzo5j21b30bjaat.jpg&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;解决方法-1&#34;&gt;解决方法&lt;/h4&gt;

&lt;p&gt;调整proxy_read_timeout，连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理
在ingress中添加注释&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;nginx.ingress.kubernetes.io/proxy-read-timeout: 600
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;这里需要注意的事该注释的value需要时number类型，不能加s，否则将不生效&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;3-增加白名单&#34;&gt;3. 增加白名单&lt;/h3&gt;

&lt;h4 id=&#34;现象-2&#34;&gt;现象&lt;/h4&gt;

&lt;p&gt;在实际的使用中，会有一部分应用需要设置只可以在办公场地的网络使用，之前使用阿里云 SLB 的时候可以针对端口进行访问控制，但是现在走 ingress ，都是从80 or 443端口进，所以需要在 ingress 设置&lt;/p&gt;

&lt;h4 id=&#34;解决方法-2&#34;&gt;解决方法&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Whitelist source range&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You can specify allowed client IP source ranges through the nginx.ingress.kubernetes.io/whitelist-source-range annotation. The value is a comma separated list of CIDRs, e.g. 10.0.0.0/24,172.10.0.1.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在 ingress 里配置 &lt;code&gt;nginx.ingress.kubernetes.io/whitelist-source-range&lt;/code&gt; ，如有多个ip段，用逗号分隔即可&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;nginx.ingress.kubernetes.io/whitelist-source-range: 10.0.0.0/24
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想全局适用，可以在阿里云 SLB 里操作，也可以将该配置加入到 &lt;code&gt;NGINX ConfigMap&lt;/code&gt; 中。&lt;/p&gt;

&lt;p&gt;image:
  caption: &amp;ldquo;Image from: &lt;a href=&#34;https://www.pexels.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&amp;ldquo;
  focal_point: &amp;ldquo;&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;preview-only-false-3&#34;&gt;preview_only: false&lt;/h2&gt;

&lt;p&gt;根据工作中遇到的实际问题，持续更新中&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;使用NGINX ingress controller的好处就是对于nginx配置相对比较熟悉，性能也不差。相关nginx配置的对应的ingress可以在 &lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/&lt;/a&gt; 上查到。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pod质量服务类别(QoS)</title>
      <link>https://guoxudong.io/post/k8s-qos/</link>
      <pubDate>Mon, 04 Mar 2019 19:18:13 +0800</pubDate>
      
      <guid>https://guoxudong.io/post/k8s-qos/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;根据Pod对象的requests和limits属性，kubernetes将Pod对象归类到BestEffort、Burstable和Guaranteed三个服务质量（Quality of Service，QoS）类别。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Guaranteed

&lt;ul&gt;
&lt;li&gt;cpu:requests=limits&lt;/li&gt;
&lt;li&gt;memory:requests=limits&lt;/li&gt;
&lt;li&gt;这类Pod具有最高优先级&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Burstable

&lt;ul&gt;
&lt;li&gt;至少一个容器设置了cpu或内存资源的requests&lt;/li&gt;
&lt;li&gt;这类Pod具有中等优先级&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;BestEffort

&lt;ul&gt;
&lt;li&gt;未有任何一个容器设置requests或limits属性&lt;/li&gt;
&lt;li&gt;这类Pod具有最低优先级&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/ad5fbf65ly1g0rv2ipzqkj20hx0edmx8.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;同级别优先级的Pod资源在OOM时，与自身的requests属性相比，其内存占用比例最大的Pod对象将被首先杀死。如上图同属Burstable类别的Pod A将先于Pod B被杀死，虽然其内存用量小，但与自身的requests值相比，它的占用比例95%要大于Pod B的80%。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>为ingress配置SSL证书，实现HTTPS访问</title>
      <link>https://guoxudong.io/post/https-ingress/</link>
      <pubDate>Sat, 29 Dec 2018 21:28:13 +0800</pubDate>
      
      <guid>https://guoxudong.io/post/https-ingress/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;devops平台率先在公司内使用kubernetes集群提供后端服务，但是由于之前一直处于探索阶段，所以使用的事http的方式提供后端服务，但是在开发统一入口后，出现了访问HTTPS页面的跨域问题，由此引出了后端服务配置SSL证书的问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;使用rancher配置ssl证书&#34;&gt;使用rancher配置SSL证书&lt;/h1&gt;

&lt;h2 id=&#34;下载ssl证书文件&#34;&gt;下载SSL证书文件&lt;/h2&gt;

&lt;p&gt;首先需要获得SSL证书文件，可以直接在阿里云SSL证书管理控制台下载&lt;/p&gt;

&lt;p&gt;选中需要下载证书，选择下载nginx证书
&lt;img src=&#34;https://guoxudong.io/images/source/zhengshu.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;将证书上传项目&#34;&gt;将证书上传项目&lt;/h2&gt;

&lt;p&gt;打开rancher，选择要使用证书的项目，点击资源中的证书&lt;/p&gt;

&lt;h2 id=&#34;将证书上传项目-1&#34;&gt;将证书上传项目&lt;/h2&gt;

&lt;p&gt;打开rancher，选择要使用证书的项目，点击资源中的证书
&lt;img src=&#34;https://guoxudong.io/images/source/https-1.png&#34; alt=&#34;image&#34; /&gt;
添加证书，点击从文件上传
&lt;img src=&#34;https://guoxudong.io/images/source/https-2.png&#34; alt=&#34;image&#34; /&gt;
上传证书文件中的秘钥和证书，点击保存即可&lt;/p&gt;

&lt;h1 id=&#34;使用yaml上传证书&#34;&gt;使用yaml上传证书&lt;/h1&gt;

&lt;p&gt;这个证书的原理其实是在相应的命名空间创建了一个包含证书信息的secrets&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
data:
    tls.crt: {私钥}
    tls.key: {证书}
kind: Secret
metadata:
    name: keking-cn
    namespace: devops-plat
type: kubernetes.io/tls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在kubernetes上运行该yaml即可&lt;/p&gt;

&lt;h1 id=&#34;rancher中证书绑定&#34;&gt;rancher中证书绑定&lt;/h1&gt;

&lt;p&gt;选中需要绑定证书的ingress，点击编辑，选中证书，保存即可（由于ingress-controller中没有绑定默认证书，所以这里不能选中默认）
&lt;img src=&#34;https://guoxudong.io/images/source/https-3.png&#34; alt=&#34;image&#34; /&gt;
保存完毕，证书即可生效&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>kubernetes集群概述</title>
      <link>https://guoxudong.io/post/k8s-topo/</link>
      <pubDate>Wed, 03 Oct 2018 12:18:13 +0800</pubDate>
      
      <guid>https://guoxudong.io/post/k8s-topo/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;随着2017年AWS，Azure和阿里云相继在其原有容器服务上新增了对kubernetes的支持，而Docker官网也在同年10月宣布同时支持Swarm好kubernetes容器编排系统。kubernetes俨然已成为容器编排领域事实上的标准，而2018年更是各大公司相继将服务迁移到kubernetes上，而kubernetes则以惊人更新速度，保持着每个季度发布一个大版本的速度高速发展着。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;kubernetes特征&#34;&gt;kubernetes特征&lt;/h1&gt;

&lt;p&gt;kubernetes是一种在一组主机上运行和协同容器化应用程序的系统，旨在提供可预测性、可拓展性与高可用性的方法来完全管理容器化应用和服务的生命周期平台。用户可以定义应用程序的运行方式，以及与其他应用程序或外部世界交互的途径，并能实现服务的扩容和缩容，执行平滑滚动更新，以及在不同版本的应用程序之间调度流量以测试功能或回滚有问题的部署。kubernetes提供了接口和可组合帆软平台原语，使得用户能够以高度的灵活性和可靠性定义及管理应用程序。&lt;/p&gt;

&lt;h1 id=&#34;kubernetes组件及网络通信&#34;&gt;kubernetes组件及网络通信&lt;/h1&gt;

&lt;p&gt;kubernetes集群的客户端可以分为两类：API Server客户端和应用程序（运行为Pod中的容器）客户端。
&lt;img src=&#34;https://guoxudong.io/images/source/kubernetes-topo.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一类客户端通常包含用户和Pod对象两种，它们通过API Server访问kubernetes集群完成管理任务，例如，管理集群上的各种资源对象。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;第二类客户端一般也包含人类用户和Pod对象两种，它们的访问目标是Pod上运行于容器中的应用程序提供的各种具体的服务，如redis或nginx等，不过，这些访问请求通常要经由Service或Ingress资源对象进行。另外，第二类客户端的访问目标对象的操作要经由第一类客户端创建和配置完成后才进行。&lt;/p&gt;

&lt;p&gt;访问API Server时，人类用户一般借助于命令行工具kubectl或图形UI（例如kubernetes dashboard）进行，也通过编程接口进行访问，包括REST API。访问Pod中的应用时，其访问方式要取决于Pod中的应用程序，例如，对于运行Nginx容器的Pod来说，其最常用工具就是浏览器。&lt;/p&gt;

&lt;p&gt;管理员（开发人员或运维人员）使用kubernetes集群的常见操作包括通过控制器创建Pod，在Pod的基础上创建Service供第二类客户端访问，更新Pod中的应用版本（更新和回滚）以及对应用规模进行扩容或缩容等，另外还有集群附件管理、存储卷管理、网络及网络策略管理、资源管理和安全管理等。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>容器技术概述</title>
      <link>https://guoxudong.io/post/con-in/</link>
      <pubDate>Thu, 30 Aug 2018 18:45:22 +0800</pubDate>
      
      <guid>https://guoxudong.io/post/con-in/</guid>
      <description>

&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;自从微服务（Microservice）的出现，出于业务的需要，IT应用模型不断的变革。开发模式从瀑布式到敏捷开发；开发、运维和测试互相配合的devops思想；应用程序架构从单体模型到分层模型再到微服务；部署方式也从面向物理机到虚拟键再到容器；应用程序的基础架构从自建机房到托管再到云计算，等等。这些变革使得IT技术应用的效率大大提升，同时却以&lt;strong&gt;更低的成本交付更高质量的产品&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;尤其是以Docker为代表的容器技术的出现，终结了devops中交付和部署环节因环节、配置及程序本身的不同而造成的动辄几种甚至十几种部署配置的困境，将它们统一在容器镜像（image）之上。这就是我在工作中遇到最先遇到的困境，同时也是我开始研究容器技术的契机。&lt;/p&gt;

&lt;p&gt;如今，越来越多的企业或组织开始开始选择以镜像文件为交付载体。容器镜像之内直接包含了应用程序及其依赖的系统环境、库、基础程序等，从而能够在容器引擎上直接运行。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;容器技术概述&#34;&gt;容器技术概述&lt;/h1&gt;

&lt;p&gt;容器是一种轻量级、可移植、自包含的软件打包技术，它使得应用程序可以在几乎任何地方以相同的方式运行。&lt;/p&gt;

&lt;p&gt;容器有应用程序本身和它的环境依赖（库和其他应用程序）两部分组成，并在宿主机（Host）操作系统的用户空间中运行，但与操作系统的其他进程互相隔离，他们的实现机制有别于VMWare、KVM、Xen等实现方案的虚拟化技术。容器与虚拟机的对比关系如下图
&lt;img src=&#34;https://guoxudong.io/images/source/vs.png&#34; alt=&#34;image&#34; /&gt;
由于同一个宿主机上的所有容器都共享其底层操作系统（内核空间），这就使得容器在体积上要比传统的虚拟机小很多。另外，启动容器无须启动整个操作系统，所以容器部署和启动的速度更快，开销更小，也更容易迁移。事实上，容器赋予了应用程序超强的可移植能力。&lt;/p&gt;

&lt;h1 id=&#34;容器技术的优势&#34;&gt;容器技术的优势&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;开发方面：“一次构建、到处运行”（Build Once, Run Anywhere）。容器意味着环境隔离和可重复性，开发人员只需为应用创建一个运行环境，并将其打包成容器便可在各种部署环境上运行，并与它所在的宿主机环境隔离。&lt;/li&gt;
&lt;li&gt;运维方面：“一次配置，运行所以”（Configure Once, Run Anything）。一旦配置好标准的容器运行时环境，服务器就可以运行任何容器，这使得运维人员的工作变得更高效、一致和可重复。容器消除了开发、测试、生产环境的不一致性。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
