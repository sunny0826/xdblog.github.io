<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MaoXian Play on MaoXian Play</title>
    <link>https://blog.maoxianplay.com/</link>
    <description>Recent content in MaoXian Play on MaoXian Play</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Feb 2019 14:07:06 +0800</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>理解十二要素应用(12-Factor)</title>
      <link>https://blog.maoxianplay.com/2019/12-factor/</link>
      <pubDate>Thu, 14 Feb 2019 14:07:06 +0800</pubDate>
      
      <guid>https://blog.maoxianplay.com/2019/12-factor/</guid>
      <description>

&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;为了更好的拥抱云原生架构，同时提高软件交付质量，开发人员必须改变他们的编码方式，并未开发者和应用程序所运行的基础架构之间创造一个新的协议，十二要素应用宣言应运而生。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;构建云原生应用程序时&#34;&gt;构建云原生应用程序时&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;使用详实的设计，尽量自动化已降低时间成本和资源花费&lt;/li&gt;
&lt;li&gt;在不同环境（测试&amp;amp;生产）和不同平台（Linux&amp;amp;Windows）中应用程序的可移植性&lt;/li&gt;
&lt;li&gt;使用适于云平台的应用程序，并了解资源分配和管理&lt;/li&gt;
&lt;li&gt;使用一致的环境，减少持续交付/部署中的错误，从而最大限度地发挥软件的敏捷性&lt;/li&gt;
&lt;li&gt;通过最少的监督和设计灾难恢复框架来扩展应用程序，实现高可用性&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;十二要素应用宣言&#34;&gt;十二要素应用宣言&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;基础代码&lt;/strong&gt;：每份部署代码都使用版本控制追踪，并在不同的平台中部署多个实例&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;依赖管理&lt;/strong&gt;：应用程序应该显式声明依赖关系，并使用工具在单独管理依赖，例如Bundler、pip和Maven&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;定义配置&lt;/strong&gt;：不同环境中的配置（例如环境变量）可能会不同，例如开发环境、预发布环境和生产环境应该在操作系统级定义&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;后端服务&lt;/strong&gt;：所有资源都要被当做应用程序自身的一部分来对待。例如数据库、消息队列这样的后端服务应该被当做附加资源来看待，在所有的环境中以相同的方式被消费&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;构建、发布、运行&lt;/strong&gt;：包括构建组件、绑定配置，根据绑定的组件和配置文件启动一个或多个实例&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;进程无状态&lt;/strong&gt;：以一个或多个无状态进程运行应用（例如master和worker），进程实例之间不共享任何内容&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;服务端口绑定&lt;/strong&gt;：应用程序应当自包含，如果有任何需要对外暴露的服务，应当使用端口绑定的形式来完成（首选HTTP）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;扩容无状态应用&lt;/strong&gt;：该架构应该强调基础平台中的无状态进程管理，而不是实现更复杂的应用程序&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;进程状态管理&lt;/strong&gt;：进程应该可以迅速地增加，并在一小段时间后正常关闭。在这些方面可实现快速可拓展性、部署更改和灾难恢复&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;持续发布和部署到生产&lt;/strong&gt;：保持环境一致，不论是预发布环境还是生产环境。这样可以保证在跨越不同的环境时获取相似的结果，有利于向生产环境持续交付&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;把日志当事件流&lt;/strong&gt;：不论是平台级的日志，还是应用级的日志，都十分重要，因为日志可以帮助你了解应用程序背后都做了什么&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;后台管理任务呗当作一次性进程运行&lt;/strong&gt;：在云原生的方法中，作为应用程序发布一部分的管理任务（例如数据库迁移）应该作为一次性进程运行，而不是作为常规应用程序长时间运行&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Docker容器启动退出解决方案</title>
      <link>https://blog.maoxianplay.com/2018/docker-quit/</link>
      <pubDate>Thu, 27 Sep 2018 19:27:03 +0800</pubDate>
      
      <guid>https://blog.maoxianplay.com/2018/docker-quit/</guid>
      <description>

&lt;h1 id=&#34;现象&#34;&gt;现象&lt;/h1&gt;

&lt;p&gt;启动docker容器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run –name [CONTAINER_NAME] [CONTAINER_ID] 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看容器运行状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker ps -a 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;发现刚刚启动的mydocker容器已经退出&lt;/p&gt;

&lt;h1 id=&#34;原因&#34;&gt;原因&lt;/h1&gt;

&lt;p&gt;docker容器的主线程（dockfile中CMD执行的命令）结束，容器会退出&lt;/p&gt;

&lt;h1 id=&#34;解决办法&#34;&gt;解决办法&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;可以使用交互式启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -i [CONTAINER_NAME or CONTAINER_ID]
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;上面的不太友好，建议使用后台模式和tty选项&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -dit [CONTAINER_NAME or CONTAINER_ID]
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Docker 容器在后台以守护态（Daemonized）形式运行，可以通过添加 -d 参数来实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run -d ubuntu:14.04 /bin/sh -c &amp;quot;while true; do echo hello world; sleep 1; done&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在脚本最后一行添加&lt;strong&gt;tail -f /dev/null&lt;/strong&gt;，这个命令永远完成不了，所以该脚本一直不会执行完，所以该容器永远不会退出。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;TIPs:退出时，使用[ctrl + D]，这样会结束docker当前线程，容器结束，可以使用[ctrl + P][ctrl + Q]退出而不终止容器运行&lt;/p&gt;

&lt;p&gt;如下命令，会在指定容器中执行指定命令，[ctrl+D]退出后不会终止容器运行&lt;/p&gt;

&lt;p&gt;docker默认会把容器内部pid=1的作为默认的程序&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>容器技术概述</title>
      <link>https://blog.maoxianplay.com/2018/con-ind/</link>
      <pubDate>Thu, 30 Aug 2018 18:45:22 +0800</pubDate>
      
      <guid>https://blog.maoxianplay.com/2018/con-ind/</guid>
      <description>

&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;自从微服务（Microservice）的出现，出于业务的需要，IT应用模型不断的变革。开发模式从瀑布式到敏捷开发；开发、运维和测试互相配合的devops思想；应用程序架构从单体模型到分层模型再到微服务；部署方式也从面向物理机到虚拟键再到容器；应用程序的基础架构从自建机房到托管再到云计算，等等。这些变革使得IT技术应用的效率大大提升，同时却以&lt;strong&gt;更低的成本交付更高质量的产品&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;尤其是以Docker为代表的容器技术的出现，终结了devops中交付和部署环节因环节、配置及程序本身的不同而造成的动辄几种甚至十几种部署配置的困境，将它们统一在容器镜像（image）之上。这就是我在工作中遇到最先遇到的困境，同时也是我开始研究容器技术的契机。&lt;/p&gt;

&lt;p&gt;如今，越来越多的企业或组织开始开始选择以镜像文件为交付载体。容器镜像之内直接包含了应用程序及其依赖的系统环境、库、基础程序等，从而能够在容器引擎上直接运行。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;容器技术概述&#34;&gt;容器技术概述&lt;/h1&gt;

&lt;p&gt;容器是一种轻量级、可移植、自包含的软件打包技术，它使得应用程序可以在几乎任何地方以相同的方式运行。&lt;/p&gt;

&lt;p&gt;容器有应用程序本身和它的环境依赖（库和其他应用程序）两部分组成，并在宿主机（Host）操作系统的用户空间中运行，但与操作系统的其他进程互相隔离，他们的实现机制有别于VMWare、KVM、Xen等实现方案的虚拟化技术。容器与虚拟机的对比关系如下图
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/vs.png&#34; alt=&#34;image&#34; /&gt;
由于同一个宿主机上的所有容器都共享其底层操作系统（内核空间），这就使得容器在体积上要比传统的虚拟机小很多。另外，启动容器无须启动整个操作系统，所以容器部署和启动的速度更快，开销更小，也更容易迁移。事实上，容器赋予了应用程序超强的可移植能力。&lt;/p&gt;

&lt;h1 id=&#34;容器技术的优势&#34;&gt;容器技术的优势&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;开发方面：“一次构建、到处运行”（Build Once, Run Anywhere）。容器意味着环境隔离和可重复性，开发人员只需为应用创建一个运行环境，并将其打包成容器便可在各种部署环境上运行，并与它所在的宿主机环境隔离。&lt;/li&gt;
&lt;li&gt;运维方面：“一次配置，运行所以”（Configure Once, Run Anything）。一旦配置好标准的容器运行时环境，服务器就可以运行任何容器，这使得运维人员的工作变得更高效、一致和可重复。容器消除了开发、测试、生产环境的不一致性。·&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>centos7.2 安装k8s v1.11.0</title>
      <link>https://blog.maoxianplay.com/2018/install-k8s/</link>
      <pubDate>Tue, 14 Aug 2018 20:07:03 +0800</pubDate>
      
      <guid>https://blog.maoxianplay.com/2018/install-k8s/</guid>
      <description>

&lt;h1 id=&#34;前言&#34;&gt;前言&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;最近由于公司业务发展到了瓶颈，原有的技术架构已经逐渐无法满足业务开发和测试的需求，出现了应用测试环境搭建复杂，有许多套（真的很多很多）应用环境，应用在持续集成/持续交付也遇到了很大的困难，经过讨论研究决定对应用和微服务进行容器化，这就是我首次直面docker和k8s的契机（好吧，我是菜鸟）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;kubernetes-介绍&#34;&gt;Kubernetes 介绍&lt;/h1&gt;

&lt;p&gt;Kubernetes 是 Google 团队发起的开源项目，它的目标是管理跨多个主机的容器，提供基本的部署，维护以及运用伸缩，主要实现语言为
Go 语言。
Kubernetes的特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;易学：轻量级，简单，容易理解&lt;/li&gt;
&lt;li&gt;便携：支持公有云，私有云，混合云，以及多种云平台&lt;/li&gt;
&lt;li&gt;可拓展：模块化，可插拔，支持钩子，可任意组合&lt;/li&gt;
&lt;li&gt;自修复：自动重调度，自动重启，自动复制&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;准备工作&#34;&gt;准备工作&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;注：以下操作都是在root权限下执行的&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;安装docker-ce，这里使用docker-ce-17.09.0.c版本，安装方法见&lt;a href=&#34;https://blog.maoxianplay.com/2018/install-docker&#34;&gt;之前的教程&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装Kubeadm&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#安装 Kubeadm 首先我们要配置好阿里云的国内源，执行如下命令：
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
EOF

#之后，执行以下命令来重建yum缓存：
yum -y install epel-releaseyum
clean all
yum makecache
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来需要安装指定版本的Kubeadm（这里要安装指定版本，因为后续依赖的镜像由于有墙无法拉取，这里我们只有指定版本的镜像），注意：&lt;strong&gt;这里是安装指定版本的Kubeadm，k8s的版本更新之快完全超出你的想象！&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum -y install kubelet-1.11.0-0
yum -y install kubeadm-1.11.0-0
yum -y install kubectl-1.11.0-0
yum -y install kubernetes-cni

#执行命令启动Kubeadm服务：
systemctl enable kubelet &amp;amp;&amp;amp; systemctl start kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置 Kubeadm 所用到的镜像
这里是重中之重，因为在国内的原因，无法访问到 Google 的镜像库，所以我们需要执行以下脚本来从 Docker Hub 仓库中获取相同的镜像，并且更改 TAG 让其变成与 Google 拉去镜像一致。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;新建一个 Shell 脚本，填入以下代码之后保存&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#docker.sh
#!/bin/bash
images=(kube-proxy-amd64:v1.11.0 kube-scheduler-amd64:v1.11.0 kube-controller-manager-amd64:v1.11.0 kube-apiserver-amd64:v1.11.0 etcd-amd64:3.2.18 coredns:1.1.3 pause-amd64:3.1 kubernetes-dashboard-amd64:v1.8.3 k8s-dns-sidecar-amd64:1.14.9 k8s-dns-kube-dns-amd64:1.14.9 k8s-dns-dnsmasq-nanny-amd64:1.14.9 )
for imageName in ${images[@]} ; do
docker pull keveon/$imageName
docker tag keveon/$imageName k8s.gcr.io/$imageName
docker rmi keveon/$imageName
done
# 个人新加的一句，V 1.11.0 必加
docker tag da86e6ba6ca1 k8s.gcr.io/pause:3.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;保存后使用chmod命令赋予脚本执行权限&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chmod -R 777 ./docker.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;执行脚本拉取镜像&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sh docker.sh
#这里就开始了漫长的拉取镜像之路
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;关闭掉swap&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo swapoff -a
#要永久禁掉swap分区，打开如下文件注释掉swap那一行
# sudo vi /etc/stab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;关闭SELinux的&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 临时禁用selinux
# 永久关闭 修改/etc/sysconfig/selinux文件设置
sed -i &#39;s/SELINUX=permissive/SELINUX=disabled/&#39; /etc/sysconfig/selinux
# 这里按回车，下面是第二条命令
setenforce 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;关闭防火墙&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl disable firewalld.service &amp;amp;&amp;amp; systemctl stop firewalld.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置转发参数&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 配置转发相关参数，否则可能会出错
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
vm.swappiness=0
EOF
# 这里按回车，下面是第二条命令
sysctl --system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;这里就完成了k8s集群搭建的准备工作，集群搭建的话以上操作结束后将操作完的系统制作成系统镜像，方便集群搭建&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;正式安装&#34;&gt;正式安装&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;以下的操作都只在主节点上进行：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;初始化镜像&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubeadm init --kubernetes-version=v1.11.0 --pod-network-cidr=10.10.0.0/16  #这里填写集群所在网段
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;之后的输出会是这样：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;I0712 10:46:30.938979   13461 feature_gate.go:230] feature gates: &amp;amp;{map[]}
[init] using Kubernetes version: v1.11.0
[preflight] running pre-flight checks
I0712 10:46:30.961005   13461 kernel_validator.go:81] Validating kernel version
I0712 10:46:30.961061   13461 kernel_validator.go:96] Validating kernel config
    [WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 18.03.1-ce. Max validated version: 17.03
    [WARNING Hostname]: hostname &amp;quot;g2-apigateway&amp;quot; could not be reached
    [WARNING Hostname]: hostname &amp;quot;g2-apigateway&amp;quot; lookup g2-apigateway on 100.100.2.138:53: no such host
[preflight/images] Pulling images required for setting up a Kubernetes cluster
[preflight/images] This might take a minute or two, depending on the speed of your internet connection
[preflight/images] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;
[kubelet] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;
[kubelet] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;
[preflight] Activating the kubelet service
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [g2-apigateway kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.16.8.62]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Generated etcd/ca certificate and key.
[certificates] Generated etcd/server certificate and key.
[certificates] etcd/server serving cert is signed for DNS names [g2-apigateway localhost] and IPs [127.0.0.1 ::1]
[certificates] Generated etcd/peer certificate and key.
[certificates] etcd/peer serving cert is signed for DNS names [g2-apigateway localhost] and IPs [172.16.8.62 127.0.0.1 ::1]
[certificates] Generated etcd/healthcheck-client certificate and key.
[certificates] Generated apiserver-etcd-client certificate and key.
[certificates] valid certificates and keys now exist in &amp;quot;/etc/kubernetes/pki&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/admin.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/kubelet.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/controller-manager.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/scheduler.conf&amp;quot;
[controlplane] wrote Static Pod manifest for component kube-apiserver to &amp;quot;/etc/kubernetes/manifests/kube-apiserver.yaml&amp;quot;
[controlplane] wrote Static Pod manifest for component kube-controller-manager to &amp;quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&amp;quot;
[controlplane] wrote Static Pod manifest for component kube-scheduler to &amp;quot;/etc/kubernetes/manifests/kube-scheduler.yaml&amp;quot;
[etcd] Wrote Static Pod manifest for a local etcd instance to &amp;quot;/etc/kubernetes/manifests/etcd.yaml&amp;quot;
[init] waiting for the kubelet to boot up the control plane as Static Pods from directory &amp;quot;/etc/kubernetes/manifests&amp;quot;
[init] this might take a minute or longer if the control plane images have to be pulled
[apiclient] All control plane components are healthy after 41.001672 seconds
[uploadconfig] storing the configuration used in ConfigMap &amp;quot;kubeadm-config&amp;quot; in the &amp;quot;kube-system&amp;quot; Namespace
[kubelet] Creating a ConfigMap &amp;quot;kubelet-config-1.11&amp;quot; in namespace kube-system with the configuration for the kubelets in the cluster
[markmaster] Marking the node g2-apigateway as master by adding the label &amp;quot;node-role.kubernetes.io/master=&#39;&#39;&amp;quot;
[markmaster] Marking the node g2-apigateway as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[patchnode] Uploading the CRI Socket information &amp;quot;/var/run/dockershim.sock&amp;quot; to the Node API object &amp;quot;g2-apigateway&amp;quot; as an annotation
[bootstraptoken] using token: o337m9.ceq32wg9g2gro7gx
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the &amp;quot;cluster-info&amp;quot; ConfigMap in the &amp;quot;kube-public&amp;quot; namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

kubeadm join 10.10.207.253:6443 --token t69z6h.lr2etdbg9mfx5r15 --discovery-token-ca-cert-hash sha256:90e3a748c0eb4cb7058f3d0ee8870ee5d746214ab0589b5e841fd5d68fec8f00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;这里注意最后一行：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubeadm join 10.10.207.253:6443 --token t69z6h.lr2etdbg9mfx5r15 --discovery-token-ca-cert-hash sha256:90e3a748c0eb4cb7058f3d0ee8870ee5d746214ab0589b5e841fd5d68fec8f00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;证明集群主节点安装成功，这里要记得保存这条命令，以便之后各个节点加入集群&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;配置kubetl认证信息&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export KUBECONFIG=/etc/kubernetes/admin.conf
# 如果你想持久化的话，直接执行以下命令【推荐】
echo &amp;quot;export KUBECONFIG=/etc/kubernetes/admin.conf&amp;quot; &amp;gt;&amp;gt; ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;安装flanel网络&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /etc/cni/net.d/

cat &amp;lt;&amp;lt;EOF&amp;gt; /etc/cni/net.d/10-flannel.conf
{
&amp;quot;name&amp;quot;: &amp;quot;cbr0&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;flannel&amp;quot;,
&amp;quot;delegate&amp;quot;: {
&amp;quot;isDefaultGateway&amp;quot;: true
}
}
EOF

mkdir /usr/share/oci-umount/oci-umount.d -p

mkdir /run/flannel/

cat &amp;lt;&amp;lt;EOF&amp;gt; /run/flannel/subnet.env
FLANNEL_NETWORK=10.244.0.0/16
FLANNEL_SUBNET=10.244.1.0/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=true
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;最后需要新建一个flannel.yml文件：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
name: flannel
rules:
- apiGroups:
    - &amp;quot;&amp;quot;
    resources:
    - pods
    verbs:
    - get
- apiGroups:
    - &amp;quot;&amp;quot;
    resources:
    - nodes
    verbs:
    - list
    - watch
- apiGroups:
    - &amp;quot;&amp;quot;
    resources:
    - nodes/status
    verbs:
    - patch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
name: flannel
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: flannel
subjects:
- kind: ServiceAccount
name: flannel
namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
name: flannel
namespace: kube-system
---
kind: ConfigMap
apiVersion: v1
metadata:
name: kube-flannel-cfg
namespace: kube-system
labels:
    tier: node
    app: flannel
data:
cni-conf.json: |
    {
    &amp;quot;name&amp;quot;: &amp;quot;cbr0&amp;quot;,
    &amp;quot;type&amp;quot;: &amp;quot;flannel&amp;quot;,
    &amp;quot;delegate&amp;quot;: {
        &amp;quot;isDefaultGateway&amp;quot;: true
    }
    }
net-conf.json: |
    {
    &amp;quot;Network&amp;quot;: &amp;quot;10.10.0.0/16&amp;quot;,    #这里换成集群所在的网段
    &amp;quot;Backend&amp;quot;: {
        &amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot;
    }
    }
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
name: kube-flannel-ds
namespace: kube-system
labels:
    tier: node
    app: flannel
spec:
template:
    metadata:
    labels:
        tier: node
        app: flannel
    spec:
    hostNetwork: true
    nodeSelector:
        beta.kubernetes.io/arch: amd64
    tolerations:
    - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
    serviceAccountName: flannel
    initContainers:
    - name: install-cni
        image: quay.io/coreos/flannel:v0.9.1-amd64
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conf
        volumeMounts:
        - name: cni
        mountPath: /etc/cni/net.d
        - name: flannel-cfg
        mountPath: /etc/kube-flannel/
    containers:
    - name: kube-flannel
        image: quay.io/coreos/flannel:v0.9.1-amd64
        command: [ &amp;quot;/opt/bin/flanneld&amp;quot;, &amp;quot;--ip-masq&amp;quot;, &amp;quot;--kube-subnet-mgr&amp;quot; ]
        securityContext:
        privileged: true
        env:
        - name: POD_NAME
        valueFrom:
            fieldRef:
            fieldPath: metadata.name
        - name: POD_NAMESPACE
        valueFrom:
            fieldRef:
            fieldPath: metadata.namespace
        volumeMounts:
        - name: run
        mountPath: /run
        - name: flannel-cfg
        mountPath: /etc/kube-flannel/
    volumes:
        - name: run
        hostPath:
            path: /run
        - name: cni
        hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
        configMap:
            name: kube-flannel-cfg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;执行：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f ./flannel.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认情况下，master节点不参与工作负载，但如果希望安装出一个all-in-one的k8s环境，则可以执行以下命令：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;让master节点成为一个node节点：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl taint nodes --all node-role.kubernetes.io/master-
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;查看节点信息：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;会看到如下的输出：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAME            STATUS     ROLES     AGE       VERSION
k8s-master      Ready      master    18h       v1.11.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;以下是节点配置&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在配置好主节点之后，就可以配置集群的其他节点了，这里建议直接安装之前做好准备工作的系统镜像
进入节点机器之后，直接执行之前保存好的命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubeadm join 10.10.207.253:6443 --token t69z6h.lr2etdbg9mfx5r15 --discovery-token-ca-cert-hash sha256:90e3a748c0eb4cb7058f3d0ee8870ee5d746214ab0589b5e841fd5d68fec8f00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行完后会看到：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[preflight] running pre-flight checks
        [WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs_wrr ip_vs_sh ip_vs ip_vs_rr] or no builtin kernel ipvs support: map[ip_vs_rr:{} ip_vs_wrr:{} ip_vs_sh:{} nf_conntrack_ipv4:{} ip_vs:{}]
you can solve this problem with following methods:
1. Run &#39;modprobe -- &#39; to load missing kernel modules;
2. Provide the missing builtin kernel ipvs support

I0725 09:59:27.929247   10196 kernel_validator.go:81] Validating kernel version
I0725 09:59:27.929356   10196 kernel_validator.go:96] Validating kernel config
[discovery] Trying to connect to API Server &amp;quot;10.10.207.253:6443&amp;quot;
[discovery] Created cluster-info discovery client, requesting info from &amp;quot;https://10.10.207.253:6443&amp;quot;
[discovery] Requesting info from &amp;quot;https://10.10.207.253:6443&amp;quot; again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &amp;quot;10.10.207.253:6443&amp;quot;
[discovery] Successfully established connection with API Server &amp;quot;10.10.207.253:6443&amp;quot;
[kubelet] Downloading configuration for the kubelet from the &amp;quot;kubelet-config-1.11&amp;quot; ConfigMap in the kube-system namespace
[kubelet] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;
[kubelet] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;
[preflight] Activating the kubelet service
[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...
[patchnode] Uploading the CRI Socket information &amp;quot;/var/run/dockershim.sock&amp;quot; to the Node API object &amp;quot;k8s-node1&amp;quot; as an annotation

This node has joined the cluster:
* Certificate signing request was sent to master and a response
was received.
* The Kubelet was informed of the new secure connection details.

Run &#39;kubectl get nodes&#39; on the master to see this node join the cluster.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;这里就表示执行完毕了，可以去主节点执行命令：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;可以看到节点已加入集群：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAME        STATUS    ROLES     AGE       VERSION
k8s-master  Ready     master    20h       v1.11.0
k8s-node1   Ready     &amp;lt;none&amp;gt;    20h       v1.11.0
k8s-node2   Ready     &amp;lt;none&amp;gt;    20h       v1.11.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这期间可能需要等待一段时间，状态才会全部变为ready&lt;/p&gt;

&lt;h1 id=&#34;kubernetes-dashboard安装&#34;&gt;kubernetes-dashboard安装&lt;/h1&gt;

&lt;p&gt;详见：&lt;a href=&#34;https://blog.maoxianplay.com/2018/dashboard-k8s&#34;&gt;kubernetes安装dashboard&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;采坑指南&#34;&gt;采坑指南&lt;/h1&gt;

&lt;p&gt;有时会出现master节点一直处于notready的状态，这里可能是没有启动flannel，只需要按照上面的教程配置好flannel，然后执行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f ./flannel.yml
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>centos7安装指定版本的docker</title>
      <link>https://blog.maoxianplay.com/2018/install-docker/</link>
      <pubDate>Tue, 14 Aug 2018 20:05:21 +0800</pubDate>
      
      <guid>https://blog.maoxianplay.com/2018/install-docker/</guid>
      <description>

&lt;h1 id=&#34;前言&#34;&gt;前言&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;在使用&lt;strong&gt;centos7&lt;/strong&gt;，并使用荫安装搬运工的时候，往往不希望安装最新版本的搬运工，而是希望安装与自己熟悉或者当前业务环境需要的版本，例如目前Kubernetes支持的最新搬运工版本为v17.03，所以就产生了安装指定版本码头工人的需求。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;安装步骤&#34;&gt;安装步骤&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;# 安装依赖包
yum install -y yum-utils device-mapper-persistent-data lvm2

# 添加Docker软件包源
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

#关闭测试版本list（只显示稳定版）
sudo yum-config-manager --enable docker-ce-edge
sudo yum-config-manager --enable docker-ce-test

# 更新yum包索引
yum makecache fast

#NO.1 直接安装Docker CE （will always install the highest  possible version，可能不符合你的需求）
yum install docker-ce

#NO.2 指定版本安装
yum list docker-ce --showduplicates|sort -r 
#找到需要安装的
yum install docker-ce-17.09.0.ce -y
#启动docker
systemctl start docker &amp;amp; systemctl enable docker
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;采坑指南&#34;&gt;采坑指南&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;当然本着万事皆有坑的原则，这里也是有坑的，在安装中也是会遇到如下的问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在执行一下命令的时候：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install docker-ce-17.03.0.ce -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;会出现如下的报错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--&amp;gt; Finished Dependency Resolution
Error: Package: docker-ce-17.03.0.ce-1.el7.centos.x86_64 (docker-ce-stable)
        Requires: docker-ce-selinux &amp;gt;= 17.03.0.ce-1.el7.centos
        Available: docker-ce-selinux-17.03.0.ce-1.el7.centos.noarch (docker-ce-stable)
            docker-ce-selinux = 17.03.0.ce-1.el7.centos
        Available: docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch (docker-ce-stable)
            docker-ce-selinux = 17.03.1.ce-1.el7.centos
        Available: docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch (docker-ce-stable)
            docker-ce-selinux = 17.03.2.ce-1.el7.centos
You could try using --skip-broken to work around the problem
You could try running: rpm -Va --nofiles --nodigest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在出现这个问题之后，需要执行以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#要先安装docker-ce-selinux-17.03.2.ce，否则安装docker-ce会报错
yum install https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm

#然后再安装 docker-ce-17.03.2.ce，就能正常安装
yum install docker-ce-17.03.2.ce-1.el7.centos
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>kubernetes安装dashboard</title>
      <link>https://blog.maoxianplay.com/2018/dashboard-k8s/</link>
      <pubDate>Tue, 14 Aug 2018 19:07:03 +0800</pubDate>
      
      <guid>https://blog.maoxianplay.com/2018/dashboard-k8s/</guid>
      <description>

&lt;h1 id=&#34;前言&#34;&gt;前言&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;在安装搭建k8s的时候，往往会遇到各种各样的问题，而安装k8s的web展示组件kubernetes-dashboard则是困难中的困难，本人在实际搭建中则被整整卡住了2天，Google和百度轮番搜索，各种技术博客和技术视频反复研究才勉强搭建成功&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;开始安装&#34;&gt;开始安装&lt;/h1&gt;

&lt;p&gt;在安装好k8s集群之后，确保集群各个节点都处于ready状态的时候，就可以安装kubernetes-dashboard了
如果没有安装好k8s集群，可以参考之前的文章：&lt;a href=&#34;https://blog.maoxianplay.com/2018/install-k8s&#34;&gt;centos7.2 安装k8s v1.11.0&lt;/a&gt;
&lt;strong&gt;创建kubernetes-dashboard.yaml文件&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Copyright 2017 The Kubernetes Authors.

#
# Licensed under the Apache License, Version 2.0 (the &amp;quot;License&amp;quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &amp;quot;AS IS&amp;quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Configuration to deploy release version of the Dashboard UI compatible with
# Kubernetes 1.8.
#
# Example usage: kubectl create -f &amp;lt;this_file&amp;gt;
# ------------------- Dashboard Secret ------------------- #
apiVersion: v1
kind: Secret
metadata:
    labels:
        k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard-certs
namespace: kube-system
type: Opaque
---
# ------------------- Dashboard Service Account ------------------- #
apiVersion: v1
kind: ServiceAccount
metadata:
    labels:
        k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard
namespace: kube-system
---
# ------------------- Dashboard Role &amp;amp; Role Binding ------------------- #
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
    name: kubernetes-dashboard-minimal
namespace: kube-system
rules:
    # Allow Dashboard to create &#39;kubernetes-dashboard-key-holder&#39; secret.
    - apiGroups: [&amp;quot;&amp;quot;]
    resources: [&amp;quot;secrets&amp;quot;]
    verbs: [&amp;quot;create&amp;quot;]
    # Allow Dashboard to create &#39;kubernetes-dashboard-settings&#39; config map.
    - apiGroups: [&amp;quot;&amp;quot;]
    resources: [&amp;quot;configmaps&amp;quot;]
    verbs: [&amp;quot;create&amp;quot;]
    # Allow Dashboard to get, update and delete Dashboard exclusive secrets.
    - apiGroups: [&amp;quot;&amp;quot;]
    resources: [&amp;quot;secrets&amp;quot;]
    resourceNames: [&amp;quot;kubernetes-dashboard-key-holder&amp;quot;, &amp;quot;kubernetes-dashboard-certs&amp;quot;]
    verbs: [&amp;quot;get&amp;quot;, &amp;quot;update&amp;quot;, &amp;quot;delete&amp;quot;]
    # Allow Dashboard to get and update &#39;kubernetes-dashboard-settings&#39; config map.
    - apiGroups: [&amp;quot;&amp;quot;]
    resources: [&amp;quot;configmaps&amp;quot;]
    resourceNames: [&amp;quot;kubernetes-dashboard-settings&amp;quot;]
    verbs: [&amp;quot;get&amp;quot;, &amp;quot;update&amp;quot;]
    # Allow Dashboard to get metrics from heapster.
    - apiGroups: [&amp;quot;&amp;quot;]
    resources: [&amp;quot;services&amp;quot;]
    resourceNames: [&amp;quot;heapster&amp;quot;]
    verbs: [&amp;quot;proxy&amp;quot;]
    - apiGroups: [&amp;quot;&amp;quot;]
    resources: [&amp;quot;services/proxy&amp;quot;]
    resourceNames: [&amp;quot;heapster&amp;quot;, &amp;quot;http:heapster:&amp;quot;, &amp;quot;https:heapster:&amp;quot;]
    verbs: [&amp;quot;get&amp;quot;]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
    name: kubernetes-dashboard-minimal
namespace: kube-system
roleRef:
    apiGroup: rbac.authorization.k8s.io
kind: Role
name: kubernetes-dashboard-minimal
subjects:
    - kind: ServiceAccount
    name: kubernetes-dashboard
namespace: kube-system
---
# ------------------- Dashboard Deployment ------------------- #
kind: Deployment
apiVersion: apps/v1beta2
metadata:
    labels:
        k8s-app: kubernetes-dashboard
name: kubernetes-dashboard
namespace: kube-system
spec:
    replicas: 1
    revisionHistoryLimit: 10
    selector:
        matchLabels:
            k8s-app: kubernetes-dashboard
    template:
        metadata:
            labels:
                k8s-app: kubernetes-dashboard
        spec:
        serviceAccountName: kubernetes-dashboard
        containers:
            - name: kubernetes-dashboard
        image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3
        ports:
            - containerPort: 9090
        protocol: TCP
        args:
        #- --auto-generate-certificates
        # Uncomment the following line to manually specify Kubernetes API server Host
        # If not specified, Dashboard will attempt to auto discover the API server and connect
        # to it. Uncomment only if the default does not work.
        #- --apiserver-host=http://10.0.1.168:8080
        volumeMounts:
        - name: kubernetes-dashboard-certs
        mountPath: /certs
        # Create on-disk volume to store exec logs
        - mountPath: /tmp
        name: tmp-volume
        livenessProbe:
            httpGet:
                scheme: HTTP
            path: /
            port: 9090
            initialDelaySeconds: 30
            timeoutSeconds: 30
        volumes:
            - name: kubernetes-dashboard-certs
        secret:
            secretName: kubernetes-dashboard-certs
            - name: tmp-volume
        emptyDir: {}
        serviceAccountName: kubernetes-dashboard
        # Comment the following tolerations if Dashboard must not be deployed on master
        tolerations:
            - key: node-role.kubernetes.io/master
        effect: NoSchedule
---
# ------------------- Dashboard Service ------------------- #
kind: Service
apiVersion: v1
metadata:
    labels:
        k8s-app: kubernetes-dashboard
name: kubernetes-dashboard
namespace: kube-system
spec:
    ports:
        - port: 9090
    targetPort: 9090
    selector:
    k8s-app: kubernetes-dashboard
# ------------------------------------------------------------
kind: Service
apiVersion: v1
metadata:
    labels:
        k8s-app: kubernetes-dashboard
name: kubernetes-dashboard-external
namespace: kube-system
spec:
    ports:
        - port: 9090
    targetPort: 9090
    nodePort: 30090
    type: NodePort
    selector:
        k8s-app: kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建kubernetes-dashboard-admin.rbac.yaml文件&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
apiVersion: v1
kind: ServiceAccount
metadata:
    labels:
        k8s-app: kubernetes-dashboard
name: kubernetes-dashboard-admin
namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
    name: kubernetes-dashboard-admin
    labels:
        k8s-app: kubernetes-dashboard
roleRef:
    apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: cluster-admin
subjects:
    - kind: ServiceAccount
name: kubernetes-dashboard-admin
namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;执行命令创建仪表盘所需要的所有容器&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl -n kube-system create -f .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;访问主节点的30090端口即可看到dashboard页面&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.maoxianplay.com/images/source/k8s-dashboard.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://blog.maoxianplay.com/about/</link>
      <pubDate>Sun, 05 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.maoxianplay.com/about/</guid>
      <description>&lt;p&gt;郭旭东，是一名在上海工作的devops工程师，目前就职于&lt;a href=&#34;https://www.keking.com&#34; rel=&#34;nofollow noreferrer&#34; target=&#34;_blank&#34;&gt;上海凯京科技&lt;/a&gt;，对新技术，尤其是devops方面的技术有着强烈的好奇与不懈的追求，欢迎加入&lt;a href=&#34;https://www.keking.com&#34; rel=&#34;nofollow noreferrer&#34; target=&#34;_blank&#34;&gt;上海凯京科技&lt;/a&gt;，一起探索与创造更好更合适的devops理念。简历可以通过首页的联系方式投递。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>