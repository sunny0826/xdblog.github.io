<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MaoXian Play on MaoXian Play</title>
    <link>https://blog.maoxianplay.com/</link>
    <description>Recent content in MaoXian Play on MaoXian Play</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Feb 2019 14:07:06 +0800</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>理解十二要素应用(12-Factor)</title>
      <link>https://blog.maoxianplay.com/2019/12-factor/</link>
      <pubDate>Thu, 14 Feb 2019 14:07:06 +0800</pubDate>
      
      <guid>https://blog.maoxianplay.com/2019/12-factor/</guid>
      <description>

&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;为了更好的拥抱云原生架构，同时提高软件交付质量，开发人员必须改变他们的编码方式，并未开发者和应用程序所运行的基础架构之间创造一个新的协议，十二要素应用宣言应运而生。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;构建云原生应用程序时&#34;&gt;构建云原生应用程序时&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;使用详实的设计，尽量自动化已降低时间成本和资源花费&lt;/li&gt;
&lt;li&gt;在不同环境（测试&amp;amp;生产）和不同平台（Linux&amp;amp;Windows）中应用程序的可移植性&lt;/li&gt;
&lt;li&gt;使用适于云平台的应用程序，并了解资源分配和管理&lt;/li&gt;
&lt;li&gt;使用一致的环境，减少持续交付/部署中的错误，从而最大限度地发挥软件的敏捷性&lt;/li&gt;
&lt;li&gt;通过最少的监督和设计灾难恢复框架来扩展应用程序，实现高可用性&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;十二要素应用宣言&#34;&gt;十二要素应用宣言&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;基础代码&lt;/strong&gt;：每份部署代码都使用版本控制追踪，并在不同的平台中部署多个实例&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;依赖管理&lt;/strong&gt;：应用程序应该显式声明依赖关系，并使用工具在单独管理依赖，例如Bundler、pip和Maven&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;定义配置&lt;/strong&gt;：不同环境中的配置（例如环境变量）可能会不同，例如开发环境、预发布环境和生产环境应该在操作系统级定义&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;后端服务&lt;/strong&gt;：所有资源都要被当做应用程序自身的一部分来对待。例如数据库、消息队列这样的后端服务应该被当做附加资源来看待，在所有的环境中以相同的方式被消费&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;构建、发布、运行&lt;/strong&gt;：包括构建组件、绑定配置，根据绑定的组件和配置文件启动一个或多个实例&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;进程无状态&lt;/strong&gt;：以一个或多个无状态进程运行应用（例如master和worker），进程实例之间不共享任何内容&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;服务端口绑定&lt;/strong&gt;：应用程序应当自包含，如果有任何需要对外暴露的服务，应当使用端口绑定的形式来完成（首选HTTP）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;扩容无状态应用&lt;/strong&gt;：该架构应该强调基础平台中的无状态进程管理，而不是实现更复杂的应用程序&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;进程状态管理&lt;/strong&gt;：进程应该可以迅速地增加，并在一小段时间后正常关闭。在这些方面可实现快速可拓展性、部署更改和灾难恢复&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;持续发布和部署到生产&lt;/strong&gt;：保持环境一致，不论是预发布环境还是生产环境。这样可以保证在跨越不同的环境时获取相似的结果，有利于向生产环境持续交付&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;把日志当事件流&lt;/strong&gt;：不论是平台级的日志，还是应用级的日志，都十分重要，因为日志可以帮助你了解应用程序背后都做了什么&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;后台管理任务呗当作一次性进程运行&lt;/strong&gt;：在云原生的方法中，作为应用程序发布一部分的管理任务（例如数据库迁移）应该作为一次性进程运行，而不是作为常规应用程序长时间运行&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>kubernetes中pod同步时区问题</title>
      <link>https://blog.maoxianplay.com/2019/pod-timezone/</link>
      <pubDate>Wed, 30 Jan 2019 20:18:13 +0800</pubDate>
      
      <guid>https://blog.maoxianplay.com/2019/pod-timezone/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;新版监控大屏于18年最后一天正式上线，之后陆续进行了几次优化和修改，最近发现一个比较大的bug，就是监控显示的时间轴不对，显示的就是和目前的时间相差8小时，这就引出了docker中的时区问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;问题的原因&#34;&gt;问题的原因&lt;/h1&gt;

&lt;p&gt;默认的情况，在K8S里启动一个容器，该容器的设置的时区是UTC0，但是对用户而言，主机环境并不在UTC0。我们在UTC8。如果不把容器的时区和主机主机设置为一致，则在查找日志等时候将非常不方便，也容易造成误解。但是K8S以及Docker容器没有一个简便的设置/开关在系统层面做配置。都需要我们从单个容器入手做设置，具体有两个方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;直接修改镜像的时间设置，好处是应用部署时无需做特殊设置，但是需要手动构建Docker镜像。&lt;/li&gt;
&lt;li&gt;部署应用时，单独读取主机的“/etc/localtime”文件，即创建pod时同步时区，无需修改镜像，但是每个应用都要单独设置。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;问题的解决&#34;&gt;问题的解决&lt;/h1&gt;

&lt;p&gt;这里我们选择第二种方法，即修改部署应用的yaml文件，创建pod时同步时区&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
    name: myweb
spec:
    replicas: 2
    template:
        metadata:
        labels:
            app: myweb
        spec:
        containers:
        - name: myweb
            image: nginx:apline
            ports:
            - containerPort: 80
        #挂载到pod中
            volumeMounts:
            - name: host-time
            mountPath: /etc/localtime    
        #需要被挂载的宿主机的时区文件
        volumes:
        - name: host-time
            hostPath:
            path: /etc/localtime
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;效果对比&#34;&gt;效果对比&lt;/h1&gt;

&lt;h2 id=&#34;修改时区前&#34;&gt;修改时区前&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.maoxianplay.com/images/source/time-1.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;修改时区后&#34;&gt;修改时区后&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.maoxianplay.com/images/source/time-2.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>为ingress配置SSL证书，实现HTTPS访问</title>
      <link>https://blog.maoxianplay.com/2018/https-ingress/</link>
      <pubDate>Sat, 29 Dec 2018 21:28:13 +0800</pubDate>
      
      <guid>https://blog.maoxianplay.com/2018/https-ingress/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;devops平台率先在公司内使用kubernetes集群提供后端服务，但是由于之前一直处于探索阶段，所以使用的事http的方式提供后端服务，但是在开发统一入口后，出现了访问HTTPS页面的跨域问题，由此引出了后端服务配置SSL证书的问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;使用rancher配置ssl证书&#34;&gt;使用rancher配置SSL证书&lt;/h1&gt;

&lt;h2 id=&#34;下载ssl证书文件&#34;&gt;下载SSL证书文件&lt;/h2&gt;

&lt;p&gt;首先需要获得SSL证书文件，可以直接在阿里云SSL证书管理控制台下载&lt;/p&gt;

&lt;p&gt;选中需要下载证书，选择下载nginx证书
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/zhengshu.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;将证书上传项目&#34;&gt;将证书上传项目&lt;/h2&gt;

&lt;p&gt;打开rancher，选择要使用证书的项目，点击资源中的证书&lt;/p&gt;

&lt;h2 id=&#34;将证书上传项目-1&#34;&gt;将证书上传项目&lt;/h2&gt;

&lt;p&gt;打开rancher，选择要使用证书的项目，点击资源中的证书
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/https-1.png&#34; alt=&#34;image&#34; /&gt;
添加证书，点击从文件上传
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/https-2.png&#34; alt=&#34;image&#34; /&gt;
上传证书文件中的秘钥和证书，点击保存即可&lt;/p&gt;

&lt;h1 id=&#34;使用yaml上传证书&#34;&gt;使用yaml上传证书&lt;/h1&gt;

&lt;p&gt;这个证书的原理其实是在相应的命名空间创建了一个包含证书信息的secrets&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
data:
    tls.crt: {私钥}
    tls.key: {证书}
kind: Secret
metadata:
    name: keking-cn
    namespace: devops-plat
type: kubernetes.io/tls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在kubernetes上运行该yaml即可&lt;/p&gt;

&lt;h1 id=&#34;rancher中证书绑定&#34;&gt;rancher中证书绑定&lt;/h1&gt;

&lt;p&gt;选中需要绑定证书的ingress，点击编辑，选中证书，保存即可（由于ingress-controller中没有绑定默认证书，所以这里不能选中默认）
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/https-3.png&#34; alt=&#34;image&#34; /&gt;
保存完毕，证书即可生效&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>阿里云部署rancher2.1采坑记</title>
      <link>https://blog.maoxianplay.com/2018/install-rancher/</link>
      <pubDate>Thu, 29 Nov 2018 18:28:13 +0800</pubDate>
      
      <guid>https://blog.maoxianplay.com/2018/install-rancher/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;近期由于公司需要将部署在ucloud上的rancher迁移到阿里云上，所以将部署到阿里云的图中遇到的问题和踩到的坑在这里进行记录&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;无法删除namespace&#34;&gt;无法删除namespace&lt;/h1&gt;

&lt;p&gt;在安装新环境的rancher之前，需要将kubernetes集群中cattle-system ns下面的cluster-agent和node-agent干掉，这里我选择直接删除cattle-system这个命名空间&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl delete ns cattle-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然而问题来了，在删除命名空间之后，这个命名空间并没有立刻被删除，而是一直处于Terminating状态，这里我专门写了一篇文章解决这个问题，这里就不再赘述&lt;/p&gt;

&lt;h1 id=&#34;阿里云证书配置&#34;&gt;阿里云证书配置&lt;/h1&gt;

&lt;p&gt;由于之前使用的ucloud的机器进行测试，使用默认自签名证书并没有使用SSL证书，所以在配置证书这里遇到的许多的问题&lt;/p&gt;

&lt;p&gt;首先根据官方文档使用权威CA机构颁发的证书，这里使用的是本公司自己的证书&lt;/p&gt;

&lt;p&gt;获取证书方法：
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/jinrussl.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;点击下载证书，选择nginx证书下载
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/zhengshu.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;之后将下载的证书上传到rancher所在服务器，并配置好数据卷挂载&lt;/p&gt;

&lt;p&gt;将下面代码的挂载地址指向证书文件，运行代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d --restart=unless-stopped \
-p 80:80 -p 443:443 \
-v /root/var/log/auditlog:/var/log/auditlog \
-e AUDIT_LEVEL=3 \
-v /etc/your_certificate_directory/fullchain.pem:/etc/rancher/ssl/cert.pem \
-v /etc/your_certificate_directory/privkey.pem:/etc/rancher/ssl/key.pem \
rancher/rancher:latest --no-cacerts
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后会自动冲dockerhub上拉取最新的rancher进行进行安装，之后使用命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker ps
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看容器是否在运行，如果运行正常，则后端的配置就完成了&lt;/p&gt;

&lt;p&gt;划重点：这是是在后端配置了证书，所以在阿里云的配置上要使用四层TCP监听&lt;/p&gt;

&lt;p&gt;这个地方可是坑了我许久，我一直在前端配置https七层监听，导致一直无法正常访问，一度已经到了怀疑人生的地步=。=&lt;/p&gt;

&lt;p&gt;之后就是简单的阿里云SLB配置四层TCP监听，这里也就不再赘述了&lt;/p&gt;

&lt;h1 id=&#34;k8s集群导入rancher&#34;&gt;k8s集群导入rancher&lt;/h1&gt;

&lt;p&gt;前后端都准备就绪，现在就可以访问rancher了，访问rancher根据页面提示进行基本配置，登录后选择添加集群&lt;/p&gt;

&lt;p&gt;选择导入现有集群
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/add.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;为集群创建一个rancher中的名称，然后根据提示将命令拷贝到k8s集群所在宿主机执行即可，注意：这里由于配置了证书，所以选择有证书，不绕过证书的那个命令执行，之后就可看到集群数据导入中
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/wating.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;等待几秒即可开心的使用rancher了！&lt;/p&gt;

&lt;h1 id=&#34;关于rancher部署后访问集群api超时问题&#34;&gt;关于rancher部署后访问集群api超时问题&lt;/h1&gt;

&lt;p&gt;经过排查，原因是阿里云在容器服务对外连接处设置了TLS双向认证，导致rancher的外网ip经常性的被拦截，导致超时&lt;/p&gt;

&lt;p&gt;解决办法：&lt;/p&gt;

&lt;p&gt;对k8s集群中rancher的cattle-cluster-agent传递内网参数，将其配置为内网连接，就可以正常访问了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl -n cattle-system patch deployments cattle-cluster-agent --patch &#39;{
    &amp;quot;spec&amp;quot;: {
        &amp;quot;template&amp;quot;: {
                &amp;quot;spec&amp;quot;: {
                    &amp;quot;hostAliases&amp;quot;: [{
                                    &amp;quot;hostnames&amp;quot;:[&amp;quot;rancher.keking.cn&amp;quot;],  #rancher的域名
                                    &amp;quot;ip&amp;quot;: &amp;quot;10.0.0.219&amp;quot;  #rancher部署地址
                                    }]
                        }
                    }
            }
}&#39;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>阿里云日志服务采集k8s日志并实现livetail功能</title>
      <link>https://blog.maoxianplay.com/2018/dashboard-k8s/</link>
      <pubDate>Mon, 29 Oct 2018 21:28:13 +0800</pubDate>
      
      <guid>https://blog.maoxianplay.com/2018/dashboard-k8s/</guid>
      <description>

&lt;h1 id=&#34;前言&#34;&gt;前言&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;目前的项目日志都是通过Logtail直接采集，投递到OSS持久化，同时可以通过阿里云日志服务、devops自建平台进行查看（虽然大部分人是直接登录ECS查看=。=），
在开始进行容器化之后，同样遇到日志的问题，目前的解决方案是阿里云日志服务持久化和展现格式化后的日志、使用rancher查看实时日志，
但是之前由于rancher平台出现一些问题，导致不能及时查看日志的情况，在这个背景下对阿里云日志服务采集k8s日志和livetail进行搭建并调研词方案是否可行。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;简介-转自阿里云官方文档&#34;&gt;简介（转自阿里云官方文档）&lt;/h1&gt;

&lt;p&gt;日志服务（Log Service，简称 LOG）是针对日志类数据的一站式服务，在阿里巴巴集团经历大量大数据场景锤炼而成。您无需开发就能快捷完成日志数据采集、消费、投递以及查询分析等功能，提升运维、运营效率，建立 DT 时代海量日志处理能力。&lt;/p&gt;

&lt;h1 id=&#34;kubernetes日志采集组件安装&#34;&gt;kubernetes日志采集组件安装&lt;/h1&gt;

&lt;h2 id=&#34;安装logtail&#34;&gt;安装Logtail&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;进入阿里云容器服务找到集群id
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/log_ser.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;通过ssh登录master节点，或者任意安装了kubectl并配置了该集群kubeconfig的服务器&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;执行命令，将${your_k8s_cluster_id}替换为集群id&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget http://logtail-release-cn-hangzhou.oss-cn-hangzhou.aliyuncs.com/kubernetes/alicloud-log-k8s-install.sh -O alicloud-log-k8s-install.sh; chmod 744 ./alicloud-log-k8s-install.sh; sh ./alicloud-log-k8s-install.sh ${your_k8s_cluster_id}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Project k8s-log-${your_k8s_cluster_id}下会自动创建名为config-operation-log的Logstore，用于存储alibaba-log-controller的运行日志。请勿删除此Logstore，否则无法为alibaba-log-controller排查问题。&lt;/li&gt;
&lt;li&gt;若您需要将日志采集到已有的Project，请执行安装命令sh ./alicloud-log-k8s-install.sh${your_k8s_cluster_id} ${your_project_name} ，并确保日志服务Project和您的Kubernetes集群在同一地域。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;该条命令其实就是执行了一个shell脚本，使用helm安装了采集kubernetes集群日志的组件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

if [ $# -eq 0 ] ; then
    echo &amp;quot;[Invalid Param], use sudo ./install-k8s-log.sh {your-k8s-cluster-id}&amp;quot;
    exit 1
fi

clusterName=$(echo $1 | tr &#39;[A-Z]&#39; &#39;[a-z]&#39;)
curl --connect-timeout 5  http://100.100.100.200/latest/meta-data/region-id

if [ $? != 0 ]; then
    echo &amp;quot;[FAIL] ECS meta server connect fail, only support alibaba cloud k8s service&amp;quot;
    exit 1
fi

regionId=`curl http://100.100.100.200/latest/meta-data/region-id`
aliuid=`curl http://100.100.100.200/latest/meta-data/owner-account-id`

helmPackageUrl=&amp;quot;http://logtail-release-$regionId.oss-$regionId.aliyuncs.com/kubernetes/alibaba-cloud-log.tgz&amp;quot;
wget $helmPackageUrl -O alibaba-cloud-log.tgz
if [ $? != 0 ]; then
    echo &amp;quot;[FAIL] download alibaba-cloud-log.tgz from $helmPackageUrl failed&amp;quot;
    exit 1
fi

project=&amp;quot;k8s-log-&amp;quot;$clusterName
if [ $# -ge 2 ]; then
    project=$2
fi

echo [INFO] your k8s is using project : $project

helm install alibaba-cloud-log.tgz --name alibaba-log-controller \
    --set ProjectName=$project \
    --set RegionId=$regionId \
    --set InstallParam=$regionId \
    --set MachineGroupId=&amp;quot;k8s-group-&amp;quot;$clusterName \
    --set Endpoint=$regionId&amp;quot;-intranet.log.aliyuncs.com&amp;quot; \
    --set AlibabaCloudUserId=&amp;quot;:&amp;quot;$aliuid \
    --set LogtailImage.Repository=&amp;quot;registry.$regionId.aliyuncs.com/log-service/logtail&amp;quot; \
    --set ControllerImage.Repository=&amp;quot;registry.$regionId.aliyuncs.com/log-service/alibabacloud-log-controller&amp;quot;

installRst=$?

if [ $installRst -eq 0 ]; then
    echo &amp;quot;[SUCCESS] install helm package : alibaba-log-controller success.&amp;quot;
    exit 0
else
    echo &amp;quot;[FAIL] install helm package failed, errno &amp;quot; $installRst
    exit 0
fi
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;命令执行后，会在kubernetes集群中的每个节点运行一个日志采集的pod：logatail-ds，该pod位于kube-system&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.maoxianplay.com/images/source/log_detail.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装完成后，可使用以下命令来查看pod状态，若状态全部成功后，则表示安装完成&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@izuf6d75c59ll4woscqmh5z ~]# helm status alibaba-log-controller
LAST DEPLOYED: Thu Nov 22 15:09:35 2018
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==&amp;gt; v1/ServiceAccount
NAME                    SECRETS  AGE
alibaba-log-controller  1        6d

==&amp;gt; v1beta1/CustomResourceDefinition
NAME                                   AGE
aliyunlogconfigs.log.alibabacloud.com  6d

==&amp;gt; v1beta1/ClusterRole
alibaba-log-controller  6d

==&amp;gt; v1beta1/ClusterRoleBinding
NAME                    AGE
alibaba-log-controller  6d

==&amp;gt; v1beta1/DaemonSet
NAME        DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE SELECTOR  AGE
logtail-ds  16       16       16     16          16         &amp;lt;none&amp;gt;         6d

==&amp;gt; v1beta1/Deployment
NAME                    DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
alibaba-log-controller  1        1        1           1          6d

==&amp;gt; v1/Pod(related)
NAME                                     READY  STATUS   RESTARTS  AGE
logtail-ds-2fqs4                         1/1    Running  0         6d
logtail-ds-4bz7w                         1/1    Running  1         6d
logtail-ds-6vg88                         1/1    Running  0         6d
logtail-ds-7tp6v                         1/1    Running  0         6d
logtail-ds-9575c                         1/1    Running  0         6d
logtail-ds-bgq84                         1/1    Running  0         6d
logtail-ds-kdlhr                         1/1    Running  0         6d
logtail-ds-lknxw                         1/1    Running  0         6d
logtail-ds-pdxfk                         1/1    Running  0         6d
logtail-ds-pf4dz                         1/1    Running  0         6d
logtail-ds-rzsnw                         1/1    Running  0         6d
logtail-ds-sqhbv                         1/1    Running  0         6d
logtail-ds-vvtwn                         1/1    Running  0         6d
logtail-ds-wwmhg                         1/1    Running  0         6d
logtail-ds-xbp4j                         1/1    Running  0         6d
logtail-ds-zpld9                         1/1    Running  0         6d
alibaba-log-controller-85f8fbb498-nzhc8  1/1    Running  0         6d
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;配置日志组件展示&#34;&gt;配置日志组件展示&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在集群内安装好日志组件后，登录阿里云日志服务控制台，就会发现有一个新的project，名称为k8s-log-{集群id}
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/log_src_de.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建Logstore
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/log-1.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据导入
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/log-2.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;选择数据类型中选择docker标准输出
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/log-3.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据源配置，这里可以使用默认的
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/log-4.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;选择数据源
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/log-5.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置好之后等待1-2分钟，日志就会进来了
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/log-6.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;为了快速查询和过滤，需要配置索引
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/log-7.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;添加容器名称、命名空间、pod名称作为索引（后续使用livetail需要）
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/log-8.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;这样就完成了一个k8s集群日志采集和展示的基本流程了&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;livetail功能使用&#34;&gt;livetail功能使用&lt;/h1&gt;

&lt;h2 id=&#34;背景介绍&#34;&gt;背景介绍&lt;/h2&gt;

&lt;p&gt;在线上运维的场景中，往往需要对日志队列中进入的数据进行实时监控，从最新的日志数据中提取出关键的信息进而快速地分析出异常原因。在传统的运维方式中，如果需要对日志文件进行实时监控，需要到服务器上对日志文件执行命令tail -f，如果实时监控的日志信息不够直观，可以加上grep或者grep -v进行关键词过滤。日志服务在控制台提供了日志数据实时监控的交互功能LiveTail，针对线上日志进行实时监控分析，减轻运维压力。&lt;/p&gt;

&lt;h2 id=&#34;使用方法&#34;&gt;使用方法&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;这里选择来源类型为kubernetes，命名空间、pod名称、容器名称为上一步新建的3个索引的内容，过滤关键字的功劳与tail命令后加的grep命令是一样的，用于关键词过滤
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/log-9.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;点击开启livetail，这时就有实时日志展示出来了
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/log-10.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;以上就是阿里云livetail日志服务功能&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>精简docker镜像</title>
      <link>https://blog.maoxianplay.com/2018/image-size/</link>
      <pubDate>Thu, 27 Sep 2018 20:28:13 +0800</pubDate>
      
      <guid>https://blog.maoxianplay.com/2018/image-size/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;精简Docker镜像的好处很多，不仅可以节省存储空间和带宽，还能减少安全隐患。优化镜像大小的手段多种多样，因服务所使用的基础开发语言不同而有差异。本文将介绍精简Docker镜像的几种通用方法。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;精简docker镜像大小的必要性&#34;&gt;精简Docker镜像大小的必要性&lt;/h1&gt;

&lt;p&gt;Docker镜像由很多镜像层（Layers）组成（最多127层），镜像层依赖于一系列的底层技术，比如文件系统(filesystems)、写时复制(copy-on-write)、联合挂载(union mounts)等技术，你可以查看Docker社区文档以了解更多有关Docker存储驱动的内容，这里就不再赘述技术细节。总的来说，Dockerfile中的每条指令都会创建一个镜像层，继而会增加整体镜像的尺寸。&lt;/p&gt;

&lt;p&gt;下面是精简Docker镜像尺寸的好处：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;减少构建时间&lt;/li&gt;
&lt;li&gt;减少磁盘使用量&lt;/li&gt;
&lt;li&gt;减少下载时间&lt;/li&gt;
&lt;li&gt;因为包含文件少，攻击面减小，提高了安全性&lt;/li&gt;
&lt;li&gt;提高部署速度&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;五点建议减小Docker镜像尺寸&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;一-优化基础镜像&#34;&gt;一、优化基础镜像&lt;/h1&gt;

&lt;p&gt;优化基础镜像的方法就是选用合适的更小的基础镜像，常用的 Linux 系统镜像一般有 Ubuntu、CentOs、Alpine，其中Alpine更推荐使用。大小对比如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;guoxudong@ubuntu ~/s&amp;gt; docker images
REPOSITORY         TAG             IMAGE ID            CREATED             SIZE
ubuntu             latest        74f8760a2a8b        8 days ago          82.4MB
alpine             latest        11cd0b38bc3c        2 weeks ago         4.41MB
centos               7           49f7960eb7e4        7 weeks ago         200MB
debian             latest        3bbb526d2608        8 days ago          101MB
guoxudong@ubuntu ~/s&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alpine是一个高度精简又包含了基本工具的轻量级Linux发行版，基础镜像只有4.41M，各开发语言和框架都有基于Alpine制作的基础镜像，强烈推荐使用它。Alpine镜像各个语言和框架支持情况，可以参考《优化Docker镜像、加速应用部署》。
查看上面的镜像尺寸对比结果，你会发现最小的镜像也有4.41M，那么有办法构建更小的镜像吗？答案是肯定的，例如 gcr.io/google_containers/pause-amd64:3.1 镜像仅有742KB。为什么这个镜像能这么小？在为大家解密之前，再推荐两个基础镜像：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;scratch镜像&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;scratch是一个空镜像，只能用于构建其他镜像，比如你要运行一个包含所有依赖的二进制文件，如Golang程序，可以直接使用scratch作为基础镜像。现在给大家展示一下上文提到的Google pause镜像Dockerfile：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM scratch
ARG ARCH
ADD bin/pause-${ARCH} /pause
ENTRYPOINT [&amp;quot;/pause&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Google pause镜像使用了scratch作为基础镜像，这个镜像本身是不占空间的，使用它构建的镜像大小几乎和二进制文件本身一样大，所以镜像非常小。当然在我们的Golang程序中也会使用。对于一些Golang/C程序，可能会依赖一些动态库，你可以使用自动提取动态库工具，比如ldd、linuxdeployqt等提取所有动态库，然后将二进制文件和依赖动态库一起打包到镜像中。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;busybox镜像&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;scratch是个空镜像，如果希望镜像里可以包含一些常用的Linux工具，busybox镜像是个不错选择，镜像本身只有1.16M，非常便于构建小镜像。&lt;/p&gt;

&lt;h1 id=&#34;二-串联-dockerfile-指令&#34;&gt;二、串联 Dockerfile 指令&lt;/h1&gt;

&lt;p&gt;大家在定义Dockerfile时，如果太多的使用RUN指令，经常会导致镜像有特别多的层，镜像很臃肿，而且甚至会碰到超出最大层数（127层）限制的问题，遵循 Dockerfile 最佳实践，我们应该把多个命令串联合并为一个 RUN（通过运算符&amp;amp;&amp;amp;和/ 来实现），每一个 RUN 要精心设计，确保安装构建最后进行清理，这样才可以降低镜像体积，以及最大化的利用构建缓存。&lt;/p&gt;

&lt;p&gt;下面是一个优化前Dockerfile：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM ubuntu
ENV VER     3.0.0 
ENV TARBALL http://download.redis.io/releases/redis-$VER.tar.gz 
# ==&amp;gt; Install curl and helper tools...
RUN apt-get update 
RUN apt-get install -y  curl make gcc 
# ==&amp;gt; Download, compile, and install...
RUN curl -L $TARBALL | tar zxv 
WORKDIR  redis-$VER 
RUN make 
RUN make install 
#...
# ==&amp;gt; Clean up...
WORKDIR / 
RUN apt-get remove -y --auto-remove curl make gcc 
RUN apt-get clean 
RUN rm -rf /var/lib/apt/lists/*  /redis-$VER 
#...
CMD [&amp;quot;redis-server&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;构建镜像，名称叫 test/test:0.1。&lt;/p&gt;

&lt;p&gt;我们对Dockerfile做优化，优化后Dockerfile：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM ubuntu
ENV VER     3.0.0 
ENV TARBALL http://download.redis.io/releases/redis-$VER.tar.gz
RUN echo &amp;quot;==&amp;gt; Install curl and helper tools...&amp;quot;  &amp;amp;&amp;amp; \ 
apt-get update                      &amp;amp;&amp;amp; \
apt-get install -y  curl make gcc   &amp;amp;&amp;amp; \
echo &amp;quot;==&amp;gt; Download, compile, and install...&amp;quot;  &amp;amp;&amp;amp; \
curl -L $TARBALL | tar zxv  &amp;amp;&amp;amp; \
cd redis-$VER               &amp;amp;&amp;amp; \
make                        &amp;amp;&amp;amp; \
make install                &amp;amp;&amp;amp; \
echo &amp;quot;==&amp;gt; Clean up...&amp;quot;  &amp;amp;&amp;amp; \
apt-get remove -y --auto-remove curl make gcc  &amp;amp;&amp;amp; \
apt-get clean                                  &amp;amp;&amp;amp; \
rm -rf /var/lib/apt/lists/*  /redis-$VER
#...
CMD [&amp;quot;redis-server&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;构建镜像，名称叫 test/test:0.2。&lt;/p&gt;

&lt;p&gt;对比两个镜像大小：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@k8s-master:/tmp/iops# docker images
REPOSITORY       TAG           IMAGE ID            CREATED             SIZE
test/test        0.2         58468c0222ed        2 minutes ago       98.1MB
test/test        0.1         e496cf7243f2        6 minutes ago       307MB
root@k8s-master:/tmp/iops#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，将多条RUN命令串联起来构建的镜像大小是每条命令分别RUN的三分之一。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;提示：&lt;/strong&gt;为了应对镜像中存在太多镜像层，Docker 1.13版本以后，提供了一个压扁镜像功能，即将 Dockerfile 中所有的操作压缩为一层。这个特性还处于实验阶段，Docker默认没有开启，如果要开启，需要在启动Docker时添加-experimental 选项，并在Docker build 构建镜像时候添加 &amp;ndash;squash 。我们不推荐使用这个办法，请在撰写 Dockerfile 时遵循最佳实践编写，不要试图用这种办法去压缩镜像。&lt;/p&gt;

&lt;h1 id=&#34;三-使用多阶段构建&#34;&gt;三、使用多阶段构建&lt;/h1&gt;

&lt;p&gt;Dockerfile中每条指令都会为镜像增加一个镜像层，并且你需要在移动到下一个镜像层之前清理不需要的组件。实际上，有一个Dockerfile用于开发（其中包含构建应用程序所需的所有内容）以及一个用于生产的瘦客户端，它只包含你的应用程序以及运行它所需的内容。这被称为“建造者模式”。Docker 17.05.0-ce版本以后支持多阶段构建。使用多阶段构建，你可以在Dockerfile中使用多个FROM语句，每条FROM指令可以使用不同的基础镜像，这样您可以选择性地将服务组件从一个阶段COPY到另一个阶段，在最终镜像中只保留需要的内容。&lt;/p&gt;

&lt;p&gt;下面是一个使用COPY &amp;ndash;from 和 FROM &amp;hellip; AS &amp;hellip; 的Dockerfile：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Compile
FROM golang:1.9.0 AS builder
WORKDIR /go/src/v9.git...com/.../k8s-monitor
COPY . .
WORKDIR /go/src/v9.git...com/.../k8s-monitor
RUN make build
RUN mv k8s-monitor /root
# Package
# Use scratch image
FROM scratch
WORKDIR /root/
COPY --from=builder /root .
EXPOSE 8080
CMD [&amp;quot;/root/k8s-monitor&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;构建镜像，你会发现生成的镜像只有上面COPY 指令指定的内容，镜像大小只有2M。这样在以前使用两个Dockerfile（一个Dockerfile用于开发和一个用于生产的瘦客户端），现在使用多阶段构建就可以搞定。&lt;/p&gt;

&lt;h1 id=&#34;四-构建业务服务镜像技巧&#34;&gt;四、构建业务服务镜像技巧&lt;/h1&gt;

&lt;p&gt;Docker在build镜像的时候，如果某个命令相关的内容没有变化，会使用上一次缓存（cache）的文件层，在构建业务镜像的时候可以注意下面两点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;不变或者变化很少的体积较大的依赖库和经常修改的自有代码分开；&lt;/li&gt;
&lt;li&gt;因为cache缓存在运行Docker build命令的本地机器上，建议固定使用某台机器来进行Docker build，以便利用cache。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面是构建Spring Boot应用镜像的例子，用来说明如何分层。其他类型的应用，比如Java WAR包，Nodejs的npm 模块等，可以采取类似的方式。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;在Dockerfile所在目录，解压缩maven生成的jar包&lt;/p&gt;

&lt;p&gt;$ unzip &lt;path-to-app-jar&gt;.jar -d app&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Dockerfile 我们把应用的内容分成4个部分COPY到镜像里面：其中前面3个基本不变，第4个是经常变化的自有代码。最后一行是解压缩后，启动spring boot应用的方式。&lt;/p&gt;

&lt;p&gt;FROM openjdk:8-jre-alpine
LABEL maintainer &amp;ldquo;opl-xws@xiaomi.com&amp;rdquo;
COPY app/BOOT-INF/lib/ /app/BOOT-INF/lib/
COPY app/org /app/org
COPY app/META-INF /app/META-INF
COPY app/BOOT-INF/classes /app/BOOT-INF/classes
EXPOSE 8080
CMD [&amp;ldquo;/usr/bin/java&amp;rdquo;, &amp;ldquo;-cp&amp;rdquo;, &amp;ldquo;/app&amp;rdquo;, &amp;ldquo;org.springframework.boot.loader.JarLauncher&amp;rdquo;]&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这样在构建镜像时候可大大提高构建速度。&lt;/p&gt;

&lt;h1 id=&#34;五-其他优化办法&#34;&gt;五、其他优化办法&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;RUN命令中执行apt、apk或者yum类工具技巧，如果在RUN命令中执行apt、apk或者yum类工具，可以借助这些工具提供的一些小技巧来减少镜像层数量及镜像大小。举几个例子：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在执行apt-get install -y 时增加选项— no-install-recommends ，可以不用安装建议性（非必须）的依赖，也可以在执行apk add 时添加选项&amp;ndash;no-cache 达到同样效果；&lt;/li&gt;
&lt;li&gt;执行yum install -y 时候， 可以同时安装多个工具，比如yum install -y gcc gcc-c++ make &amp;hellip;。将所有yum install 任务放在一条RUN命令上执行，从而减少镜像层的数量；&lt;/li&gt;
&lt;li&gt;组件的安装和清理要串联在一条指令里面，如 apk &amp;ndash;update add php7 &amp;amp;&amp;amp; rm -rf /var/cache/apk/* ，因为Dockerfile的每条指令都会产生一个文件层，如果将apk add &amp;hellip;和 rm -rf &amp;hellip; 命令分开，清理无法减小apk命令产生的文件层的大小。 Ubuntu或Debian可以使用 rm -rf /&lt;strong&gt;var&lt;/strong&gt;/lib/apt/lists/* 清理镜像中缓存文件；CentOS等系统使用yum clean all 命令清理。&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;压缩镜像&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Docker 自带的一些命令还能协助压缩镜像，比如 export 和 import&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d test/test:0.2
$ docker export 747dc0e72d13 | docker import - test/test:0.3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用这种方式需要先将容器运行起来，而且这个过程中会丢失镜像原有的一些信息，比如：导出端口，环境变量，默认指令。&lt;/p&gt;

&lt;p&gt;查看这两个镜像history信息，如下，可以看到test/test:0.3 丢失了所有的镜像层信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@k8s-master:/tmp/iops# docker history test/test:0.3
IMAGE               CREATED             CREATED BY          SIZE                COMMENT
6fb3f00b7a72        15 seconds ago                          84.7MB              Imported from -
root@k8s-master:/tmp/iops# docker history test/test:0.2
IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT
58468c0222ed        2 hours ago         /bin/sh -c #(nop)  CMD [&amp;quot;redis-server&amp;quot;]         0B     
1af7ffe3d163        2 hours ago         /bin/sh -c echo &amp;quot;==&amp;gt; Install curl and helper...   15.7MB 
8bac6e733d54        2 hours ago         /bin/sh -c #(nop)  ENV TARBALL=http://downlo...   0B     
793282f3ef7a        2 hours ago         /bin/sh -c #(nop)  ENV VER=3.0.0                0B     
74f8760a2a8b        8 days ago          /bin/sh -c #(nop)  CMD [&amp;quot;/bin/bash&amp;quot;]            0B     
&amp;lt;missing&amp;gt;           8 days ago          /bin/sh -c mkdir -p /run/systemd &amp;amp;&amp;amp; echo &#39;do...   7B
&amp;lt;missing&amp;gt;           8 days ago          /bin/sh -c sed -i &#39;s/^#\s*\(deb.*universe\)$...   2.76kB
&amp;lt;missing&amp;gt;           8 days ago          /bin/sh -c rm -rf /var/lib/apt/lists/*          0B
&amp;lt;missing&amp;gt;           8 days ago          /bin/sh -c set -xe   &amp;amp;&amp;amp; echo &#39;#!/bin/sh&#39; &amp;gt; /...   745B   
&amp;lt;missing&amp;gt;           8 days ago          /bin/sh -c #(nop) ADD file:5fabb77ea8d61e02d...   82.4MB 
root@k8s-master:/tmp/iops#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;社区里还有很多压缩工具，比如Docker-squash ，用起来更简单方便，并且不会丢失原有镜像的自带信息，大家有兴趣可以试试。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Docker容器启动退出解决方案</title>
      <link>https://blog.maoxianplay.com/2018/docker-quit/</link>
      <pubDate>Thu, 27 Sep 2018 19:27:03 +0800</pubDate>
      
      <guid>https://blog.maoxianplay.com/2018/docker-quit/</guid>
      <description>

&lt;h1 id=&#34;现象&#34;&gt;现象&lt;/h1&gt;

&lt;p&gt;启动docker容器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run –name [CONTAINER_NAME] [CONTAINER_ID] 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看容器运行状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker ps -a 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;发现刚刚启动的mydocker容器已经退出&lt;/p&gt;

&lt;h1 id=&#34;原因&#34;&gt;原因&lt;/h1&gt;

&lt;p&gt;docker容器的主线程（dockfile中CMD执行的命令）结束，容器会退出&lt;/p&gt;

&lt;h1 id=&#34;解决办法&#34;&gt;解决办法&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;可以使用交互式启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -i [CONTAINER_NAME or CONTAINER_ID]
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;上面的不太友好，建议使用后台模式和tty选项&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -dit [CONTAINER_NAME or CONTAINER_ID]
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Docker 容器在后台以守护态（Daemonized）形式运行，可以通过添加 -d 参数来实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run -d ubuntu:14.04 /bin/sh -c &amp;quot;while true; do echo hello world; sleep 1; done&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在脚本最后一行添加&lt;strong&gt;tail -f /dev/null&lt;/strong&gt;，这个命令永远完成不了，所以该脚本一直不会执行完，所以该容器永远不会退出。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;TIPs:退出时，使用[ctrl + D]，这样会结束docker当前线程，容器结束，可以使用[ctrl + P][ctrl + Q]退出而不终止容器运行&lt;/p&gt;

&lt;p&gt;如下命令，会在指定容器中执行指定命令，[ctrl+D]退出后不会终止容器运行&lt;/p&gt;

&lt;p&gt;docker默认会把容器内部pid=1的作为默认的程序&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>容器技术概述</title>
      <link>https://blog.maoxianplay.com/2018/con-ind/</link>
      <pubDate>Thu, 30 Aug 2018 18:45:22 +0800</pubDate>
      
      <guid>https://blog.maoxianplay.com/2018/con-ind/</guid>
      <description>

&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;自从微服务（Microservice）的出现，出于业务的需要，IT应用模型不断的变革。开发模式从瀑布式到敏捷开发；开发、运维和测试互相配合的devops思想；应用程序架构从单体模型到分层模型再到微服务；部署方式也从面向物理机到虚拟键再到容器；应用程序的基础架构从自建机房到托管再到云计算，等等。这些变革使得IT技术应用的效率大大提升，同时却以&lt;strong&gt;更低的成本交付更高质量的产品&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;尤其是以Docker为代表的容器技术的出现，终结了devops中交付和部署环节因环节、配置及程序本身的不同而造成的动辄几种甚至十几种部署配置的困境，将它们统一在容器镜像（image）之上。这就是我在工作中遇到最先遇到的困境，同时也是我开始研究容器技术的契机。&lt;/p&gt;

&lt;p&gt;如今，越来越多的企业或组织开始开始选择以镜像文件为交付载体。容器镜像之内直接包含了应用程序及其依赖的系统环境、库、基础程序等，从而能够在容器引擎上直接运行。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;容器技术概述&#34;&gt;容器技术概述&lt;/h1&gt;

&lt;p&gt;容器是一种轻量级、可移植、自包含的软件打包技术，它使得应用程序可以在几乎任何地方以相同的方式运行。&lt;/p&gt;

&lt;p&gt;容器有应用程序本身和它的环境依赖（库和其他应用程序）两部分组成，并在宿主机（Host）操作系统的用户空间中运行，但与操作系统的其他进程互相隔离，他们的实现机制有别于VMWare、KVM、Xen等实现方案的虚拟化技术。容器与虚拟机的对比关系如下图
&lt;img src=&#34;https://blog.maoxianplay.com/images/source/vs.png&#34; alt=&#34;image&#34; /&gt;
由于同一个宿主机上的所有容器都共享其底层操作系统（内核空间），这就使得容器在体积上要比传统的虚拟机小很多。另外，启动容器无须启动整个操作系统，所以容器部署和启动的速度更快，开销更小，也更容易迁移。事实上，容器赋予了应用程序超强的可移植能力。&lt;/p&gt;

&lt;h1 id=&#34;容器技术的优势&#34;&gt;容器技术的优势&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;开发方面：“一次构建、到处运行”（Build Once, Run Anywhere）。容器意味着环境隔离和可重复性，开发人员只需为应用创建一个运行环境，并将其打包成容器便可在各种部署环境上运行，并与它所在的宿主机环境隔离。&lt;/li&gt;
&lt;li&gt;运维方面：“一次配置，运行所以”（Configure Once, Run Anything）。一旦配置好标准的容器运行时环境，服务器就可以运行任何容器，这使得运维人员的工作变得更高效、一致和可重复。容器消除了开发、测试、生产环境的不一致性。·&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>centos7.2 安装k8s v1.11.0</title>
      <link>https://blog.maoxianplay.com/2018/install-k8s/</link>
      <pubDate>Tue, 14 Aug 2018 20:07:03 +0800</pubDate>
      
      <guid>https://blog.maoxianplay.com/2018/install-k8s/</guid>
      <description>

&lt;h1 id=&#34;前言&#34;&gt;前言&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;最近由于公司业务发展到了瓶颈，原有的技术架构已经逐渐无法满足业务开发和测试的需求，出现了应用测试环境搭建复杂，有许多套（真的很多很多）应用环境，应用在持续集成/持续交付也遇到了很大的困难，经过讨论研究决定对应用和微服务进行容器化，这就是我首次直面docker和k8s的契机（好吧，我是菜鸟）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;kubernetes-介绍&#34;&gt;Kubernetes 介绍&lt;/h1&gt;

&lt;p&gt;Kubernetes 是 Google 团队发起的开源项目，它的目标是管理跨多个主机的容器，提供基本的部署，维护以及运用伸缩，主要实现语言为
Go 语言。
Kubernetes的特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;易学：轻量级，简单，容易理解&lt;/li&gt;
&lt;li&gt;便携：支持公有云，私有云，混合云，以及多种云平台&lt;/li&gt;
&lt;li&gt;可拓展：模块化，可插拔，支持钩子，可任意组合&lt;/li&gt;
&lt;li&gt;自修复：自动重调度，自动重启，自动复制&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;准备工作&#34;&gt;准备工作&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;注：以下操作都是在root权限下执行的&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;安装docker-ce，这里使用docker-ce-17.09.0.c版本，安装方法见&lt;a href=&#34;https://blog.maoxianplay.com/2018/install-docker&#34;&gt;之前的教程&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装Kubeadm&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#安装 Kubeadm 首先我们要配置好阿里云的国内源，执行如下命令：
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
EOF

#之后，执行以下命令来重建yum缓存：
yum -y install epel-releaseyum
clean all
yum makecache
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来需要安装指定版本的Kubeadm（这里要安装指定版本，因为后续依赖的镜像由于有墙无法拉取，这里我们只有指定版本的镜像），注意：&lt;strong&gt;这里是安装指定版本的Kubeadm，k8s的版本更新之快完全超出你的想象！&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum -y install kubelet-1.11.0-0
yum -y install kubeadm-1.11.0-0
yum -y install kubectl-1.11.0-0
yum -y install kubernetes-cni

#执行命令启动Kubeadm服务：
systemctl enable kubelet &amp;amp;&amp;amp; systemctl start kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置 Kubeadm 所用到的镜像
这里是重中之重，因为在国内的原因，无法访问到 Google 的镜像库，所以我们需要执行以下脚本来从 Docker Hub 仓库中获取相同的镜像，并且更改 TAG 让其变成与 Google 拉去镜像一致。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;新建一个 Shell 脚本，填入以下代码之后保存&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#docker.sh
#!/bin/bash
images=(kube-proxy-amd64:v1.11.0 kube-scheduler-amd64:v1.11.0 kube-controller-manager-amd64:v1.11.0 kube-apiserver-amd64:v1.11.0 etcd-amd64:3.2.18 coredns:1.1.3 pause-amd64:3.1 kubernetes-dashboard-amd64:v1.8.3 k8s-dns-sidecar-amd64:1.14.9 k8s-dns-kube-dns-amd64:1.14.9 k8s-dns-dnsmasq-nanny-amd64:1.14.9 )
for imageName in ${images[@]} ; do
docker pull keveon/$imageName
docker tag keveon/$imageName k8s.gcr.io/$imageName
docker rmi keveon/$imageName
done
# 个人新加的一句，V 1.11.0 必加
docker tag da86e6ba6ca1 k8s.gcr.io/pause:3.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;保存后使用chmod命令赋予脚本执行权限&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chmod -R 777 ./docker.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;执行脚本拉取镜像&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sh docker.sh
#这里就开始了漫长的拉取镜像之路
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;关闭掉swap&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo swapoff -a
#要永久禁掉swap分区，打开如下文件注释掉swap那一行
# sudo vi /etc/stab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;关闭SELinux的&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 临时禁用selinux
# 永久关闭 修改/etc/sysconfig/selinux文件设置
sed -i &#39;s/SELINUX=permissive/SELINUX=disabled/&#39; /etc/sysconfig/selinux
# 这里按回车，下面是第二条命令
setenforce 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;关闭防火墙&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl disable firewalld.service &amp;amp;&amp;amp; systemctl stop firewalld.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置转发参数&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 配置转发相关参数，否则可能会出错
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
vm.swappiness=0
EOF
# 这里按回车，下面是第二条命令
sysctl --system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;这里就完成了k8s集群搭建的准备工作，集群搭建的话以上操作结束后将操作完的系统制作成系统镜像，方便集群搭建&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;正式安装&#34;&gt;正式安装&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;以下的操作都只在主节点上进行：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;初始化镜像&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubeadm init --kubernetes-version=v1.11.0 --pod-network-cidr=10.10.0.0/16  #这里填写集群所在网段
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;之后的输出会是这样：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;I0712 10:46:30.938979   13461 feature_gate.go:230] feature gates: &amp;amp;{map[]}
[init] using Kubernetes version: v1.11.0
[preflight] running pre-flight checks
I0712 10:46:30.961005   13461 kernel_validator.go:81] Validating kernel version
I0712 10:46:30.961061   13461 kernel_validator.go:96] Validating kernel config
    [WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 18.03.1-ce. Max validated version: 17.03
    [WARNING Hostname]: hostname &amp;quot;g2-apigateway&amp;quot; could not be reached
    [WARNING Hostname]: hostname &amp;quot;g2-apigateway&amp;quot; lookup g2-apigateway on 100.100.2.138:53: no such host
[preflight/images] Pulling images required for setting up a Kubernetes cluster
[preflight/images] This might take a minute or two, depending on the speed of your internet connection
[preflight/images] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;
[kubelet] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;
[kubelet] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;
[preflight] Activating the kubelet service
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [g2-apigateway kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.16.8.62]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Generated etcd/ca certificate and key.
[certificates] Generated etcd/server certificate and key.
[certificates] etcd/server serving cert is signed for DNS names [g2-apigateway localhost] and IPs [127.0.0.1 ::1]
[certificates] Generated etcd/peer certificate and key.
[certificates] etcd/peer serving cert is signed for DNS names [g2-apigateway localhost] and IPs [172.16.8.62 127.0.0.1 ::1]
[certificates] Generated etcd/healthcheck-client certificate and key.
[certificates] Generated apiserver-etcd-client certificate and key.
[certificates] valid certificates and keys now exist in &amp;quot;/etc/kubernetes/pki&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/admin.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/kubelet.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/controller-manager.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/scheduler.conf&amp;quot;
[controlplane] wrote Static Pod manifest for component kube-apiserver to &amp;quot;/etc/kubernetes/manifests/kube-apiserver.yaml&amp;quot;
[controlplane] wrote Static Pod manifest for component kube-controller-manager to &amp;quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&amp;quot;
[controlplane] wrote Static Pod manifest for component kube-scheduler to &amp;quot;/etc/kubernetes/manifests/kube-scheduler.yaml&amp;quot;
[etcd] Wrote Static Pod manifest for a local etcd instance to &amp;quot;/etc/kubernetes/manifests/etcd.yaml&amp;quot;
[init] waiting for the kubelet to boot up the control plane as Static Pods from directory &amp;quot;/etc/kubernetes/manifests&amp;quot;
[init] this might take a minute or longer if the control plane images have to be pulled
[apiclient] All control plane components are healthy after 41.001672 seconds
[uploadconfig] storing the configuration used in ConfigMap &amp;quot;kubeadm-config&amp;quot; in the &amp;quot;kube-system&amp;quot; Namespace
[kubelet] Creating a ConfigMap &amp;quot;kubelet-config-1.11&amp;quot; in namespace kube-system with the configuration for the kubelets in the cluster
[markmaster] Marking the node g2-apigateway as master by adding the label &amp;quot;node-role.kubernetes.io/master=&#39;&#39;&amp;quot;
[markmaster] Marking the node g2-apigateway as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[patchnode] Uploading the CRI Socket information &amp;quot;/var/run/dockershim.sock&amp;quot; to the Node API object &amp;quot;g2-apigateway&amp;quot; as an annotation
[bootstraptoken] using token: o337m9.ceq32wg9g2gro7gx
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the &amp;quot;cluster-info&amp;quot; ConfigMap in the &amp;quot;kube-public&amp;quot; namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

kubeadm join 10.10.207.253:6443 --token t69z6h.lr2etdbg9mfx5r15 --discovery-token-ca-cert-hash sha256:90e3a748c0eb4cb7058f3d0ee8870ee5d746214ab0589b5e841fd5d68fec8f00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;这里注意最后一行：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubeadm join 10.10.207.253:6443 --token t69z6h.lr2etdbg9mfx5r15 --discovery-token-ca-cert-hash sha256:90e3a748c0eb4cb7058f3d0ee8870ee5d746214ab0589b5e841fd5d68fec8f00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;证明集群主节点安装成功，这里要记得保存这条命令，以便之后各个节点加入集群&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;配置kubetl认证信息&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export KUBECONFIG=/etc/kubernetes/admin.conf
# 如果你想持久化的话，直接执行以下命令【推荐】
echo &amp;quot;export KUBECONFIG=/etc/kubernetes/admin.conf&amp;quot; &amp;gt;&amp;gt; ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;安装flanel网络&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /etc/cni/net.d/

cat &amp;lt;&amp;lt;EOF&amp;gt; /etc/cni/net.d/10-flannel.conf
{
&amp;quot;name&amp;quot;: &amp;quot;cbr0&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;flannel&amp;quot;,
&amp;quot;delegate&amp;quot;: {
&amp;quot;isDefaultGateway&amp;quot;: true
}
}
EOF

mkdir /usr/share/oci-umount/oci-umount.d -p

mkdir /run/flannel/

cat &amp;lt;&amp;lt;EOF&amp;gt; /run/flannel/subnet.env
FLANNEL_NETWORK=10.244.0.0/16
FLANNEL_SUBNET=10.244.1.0/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=true
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;最后需要新建一个flannel.yml文件：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
name: flannel
rules:
- apiGroups:
    - &amp;quot;&amp;quot;
    resources:
    - pods
    verbs:
    - get
- apiGroups:
    - &amp;quot;&amp;quot;
    resources:
    - nodes
    verbs:
    - list
    - watch
- apiGroups:
    - &amp;quot;&amp;quot;
    resources:
    - nodes/status
    verbs:
    - patch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
name: flannel
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: flannel
subjects:
- kind: ServiceAccount
name: flannel
namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
name: flannel
namespace: kube-system
---
kind: ConfigMap
apiVersion: v1
metadata:
name: kube-flannel-cfg
namespace: kube-system
labels:
    tier: node
    app: flannel
data:
cni-conf.json: |
    {
    &amp;quot;name&amp;quot;: &amp;quot;cbr0&amp;quot;,
    &amp;quot;type&amp;quot;: &amp;quot;flannel&amp;quot;,
    &amp;quot;delegate&amp;quot;: {
        &amp;quot;isDefaultGateway&amp;quot;: true
    }
    }
net-conf.json: |
    {
    &amp;quot;Network&amp;quot;: &amp;quot;10.10.0.0/16&amp;quot;,    #这里换成集群所在的网段
    &amp;quot;Backend&amp;quot;: {
        &amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot;
    }
    }
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
name: kube-flannel-ds
namespace: kube-system
labels:
    tier: node
    app: flannel
spec:
template:
    metadata:
    labels:
        tier: node
        app: flannel
    spec:
    hostNetwork: true
    nodeSelector:
        beta.kubernetes.io/arch: amd64
    tolerations:
    - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
    serviceAccountName: flannel
    initContainers:
    - name: install-cni
        image: quay.io/coreos/flannel:v0.9.1-amd64
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conf
        volumeMounts:
        - name: cni
        mountPath: /etc/cni/net.d
        - name: flannel-cfg
        mountPath: /etc/kube-flannel/
    containers:
    - name: kube-flannel
        image: quay.io/coreos/flannel:v0.9.1-amd64
        command: [ &amp;quot;/opt/bin/flanneld&amp;quot;, &amp;quot;--ip-masq&amp;quot;, &amp;quot;--kube-subnet-mgr&amp;quot; ]
        securityContext:
        privileged: true
        env:
        - name: POD_NAME
        valueFrom:
            fieldRef:
            fieldPath: metadata.name
        - name: POD_NAMESPACE
        valueFrom:
            fieldRef:
            fieldPath: metadata.namespace
        volumeMounts:
        - name: run
        mountPath: /run
        - name: flannel-cfg
        mountPath: /etc/kube-flannel/
    volumes:
        - name: run
        hostPath:
            path: /run
        - name: cni
        hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
        configMap:
            name: kube-flannel-cfg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;执行：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f ./flannel.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认情况下，master节点不参与工作负载，但如果希望安装出一个all-in-one的k8s环境，则可以执行以下命令：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;让master节点成为一个node节点：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl taint nodes --all node-role.kubernetes.io/master-
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;查看节点信息：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;会看到如下的输出：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAME            STATUS     ROLES     AGE       VERSION
k8s-master      Ready      master    18h       v1.11.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;以下是节点配置&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在配置好主节点之后，就可以配置集群的其他节点了，这里建议直接安装之前做好准备工作的系统镜像
进入节点机器之后，直接执行之前保存好的命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubeadm join 10.10.207.253:6443 --token t69z6h.lr2etdbg9mfx5r15 --discovery-token-ca-cert-hash sha256:90e3a748c0eb4cb7058f3d0ee8870ee5d746214ab0589b5e841fd5d68fec8f00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行完后会看到：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[preflight] running pre-flight checks
        [WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs_wrr ip_vs_sh ip_vs ip_vs_rr] or no builtin kernel ipvs support: map[ip_vs_rr:{} ip_vs_wrr:{} ip_vs_sh:{} nf_conntrack_ipv4:{} ip_vs:{}]
you can solve this problem with following methods:
1. Run &#39;modprobe -- &#39; to load missing kernel modules;
2. Provide the missing builtin kernel ipvs support

I0725 09:59:27.929247   10196 kernel_validator.go:81] Validating kernel version
I0725 09:59:27.929356   10196 kernel_validator.go:96] Validating kernel config
[discovery] Trying to connect to API Server &amp;quot;10.10.207.253:6443&amp;quot;
[discovery] Created cluster-info discovery client, requesting info from &amp;quot;https://10.10.207.253:6443&amp;quot;
[discovery] Requesting info from &amp;quot;https://10.10.207.253:6443&amp;quot; again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &amp;quot;10.10.207.253:6443&amp;quot;
[discovery] Successfully established connection with API Server &amp;quot;10.10.207.253:6443&amp;quot;
[kubelet] Downloading configuration for the kubelet from the &amp;quot;kubelet-config-1.11&amp;quot; ConfigMap in the kube-system namespace
[kubelet] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;
[kubelet] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;
[preflight] Activating the kubelet service
[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...
[patchnode] Uploading the CRI Socket information &amp;quot;/var/run/dockershim.sock&amp;quot; to the Node API object &amp;quot;k8s-node1&amp;quot; as an annotation

This node has joined the cluster:
* Certificate signing request was sent to master and a response
was received.
* The Kubelet was informed of the new secure connection details.

Run &#39;kubectl get nodes&#39; on the master to see this node join the cluster.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;这里就表示执行完毕了，可以去主节点执行命令：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;可以看到节点已加入集群：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAME        STATUS    ROLES     AGE       VERSION
k8s-master  Ready     master    20h       v1.11.0
k8s-node1   Ready     &amp;lt;none&amp;gt;    20h       v1.11.0
k8s-node2   Ready     &amp;lt;none&amp;gt;    20h       v1.11.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这期间可能需要等待一段时间，状态才会全部变为ready&lt;/p&gt;

&lt;h1 id=&#34;kubernetes-dashboard安装&#34;&gt;kubernetes-dashboard安装&lt;/h1&gt;

&lt;p&gt;详见：&lt;a href=&#34;https://blog.maoxianplay.com/2018/dashboard-k8s&#34;&gt;kubernetes安装dashboard&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;采坑指南&#34;&gt;采坑指南&lt;/h1&gt;

&lt;p&gt;有时会出现master节点一直处于notready的状态，这里可能是没有启动flannel，只需要按照上面的教程配置好flannel，然后执行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f ./flannel.yml
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>centos7安装指定版本的docker</title>
      <link>https://blog.maoxianplay.com/2018/install-docker/</link>
      <pubDate>Tue, 14 Aug 2018 20:05:21 +0800</pubDate>
      
      <guid>https://blog.maoxianplay.com/2018/install-docker/</guid>
      <description>

&lt;h1 id=&#34;前言&#34;&gt;前言&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;在使用&lt;strong&gt;centos7&lt;/strong&gt;，并使用荫安装搬运工的时候，往往不希望安装最新版本的搬运工，而是希望安装与自己熟悉或者当前业务环境需要的版本，例如目前Kubernetes支持的最新搬运工版本为v17.03，所以就产生了安装指定版本码头工人的需求。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;安装步骤&#34;&gt;安装步骤&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;# 安装依赖包
yum install -y yum-utils device-mapper-persistent-data lvm2

# 添加Docker软件包源
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

#关闭测试版本list（只显示稳定版）
sudo yum-config-manager --enable docker-ce-edge
sudo yum-config-manager --enable docker-ce-test

# 更新yum包索引
yum makecache fast

#NO.1 直接安装Docker CE （will always install the highest  possible version，可能不符合你的需求）
yum install docker-ce

#NO.2 指定版本安装
yum list docker-ce --showduplicates|sort -r 
#找到需要安装的
yum install docker-ce-17.09.0.ce -y
#启动docker
systemctl start docker &amp;amp; systemctl enable docker
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;采坑指南&#34;&gt;采坑指南&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;当然本着万事皆有坑的原则，这里也是有坑的，在安装中也是会遇到如下的问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在执行一下命令的时候：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install docker-ce-17.03.0.ce -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;会出现如下的报错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--&amp;gt; Finished Dependency Resolution
Error: Package: docker-ce-17.03.0.ce-1.el7.centos.x86_64 (docker-ce-stable)
        Requires: docker-ce-selinux &amp;gt;= 17.03.0.ce-1.el7.centos
        Available: docker-ce-selinux-17.03.0.ce-1.el7.centos.noarch (docker-ce-stable)
            docker-ce-selinux = 17.03.0.ce-1.el7.centos
        Available: docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch (docker-ce-stable)
            docker-ce-selinux = 17.03.1.ce-1.el7.centos
        Available: docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch (docker-ce-stable)
            docker-ce-selinux = 17.03.2.ce-1.el7.centos
You could try using --skip-broken to work around the problem
You could try running: rpm -Va --nofiles --nodigest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在出现这个问题之后，需要执行以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#要先安装docker-ce-selinux-17.03.2.ce，否则安装docker-ce会报错
yum install https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm

#然后再安装 docker-ce-17.03.2.ce，就能正常安装
yum install docker-ce-17.03.2.ce-1.el7.centos
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://blog.maoxianplay.com/about/</link>
      <pubDate>Sun, 05 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.maoxianplay.com/about/</guid>
      <description>&lt;p&gt;郭旭东，是一名在上海工作的devops工程师，目前就职于&lt;a href=&#34;https://www.keking.com&#34; rel=&#34;nofollow noreferrer&#34; target=&#34;_blank&#34;&gt;上海凯京科技&lt;/a&gt;，对新技术，尤其是devops方面的技术有着强烈的好奇与不懈的追求，欢迎加入&lt;a href=&#34;https://www.keking.com&#34; rel=&#34;nofollow noreferrer&#34; target=&#34;_blank&#34;&gt;上海凯京科技&lt;/a&gt;，一起探索与创造更好更合适的devops理念。简历可以通过首页的联系方式投递。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>